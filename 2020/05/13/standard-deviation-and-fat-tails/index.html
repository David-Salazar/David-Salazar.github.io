<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>Standard Deviation and Fat Tails - David Salazar&#39;s blog</title>
<meta property="og:title" content="Standard Deviation and Fat Tails - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Standard Deviation and Fat Tails</h1>

    
    <span class="article-date">2020-05-13</span>
    

    <div class="article-content">
      


<div id="statistical-consequences-of-fat-tails" class="section level1">
<h1>Statistical Consequences of Fat Tails</h1>
<p>In this post, I’ll continue to explore with Monte-Carlo simulations the ideas in Nassim Taleb’s latest book: <a href="https://arxiv.org/abs/2001.10488">Statistical Consequences of Fat Tails</a>. In other posts, I have look at <a href="2020-04-17-fat-vs-thin-does-lln-work.html">the persistent small sample effect that plagues the mean estimates under fat tails</a> as a consequence of the loooong pre-asymptotics of the law of large numbers. Also, how this in turn plagues other statistical techniques <a href="2020-04-27-spurious-pca-under-thick-tails.html">such as PCA</a>. This time, I’ll explore what Taleb finds out by turning his attention to the much misunderstood standard deviation.</p>
</div>
<div id="standard-deviation-under-fat-tails" class="section level1">
<h1>Standard deviation under Fat Tails</h1>
<p>In Chapter 4 of his book, Taleb explores how Standard Deviation (SD) is problematic:</p>
<ol style="list-style-type: decimal">
<li>Widely misunderstood. People mistake Standard Deviation with Mean Absolute Deviation (MAD).</li>
<li>That this confusion is benign under thin tailed distributions but an unforgivable mistake under fat tails.</li>
</ol>
<p>Let’s try to replicate the second point.</p>
<div id="game-plan" class="section level2">
<h2>Game Plan</h2>
<p>First, I’ll begin by exploring in what sense, under a Gaussian, the Standard Deviation (SD) is more “efficient estimator” than the Mean Absolute Deviation (MAD).</p>
<p>Secondly, in order to gauge the problems that start to arise with standard deviation (SD) as a measure of the scale of the distribution, Taleb uses the same trick as before: stochastize the volatility by switching between 2 normals with same means but different variances. The larger the difference between the variances, the fatter the tail of the resulting random variable. I tried to play with this idea in <a href="2020-05-09-what-does-it-mean-to-fatten-the-tails.html">this post</a>. Therefore, by analyzing the relationship between SD and MAD as we increase this difference, we get a type of derivative: as we increase the fat-tailedness, the difference between Standard Deviation (SD) and Mean Absolute Deviation (MAD) grows.</p>
</div>
<div id="efficiency-in-mediocristan" class="section level2">
<h2>Efficiency in Mediocristan</h2>
<p>When deciding between estimators, statistics textbooks end up talking about two asymptotic properties of the estimator: consistency and efficiency. Consistency concerns itself with the asymptotic accuracy of an estimator and efficiency with the asymptotic variance of the estimator. That is, consistency means that the estimator with lots of observations has a distribution tightly centered around the parameter. Whereas efficiency is something more esoteric but it means that the asymptotic variance is the lowest it can possibly be.</p>
<div id="efficiency-under-the-poisson" class="section level3">
<h3>Efficiency under the Poisson</h3>
<p>To get some intuition around why we should care about the variance of an estimator, let’s play with <span class="math inline">\(X \sim Poisson(\lambda)\)</span>. Note that <span class="math inline">\(E(X) = Var(X) = \lambda\)</span>. We could use either the estimate for the mean or the variance to estimate <span class="math inline">\(\lambda\)</span>. Which is better?</p>
<pre class="r"><code>lambda_estimates &lt;- function(size) {
  sample &lt;- rpois(size, lambda = 1)
  mean_estimate &lt;- mean(sample)
  variance_estimate &lt;- var(sample)
  data.frame(sample_size = size, mean_estimate, variance_estimate) 
}

montecarlo_lambda &lt;- function(size) {
  rerun(1000, lambda_estimates(size)) %&gt;% 
    bind_rows() %&gt;% 
    pivot_longer(-sample_size, names_to = &quot;estimator&quot;, values_to = &quot;estimate&quot;)
}

c(10, 100, 500) %&gt;% 
  map_df(montecarlo_lambda) %&gt;% 
  mutate(sample_size = glue::glue(&quot;sample size: {sample_size}&quot;)) -&gt; lambda_est 
lambda_est %&gt;% 
  ggplot(aes(x = estimate, y = sample_size)) +
  geom_density_ridges(aes(fill = estimator), alpha = 0.5, color = &quot;black&quot;) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;X&quot;) +
  theme(legend.position = &quot;bottom&quot;) +
  labs(x = &quot;&quot;, 
       title = &quot;Estimating Poisson&#39;s lambda = 1&quot;,
       subtitle = &quot;Mean estimate has lower variance&quot;,
       caption = &quot;1000 monte carlo simulations for each sample size&quot;) +
  scale_fill_viridis_d()</code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Thus, even though both estimators put a lot of their mass close to the true value, the Mean estimate has a lower variance than the Variance estimate of <span class="math inline">\(\lambda\)</span>. And therefore is a safer bet to estimate the <span class="math inline">\(\lambda\)</span> parameter with the mean. Indeed, there’s a mathematical bound for the variance of an estimator of <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(\dfrac{\lambda}{n}\)</span> which only the mean estimator satisties.</p>
<p>This is an important results: <strong>the variance estimate is too sensitive to outliers and we should prefer a natural weighting of the observations</strong>. However, with a Poisson and its thin tails, this is no big deal.</p>
<p>So far, we have talked about the intuition. If one wanted to compare the two estimator’s asymptotic efficiency, then one would compare the following ratio:</p>
<p><span class="math display">\[ \dfrac{(Var(S^2)}{Var(\bar{X})} \]</span></p>
<p>Which is the asymptotic Relative efficiency of the two estimators. That is, which of them has a lower variance as the number of available observations becomes increasingly large.</p>
<pre class="r"><code>montecarlo_lambda_efficiency &lt;- function(size) {
  rerun(1000, lambda_estimates(size)) %&gt;% 
    bind_rows() %&gt;% 
    pivot_longer(-sample_size, names_to = &quot;estimator&quot;, values_to = &quot;estimate&quot;) %&gt;% 
    group_by(sample_size, estimator) %&gt;% 
    summarise(variance_estimator = var(estimate))
}

c(100, 500, 1000, 5000, 10000) %&gt;% 
  map_df(montecarlo_lambda_efficiency) -&gt; lambda_efficiency

lambda_efficiency %&gt;% 
  pivot_wider(sample_size, names_from = estimator, values_from = variance_estimator) %&gt;% 
  mutate(are = variance_estimate/mean_estimate) %&gt;% 
  ggplot(aes(x = sample_size, y = are)) +
    geom_point() +
    geom_line() +
    scale_x_log10() +
    expand_limits(x = 100, y = 0) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) + 
  labs(x = &quot;sample size (log scale)&quot;,
       y = &quot;Asymptotic Relative Efficiency&quot;,
       title = &quot;ARE Mean and Variance to estimate Poisson&#39;s lambda&quot;,
       subtitle = &quot;Mean estimate is ~ 3x more efficient&quot;)</code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="efficiency-under-the-gaussian" class="section level3">
<h3>Efficiency under The Gaussian</h3>
<p>Now let’s start playing with a Gaussian with known mean 0. Imagine you have to estimate the scale of the distribution, whatever that may mean. We have two candidates: the SD or the MAD. Which one should we prefer? Just as with the Poisson, let’s play with with some Monte-Carlo simulations and calculate the ARE:</p>
<p><span class="math display">\[  \dfrac{(\dfrac{Var(SD^2)}{E(SD)^2})}{(\dfrac{Var({MAD})}{E({MAD})^2})} \]</span></p>
<p>Note that SD and MAD are estimating different measures of the scale of the distribution Therefore, instead of comparing their variances, we compare their coefficients of variation.</p>
<pre class="r"><code>gaussian_estimates &lt;- function(size) {
  sample &lt;- rnorm(size)
  standard_deviation &lt;- sqrt(mean(sample^2))
  mad &lt;- sum(abs(sample))/size 
  data.frame(sample_size = size, standard_deviation, mad)
}

montecarlo_sigma &lt;- function(size) {
  rerun(1000, gaussian_estimates(size)) %&gt;% 
    bind_rows() %&gt;% 
    pivot_longer(-sample_size, names_to = &quot;estimator&quot;, values_to = &quot;estimate&quot;) %&gt;% 
    group_by(sample_size, estimator) %&gt;% 
    summarise(variance_estimator = var(estimate)/mean(estimate)^2)
}

c(10, 100, 500, 1000, 10000, 20000, 50000 ) %&gt;% 
  map_df(montecarlo_sigma) -&gt; gaussian_est

gaussian_est %&gt;% 
  pivot_wider(sample_size, names_from = estimator, values_from = variance_estimator) %&gt;% 
  mutate(are = standard_deviation/mad) %&gt;% 
  ggplot(aes(sample_size, are)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 0.875), linetype = 2, color = &quot;red&quot;) +
   scale_x_log10() +
    expand_limits(x = 100) +
  scale_y_continuous(limits = c(0, 1)) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  labs(title = &quot;Asymptotic Relative Efficiency SD/MAD under Mediocristan&quot;,
       subtitle = &quot;Variance of SD is ~ 0.875 that of MAD&quot;,
       y = &quot;Asymptotic Relative Efficiency&quot;,
       x = &quot;Sample size (log scale)&quot;,
       caption = &quot;Red is theoretical ARE = 0.875&quot;) </code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Great, as our results replicate what Taleb did with math.</p>
<p>Thus, in a strict sense, <strong>when we are dealing with samples of Gaussian</strong>, it is more efficient to use SD than MAD. However, what happens when we slip away into fatter-tails?</p>
</div>
</div>
<div id="what-happens-to-sds-efficiency-as-we-fatten-the-tails" class="section level2">
<h2>What happens to SD’s efficiency as we fatten the tails?</h2>
<p>Taleb introduces an incredible heuristic ( <a href="2020-05-09-what-does-it-mean-to-fatten-the-tails.html">which I already examined in this post</a> ) that works perfectly to get the intuition of fattening a distribution. In Taleb’s words: “The equivalent of a functional derivative that provides good grasp for local sensitivities”</p>
<p>The heuristic is a simple <strong>switching between two gaussians with different standard deviations</strong>. Thus, we stochastize the variance of the distrbution. With probability <span class="math inline">\(1-p\)</span>, we switch to a <span class="math inline">\(Normal(0, \sigma)\)</span>; with probability <span class="math inline">\(p\)</span>, we switch to <span class="math inline">\(Normal(0, \sigma \sqrt{(1+a)})\)</span>.</p>
<p>That is, if <span class="math inline">\(p\)</span> is small, there’s a tiny probability that we switch to a gaussian with a higher standard deviation and that will likely generate outliers in comparison to the rest of the distribution. The higher <span class="math inline">\(a\)</span>, the larger the scale of the distribution. Thus, <strong>given that the tail probabilities are convex to the scale of the distribution, the higher <span class="math inline">\(a\)</span>, the more we fatten the distribution</strong>. Let’s sample from this distribution to have a quick look:</p>
<pre class="r"><code># simulate fattened
fatten_simulations &lt;- function(samples, a, p, sigma  = 1)  {
  
  # create vector to store sims
  sims &lt;- vector(length = samples)
  # sample with probability p
  p_location &lt;- rbernoulli(samples, p = p)
  
  sims[p_location] &lt;- rnorm(sum(p_location), sd = sigma *sqrt(1+a))
  sims[!p_location] &lt;- rnorm(sum(!p_location), sd = sigma)
  return(sims)
}

samples &lt;- 10000
# simulated fattened
a_s &lt;- seq(0, 100, length.out = 5)
names(a_s) &lt;- unlist(map(a_s, ~ glue::glue(&quot;a = {.x/5}&quot;)))

a_s%&gt;% 
  map_df(~ fatten_simulations(samples, a = .x, p = 0.01, sigma = 5)) %&gt;% 
  mutate(simulation = 1:samples) %&gt;% 
  pivot_longer(-simulation, names_to = &quot;a_s&quot;) %&gt;% 
  mutate(a_s = fct_inorder(a_s)) -&gt; fattend_different_as
fattend_different_as %&gt;% 
  ggplot(aes(sample = value, color = a_s)) +
  stat_qq(alpha = 0.4) +
  stat_qq_line() +
  scale_color_viridis_d(direction = -1) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;QQ-plot for different stochastic volatilties&quot;,
       subtitle = &quot;Tail values are convex to the scale of the distribution. Higher a, larger scale&quot;,
       color = &quot;&quot;,
       caption = &quot;Theoretical distribution is the normal. parameter a as a multiple of sigma&quot;) +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Therefore, if we calculate the ARE for different values of <span class="math inline">\(a\)</span>, we are calculating its sensibility as we move close to fatter distributions. Let’s check whether SD keeps being more efficient than MAD:</p>
<pre class="r"><code>fatten_estimates &lt;- function(size, a, p, sigma) {
  sample &lt;- fatten_simulations(size, a, p, sigma = sigma) 
  standard_deviation &lt;- sqrt(mean(sample^2))
  mad &lt;- mean(abs(sample)) 
  data.frame(sample_size = size, standard_deviation, mad)
}

montecarlo_sigma_fattened &lt;- function(size, a, p, sigma) {
  rerun(3000, fatten_estimates(size, a, p, sigma)) %&gt;% 
    bind_rows() %&gt;% 
    pivot_longer(-sample_size, names_to = &quot;estimator&quot;, values_to = &quot;estimate&quot;) %&gt;% 
    group_by(estimator) %&gt;% 
    summarise(variance_estimator = var(estimate)/mean(estimate)^2) %&gt;% 
    mutate(a = a)
}

a_s &lt;- seq(0, 100, length.out = 40)
names(a_s) &lt;- unlist(map(a_s, ~ glue::glue(&quot;a = {.x}&quot;)))
a_s%&gt;% 
  map_df(~montecarlo_sigma_fattened (size = 1000, a = .x, p = 0.01, sigma = 5)) -&gt;fattened_are
fattened_are %&gt;% 
  pivot_wider(a, names_from = estimator, values_from = variance_estimator) %&gt;% 
  mutate(are = standard_deviation/mad,
         a = a / 5) %&gt;% 
  ggplot(aes(a, are)) +
  geom_point() +
  expand_limits(x = 0, y = 0)  +
  hrbrthemes::theme_ipsum_rc() +
  labs(x = &quot;parameter a as a multiple of sigma&quot;,
       title = &quot;Asymptotic Relative Efficiency of SD vs MAD&quot;,
       subtitle = &quot;Small outliers (~ 2 SD upwards) eat away the efficiency of SD&quot;,
       y = &quot;Asymptotic Relative Efficiency&quot;)</code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Per Taleb:</p>
<blockquote>
<p>A minute presence of outliers makes MAD more “efﬁcient” than STD. Small “outliers” of 5 standard deviations cause MAD to be ﬁve times more efﬁcient.</p>
</blockquote>
<p>Therefore, as we move towards distribution with fatter tails, we move to a place where standard deviation is worse than useless: it is dangerous. The problem compounds as the tails are precisely convex to the scale of the distribution.</p>
<p>Thus, to estimate the tails with SD is to play with fire: we are verly likely to make mistakes that will compound at the part of the distribution that matters the most. As Taleb says, this is also one of the misunderstandings of the Black Swan: the problem is not only that the extreme plays a large role; but also that we can very easily produce bogus estimates of the probabilities at the tail.</p>
<div id="why" class="section level3">
<h3>Why?</h3>
<p>The problem arises from the weighting that SD performs. The larger the observation, the even larger weight will have on the Standard Deviation. Whereas MAD uses a more natural weighting, where the weight is equal to each observation. This becomes evident if we examine the cumulative SD and cumulative MAD, where every once in a while we get an extreme value.</p>
<p>To simulate this, let’s sample from a Cauchy distribution where there is not even a defined first moment. In sample, however, we can calculate anything:</p>
<pre class="r"><code>samples &lt;- rcauchy(10000)
cumstd &lt;- sqrt(cumvar(samples))
cummad &lt;- cummean(abs(samples - cummean(samples)))

data.frame(obs = 1:10000, cumstd, cummad) %&gt;% 
  filter(obs &gt; 2) %&gt;% 
  pivot_longer(-obs) %&gt;% 
  ggplot(aes(obs, value)) +
    geom_line(aes(color = name)) +
    hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  scale_color_brewer(palette = &quot;Set1&quot;) +
  labs(color = &quot;&quot;,
       title = &quot;The SD is way too sensible to outliers&quot;)</code></pre>
<p><img src="/post/2020-05-13-standard-deviation-and-fat-tails_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>SD is only optimal in Mediocristan. When we start moving toward Extremistan, SD blows up due to being a sum of squares. Even relatively small deviations wash away the efficiency of SD over MAD. Thus, when dealing with fat-tails, SD should not be used as it can lead to very imprecise estimates of the probability at the tails. Thus, SD is not an appropriate measure of the scale of the distribution for fat tails. It is much better to use the MAD. That is, <strong>MAD is a better estimator of <span class="math inline">\(E(|X-\mu|)\)</span> than SD as an estimator of <span class="math inline">\(\sigma\)</span>.</strong></p>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

