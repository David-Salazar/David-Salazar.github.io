<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Understanding Taleb&#39;s kappa metric - Dilettanting Data Science</title>
<meta property="og:title" content="Understanding Taleb&#39;s kappa metric - Dilettanting Data Science">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">Understanding Taleb&#39;s kappa metric</h1>

    
    <span class="article-date">2020/06/07</span>
    
    

    <div class="article-content">
      


<p>Taleb’s kappa metric is a fundamentally <em>preasymptotic</em> metric that allows us to <strong>compare different distributions’ preasymptotic properties relative to the preasymptotic properties of the Gaussian</strong>. The preasymptotic properties are the convergence of the normal regarding the CLT and the LLN.</p>
<div id="context" class="section level1">
<h1>Context</h1>
<p>I am currently reading through Nassim Taleb’s latest <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">technical book: Statistical Consequences of Fat Tails</a>. In it, Taleb shows, among other things, how <strong>the rate of convergence of statistics’ two main theorems, the Central limit Theorem (CLT) and the Law of Large Numbers (LLN), varies across different distributions</strong>. For example, for the Gaussian, we can invoke both theorems with a relatively small number of observations. However, once we enter into Extremistan territory, where the tails of distribution are fat, things start to change. We cannot invoke <em>any</em> of the theorems at even moderately large sample sizes. As Taleb says:</p>
<blockquote>
<p>Statistics is never standard.</p>
</blockquote>
<p>That is, you cannot just invoke the theorems regardless of the distribution and regardless of the sample size and call it a day. Proof of this is the unreliability of many estimators under fat tails. For example, the <a href="2020-04-17-fat-vs-thin-does-lln-work.html">sample mean</a>, <a href="2020-04-27-spurious-pca-under-thick-tails.html">PCA</a>, <a href="2020-05-22-correlation-is-not-correlation.html">sample correlation</a>, <a href="2020-05-26-r-squared-and-fat-tails.html"><span class="math inline">\(R^2\)</span></a> and other <a href="2020-06-02-lln-for-higher-p-moments.html">higher moments estimators</a> turn into pure noise when we are dealing with fat-tails and not large enough samples sizes.</p>
</div>
<div id="what-question-is-talebs-kappa-trying-to-solve" class="section level1">
<h1>What question is Taleb’s kappa trying to solve?</h1>
<p>However, the distinction between the distributions is not crystal clear. <em>How far of the Gaussian’s preasymptotic properties are a Pareto with <span class="math inline">\(\alpha = 3\)</span> and a Student-t with the same degree?</em> <strong>Taleb’s <span class="math inline">\(\kappa\)</span> metric attempts to solve this</strong>. Many statisticians only know what happens when <span class="math inline">\(n = 1\)</span> and when <span class="math inline">\(n = \infty\)</span>. In the middle, they say <span class="math inline">\(n = 30\)</span> for a Gaussian is enough to invoke both the CLT and the LLN. However, what about all the other distributions? How far is a Pareto with <span class="math inline">\(n = 100\)</span> of the standard set by the Gaussian? With Taleb’s <span class="math inline">\(\kappa\)</span> we can answer these types of questions.</p>
<p>Intuitively, once we sum over 30 something i.i.d Gaussians, we can rightly invoke the CLT and the LLN. With 1 observation, not so much. <em>What happend in this interval?</em> The 29 additional observations <strong>were enough data to increase the stability</strong> of the observed sample mean. Taleb’s <span class="math inline">\(\kappa\)</span> helps us figure out <strong>the equivalent number of observations</strong> we need to observe this <em>same type of stability</em> with other distributions.</p>
<div id="the-distribution-of-the-partial-sum" class="section level2">
<h2>The distribution of the partial sum</h2>
<p><em>What can derail our mean estimation?</em> <strong>An observation so large, relative to our partial sum, that shifts dramatically our mean estimation</strong>. Mathematically, we are studying a partial sum of identical copies of a random variable: <span class="math inline">\(S_n = X_1 + X_2 + \cdots + X_n\)</span>. More precisely, <em>the scale</em> of the distribution as <span class="math inline">\(n\)</span> grows. Let’s define the scale of the distribution as the Mean Absolute Deviation thus: <span class="math inline">\(\mathbb{M}(n) = E[S_n - E[S_n]]\)</span>.</p>
<p>Intuitively, as we add more variables to the partial sum, the scale of the distribution must grow. However, as the magnitude of the partial sum grows, the <em>added scale</em> that any <strong>particular instantiation can possibly induce becomes smaller and smaller</strong>. The difference across the distributions will depend on when this kind of behavior starts. That is, <strong>when will the partial sum be much larger than any possible instantiation of the random variable?</strong></p>
<p>For the Gaussian, this behavior sets in instantly. Indeed, the <span class="math inline">\(\mathbb{M}\)</span> grows incredibly slowly. Adding <span class="math inline">\(n\)</span> observations barely changes the scale of <span class="math inline">\(S_n\)</span>. Here is the graph of the <span class="math inline">\(\mathbb{M}(n)\)</span> as a function of <span class="math inline">\(n\)</span>:</p>
<p><img src="/post/2020-06-07-understanding-taleb-s-kappa-metric_files/figure-html/mad.gaussian-1.png" width="768" /></p>
<p>Taleb’s <span class="math inline">\(\kappa_{n_0, n}\)</span> measures the “rate of convergence” of <span class="math inline">\(\mathbb{n}\)</span>. Given that we have have <span class="math inline">\(n_0\)</span> observations, how does the scale of the distribution of <span class="math inline">\(S_n\)</span> grow by adding <span class="math inline">\(n\)</span> observations? As it can be seen in the plot, adding observations barely increases the scale of <span class="math inline">\(S_n\)</span>. Interestingly, Taleb defines <span class="math inline">\(\kappa_{n_0, n}\)</span> such that it is always zero for a Gaussian. Therefore, <span class="math inline">\(k_{n_0, n}\)</span>, <em>for other distributions</em>, measures <strong>how much has the scale of <span class="math inline">\(S_n\)</span> changed</strong> as a result of adding <span class="math inline">\(n\)</span> observations <em>above the change that we would expect for a Gaussian</em>.</p>
</div>
</div>
<div id="why-is-it-different-for-fat-tailed-variables" class="section level1">
<h1>Why is it different for fat-tailed variables?</h1>
<p>We can calculate “a derivative” of <span class="math inline">\(\kappa\)</span> with respective to fat-tailedness thus: using heuristic to fatten the tails of a Gaussian by stochastizing the variance of a mixture of Gaussians. The larger the <span class="math inline">\(a\)</span> parameter, the fatter the tails of the resulting mixture:</p>
<p><img src="/post/2020-06-07-understanding-taleb-s-kappa-metric_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>That is, as the tails of the distribution grow fatter, the resulting <em>added scale</em> by adding <span class="math inline">\(n\)</span> summands to <span class="math inline">\(S_{n0}\)</span> grows. Thus, <span class="math inline">\(\kappa_{n_0, n}\)</span> grows, too. Why? This question is equivalent to the question of <em>degree of concentration</em> for random variables.</p>
<p>For thin-tailed variables, <strong>no single observation can define the statistical properties of the partial sum</strong>. Intuitively, think about the partial sum of the heights of people. Very quickly, the combined height of 10 people is much, much larger than the height of any single individual. Thus, the rate of growth of the <span class="math inline">\(\mathbb{M}(n)\)</span> will quickly level off. Therefore, as this type of behavior kicks in, we can be sure that <em>adding</em> observations will increase the <strong>stability</strong> of our sample mean. In other words, <strong>this is when we can invoke the LLN</strong> to estimate the sample mean.</p>
<p>Whereas for fat-tailed variables this behavior will only kick in much later. <strong>A single observation can define the statistical properties of the partial sum</strong>. Why? Think about the partial sum of the wealth of people. Even at very large <span class="math inline">\(n\)</span>, there is still a chance that we sample an hyper rich individual that has the double of the money of all the other people we’ve sampled so far. Thus, the scale of the distribution keeps growing a lot as we add new individuals. This behavior will only stop when we have an astronomically large number of people such that even if we sample Bill Gates, the partial sum won’t change that much.</p>
<p>Indeed, Taleb has a fantastic plot to show how the scale of the partial sum grows with the number of summands for different distributions:</p>
<p><img src="/images/scalemad.png" /></p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

