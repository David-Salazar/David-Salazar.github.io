<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>R-squared and fat tails - David Salazar&#39;s blog</title>
<meta property="og:title" content="R-squared and fat tails - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">R-squared and fat tails</h1>

    
    <span class="article-date">2020-05-26</span>
    

    <div class="article-content">
      


<div id="r-squared-and-fat-tails" class="section level1">
<h1>R-squared and Fat-tails</h1>
<p>This post continues to explore how common statistical methods are unreliable and dangerous when we are dealing with fat-tails. So far, we have seen how the distribution of the <a href="2020-04-17-fat-vs-thin-does-lln-work.html">sample mean</a>, <a href="2020-04-27-spurious-pca-under-thick-tails.html">PCA</a> and <a href="2020-05-22-correlation-is-not-correlation.html">sample correlation</a> turn into pure noise when we are dealing with fat-tails. In this post, I’ll show the same for <span class="math inline">\(R^2\)</span> (i.e., coefficient of determination). Remember, it is a random variable that we are estimating and thefore has its own distribution.</p>
<p>In short, the goal is to justify with simulations Nassim Taleb’s conclusion in his latest <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">technical book</a> regarding R-squared:</p>
<blockquote>
<p>When a fat tailed random variable is regresed against a thin tailed one, the coefficient of determination <span class="math inline">\(R^2\)</span> will be biased higher, and requires a much larger sample size to converge (if it ever does)</p>
</blockquote>
<div id="gameplan" class="section level2">
<h2>Gameplan</h2>
<p>I’ll follow the same gameplan as usual: explore with Monte-Carlo the distribution of our estimator in both Mediocristan and Extremistan.</p>
</div>
<div id="mediocristan" class="section level2">
<h2>Mediocristan</h2>
<p>Assume the usual scenario in a Gaussian regression: Gaussian errors.</p>
<pre class="r"><code># simulate
n &lt;- 10^6
x &lt;- rnorm(n)
y &lt;- rnorm(n, mean = 0.2 + 1 * x)</code></pre>
<p>Let’s plot (some of ) the data:</p>
<pre class="r"><code>data.frame(x, y)[sample(n, 10^4), ] %&gt;% 
  ggplot(aes(x, y)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;Gaussian Regression&quot;)</code></pre>
<p><img src="/post/2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>glue::glue(&quot;The correlation coefficient is: {round(cor(x,y), 2)}&quot;)</code></pre>
<pre><code>## The correlation coefficient is: 0.71</code></pre>
<p>Then, the <span class="math inline">\(R^2\)</span> should be the squared of this: <span class="math inline">\(0.50\)</span></p>
<pre class="r"><code>fit &lt;- lm(y ~ 1 + x, data = data.frame(x, y))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ 1 + x, data = data.frame(x, y))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8839 -0.6735  0.0003  0.6743  4.7843 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.1994577  0.0009997   199.5   &lt;2e-16 ***
## x           0.9994888  0.0009987  1000.8   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9997 on 999998 degrees of freedom
## Multiple R-squared:  0.5004, Adjusted R-squared:  0.5004 
## F-statistic: 1.002e+06 on 1 and 999998 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>broom::glance(fit)$r.squared</code></pre>
<pre><code>## [1] 0.5004092</code></pre>
<p>Which indeed it is<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Let’s create a Monte-Carlo function to simulate smaller samples and check the convergence of the <span class="math inline">\(R^2\)</span>.</p>
<pre class="r"><code>simulate_R_two &lt;- function(n = 30) {
  x &lt;- rnorm(n)
  y &lt;- rnorm(n, mean = 0.2 + 1 * x )
  fit &lt;- lm(y ~ 1 + x, data = data.frame(x, y))
  r2 &lt;- broom::glance(fit)$r.squared
  data.frame(r_squared = r2)
}

rerun(1000, simulate_R_two()) %&gt;% 
  bind_rows() -&gt; r_squareds_30

rerun(1000, simulate_R_two(n = 100)) %&gt;% 
  bind_rows() -&gt; r_squareds_100

rerun(1000, simulate_R_two(n = 1000)) %&gt;% 
  bind_rows() -&gt; r_squareds_1000</code></pre>
<p>Let’s plot the results</p>
<pre class="r"><code>data.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %&gt;% 
  rename(sample_30 = r_squared,
         sample_100 = r_squared.1,
         sample_1000 = r_squared.2) %&gt;% 
  pivot_longer(-sim, names_to = &quot;sample&quot;, values_to = &quot;r_squared&quot;) %&gt;% 
  mutate(sample = str_extract(sample, &quot;\\d+&quot;),
         sample = glue::glue(&quot;{sample} obs per sample&quot;),
         sample = factor(sample)) %&gt;% 
  ggplot(aes(r_squared, fill = sample)) +
  geom_histogram(color = &quot;black&quot;, alpha = 0.5, binwidth = 0.05) +
  geom_vline(aes(xintercept = 0.5), linetype = 2, color = &quot;red&quot;) +
  facet_wrap(~sample) +
  scale_fill_viridis_d() +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(caption = &quot;Binwidth is 0.05&quot;,
       title = &quot;Mediocristan: Distribution of R-squared values&quot;,
       subtitle = &quot;Gaussian Regression. True R^2 shown as red line.&quot;,
       x = &quot;R squared&quot;)</code></pre>
<p><img src="/post/2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Therefore, when we are dealing with randomness coming from Mediocristan, we can reliably use our estimates of the R-squared. They converge at a good pace toward the true vlue.</p>
</div>
<div id="extremistan" class="section level2">
<h2>Extremistan</h2>
<p>Now, let’s swtich pace and sample from Extremistan. Imagine then, our same simulation as before. However, instead of our noise coming from a Gaussian, our noise will come from a Pareto with tail exponent of <span class="math inline">\(1.5\)</span> (theoretical mean exists but higher moments do not). Let’s simulate:</p>
<pre class="r"><code>n &lt;- 10^5

x &lt;- rnorm(n)

pareto_errors &lt;- (1/runif(n)^(1/1.5))

y &lt;- 0.2 + 10*x + pareto_errors</code></pre>
<p>Before we plot, let’s think through what exactly is <span class="math inline">\(R^2\)</span>: it defines the proportion of the total variance of our outcome variable that is explained by our model. However, when the errors are Pareto distributed, our outcome variable is also Pareto distributed (with the same tail exponent). Therefore, the outcome variable won’t have a theoretical variance. That is, it will have an infinite variance. As you can imagine, no matter what variance the model explains, it is going to be tiny in comparison to the total variance. Thus, we arrive at the following: the true <span class="math inline">\(R^2\)</span> is zero. That is: <span class="math inline">\(E[R^2] = 0\)</span></p>
<pre class="r"><code>data.frame(x, y) %&gt;% 
  ggplot(aes(x, y)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;Pareto Regression&quot;)</code></pre>
<p><img src="/post/2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>fit &lt;- lm(y ~ 1 + x, data = data.frame(x, y))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ 1 + x, data = data.frame(x, y))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
##   -2.00   -1.70   -1.32   -0.38 2439.07 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.10987    0.03511   88.58   &lt;2e-16 ***
## x           10.03016    0.03508  285.96   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.1 on 99998 degrees of freedom
## Multiple R-squared:  0.4499, Adjusted R-squared:  0.4499 
## F-statistic: 8.177e+04 on 1 and 99998 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Even with <span class="math inline">\(10^5\)</span> observations, we are way off mark here. This is the same problem as we had with the other estimators. There isn’t enough data. As Taleb says:</p>
<blockquote>
<p><span class="math inline">\(R^2\)</span> … is a stochastic variable that will be extremely sample dependent, and only stabilize for large n, perhaps even astronomically large n</p>
</blockquote>
<p>To show this, let’s create with Monte-Carlo simulations the distribution of the sample R-squared:</p>
<pre class="r"><code>simulate_R_two &lt;- function(n = 30) {
  x &lt;- rnorm(n)
  pareto_errors &lt;- (1/runif(n)^(1/1.5))
  y &lt;- 0.2 + 10*x + pareto_errors
  fit &lt;- lm(y ~ 1 + x, data = data.frame(x, y))
  r2 &lt;- broom::glance(fit)$r.squared
  data.frame(r_squared = r2)
}

rerun(1000, simulate_R_two()) %&gt;% 
  bind_rows() -&gt; r_squareds_30

rerun(1000, simulate_R_two(n = 100)) %&gt;% 
  bind_rows() -&gt; r_squareds_100

rerun(1000, simulate_R_two(n = 1000)) %&gt;% 
  bind_rows() -&gt; r_squareds_1000</code></pre>
<p>Let’s plot our results:</p>
<pre class="r"><code>data.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %&gt;% 
  rename(sample_30 = r_squared,
         sample_100 = r_squared.1,
         sample_1000 = r_squared.2) %&gt;% 
  pivot_longer(-sim, names_to = &quot;sample&quot;, values_to = &quot;r_squared&quot;) %&gt;% 
  mutate(sample = str_extract(sample, &quot;\\d+&quot;),
         sample = glue::glue(&quot;{sample} obs per sample&quot;),
         sample = factor(sample)) %&gt;% 
  ggplot(aes(r_squared, fill = sample)) +
  geom_histogram(color = &quot;black&quot;, alpha = 0.5, binwidth = 0.05) +
  geom_vline(aes(xintercept = 0), linetype = 2, color = &quot;red&quot;) +
  facet_wrap(~sample) +
  scale_fill_viridis_d() +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(caption = &quot;Binwidth is 0.05&quot;,
       title = &quot;Extremistan: Distribution of R-squared values&quot;,
       subtitle = &quot;Pareto (infinite variance) Regression. True R-squared is zero&quot;,
       x = &quot;R squared&quot;)</code></pre>
<p><img src="/post/2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>As Taleb reminds us, <span class="math inline">\(R^2\)</span> is a stochastic variable. When the variance of our outcome variable approaches infinity, the <span class="math inline">\(E[R^2] \to 0\)</span>. However, to get this result in sample we must get a good estimate of the variance of our outcome variable in the first place. As we have seen, the Law of Large Numbers is way too slow to be useful when dealing with fat-tailed variables. Therefore, to get a good estimate of <span class="math inline">\(R^2\)</span> we will need an astronomically large sample size; otherwise, we will be estimating noise.</p>
<p>To conclude, <span class="math inline">\(R^2\)</span> should not be used when we are dealing in Extremistan. Whatever we estimate, it’s going to be pure noise. Even when the variance is not undefined, it will still be biased upwards.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Notice that here I am doing a circular argument because I know that <span class="math inline">\(R^2\)</span>, in Mediocristan, can be reliably estimated from this simulation. It is only for explanatory purposes (and out of lazyness) that I haven’t done the “right thing” and use boring algebra to derive <span class="math inline">\(R^2\)</span><a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

