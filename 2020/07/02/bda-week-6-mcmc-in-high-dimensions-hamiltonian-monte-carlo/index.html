<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo - Dilettanting Data Science</title>
<meta property="og:title" content="BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo</h1>

    
    <span class="article-date">2020/07/02</span>
    
    

    <div class="article-content">
      


<p>In the last couple of weeks, We’ve seen how the most difficult part of Bayesian Statistics is computing the posterior distribution. In particular, in the last week, we’ve studied the <a href="https://david-salazar.github.io/2020/06/29/bayesian-data-analysis-week-5-metropolis/">Metropolis Algorithm</a>. In this blogpost, I’ll study why Metropolis <strong>does not scale well enough to high dimensions</strong> and give an <em>intuitive explanation</em> of our best alternative: <strong>Hamiltonian Monte Carlo</strong> (HMC).</p>
<p>This blogpost is my personal digestion of the excellent content that Michael Betancourt has put out there to explain HMC. I particularly enjoyed his <a href="https://www.youtube.com/watch?v=jUSZboSq1zg&amp;t=1638s">YouTube lecture</a> and his blogpost about <a href="https://betanalpha.github.io/assets/case_studies/probabilistic_computation.html">Probabilistic computation</a>.</p>
<div id="metropolis-algorithm" class="section level2">
<h2>Metropolis Algorithm</h2>
<p>As we’ve seen, the Metropolis algorithm is just a random walk through parameter space where the proposed jumps are corrected by a comparison of posterior densities to determine whether to move or not. That is, we can see the Metropolis algorithm as:</p>
<blockquote>
<p>A stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density until it finds the mode and then only sometimes accepting steps that decrease the posterior density.</p>
</blockquote>
<p>Thus, as long as the algorithm has run long enough to find the posterior mode, <strong>and the area around the mode is a good representation of the overall posterior</strong>, the Metropolis Algorithm will work. Sadly, this assumption only holds for lower dimensions</p>
</div>
<div id="an-expectation-in-low-dimensions" class="section level2">
<h2>An Expectation in low dimensions</h2>
<p>Fundamentally, we sample from the posterior in order to compute Monte-Carlo estimators. That is, we want to approximate expectations:</p>
<p><span class="math display">\[
\mathbb{E}_{\pi}[f]=\int_{Q} \mathrm{d} q \pi(q) f(q)
\]</span></p>
<p>With the Metropolis algorithm, we are saying that we can approximate these expectation by computing at values near the mode of <span class="math inline">\(\pi(q)\)</span> and occasionally exploring lower density parts. Which is a pretty good approximation of most unimodal low dimensional probability distributions. However, this entirely ignores the contribution of <span class="math inline">\(\mathrm{d} q\)</span>, the differential volume, to the integral.</p>
</div>
<div id="an-expectation-in-high-dimensions" class="section level2">
<h2>An Expectation in high dimensions</h2>
<p>However, as the number of dimensions grows, so does the importance of <span class="math inline">\(\mathrm{d} q\)</span>, the differential volume, to the integral. Therefore, in higher dimensions, ignoring the differential volume when computing the expectation guarantees biased samples. Why does this happen? This is due to a phenomenon called <strong>concentration of measure</strong>.</p>
<p>Intuitively, an integral is nothing more than a weighted sum of parts of the distribution with height equal to function heights. However, as dimensions grows, <strong>the contribution of any single part of the distribution to the integral decreases very rapidly</strong>. Why? Because, the relative differential volume of this single part of the distribution decreases as we increase the number of dimensions. Michael’s Betancourt box-experiment is very illuminating.</p>
<div id="corner-madness" class="section level3">
<h3>Corner Madness</h3>
<blockquote>
<p>Let’s begin with a simple example - a square box representing the ambient space. In each dimension we will partition the box into three smaller boxes, with the central box corresponding to the neighborhood around the mode and the outer two side boxes corresponding to the rest of the space.</p>
</blockquote>
<p>In one dimension, the central box encapsulates one third of the total volume. In two dimensions, there are nine boxes, thus the central box represents only 1/9 of the volume. In three dimensions, we must fill the corners of the corners with more boxes, thus creating 27 boxes. The central box now only represents 1/27 of the total volume. We can keep increasing dimensions with the aid of Monte-Carlo samples.</p>
<blockquote>
<p>The relative importance of the central box is its probability with respect to a uniform distribution over the enclosing box, which we can estimate by sampling from that uniform distribution and computing the Monte Carlo estimator of the expectation value of the corresponding inclusion function</p>
</blockquote>
<p><span class="math display">\[
P(CentralBox) \approx \dfrac{N_{centralBox}}{N}
\]</span></p>
<pre class="r"><code>N &lt;- 10000
Ds &lt;- 1:10
prob_means &lt;- 0 * Ds

for (D in Ds) {
  is_central_samples &lt;- rep(1, N)
  for (n in 1:N) {
    # Sample a new point one dimension at a time
    for (d in 1:D) {
      q_d &lt;- runif(1, -3, 3)
      
    # If the sampled component is not contained 
    # within the central interval then set the 
    # inclusion function to zero
      if (-1 &gt; q_d || q_d &gt; 1)
        is_central_samples[n] &lt;- 0
    }
  }
  # Estimate the relative volume as a probability
  prob_means[D] &lt;- mean(is_central_samples)
}

data.frame(dimensions = Ds, prob_central_box = prob_means) %&gt;% 
  ggplot(aes(dimensions, prob_central_box)) +
  geom_step(color = &quot;dodgerblue4&quot;) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = 1:10) +
  labs(y = &quot;Probability of central box&quot;,
       subtitle = &quot;As dimensions grow, volume of a single box decreases rapidly&quot;,
       title = &quot;Central Box Volume in high dimensions&quot;)</code></pre>
<p><img src="/post/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo_files/figure-html/box-1.png" width="768" /></p>
</div>
<div id="all-that-matters-the-typical-set" class="section level3">
<h3>All that matters: the typical set</h3>
<p>Thus, the volume of a single neighborhood of the distribution decreases very rapidly as the number of dimensions grow. In our expectation, thus, the mode of the density <span class="math inline">\(\pi(q)\)</span> represents a neighborhood of the distribution with less and less volume and thus less and less probability mass. Indeed, in higher dimensions, our earlier <strong>intuition of a a probability distribution as concentrating around the mode and quickly decreasing is terribly wrong.</strong></p>
<p>Indeed, the probability of mass of any neighborhood of the distribution really depends on the interplay between its volume and the density. Without both of them, the probability mass at that neighborhood the distribution vanishes. Indeed, Michael Betancourt’s has a great plot showing where the volume and the target density interact to create large probability mass:</p>
<p><img src="/images/typicalset.PNG" /></p>
<blockquote>
<p>The neighborhood immediately around the mode features large densities, but in more
than a few dimensions the small volume of that neighborhood prevents it from having much
contribution to any expectation. On the other hand, the complimentary neighborhood far
away from the mode features a much larger volume, but the vanishing densities lead to
similarly negligible contributions expectations. The only significant contributions come
from the neighborhood between these two extremes known as the typical set</p>
</blockquote>
<p>Therefore, to calculate an expectation <strong>it suffices to sample from the typical set</strong> instead of the <em>entire</em> parameter space. The question, then, is what is exactly the shape of this typical set?</p>
</div>
<div id="a-surface-concentrating-away-from-the-mode" class="section level3">
<h3>A surface concentrating away from the mode</h3>
<p>As the number of dimensions grows and grows, the increasing tension between probability density and volume yields a worse compromise, and those compromises are found at points further and further from the mode. <strong>This is known as concentration of measure</strong>. Thus:</p>
<blockquote>
<p>The repulsion of the typical set away from the mode implies that the relative breadth of the typical set, how far its fuzziness diffuses, shrinks with increasing dimension. As we consider higher-dimensional spaces the typical set becomes ever more thin and gossamer, and all the harder to find and explore.</p>
</blockquote>
<p>Thus, the typical set, as the number of dimensions grow, becomes a narrow ridge of probability that becomes very difficult to explore with guess and check algorithms like the Metropolis algorithm.</p>
</div>
</div>
<div id="metropolis-and-the-typical-set" class="section level2">
<h2>Metropolis and the typical set</h2>
<p>The Metropolis Algorithm is not equipped to sample from the typical set. As we’ve seen, it’s a mode finding algorithm that explores the neighborhood around it. However, this neighborhood, as we’ve just seen, has barely any volume in high dimensions and thus have barely any probability mass. Indeed, given the narrowness of the typical set, almost all proposed jumps of the random walk will be outside the typical set and thus will be rejected.</p>
<p>Therefore, <strong>the Metropolis Algorithm in high dimensions results in inaction</strong>: the Metropolis algorithm will end up without moving at all for a long time. Thus, being a waste of our computational resources.</p>
<p>In two dimensions, we can visualize a typical set as a thin donut: at any point within it, there’s only a narrow ridge of points next to it that also belong to the typical set. <strong>Any ad-hoc proposal distribution that guesses in which direction we should move will be biased to propose points outside the typical set that will be rejected once we compare the Metropolis acceptance probability</strong>.</p>
<p>Chi Feng <a href="https://chi-feng.github.io/mcmc-demo/">has awesome interactive visualizations</a> that represent exactly this conundrum:</p>
<p><img src="/images/metropolis.gif" /></p>
</div>
<div id="hamiltonian-monte-carlo" class="section level1">
<h1>Hamiltonian Monte-Carlo</h1>
<p>So far, we’ve identified the fundamental problem with the random walk in the Metropolis algorithm: <em>in higher dimensions</em>, its <strong>adhoc proposal distribution guesses too many dumb jumps that take us out of the narrow ridge of high probability mass that is the typical set</strong>. How to fix this? Create a proposal distribution that takes into account the geometry of the typical set and makes intelligent proposal?</p>
<p>An intelligent proposal should make proposals within the typical set AND make proposals that are far away from where we currently are. Thus, we will explore the typical set as efficiently as possible. Hamiltonian Monte-Carlo algorithms, when well tuned, satisfy both conditions.</p>
<div id="crafting-a-proposal-distribution-for-the-typical-set" class="section level2">
<h2>Crafting a proposal distribution for the typical set</h2>
<p>We’ve identified that an intelligent proposal distribution will encode the geometry of the typical set. A natural way of encoding this geometry is with a <em>vector field</em> <strong>aligned with the typical set</strong>. Thus, instead of taking a random-walk through parameter space, we will simply follow the vector field for some time. Therefore:</p>
<blockquote>
<p>Continuing this process traces out a coherent trajectory through the typical set that efficiently moves us far away from the initial point to new, unexplored regions of the typical set as quickly as possible.</p>
</blockquote>
<p>The question, then, is how to create these vector field? A first approximation is to exploit the differential structure of the posterior thoruhg the gradient. However, following the gradient pulls us away from the typical set and into the mode of the posterior. Here comes Physics to the rescue. This is an equivalent problem to the problem of placing a satellite in a stable orbit around a hypothetical planet.</p>
<p>The key to do it is to endow our walk through the typical set by with enough momentum (<span class="math inline">\(p\)</span>) to counteract the gravitation attraction of the mode. Therefore:</p>
<blockquote>
<p>We can <em>twist</em> the gradient vector field into a vector field aligned with the typical set if we expand our original probabilistic system with the introduction of auxiliary momentum parameters.</p>
</blockquote>
<div id="augmenting-with-the-momenta" class="section level3">
<h3>Augmenting with the momenta</h3>
<p>We augment our probabilistic system with the momenta <span class="math inline">\(p\)</span> thus:</p>
<p><span class="math display">\[
\pi(q, p)=\pi(p \mid q) \pi(q)
\]</span></p>
<p>Which allows us to ignore the momenta when necessary by marginalizing it. The joint density <span class="math inline">\(\pi(q, p)\)</span> is constructed following Hamiltonian dynamics. Therefore:</p>
<blockquote>
<p>Following the Hamiltonian vector field for some time, <span class="math inline">\(t\)</span>, generates trajectories, <span class="math inline">\(\phi_{t}(q, p),\)</span> that rapidly move through phase <span class="math inline">\((q_n, p_n)\)</span> space while being constrained to the typical set. Projecting these trajectories back down onto the target parameter space finally yields the efficient exploration of the target typical set for which we are searching.</p>
</blockquote>
</div>
<div id="stages-to-generate-a-proposal-distribution" class="section level3">
<h3>3 stages to generate a proposal distribution</h3>
<p>Suppose you are at <span class="math inline">\(q_0\)</span>. How to generate a new proposal?</p>
<ol style="list-style-type: decimal">
<li>We start by sampling a momentum from <span class="math inline">\(p_0\)</span> from:</li>
</ol>
<p><span class="math display">\[
p_0 \sim \pi(p | q)
\]</span></p>
<p>We know are in the phase space <span class="math inline">\((q_0, p_0)\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>If we follow the vector field that we have created <span class="math inline">\(\phi_{t}(q_0, p_0) = q^*, p^*,\)</span> by integrating for some time <span class="math inline">\(t\)</span>, we coherently push the Markov transition away from the initial point while staying confined to the joint typical set. This movement through the field space is done via discrete <span class="math inline">\(L\)</span> <em>‘leapfrog steps’</em>. Stan’s implementation of HMC automatically chooses the leapfrog steps and their respective length.</p></li>
<li><p>If we marginalize the momenta, we project it away and go back to the parameter space with a proposal <span class="math inline">\(q^*\)</span>.</p></li>
</ol>
<p>Because we have followed the vector field in phase space and projected the momenta away, if we started in the typical set, our proposal is guaranteed to be on the typical set, too. <strong>Thus, allowing extremely efficient exploration of the typical set.</strong></p>
<p>Finally, we will accept or reject our proposed jump with a Metropolis-Hastings ratio.</p>
<p>We can visualize the incredible efficiency that Hamiltonian Monte Carlo can deliver. In the exact same example as with the donut we just saw with Metropolis, HMC samples much more efficiently.</p>
<p><img src="/images/hmc.gif" /></p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>High dimensional probability distributions are difficult to understand. Instead of piling much of the probability mass around the mode, the probability mass lies at the intersection of the neighborhoods where there is volume and there is target density. <strong>This area of probability mass is a narrow surface that lies far from the mode and is called the typical set.</strong> Random walk algorithms, like Metropolis, are fundamentally ill-suited to explore such narrow surfaces: their ad-hoc proposal distributions are biased to propose jumps that lie outside the typical set and thus the jumps are rejected. The end result, the chains explore the typical set very inefficiently.</p>
<p>On the other hand, <strong>Hamiltonian Monte Carlo (HMC) algorithms are precisely constructed to exploit the geometry of the typical set</strong> and make intelligent proposals. By borrowing Hamiltonian dynamics from physics, we create proposals that follow a vector-field that is aligned with the typical set. Thus, our proposals will lie on the typical set and will be much more likely to be accepted than the proposals of a random walk like those of Metropolis. Therefore, HMC allows us to efficiently explore the typical set.</p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

