<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Causality: Invariance under Interventions - Dilettanting Data Science</title>
<meta property="og:title" content="Causality: Invariance under Interventions - Dilettanting Data Science">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Causality: Invariance under Interventions</h1>

    
    <span class="article-date">2020/07/22</span>
    
    

    <div class="article-content">
      


<p>In the <a href="https://david-salazar.github.io/2020/07/18/causality-bayesian-networks/">last post</a> we saw how two causal models can yield the same testable implications and thus cannot be distinguished from data alone. That is, we cannot gain causal understanding from data alone. Does that mean that we cannot ever gain causal understanding? Far from it; it just means that we must have a causal model.</p>
<p><strong>Thus, causal effects cannot be estimated from the data itself without a causal story.</strong> In this blogpost, I’ll show how exactly the combination between causal models and observational data can lead us into estimating causal effects. In short, causal effects can be estimated by leveraging the invariant information that the pre-intervention distribution can provide. Doing so, we connect pre-intervention probabilities with the post-intervention probabilities that define the causal effect.</p>
<div id="defining-the-causal-effect-with-the-do-operator" class="section level2">
<h2>Defining the causal effect with the do-operator</h2>
<p>Fundamentally, we cannot gain causal understanding with data because <strong>the data we see could have been generated by many a causal models</strong>. That is, the associations we see, <span class="math inline">\(P(Y | X)\)</span>, can be the result of many interactions; <strong>some of them causal and some purely observational</strong>. We can say that any statistically meaningful association is <em>the result of a causal relationship</em> <strong>somewhere in the system</strong>, but <em>not necessarily</em> of the causal effect of interest, <span class="math inline">\(X \rightarrow Y\)</span>.</p>
<p>To disentangle this confusion, then, let’s define a causal effect. <a href="https://fabiandablander.com/r/Causal-Inference.html">Following Pearl</a>, we will take an <strong>interventionist position</strong> and say that a variable <span class="math inline">\(X\)</span> has a causal influence on <span class="math inline">\(Y\)</span> if intervening to change <span class="math inline">\(X\)</span> leads to changes in <span class="math inline">\(Y\)</span>. Intervening on <span class="math inline">\(X\)</span> means lifting <span class="math inline">\(X\)</span> from whatever mechanism previously defined its value and now set it to a particular value <span class="math inline">\(X=x\)</span> in an exogenous way.</p>
<p>Thus, the <strong>causal effect</strong> is defined as a <em>function</em> from the values <span class="math inline">\(X\)</span> can take to the space of <em>probability distributions</em> on <span class="math inline">\(Y\)</span>. For example, if <span class="math inline">\(X := x\)</span>, then we arrive at the <strong>interventional distribution</strong> <span class="math inline">\(P(Y| \text{do}(x))\)</span>: the population distribution of <span class="math inline">\(Y\)</span> if <em>everyone</em> in the population had their <span class="math inline">\(X\)</span> value fixed at <span class="math inline">\(x\)</span>.</p>
<p>The <span class="math inline">\(\text{do}\)</span> operator defines the exogenous process through which we have intervened to set the value of <span class="math inline">\(X := x\)</span>. Finally, we derive <span class="math inline">\(P(Y| \text{do}(x))\)</span> for every possible <span class="math inline">\(x\)</span> and test whether the distribution changes as we change the value <span class="math inline">\(X\)</span> takes.</p>
<p>Therefore, to study the causal effect of <span class="math inline">\(X\)</span> is to change the system by determining the value of <span class="math inline">\(X\)</span> outside of it and seeing how the effects cascade thorough the system. However, before we change a system we must define it. How to represent the system? With a Causal Graph!</p>
</div>
<div id="causal-graphs" class="section level2">
<h2>Causal Graphs</h2>
<p>The question, then, becomes: <strong>how can we simulate the effects of intervening in the causal system?</strong>. First, however, we must define the system in question.</p>
<p>Let each node represent one of the variables of interest. We will draw an arrow from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> if there is a <strong>direct causal effect</strong> from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span> for at least one individual. Alternatively, <strong>the lack of an arrow</strong> means that <em>there’s no</em> causal effect for any individual in the population. We will assume that the system is adequately written if <strong>all common causes</strong> of <em>any pair</em> of variables on the graph are <strong>themselves on the graph</strong>. Finally, we’ll say that a variable is always a cause of its descendants.</p>
<p>We will link Causal Graphs to Bayesian graphs by assuming that each variable, conditional on its parents, is independent of any variable for which it is not a cause (i.e., all its predecessors). In turn, this will imply that the <strong>Graph defines the same recursive decomposition of the joint distribution</strong> as a <em>Bayesian Graph</em>:</p>
<p><span class="math display">\[
P\left(x_{1}, \ldots, x_{n}\right)=\prod_{j} P\left(x_{j} \mid pa_j\right)
\]</span></p>
<p>Thereby, we can derive, using the d-separation criterion, <strong>testable implications</strong> of our causal models.</p>
<p>To make things more concrete, let’s work with the following fork: let’s say that a new treatment is developed to reduce cholesterol. However, women take the treatment more/less than men and have higher/lower levels of cholesterol. How to compute the causal effect of the treatment on cholesterol?</p>
<p><img src="/post/2020-07-22-causality-invariance-under-interventions_files/figure-html/drug,%20coffee-1.png" width="672" /></p>
</div>
<div id="interventions-eliminating-incoming-arrows" class="section level2">
<h2>Interventions: Eliminating incoming arrows</h2>
<p>Intervening on <span class="math inline">\(X\)</span> such that <span class="math inline">\(\text{do(X = 1)}\)</span> amounts to curtailing the previous mechanism that defined <span class="math inline">\(X\)</span>. In Graph lingo: <strong>eliminate the incoming arrows into <span class="math inline">\(X\)</span></strong>: <em>gender no longer cause <span class="math inline">\(X\)</span></em>. Therefore, we eliminate the arrow from Gender into treatment. Thus, an intervention is equivalent to eliminating arrows in a Causal Graph. Let’s label this new graph <span class="math inline">\(G_m\)</span></p>
<p><img src="/post/2020-07-22-causality-invariance-under-interventions_files/figure-html/drug-eliminated-1.png" width="672" /></p>
</div>
<div id="invariant-probabilities-under-intervention" class="section level2">
<h2>Invariant probabilities under intervention</h2>
<p>The mutilated graph is still a Causal Graph. Thus, it implies a <em>particular decomposition of the joint probability</em> (<span class="math inline">\(P_m\)</span>) of it’s own. With respect to this post-intervention distribution, we can define the causal effect: <span class="math inline">\(P(Y=y|\text{do}(X=x)) := P_m (Y=y|X=x)\)</span>. However, this new post-intervention distribution <span class="math inline">\(P_m\)</span> is <strong>not totally disconnected</strong> from the pre-intervention distribution (<span class="math inline">\(P\)</span>) that we can study with observational data.</p>
<p>There are two <strong>invariant qualities</strong> that are the same in the <em>pre-intervention and post-intervention</em> distribution:</p>
<ul>
<li><p>Our intervention is <strong>atomic</strong>: there are no side effects that alter the way non-descendants of <span class="math inline">\(X\)</span> are determined. Thus, <span class="math inline">\(P_m(Z=z| X=x) = P(Z=z)\)</span>.</p></li>
<li><p>The conditional probability <span class="math inline">\(Y\)</span> is invariant, because the mechanism by which Y responds to <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> remains the same, <em>regardless</em> of whether <span class="math inline">\(X\)</span> <strong>changes spontaneously or by deliberate manipulation</strong>. Thus; <span class="math inline">\(P_m(Y| X=x, Z = z) = P(Y|X=x, Z=z)\)</span>.</p></li>
</ul>
<div id="connecting-pre-intervention-probabilities-with-post-treament" class="section level3">
<h3>Connecting pre-intervention probabilities with post-treament</h3>
<p>Therefore, using probability laws and our independence assumption between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> in the mutilated graph, we define the causal effect in terms of post-intervention distribution thus:</p>
<p><span class="math display">\[
\begin{array}{l}
P(Y = y | do(X=x)) := P_m (Y=y|X=x) \\
=\sum_{z} P_{m}(Y=y \mid X=x, Z=z) P_{m}(Z=z \mid X=x) \\
=\sum_{z} P_{m}(Y=y \mid X=x, Z=z) P_{m}(Z=z)
\end{array}
\]</span>
Luckily, all the terms invariant: both terms can be <strong>connected to the original pre-intervention</strong> probability distribution:</p>
<p><span class="math display">\[
P(Y=y \mid d o(X=x))=\sum_{z} P(Y=y \mid X=x, Z=z) P(Z=z)
\]</span>
Therefore, we arrive at a definition of the <strong>causal effect in terms of the pre-treatment distribution</strong>. Thus, we can <strong>estimate the causal effect from observational studies</strong> without the need of <em>actually carrying out</em> the intervention.</p>
</div>
</div>
<div id="the-adjustment-formula" class="section level2">
<h2>The Adjustment Formula</h2>
<p>More generally, we define the causal effect in terms of pre-intervention probability thus. Given a graph <span class="math inline">\(G\)</span> in which a set of variables <span class="math inline">\(pa\)</span> are designated as the parents of <span class="math inline">\(X\)</span>, the causal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is given by:</p>
<p><span class="math display">\[
P(Y=y|\text{do}(X=x)) = \sum_{z} P(Y=y | X=x, P A=z) P(pa=z)
\]</span>
Therefore, we can conclude why it is necessary to have a causal story to be able to estimate the causal effect: <strong>to identify the parents of <span class="math inline">\(X\)</span> and adjust for them</strong>: first condition <span class="math inline">\(P(Y=y| X =x)\)</span> on <span class="math inline">\(PA\)</span> and then average the result, weighted the prior probability of <span class="math inline">\(pa = z\)</span>.</p>
</div>
<div id="an-example" class="section level2">
<h2>An example</h2>
<p>Let’s follow our thought experiment with our previous graph. In the <a href="https://fabiandablander.com/r/Causal-Inference.html">experiment</a>, we observe both men and women who decide whether they take the drug or not. The results are the following:</p>
<p><style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#bsbjrkxjvf .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#bsbjrkxjvf .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#bsbjrkxjvf .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#bsbjrkxjvf .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#bsbjrkxjvf .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#bsbjrkxjvf .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#bsbjrkxjvf .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#bsbjrkxjvf .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#bsbjrkxjvf .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#bsbjrkxjvf .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#bsbjrkxjvf .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#bsbjrkxjvf .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#bsbjrkxjvf .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#bsbjrkxjvf .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#bsbjrkxjvf .gt_from_md > :first-child {
  margin-top: 0;
}

#bsbjrkxjvf .gt_from_md > :last-child {
  margin-bottom: 0;
}

#bsbjrkxjvf .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#bsbjrkxjvf .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#bsbjrkxjvf .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#bsbjrkxjvf .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#bsbjrkxjvf .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#bsbjrkxjvf .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#bsbjrkxjvf .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#bsbjrkxjvf .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#bsbjrkxjvf .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#bsbjrkxjvf .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#bsbjrkxjvf .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#bsbjrkxjvf .gt_left {
  text-align: left;
}

#bsbjrkxjvf .gt_center {
  text-align: center;
}

#bsbjrkxjvf .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#bsbjrkxjvf .gt_font_normal {
  font-weight: normal;
}

#bsbjrkxjvf .gt_font_bold {
  font-weight: bold;
}

#bsbjrkxjvf .gt_font_italic {
  font-style: italic;
}

#bsbjrkxjvf .gt_super {
  font-size: 65%;
}

#bsbjrkxjvf .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="bsbjrkxjvf" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Recovered</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">N</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1">Treatment</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">Gender</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr>
      <td class="gt_row gt_center">81</td>
      <td class="gt_row gt_center">87</td>
      <td class="gt_row gt_center">1</td>
      <td class="gt_row gt_left">Male</td>
    </tr>
    <tr>
      <td class="gt_row gt_center">234</td>
      <td class="gt_row gt_center">270</td>
      <td class="gt_row gt_center">0</td>
      <td class="gt_row gt_left">Male</td>
    </tr>
    <tr>
      <td class="gt_row gt_center">192</td>
      <td class="gt_row gt_center">263</td>
      <td class="gt_row gt_center">1</td>
      <td class="gt_row gt_left">Female</td>
    </tr>
    <tr>
      <td class="gt_row gt_center">55</td>
      <td class="gt_row gt_center">80</td>
      <td class="gt_row gt_center">0</td>
      <td class="gt_row gt_left">Female</td>
    </tr>
  </tbody>
  
  
</table></div>
When we study the data across genders, we find out that the patients who didn’t take the drug had a higher rate of recovery:</p>
<p><img src="/post/2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>However, once we separate the data by gender, the opposite picture arises:</p>
<p><img src="/post/2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>We have a case of Simpson’s Paradox! Let’s use the causal knowledge embedded in our graph to estimate the true causal effect of the treatment. Given that Gender is the only parent of Treatment, we will adjust for it:</p>
<p><span class="math display">\[
P(Y=1 \mid d o(X=1))=\frac{0.93(87+270)}{700}+\frac{0.73(263+80)}{700}=0.832
\]</span>
while, similarly,
<span class="math display">\[
P(Y=1 \mid d o(X=0))=\frac{0.87(87+270)}{700}+\frac{0.69(263+80)}{700}=0.7818
\]</span>
Thus, comparing the effect of drug-taking <span class="math inline">\((X=1)\)</span> to the effect of nontaking <span class="math inline">\((X=0),\)</span> we obtain
<span class="math display">\[
A C E=P(Y=1 \mid d o(X=1))-P(Y=1 \mid d o(X=0))=0.832-0.7818=0.0502
\]</span></p>
<p>However, if Gender had not been a parent of Treatment (i.e., if both Genders decide to take the treatment equally), our Causal effect would be different because we would adjust for Gender in the first place.</p>
</div>
<div id="identifiable" class="section level2">
<h2>Identifiable</h2>
<p>We’ve estimated causal effects with a pretty simple strategy: adjust for the parents of the exposure and average those effects weighted by the probability of the parents.</p>
<p>Therefore, according to our strategy, the causal effect will be <strong>identifiable</strong> whenever both <span class="math inline">\(X, Y\)</span> and the parents of <span class="math inline">\(X\)</span>, <span class="math inline">\(pa\)</span> are measured. Whenever measurements for some of them are missing, we must use other techniques to estimate the causal effect.</p>
</div>
<div id="addendum-rct" class="section level2">
<h2>Addendum: RCT</h2>
<p>Randomized Control Trials are sometimes referred to as the gold standard in causal inference. However, in our framework, they are nothing more than a <strong>different graph surgery</strong>. Whereas before we cut all the incoming arrows into treatment, now we replace all the incoming arrows with only with one arrow that signifies the randomization of the treatment:</p>
<p><img src="/post/2020-07-22-causality-invariance-under-interventions_files/figure-html/rct-1.png" width="768" /></p>
<p>Therefore, now we must simply adjust by randomization to estimate the causal effect of treatment. Does that mean that they are not useful? No, they will always have the upper hand when we are uncertain about our causal model. If there is another parent of treatment that we are not accounting for, Randomization will offer a clean solution.</p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

