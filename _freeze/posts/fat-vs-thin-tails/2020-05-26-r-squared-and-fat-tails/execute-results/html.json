{
  "hash": "36494e5a7d74e2ef3efed8dfa4193352",
  "result": {
    "markdown": "---\ntitle: R-squared and fat tails\nauthor: ''\ndate: '2020-05-26'\nslug: r-squared-and-fat-tails\ncategories: []\ntags: []\n---\n\n\n\n\n# R-squared and Fat-tails\n\nThis post continues to explore how common statistical methods are unreliable and dangerous when we are dealing with fat-tails. So far, we have seen how the distribution of the [sample mean](2020-04-17-fat-vs-thin-does-lln-work.html), [PCA](2020-04-27-spurious-pca-under-thick-tails.html) and [sample correlation](2020-05-22-correlation-is-not-correlation.html) turn into pure noise when we are dealing with fat-tails. In this post, I'll show the same for $R^2$ (i.e., coefficient of determination). Remember, it is a random variable that we are estimating and thefore has its own distribution. \n\nIn short, the goal is to justify with simulations Nassim Taleb's conclusion in his latest [technical book](https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf) regarding R-squared:\n\n> When a fat tailed random variable is regresed against a thin tailed one, the coefficient of determination $R^2$ will be biased higher, and requires a much larger sample size to converge (if it ever does)\n\n## Gameplan\n\nI'll follow the same gameplan as usual: explore with Monte-Carlo the distribution of our estimator in both Mediocristan and Extremistan. \n\n## Mediocristan\n\nAssume the usual scenario in a Gaussian regression: Gaussian errors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate\nn <- 10^6\nx <- rnorm(n)\ny <- rnorm(n, mean = 0.2 + 1 * x)\n```\n:::\n\n\nLet's plot (some of ) the data:\n\n\n::: {.cell hash='2020-05-26-r-squared-and-fat-tails_cache/html/unnamed-chunk-3_5d452cef32635494058824d64ac24dad'}\n\n```{.r .cell-code}\ndata.frame(x, y)[sample(n, 10^4), ] %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Gaussian Regression\")\n```\n\n::: {.cell-output-display}\n![](2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglue::glue(\"The correlation coefficient is: {round(cor(x,y), 2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe correlation coefficient is: 0.71\n```\n:::\n:::\n\n\nThen, the $R^2$ should be the squared of this: $0.50$ \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ 1 + x, data = data.frame(x, y))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = data.frame(x, y))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8839 -0.6735  0.0003  0.6743  4.7843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.1994577  0.0009997   199.5   <2e-16 ***\nx           0.9994888  0.0009987  1000.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9997 on 999998 degrees of freedom\nMultiple R-squared:  0.5004,\tAdjusted R-squared:  0.5004 \nF-statistic: 1.002e+06 on 1 and 999998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::glance(fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5004092\n```\n:::\n:::\n\n\nWhich indeed it is^[Notice that here I am doing a circular argument because I know that $R^2$, in Mediocristan, can be reliably estimated from this simulation. It is only for explanatory purposes (and out of lazyness) that I haven't done the \"right thing\" and use boring algebra to derive $R^2$]. Let's create a Monte-Carlo function to simulate smaller samples and check the convergence of the $R^2$.\n\n\n::: {.cell hash='2020-05-26-r-squared-and-fat-tails_cache/html/unnamed-chunk-7_2d902cf040e68cdec9357daf95ec7cec'}\n\n```{.r .cell-code}\nsimulate_R_two <- function(n = 30) {\n  x <- rnorm(n)\n  y <- rnorm(n, mean = 0.2 + 1 * x )\n  fit <- lm(y ~ 1 + x, data = data.frame(x, y))\n  r2 <- broom::glance(fit)$r.squared\n  data.frame(r_squared = r2)\n}\n\nrerun(1000, simulate_R_two()) %>% \n  bind_rows() -> r_squareds_30\n\nrerun(1000, simulate_R_two(n = 100)) %>% \n  bind_rows() -> r_squareds_100\n\nrerun(1000, simulate_R_two(n = 1000)) %>% \n  bind_rows() -> r_squareds_1000\n```\n:::\n\n\nLet's plot the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %>% \n  rename(sample_30 = r_squared,\n         sample_100 = r_squared.1,\n         sample_1000 = r_squared.2) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"r_squared\") %>% \n  mutate(sample = str_extract(sample, \"\\\\d+\"),\n         sample = glue::glue(\"{sample} obs per sample\"),\n         sample = factor(sample)) %>% \n  ggplot(aes(r_squared, fill = sample)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0.5), linetype = 2, color = \"red\") +\n  facet_wrap(~sample) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"none\") +\n  labs(caption = \"Binwidth is 0.05\",\n       title = \"Mediocristan: Distribution of R-squared values\",\n       subtitle = \"Gaussian Regression. True R^2 shown as red line.\",\n       x = \"R squared\")\n```\n\n::: {.cell-output-display}\n![](2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nTherefore, when we are dealing with randomness coming from Mediocristan, we can reliably use our estimates of the R-squared. They converge at a good pace toward the true vlue. \n\n## Extremistan\n\nNow, let's swtich pace and sample from Extremistan. Imagine then, our same simulation as before. However, instead of our noise coming from a Gaussian, our noise will come from a Pareto with tail exponent of $1.5$ (theoretical mean exists but higher moments do not). Let's simulate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10^5\n\nx <- rnorm(n)\n\npareto_errors <- (1/runif(n)^(1/1.5))\n\ny <- 0.2 + 10*x + pareto_errors\n```\n:::\n\n\nBefore we plot, let's think through what exactly is $R^2$: it defines the proportion of the total variance of our outcome variable that is explained by our model. However, when the errors are Pareto distributed, our outcome variable is also Pareto distributed (with the same tail exponent). Therefore, the outcome variable won't have a theoretical variance. That is, it will have an infinite variance. As you can imagine, no matter what variance the model explains, it is going to be tiny in comparison to the total variance. Thus, we arrive at the following: the true $R^2$ is zero. That is: $E[R^2] = 0$\n\n\n::: {.cell hash='2020-05-26-r-squared-and-fat-tails_cache/html/unnamed-chunk-10_e1ccc59a42104184682850a4ba9cb8a2'}\n\n```{.r .cell-code}\ndata.frame(x, y) %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Pareto Regression\")\n```\n\n::: {.cell-output-display}\n![](2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(y ~ 1 + x, data = data.frame(x, y))\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = data.frame(x, y))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n    -4     -2     -2     -1  39405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.5633     0.3988   8.935   <2e-16 ***\nx            10.4619     0.3993  26.204   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 126.1 on 99998 degrees of freedom\nMultiple R-squared:  0.00682,\tAdjusted R-squared:  0.00681 \nF-statistic: 686.6 on 1 and 99998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nEven with $10^5$ observations, we are way off mark here. This is the same problem as we had with the other estimators. There isn't enough data. As Taleb says:\n\n> $R^2$ ... is a stochastic variable that will be extremely sample dependent, and only stabilize for large n, perhaps even astronomically large n\n\nTo show this, let's create with Monte-Carlo simulations the distribution of the sample R-squared:\n\n\n::: {.cell hash='2020-05-26-r-squared-and-fat-tails_cache/html/unnamed-chunk-12_17e7b3fb58c2008cf9b05bf2d57a360c'}\n\n```{.r .cell-code}\nsimulate_R_two <- function(n = 30) {\n  x <- rnorm(n)\n  pareto_errors <- (1/runif(n)^(1/1.5))\n  y <- 0.2 + 10*x + pareto_errors\n  fit <- lm(y ~ 1 + x, data = data.frame(x, y))\n  r2 <- broom::glance(fit)$r.squared\n  data.frame(r_squared = r2)\n}\n\nrerun(1000, simulate_R_two()) %>% \n  bind_rows() -> r_squareds_30\n\nrerun(1000, simulate_R_two(n = 100)) %>% \n  bind_rows() -> r_squareds_100\n\nrerun(1000, simulate_R_two(n = 1000)) %>% \n  bind_rows() -> r_squareds_1000\n```\n:::\n\n\nLet's plot our results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %>% \n  rename(sample_30 = r_squared,\n         sample_100 = r_squared.1,\n         sample_1000 = r_squared.2) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"r_squared\") %>% \n  mutate(sample = str_extract(sample, \"\\\\d+\"),\n         sample = glue::glue(\"{sample} obs per sample\"),\n         sample = factor(sample)) %>% \n  ggplot(aes(r_squared, fill = sample)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0), linetype = 2, color = \"red\") +\n  facet_wrap(~sample) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"none\") +\n  labs(caption = \"Binwidth is 0.05\",\n       title = \"Extremistan: Distribution of R-squared values\",\n       subtitle = \"Pareto (infinite variance) Regression. True R-squared is zero\",\n       x = \"R squared\")\n```\n\n::: {.cell-output-display}\n![](2020-05-26-r-squared-and-fat-tails_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nAs Taleb reminds us, $R^2$ is a stochastic variable. When the variance of our outcome variable approaches infinity, the $E[R^2] \\to 0$. However, to get this result in sample we must get a good estimate of the variance of our outcome variable in the first place. As we have seen, the Law of Large Numbers is way too slow to be useful when dealing with fat-tailed variables. Therefore, to get a good estimate of $R^2$ we will need an astronomically large sample size; otherwise, we will be estimating noise. \n\nTo conclude, $R^2$ should not be used when we are dealing in Extremistan. Whatever we estimate, it's going to be pure noise. Even when the variance is not undefined, it will still be biased upwards.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}