[
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "",
    "text": "We’ve seen how the language of causality require an exogenous intervention on the values of \\(X\\); so far we’ve studied interventions on all the population, represented by the expression \\(do(X)\\). Nevertheless, with this language, there are plenty of interventions that remain outside our realm: most notably, counterfactual expressions where the antecedent is in contradiction with the observed behavior: there’s a clash between the observed world and the hypothetical world of interest.\nTo solve this conundrum we will need to set up a more elaborate language whereby we leverage the invariant information from the observed world into the hypothetical world.\nThese type of counterfactual queries are fundamental in our study of causality; as they enable us to answer such questions as: interventions on sub-populations; additive interventions; mediation analysis through ascertaining direct and indirect effects; and to study probability of causation (sufficient and necessary causes).\nIn this post, all citations come from Pearl’s book: Causality."
  },
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#game-plan",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#game-plan",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "Game Plan",
    "text": "Game Plan\nIn this blogpost, we will define Structural Causal Models (SCM) and explore how they encapsulate all the information we need in order to study counterfactuals. First, we’ll analyze why we cannot use the do-calculus to study counterfactuals where the antecedent contradicts the observed world. Secondly, we will define counterfactuals as derived properties of SCM and realize how interventional data undetermines counterfactual information. Thirdly, we will formulate a SCM to put what we have learned into action."
  },
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#we-change-by-taking-the-road-less-traveled-by",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#we-change-by-taking-the-road-less-traveled-by",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "We change by taking the road less traveled by",
    "text": "We change by taking the road less traveled by\nLet’s play with Frost’s famous poem:\n\nTwo roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference.\n\nWhat if Frost hadn’t taken the road less traveled by? Let’s say that by taking the road less traveled by, it took Frost \\(Y=1\\) hour of driving time. Given how long it took him on the road less traveled by, how long would it had taken him on the other road?\n\\[\nE[ Y| \\  \\text{do(Other road)}, Y = 1]\n\\] There’s a clash between the \\(Y\\) we are trying to estimate and the observed \\(Y = 1\\). Unfortunately, the do-operator does not offer us the possibility of distinguishing between the two variables themselves: one standing for the \\(Y\\) if we take the road less traveled by, the other \\(Y\\) for the hypothetical \\(Y\\) if Frost had taken the other road. That is, the different \\(y\\)’s are events occurring in different worlds.\nBecause the do-calculus offers no way of connecting the information across the different worlds, it means that we cannot use interventional experiments to estimate the counterfactuals. Indeed, the Frost after taking the road less traveled by is a very different Frost than he was before taking any of the roads."
  },
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#the-ladder-of-causation",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#the-ladder-of-causation",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "The Ladder of Causation",
    "text": "The Ladder of Causation\nBefore, we had seen that observational information is not enough to distinguish between different causal diagrams. We’ll show that the same thing happens with counterfactuals: information from interventions is not enough to distinguish between different Structural Causal Diagrams. Indeed, prediction, intervention and counterfactuals represent a natural hierarchy of reasoning tasks, with increasing levels of refinement and increasing demands on the knowledge required to accomplish them. Pearl calls this hierarchy the Ladder of Causation:\n\nWhereas for prediction one only needs a joint distribution function, the analysis of intervention requires a causal structure; finally, processing counterfactuals requires information about the functional relationships that determine that determine the variables and/or the distribution of the omitted factors. We will encode all this necessary information with Structural Causal Models (SCM)."
  },
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#defining-counterfactuals",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#defining-counterfactuals",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "Defining Counterfactuals",
    "text": "Defining Counterfactuals\nA Structural Causal Model is a triplet of Unobserved Exogenous Variables (\\(U\\)) called background variables, Observed Endogenous Variables (\\(V\\)) and Functional relationships (\\(F\\)) that map for each \\(V_i\\) from their respective domain \\(U_i \\cup Pa_i\\) (\\(Pa_i\\) are the parents of \\(i\\)) into \\(V_i\\) thus:\n\\[\nv_i = f_i(Pa_i, u_i)\n\\]\nEvery SCM can be associated with a causal DAG. However, the graph merely identifies the endogenous and background variables; it does not specify the functional form of \\(f_i\\) nor the distribution of the background variables.\nA counterfactual is defined by a submodel, \\(M_x\\), where the the functional relationship for \\(X\\) is replaced to make \\(X=x\\) hold true under any \\(u\\). Thus, the potential response of \\(Y\\) to action \\(do(X=x)\\) denoted by \\(Y_x(u)\\) is the solution for \\(Y\\) on the set of equations \\(F_x\\) in \\(M_x\\).\nIf we define a probability function over the background variables, \\(P(u)\\), we can define the probability over the endogenous variables thus:\n\\[\nP(Y = y) := \\sum_{u | Y(u) = y} P(u)\n\\]\nTherefore, the probability of counterfactual statements is thus derived:\n\\[\nP(Y_x = y) := \\sum_{u | Y_x(u) = y} P(u)\n\\]\nNote that we can define \\(P(Y = y | do(X=x)) = P(Y_x = y)\\). This solution to the SCM coincides with the truncated factorization obtained by pruning arrows from a causal DAG.\n\nConnecting different worlds through background variables\nThe determining feature of most counterfactuals is that we are interested in a conditional probability such that the information we are updating on is in contradiction with the counterfactual antecedent. In math terms:\n\\[\nP(Y_{x'} = y' | X = x, Y = y) = \\sum_u P(Y_{x'}(u) = y') P(u | x, y)\n\\] First, notice that we are using the information from one causal world (\\(<M, u>\\)) where we observe \\((X=x, Y = y)\\) to find out the probability of a statement \\(Y_x\\) in a different causal world (\\(<M_x, u>\\)). That is, the counterfactual antecedent “must be evaluated under the same background conditions as those prevailing in the observed world”.\n“The background variables are thus the main carriers of information from the actual world to the hypothetical world; they serve as the guardians of invariance.” To do so, we must first update our knowledge of \\(P(u)\\) to obtain \\(P(u|x, y)\\). Therefore, to be able to answer counterfactual queries we must have a distribution for the background variables. Indeed, this key step is known as abduction: reasoning from evidence (\\((x, y)\\) observed) to explanation (the background variables).\nThis is the fundamental characteristic counterfactual statements: we need to route the impact of known facts through U”.\n\n\nAfter abduction comes action and prediction\nOnce we have evaluated the prevailing background conditions, we use these in the sub-model \\(M_{x'}\\), where \\(x'\\) is the antecedent of the counterfactual. Finally, we use the equations in this modified SCM to predict the probability of \\(Y_{x'}\\), the consequence of the counterfactual.\nPearl has a great temporal metaphor for this whole process:\n\nAbduction explains the past (U) in light of the current evidence. The action bends the course of history to comply with the hypothetical condition \\(X= x'\\). Finally, we predict the future based on our new understanding of the past and our newly established condition, \\(X=x'\\)."
  },
  {
    "objectID": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#probabilities-for-the-dead",
    "href": "posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html#probabilities-for-the-dead",
    "title": "Causality: Counterfactuals - Clash of Worlds",
    "section": "Probabilities for the dead",
    "text": "Probabilities for the dead\nLet’s formulate the following example that will show why information from interventions undetermines counterfactuals and that will serve as practice in computing counterfactuals. The example is taken from the excellent paper from Bareinboim (et alter) (PDF):\nLet ( ^{}=={U_{1}, U_{2}}, ={X, Y}, ^{}, P(U), ) where [ ^{*}={\n\\[\\begin{array}{ll}\nX & \\leftarrow U_{1} \\\\\nY & \\leftarrow U_{2}\n\\end{array}\\]\n. ]\nand \\(U_1, U_2\\) are binary.\nNotice that we expect that any intervention will lead us to conclude that the treatment \\(X\\) is not effective: \\(P(Y| do(X)) = P(Y)\\). Suppose that we conclude exactly this with a RCT.\nIs this intervention evidence enough to argue for \\(\\mathcal{M}^{*}\\)? No! Interventional information undetermines counterfactual information. Notice that other SCM, \\(\\mathcal{M}^{'}\\), is also consistent with such causal effects and yet leads to a different counterfactual answer:\n\\[\n\\mathcal{F}^{\\prime}=\\left\\{\\begin{array}{ll}\nX & \\leftarrow U_{1} \\\\\nY & \\leftarrow X U_{2}+(1-X)\\left(1-U_{2}\\right)\n\\end{array}\\right.\n\\]\nIn both \\(\\mathcal{M}^{'}\\) and \\(\\mathcal{M}^{*}\\), we expect an intervention on \\(X\\) to lead to no causal effect: \\(P(Y| do(X)) = P(Y)\\).\n\nHowever, notice that they lead to very different answers for counterfactual queries. Suppose, then, that you have a patient \\(S\\) that took the treatment and died: what is the probability that ( S ) would have survived had they not been treated? We write this as ( P(Y_{X=0}=1 X=1, Y=0), )\n\nIn ( ^{}, ) we have ( P{{}}(Y_{X=0}=1 X=1, Y=0)=0, ) whereas in ( ^{} ) we have the exact opposite pattern, ( P{{}}(Y_{X=0}=1 X=1, Y=0)=1 ). These two models thus make diametrically opposed predictions about whether ( S ) would have survived had they not taken the treatment.\n\nIn other words, the best explanation for ( S ) ’s death may be completely different depending on whether the world is like ( ^{} ) or ( ^{} ). In ( ^{}, S ) would hav died anyway, while in ( ^{}, S ) would actually have survived, if only they had not been give the treatment."
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html",
    "title": "Causality: To adjust or not to adjust",
    "section": "",
    "text": "In this blogpost, I’ll simulate data to show how conditioning on as many variables as possible is not a good idea. Sometimes, conditioning can lead to de-confound an effect; other times, however, conditioning on a variable can create unnecessary confounding and bias the effect that we are trying to understand.\nIt all depends on our causal story: by applying the backdoor-criterion to our Causal Graph, we can derive an unambiguous answer to decide which variables should we use as controls in our statistical analysis."
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#motivation",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#motivation",
    "title": "Causality: To adjust or not to adjust",
    "section": "Motivation",
    "text": "Motivation\nIn the last couple of posts, we’ve seen how to define causal effects in terms of interventions and how to represent these interventions in terms of graph surgeries. We’ve also seen that observational data undetermines interventional data. Therefore, we cannot gain causal understanding unless we accompany our data-analysis with a causal story.\nIn particular, we’ve seen that by leveraging the invariant qualities under intervention, we are able to estimate causal effects with the adjustment formula by conditioning on the parents of the exposure \\(X\\):\n\\[\nP(Y=y|\\text{do}(X=x)) = \\sum_{z} P(Y=y | X=x, pa=z) P(pa=z)\n\\] However, what happens when we do not measure the parents of \\(X\\) and thus cannot adjust for them? Can we adjust for some other observed variable(s)?"
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#of-confounding",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#of-confounding",
    "title": "Causality: To adjust or not to adjust",
    "section": "Of confounding",
    "text": "Of confounding\nLet’s say that we are interested in estimating the causal effect of \\(X\\) on \\(Y\\) and we assume the following Causal Graph:\n\nexample <- dagify(z ~ b + c,\n                  a ~ b,\n                  d ~ c,\n                  x ~ a + z,\n                  w ~ x,\n                  y ~ w + z + d)\nggdag(example) +\n  labs(title = \"We want to estimate x -> y\",\n       subtitle = TeX(\"But we don't observe $A$ and thus we cannot use the adjustment formula\"))\n\n\n\n\nThe adjustment formula above states that we should adjust for the parents of \\(x\\). However, assume that we cannot measure \\(a\\). Can we still estimate the causal effect?\nTo answer this question we must understand why is that it’s useful to adjust for the parents. First, remember that in a Causal Graph, the statistical information flows freely in the Graph, regardless of the direction of the arrows. Therefore, in the above graph, the causal information from \\(c\\) to \\(y\\) may end up getting picked up by \\(x\\) or vice versa.\nWe condition on the parents of \\(x\\) such that we block all the information coming from other causal relationships. Thus, we don’t end up adding up the causal effect of some other variable in the process. However, if we cannot control by its parents, it’s possible that some of this causal effect coming from other variables will be picked up by \\(X\\) through the arrows that go into it. Therefore, we will have confounding bias. Which we defined thus:\n\\[\nP(Y | \\text{do(X = x)}) \\neq P(Y| X=x)\n\\]\n\nBlocking non-causal paths\nFor example, in the above graph there are the following 4 paths from \\(X\\) into \\(Y\\):\n\nggdag_paths(example, from = 'x', to = 'y', shadow = T) +\n  labs(title = \"Paths from x into y\",\n       subtitle = \"Of 4 possible paths, only one of them is a causal path\") \n\n\n\n\nWe have 4 paths but only 1 of them is causal: only one is a direct descendant of \\(X\\). Therefore, if we want to estimate the causal effect, we must make sure that all the other non-causal paths (the backdoor paths that have arrows into X) are blocked. Luckily, we already defined a graphical algorithm to find out which variables we must adjust for in order to block some path: the d-separation criterion.\nTherefore, we arrive at the following four possible adjustment sets that guarantee that all non-causal paths will be blocked. By conditioning on them, we will correctly estimate the causal effect:\n\nggdag_adjustment_set(example, exposure = 'x', outcome = 'y', shadow = T)\n\n\n\n\nLet’s see why each of the four available adjustment set works:\n\nBy conditioning on \\(z\\), we open a collider between \\(b\\) and \\(c\\). However, the causal effect of \\(c\\) on \\(y\\) does not get picked up by \\(x\\) because we are blocking the path by conditioning on \\(a\\).\nBy conditioning on \\(z\\), we open a collider between \\(b\\) and \\(c\\). However, given that we adjust for \\(b\\), the effect of \\(c\\) on \\(y\\) won’t be picked up by \\(x\\).\nBy conditioning on \\(z\\), we open a collider between \\(b\\) and \\(c\\). However, given that we adjust for \\(c\\), the effect of \\(c\\) on \\(y\\) won’t be picked up by \\(x\\).\nBy conditioning on \\(z\\), we open a collider between \\(b\\) and \\(c\\). However, by adjusting for \\(d\\), we block the effect of \\(c\\) on \\(y\\) and thus this effect won’t get picked up by \\(x\\)."
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#the-backdoor-criterion",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#the-backdoor-criterion",
    "title": "Causality: To adjust or not to adjust",
    "section": "The Backdoor Criterion",
    "text": "The Backdoor Criterion\nGiven that we cannot directly adjust by the parents of \\(x\\), what variables should we condition on to obtain the correct effect? The question boils down to finding a set of variables that satisfy the backdoor criterion:\nGiven an ordered pair of variables ( (X, Y) ) in a directed acyclic graph ( G, ) a set of variables ( Z ) satisfies the backdoor criterion relative to ( (X, Y) ) if no node in ( ) is a descendant of ( X, ) and ( ) blocks every path between ( X ) and ( Y ) that contains an arrow into X. If a set of variables ( Z ) satisfies the backdoor criterion for ( X ) and ( Y, ) then the causal effect of ( X ) on ( Y ) is given by the formula [ P(Y=y d o(X=x))=_{z} P(Y=y X=x, Z=z) P(Z=z) ]\nNote that the parents of \\(X\\) always satisfy the backdoor criterion. Notice also, quite importantly, that the criterion simultaneously says which variables we should use as control variables and which ones we shouldn’t. Indeed, by adjusting for the wrong variable we may end up opening a non-causal path into \\(x\\) and thus introducing confounding bias into our estimates."
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#correclty-adjusting",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#correclty-adjusting",
    "title": "Causality: To adjust or not to adjust",
    "section": "Correclty adjusting",
    "text": "Correclty adjusting\nLet’s keep working with our current Graphical Model.\n\nggdag_exogenous(example)\n\n\n\n\nWe can simulate data coming from such a model thus :\n\nn <- 500\nb <- rnorm(n)\nc <- rnorm(n)\nz <- 2*b - 3*c + rnorm(n)\na <- -b + rnorm(n)\nx <- 2*a -2*z + rnorm(n)\nd <- 1.5*c + rnorm(n)\nw <- x*2 + rnorm(n)\ny <- 5*w -2*d- 5*z + rnorm(n)\n\nThus, the causal effect of one unit of \\(x\\) on \\(y\\) is always \\(10 = 5 \\times 2\\). Notice that we have four possible adjustment sets to estimate these causal query and all of these possibilities should yield the same answer. Therefore, this constraint becomes yet another testable implication of our model.\nLet’s fit a Gaussian Linear regression with each of the possible adjustment sets and compare the results. But first let’s fit a naive model with no adjustment whatsoever.\n\ndata <- data.frame(b, c, z, a, x, d, w, y)\nmodel_one <- stan_glm(y ~ x, data = data, refresh = 0) \n\nmodel_one %>% \n  spread_draws(x) %>% \n  ggplot(aes(x)) +\n  stat_halfeye(alpha = 0.6) + \n  annotate(\"text\", x = 10.2, y = 0.7, label = \"True causal effect\", color = \"red\", \n           family = theme_get()$text[[\"family\"]]) +\n  hrbrthemes::theme_ipsum_rc(grid = \"y\")  +\n  geom_vline(aes(xintercept = 10), linetype = 2, color = \"red\") +\n  labs(title = \"Causal Inference from Model y ~ x\",\n       subtitle = \"This estimation overestimates the causal effect due to confounding bias\")\n\n\n\n\nAs expected, our estimations are off. Following the backdoor criterion, the following estimations should be unbiased:\n\nmodel_two <- stan_glm(y ~ x + a +z, data = data, refresh = 0)\n\n\n\n\n\n\nMuch better! Let’s check the other models:\n\nmodel_three <- stan_glm(y ~ x + b +z, data = data, refresh = 0)\n\n\n\n\n\n\nAlso a correct estimation! Let’s fit the model where we adjust by \\(c\\):\n\nmodel_four <- stan_glm(y ~ x + c +z, data = data, refresh = 0)\n\n\n\n\n\n\nFinally, the model where we condition on \\(z, d\\):\n\nmodel_five <- stan_glm(y ~ x + d +z, data = data, refresh = 0)"
  },
  {
    "objectID": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#incorreclty-adjusting",
    "href": "posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html#incorreclty-adjusting",
    "title": "Causality: To adjust or not to adjust",
    "section": "Incorreclty adjusting",
    "text": "Incorreclty adjusting\nAdjusting on a variable can sometimes open backdoor paths that would had otherwise remained closed. Take the following example:\n\nhurting <- dagify(Y ~ X + u1,\n                  X ~ u2,\n                  L ~ u1 + u2)\nggdag(hurting) +\n  labs(title = \"If we don't condition on L, the only backdoor path between X and Y is closed\")\n\n\n\n\nNotice that there is only one backdoor path for \\(X\\). However, it contains a collider: \\(L\\). Therefore, unless we condition on \\(L\\), the backdoor path will remain closed. In this case, we must not adjust for any of the variables, as there is no confounding bias: \\(P(Y | do (X = x)) = P(Y| X = x)\\).\nIf we are naive, and we consider that controlling for observables is always a step in the right direction toward causal inference, we will end up with the wrong inference. By condition on \\(L\\), we open up a collider that will create a relationship between \\(u_1\\) and \\(u_2\\); therefore, \\(X\\) will pick up the causal effect of \\(u_1\\) on \\(y\\).\n\nggdag_adjust(hurting, var = \"L\") +  \n  scale_color_brewer(type = \"qual\", palette = \"Set2\")  +\n  labs(title = \"Conditioning on a collider opens a backdoor path between X and Y\")\n\n\n\n\nWe can easily confirm this insight by simulating some data and fitting models.\n\nu1 <- rnorm(n)\nu2 <- rnorm(n)\nx <- 2*u2 + rnorm(n)\ny <- 2*x -5*u1 + rnorm(n)\nl <- u1 + u2 + rnorm(n)\ndata <- data.frame(u1, u2, x, y, l)\n\nThus, the correct causal effect of one unit of \\(x\\) on \\(y\\) is always 2.\nFirst, let’s fit the correct model where we don’t condition on any of the other variables:\n\nmodel_correct <- stan_glm(y ~ x, data = data, refresh = 0)\n\n\n\n\n\n\nHowever, if we adjust for \\(L\\), our estimations will be biased:\n\nmodel_incorrect <- stan_glm(y ~ x + l, data = data, refresh =0)"
  },
  {
    "objectID": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html",
    "href": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html",
    "title": "Causality: Regret? Look at Effect of Treatment on the Treated",
    "section": "",
    "text": "Regret about our actions stems from a counterfactual question: What if I had acted differently?. Therefore, to answer such question, we need a more elaborate language than the one we need to answer prediction or intervention questions. Why? Because we need to compare what happened with what would had happened if we had acted differently. We need to compute the Effect of Treatment on the Treated (ETT).\nTo compute the ETT, we need to formulate a Structural Causal Model and leverage the invariant qualities across the observed world and the hypothetical world: the unobserved background variables. Indeed, the definition of the Effect of Treatment on the Treated (ETT) is defined for a binary treatment thus:\n\\[\nE[Y_1 - Y_0 | X = 1] \\\\\n= \\sum_u [P(y_1 | x_1, u) - P(y_1 | x_0, u)] P(u |x_1)\n\\]\nOf course, we don’t have access to the background variables. In this post, we will learn to answer two questions: when is the ETT identifiable? And if so, can we give an estimator for such counterfactual in terms of non-experimental data?\nWe will first study a binary treatment and answer both questions. Then, we will tackle the more general case of any treatment."
  },
  {
    "objectID": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html#a-motivating-binary-example",
    "href": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html#a-motivating-binary-example",
    "title": "Causality: Regret? Look at Effect of Treatment on the Treated",
    "section": "A Motivating binary example",
    "text": "A Motivating binary example\nThe following example is taken from Pearl’s (et alter) book Causal Inference in Statistics: A primer.\n\nImagine an average adolescent: Joe. He has started smoking ever since he began High School. Should he regret his decision? That is, given that he has started smoking, has he significantly increased his chances of suffering from lung cancer compared to his chances had he never begun in the first place?\n\nTherefore, what Joe cares about is the Effect of Treatment on the Treated: \\(E[Cancer_1 - Cancer_0 | Smoking = 1]\\). If ETT > 0, having smoked has caused a higher chance of lung cancer for Joe compared to the hypothetical world where he had never smoked in the first place. How can we calculate the ETT?\n\\[\n\\begin{aligned}\nE T T &=E\\left[Y_{x}-Y_{x^{\\prime}} \\mid X=x\\right] \\\\\n&=E[Y_x \\mid X=x]-E\\left[Y_{x^{\\prime}} \\mid X=x\\right]\n\\end{aligned}\n\\]\nThe challenge, thus, is to estimate the counterfactual expression \\(E\\left[Y_{x^{\\prime}} \\mid X=x\\right]\\)\n\nExpressing ETT in terms of observational data and experimental data\nOur treatment is binary. Therefore, let’s begin by using the law of total probability thus to write \\(E[Y_x]\\)\n\\[\nE\\left[Y_{x}\\right]=E\\left[Y_{x} \\mid X=x\\right] P(X=x)+E\\left[Y_{x} \\mid X=x^{\\prime}\\right] P\\left(X=x^{\\prime}\\right)\n\\]\nWe will use the consistency axiom: \\(E[Y_x | X = x] = E[Y | X = x]\\); that is, a counterfactual predicated on an actual observation is no counterfactual.\nTherefore, we can re-write the above expression thus:\n\\[\nE\\left[Y_{x}\\right]=E[Y \\mid X=x] P(X=x)+E\\left[Y_{x} \\mid X=x^{\\prime}\\right] P\\left(X=x^{\\prime}\\right)\n\\]\nIn the above expression there’s only one term that cannot be computed using observational data, \\(E\\left[Y_{x} \\mid X=x^{\\prime}\\right]\\): the same term that causes trouble in our ETT estimation. Let’s solve for it:\n\\[\nE\\left[Y_{x} \\mid X=x^{\\prime}\\right]=\\frac{E\\left[Y_{x'}\\right]-E[Y \\mid X=x] P(X=x)}{P\\left(X=x^{\\prime}\\right)} \\\\\nE\\left[Y_{x} \\mid X=x^{\\prime}\\right]=\\frac{E\\left[Y | do(X = X')\\right]-E[Y \\mid X=x] P(X=x)}{P\\left(X=x^{\\prime}\\right)}\n\\]\nBy plugging-in this term, we can express our ETT with terms that can be computed with a mix of observational and experimental data.\n\\[\n\\begin{aligned}\nE T T &=E\\left[Y_{x}-Y_{x^{\\prime}} \\mid X=x\\right] \\\\\n&=E[Y \\mid X=x]-E\\left[Y_{x^{\\prime}} \\mid X=x\\right] \\\\\n&=E[Y \\mid X=x]-\\frac{E\\left[Y \\mid d o\\left(X=x^{\\prime}\\right)\\right]-E\\left[Y \\mid X=x^{\\prime}\\right] P\\left(X=x^{\\prime}\\right)}{P(X=x)}\n\\end{aligned}\n\\]\nTherefore, if the treatment is binary, whenever the causal effect of \\(X\\) can be identified, the ETT can also be identified.\n\n\nGoing back to Joe\nLet’s go back to our motivating example. We can express the ETT using the above derivation:\n\\[\n\\begin{aligned}\nE T T &=E\\left[Y_{1}-Y_{0} \\mid X=1\\right] \\\\\n&=E[Y \\mid X=1]-E\\left[Y_{0} \\mid X=1\\right] \\\\\n&=E[Y \\mid X=1]-\\frac{E[Y \\mid d o(X=0)]-E[Y \\mid X=0] P(X=0)}{P(X=1)}\n\\end{aligned}\n\\]\nThus, we can estimate the ETT with only observational data if we can estimate \\(E[Y \\mid d o(X=0)]\\) with observational data. Given that we know that we cannot estimate causal effects without making causal assumptions, let’s formulate ours.\nLet’s say that our causal DAG for the effects of Smoking on Cancer is the following:\n\nexample <- dagify(x ~ u,\n                  m ~ x,\n                  y ~ u + m,\n                  labels = c(\"x\" = \"Smoking\",\n                             \"y\" = \"Cancer\",\n                             \"m\" = \"Tar\",\n                             \"u\" = \"Genotype\"),\n                  latent = \"u\",\n                  exposure = \"x\",\n                  outcome = \"y\")\n\n\n\n\n\n\nTherefore, we can use the front-door formula to estimate the causal effect of smoking: \\(E\\left[Y \\mid d o\\left(X=x^{\\prime}\\right)\\right]\\).\nSuppose, then, that we collect the following data:\n\nThen, using the front-door criterion, the causal effect \\(E[Y \\mid d o(X=0)]\\) is:\n\\[\n\\begin{aligned}\nE[Y \\mid d o(X=0)]=& \\sum_{z} P(Z=z \\mid X=0) \\sum_{x^{\\prime}} P\\left(Y=1 \\mid X=x^{\\prime}, Z=z\\right) P\\left(X=x^{\\prime}\\right) \\\\\n=& P(Z=1 \\mid X=0)[P(Y=1 \\mid X=1, Z=1) P(X=1)\\\\\n&+P(Y=1 \\mid X=0, Z=1) P(X=0)] \\\\\n&+P(Z=0 \\mid X=0)[P(Y=1 \\mid X=1, Z=0) P(X=1)\\\\\n&+P(Y=1 \\mid X=0, Z=0) P(X=0)] \\\\\n=& 20 / 400 *[0.15 * 0.5+0.95 * 0.5]+380 / 400 *[0.1 * 0.5+0.9 * 0.5] \\\\\n=& 0.5025\n\\end{aligned}\n\\]\nFinally, we can calculate the ETT for Joe:\n[\n\\[\\begin{aligned}\nE T T &=E\\left[Y_{1}-Y_{0} \\mid X=1\\right] \\\\\n&=E[Y \\mid X=1]-E\\left[Y_{0} \\mid X=1\\right] \\\\\n&=E[Y \\mid X=1]-\\frac{E[Y \\mid d o(X=0)]-E[Y \\mid X=0] P(X=0)}{P(X=1)} \\\\\n&=0.15-\\frac{0.5025-0.9025 * 0.5}{0.5} \\\\\n&=0.0475>0\n\\end{aligned}\\]\n]\nTherefore, given that \\(ETT>0\\), by smoking Joe has increased his chances of suffering from Cancer. Thus, he should feel regret: the causal effect smoking has had in his life has been to increase his chances of suffering from cancer, relative to those chances in the hypothetical world where he never smoked in the first place."
  },
  {
    "objectID": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html#the-more-general-case",
    "href": "posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html#the-more-general-case",
    "title": "Causality: Regret? Look at Effect of Treatment on the Treated",
    "section": "The more general case",
    "text": "The more general case\nLet’s say that our treatment is discrete, but not binary. Is the Effect of Treatment on the Treated (ETT) identifiable? Pearl and Shipster have given an answer to this question using C-components\n\nIdentifiability using C-components\nRemember, two variables are assigned to the same c-component iff they are connected by a bi-directed path. The c-components themselves induce a factorization of the joint probability distribution in terms of c-factors: post-intervention distribution of the variables in the respective c-component under an intervention on all the other variables.\nJust as before the causal effect was identified when \\(X\\) and its children are in different C-components (i.e., there’s no bi-directed path between \\(X\\) and its children that are also ancestors of \\(Y\\)), the necessary counterfactual expression to compute the ETT, \\(P(Y_x = y|x')\\), is identifiable if and only if \\(X\\) and its children are in different C-components.\nIndeed, whereas before (when we were trying to estimate the causal effect) we summed out \\(x\\) from the c-factor, we now replace \\(x\\) by \\(x'\\) from the c-factor and divide by \\(P(x')\\). Then, we take the decomposition induced by the c-factors and marginalize and condition on the appropriate variables to get the variable of interest.\nThat is, the same test is a sufficient test for causal effects identifiability and both a necessary and sufficient test for ETT identifiability.\n\n\nConfirming our former result\nLet’s take our former example of the causal model of Smoking on Cancer. This time, we will use bi-directed paths to show that there’s an unobserved confounder:\n\nexample <- dagify(x ~~ y,\n                  m ~ x,\n                  y ~ m)\n\n\ntidy_dagitty(example, layout = \"nicely\", seed = 2) %>% \n  node_descendants(\"x\") %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, edge_linetype = linetype, color = descendant)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"))) +\n  geom_dag_point() + \n  geom_dag_text(col = \"white\") +\n  labs(title = \"The ETT is identifiable!\",\n       subtitle = \"Because there's no bi-directed path between x and m\")\n\n\n\n\nSince \\(X\\) has no bi-directed path to its child \\(m\\), the counterfactual query \\(P(Y_x = y | x')\\) is identifiable. Thus, the ETT is identifiable; confirming what we have done until now.\nHowever, if it were not binary, we could derive an estimator for the ETT using the induced factorization by the c-components. First, we replace with \\(x'\\) in the c-component where \\(x\\) is:\n\\[\nP(x, y | do(m)) = P(y|m, x') P(x')\n\\]\nWhereas the other c-component is\n\\[\nP(m| do(y, x)) = P(m| do(x)) = P(m|x)\n\\]\nTherefore, the conditional distribution on \\(x'\\)\nConditioning on \\(x'\\) and marginalizing \\(m\\):\n\\[\nP(y_x | x') = (\\sum_z P(y| z, x') P(x') P(z | x))/(P(x'))\n\\]\nThat is, we replace \\(x\\) with \\(x'\\) in the c-component where \\(x\\) is, we condition on \\(x'\\) by dividing the joint density and marginalize \\(m\\). Thus, we derive the estimator for the ETT in this model by using the c-factors.\n\n\nNot identifiable\nNow let’s work with an example where the causal effect is identifiable, yet the counterfactual query \\(P(Y_x = y | x')\\) is not.\n\nexample_not <- dagify(s ~~ y,\n                      x ~~ s, \n                      x ~ z,\n                      z ~ s,\n                      y ~ x\n                      )\n\n\ntidy_dagitty(example_not, layout = \"nicely\", seed = 2) %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>% \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend, edge_linetype = linetype)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"))) +\n  geom_dag_point() + \n  geom_dag_text(col = \"white\") +\n  labs(title = \"The ETT is not identifiable\",\n       subtitle = \"X is connected by a bi-directed path with S\")"
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html",
    "title": "Causality: Testing Identifiability",
    "section": "",
    "text": "We’ve defined causal effects as an interventional distribution and posit two identification strategies to estimate them: the back-door and the front-door criteria. However, we cannot always use these criteria; sometimes, we cannot measure the necessary variables to use either of them.\nMore generally, given a causal model and some incomplete set of measurements, when is the causal effect of interest identifiable? In this blog post, we will develop a graphical criterion to answer this question by exploiting the concept of c-components. Finally, we will put the criterion in practice with multiple examples."
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#all-you-can-estimate-markov-models",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#all-you-can-estimate-markov-models",
    "title": "Causality: Testing Identifiability",
    "section": "All you can estimate: Markov Models",
    "text": "All you can estimate: Markov Models\nWhen we can obtain measurements of all the variables in the causal model, we say that our causal model is Markovian. In this case, the adjustment formula is our identification strategy: any causal effect \\(X \\rightarrow Y\\) is identifiable if we have measurements of the parents of \\(X\\), \\(Pa(X)\\).\n\\[\nP(Y=y|\\text{do}(X=x)) = \\sum_{z} P(Y=y | X=x, Pa(X)=z) P(Pa(X)=z)\n\\] What happens when you cannot observe the parents of \\(x\\)?"
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#semi-markovian-models",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#semi-markovian-models",
    "title": "Causality: Testing Identifiability",
    "section": "Semi-Markovian Models",
    "text": "Semi-Markovian Models\nIf a variable that is unobserved has two descendants in the graph, the Markovian property is violated. We may or may not be able to use the adjustment formula. For example, if one of the parents of \\(X\\) is unobserved, we cannot use it as our identification strategy. Even then, we may be able to use either the back-door or the front-door criteria.\nLet’s start studying the problem with the following example. In this case, a bi-directed dashed edge represents a hidden common cause between the variables. We refer to all unmeasured variables by \\(U\\), all of the observed variables by \\(V\\)\n\nexample <- dagify(x ~~ z2,\n                  z1 ~ x,\n                  z1 ~ z2,\n                  z1 ~~ y,\n                  y ~ x +z1 +z2)\n\n\n\n\n\n\nTo identify the causal effect of \\(X\\) on all of the other observed variables \\(v\\), we must be able to estimate the post intervention probabilities, \\(P(v | do(X))\\), from the pre-intervention probabilities that we can observe.\nTo begin to study this question, we must remember that our causal model is simultaneously a probabilistic model. In particular, they induce a decomposition of the joint probability distribution because each variable is independent of all its non-descendants given its direct parents in the graph. However, when our model contains unobserved confounders, we must marginalize them in order to obtain the joint probability distribution of the observed variables:\n\\[\nP(v) = \\sum_u \\prod_i P(v_i| pa_i, u^i) P(u)\n\\]\nIn this case, the decomposition of the observables is given by:\n\\[\n\\begin{aligned}\nP(v)=& \\sum_{u_{1}} P\\left(x \\mid u_{1}\\right) P\\left(z_{2} \\mid z_{1}, u_{1}\\right) P\\left(u_{1}\\right) \\\\\n& \\quad \\sum_{u_2} P\\left(z_{1} \\mid x, u_{2}\\right) P\\left(y \\mid x, z_{1}, z_{2}, u_{2}\\right) P\\left(u_{2}\\right)\n\\end{aligned}\n\\]\nGiven that \\(P(v|do(X=x))\\) represents an intervention, it can be represented by truncating the above expression such that we do not calculate the probability of \\(X\\):\n\\[\n\\begin{aligned}\nP(v | do(X))=& \\sum_{u_{1}} P\\left(z_{2} \\mid z_{1}, u_{1}\\right) P\\left(u_{1}\\right) \\\\\n& \\cdot \\sum_{u_2} P\\left(z_{1} \\mid x, u_{2}\\right) P\\left(y \\mid x, z_{1}, z_{2}, u_{2}\\right) P\\left(u_{2}\\right)\n\\end{aligned}\n\\]\nCan we express \\(P(v | do(X))\\) in terms of observed variables? First, we must take a brief de-tour by confounded components."
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#confounded-components",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#confounded-components",
    "title": "Causality: Testing Identifiability",
    "section": "Confounded components",
    "text": "Confounded components\nNotice that in both expressions the unobserved confounders partition into disjoint groups the observed variables: two variables are assigned to the same group if and only if they are connected by a bi-directed path. Each group, \\(S_k\\), is called a confounded component (c-component). In this case, we have two c-components that induce two factorizations (c-factors):\n\\[\nQ_{1}=\\sum_{u_{1}} P\\left(x \\mid u_{1}\\right) P\\left(z_{2} \\mid z_{1}, u_{1}\\right) P\\left(u_{1}\\right) \\\\\nQ_{2}=\\sum_{u_2} P\\left(z_{1} \\mid x, u_{2}\\right) P\\left(y \\mid x, z_{1}, z_{2}, u_{2}\\right) P\\left(u_{2}\\right)\n\\]\nNotice that each (c-factor) \\(Q_k\\) can be interpreted as the post-intervention distribution of the variables in \\(S_k\\) under an intervention on all the other variables. Observe that we can express the joint observed distribution as a product of the c-factors:\n\\[\nP(v) = Q_1 \\cdot Q_2\n\\] We can in turn define \\(P(v | do(X))\\) in terms of \\(Q_1, Q_2\\) if we marginalize \\(P(x| u_1)\\) out of \\(Q_1\\):\n\\[\nP(v | do(X)) = Q_2 \\sum_x Q_1 = Q_2 \\cdot Q_1^x\n\\]\nTherefore, \\(P(v | do(X))\\) will be identifiable if: (a) we can compute the post-intervention probabilities \\(Q_1, Q_2\\) in terms of pre-intervention probabilities; and (b) we can marginalize \\(x\\) out of the estimated \\(Q_1\\) with pre-intervention probabilities to compute \\(Q_1^x\\).\nIn fact, Tian and Pearl (PDF) show that each c-factor is always identifiable. Therefore, the only condition to compute \\(P(v | do(X))\\) if and only if \\(Q_1^x\\) is identifiable, too. In this case:\n\\[\nQ_1 = P(z_2 | x, z_1) P(x)\n\\] Thus, we can marginalize \\(x\\) out of \\(Q_1\\) by summing over the values of \\(X\\).\n\\[\nQ_1^x = \\sum_{x'}  P(z_2 | x', z_1) P(x')\n\\]\nFinally, our estimate for \\(P(v | do(X))\\) is the following:\n\\[\nP(v | do(X)) = \\frac{P(v)}{Q_1} \\sum_{x^{\\prime}} P\\left(z_{2} \\mid x^{\\prime}, z_{1}\\right) P\\left(x^{\\prime}\\right)\n\\] Let’s generalize from this example."
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#a-general-criteria-for-identification",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#a-general-criteria-for-identification",
    "title": "Causality: Testing Identifiability",
    "section": "A general criteria for identification",
    "text": "A general criteria for identification\nFirst, notice that for any graph with bi-directed paths, we can decompose the joint probability distribution by using the partition into c-components and their respective c-factors:\n\\[\nP(v)=\\prod_{j=1}^{k} Q_{j}  \n\\]\nAlso notice that the truncated distribution generated by intervening on \\(x\\) can be represented with c-factors thus:\n\\[\nP(v | do(X=x))=Q_{x}^{x} \\prod_{i} Q_{i}\n\\] Where \\(Q_{x}^{x}\\) is the c-factor where \\(x\\) is located once we remove \\(x\\) from the factorization. Therefore, \\(P(v | do(X=x)\\) is identifiable if \\(Q_{x}^{x}\\) is identifiable, too.\nIn fact, Tian and Pearl (PDF) show that \\(Q_{x}^{x}\\) is identifiable if and only if there is no bi-directed path (a path with only bi-directed edges) connecting \\(X\\) to any of its descendants. Therefore, we arrive at the following test to decide whether \\(P(v | do(X=x)\\) is identifiable:\n\n\\(P(v | do(X=x)\\) is identifiable if and only if there is no bi-directed path connecting \\(X\\) to any of its descendants.\n\nNotice that if \\(P(v | do(X=x)\\) is identifiable so it is \\(P(Y | do(X=x))\\). Therefore, our criterion is sufficient to determine whether \\(P(v | do(X=x))\\) is non-identifiable. Given that we are only interested in the causal effect on a single variable \\(Y\\), we can simplify the problem by only considering the subgraph of all the variables that are ancestors of \\(Y\\)\n\nIntuition\nWhat is the intuition of our identifiability test? The key to identifiability lies not in blocking back-door paths between X and Y but, rather, in blocking back-door paths between \\(X\\) and any of its descendants that is an ancestor of \\(Y\\). Thus, by blocking these paths, we can ascertain which part of the association we observe is spurious and which genuinely causative.\nLet’s put this intuition into practice with the following examples."
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#examples",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#examples",
    "title": "Causality: Testing Identifiability",
    "section": "Examples",
    "text": "Examples\n\nFirst Example\nLet’s start with our former example. Why was it identifiable? All the other variables are ancestors of \\(Y\\). Therefore, we cannot simplify the problem. We must look, then, if there is a bi-directed path between \\(X\\) and its children:\n\ntidy_dagitty(example, layout = \"nicely\", seed = 2) %>% \n  node_descendants(\"x\") %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, color = descendant)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"), edge_linetype = linetype)) +\n  geom_dag_point() +\n  geom_dag_text(col = \"white\") +\n  labs(title = \"The causal effect of X is identifiable\",\n       subtitle = \"There's no bi-directed path between X and its descendats\")\n\n\n\n\nGiven that there is no bi-directed path between \\(X\\) and its descendants, the causal effect of \\(X\\) is identifiable.\n\n\nSecond Example\nLet’s take another example:\n\nnon_identifiable_example <- dagify(x ~ z,\n                                   x ~~~ z,\n                                   x ~~ y,\n                                   w ~ x,\n                                   w ~~ z,\n                                   y ~ w,\n                                   y ~~ z)\n\n\n\n\n\n\nTo find out whether the effect is identifiable, we look for a bi-directed path between \\(X\\) and its descendants. If there is none, the effect is identifiable.\n\ntidy_dagitty(non_identifiable_example, layout = \"nicely\", seed = 2) %>% \n  node_descendants(\"x\") %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, color = descendant)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"), edge_linetype = linetype)) +\n  geom_dag_point() +\n  geom_dag_text(col = \"white\")\n\n\n\n\nNotice that there is a bi-directed path from \\(X\\) to \\(W\\) (which is one of its descendants) through \\(Z\\). Then, according to our criterion, the effect is non-identifiable.\n\n\nThird example\nFinally, let’s end with the following example:\n\nthird_example <- dagify(z1 ~ x + z2,\n                        x ~ z2,\n                        x ~~ z2,\n                        x ~~ y, \n                        z2 ~~ y,\n                        z3 ~ z2,\n                        x ~~ z3,\n                        y ~ z1 + z3)\n\n\n\n\n\n\nAs in the previous examples, we look for a bi-directed path between \\(X\\) and its descendants.\n\ntidy_dagitty(third_example, layout = \"nicely\", seed = 2) %>% \n  node_descendants(\"x\") %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, color = descendant)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"), edge_linetype = linetype)) +\n  geom_dag_point() +\n  geom_dag_text(col = \"white\")\n\n\n\n\nNotice that \\(X\\) has no bi-directed path with its only descendant that is not \\(Y\\). Therefore, the causal effect is identifiable."
  },
  {
    "objectID": "posts/causality/2020-07-31-causality-testing-identifiability.html#what-about-a-necessary-condition-for-identifiability",
    "href": "posts/causality/2020-07-31-causality-testing-identifiability.html#what-about-a-necessary-condition-for-identifiability",
    "title": "Causality: Testing Identifiability",
    "section": "What about a necessary condition for identifiability?",
    "text": "What about a necessary condition for identifiability?\nOur stated test is sufficient but not necessary for identifiability. Is there a necessary and sufficient condition? Yes, there is such an algorithm by Pearl and Shipster (PDF). It extends the ideas we’ve seen in this post and returns an estimator of the causal effect in question in terms of pre-intervention probabilities. It is complete and equivalent to Pearl’s do-calculus.\nIn R, the causaleffect package has an implementation of this algorithm. It can be used thus for our first example:\n\n\n\n\n\n\nfirst_example_igraph <- graph.formula(x -+ z_2,\n                                      z_2 -+ x, \n                                      x -+ z_1,\n                                      z_2 -+ z_1,\n                                      z_1 -+ y,\n                                      y -+ z_1,\n                                      x -+ y,\n                                      z_1 -+ y,\n                                      z_2 -+ y, simplify = FALSE) %>% \n                      set.edge.attribute(\"description\", index = c(1, 2, 5, 6), \"U\")\nce <- causal.effect(y = \"y\", x = \"x\", z = NULL, G = first_example_igraph, expr = TRUE)\nplot(TeX(ce), cex = 3)"
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html",
    "title": "Causality: The front-door criterion",
    "section": "",
    "text": "In a past blogpost, I’ve explore the backdoor criterion: a simple graphical algorithm, we can define which variables we must include in our analysis in order to cancel out all the information coming from different causal relationships than the one we are interested. However, these variables are not always measured. What else can we do?\nIn this blogpost, I’ll explore the front-door criterion: i) an intuitive proof of why it works; (ii) how to estimate it; (iii) what are its fundamental assumptions; finally, (iv) an experiment with monte-carlo samples. Whereas the back-door criterion blocks all the non-causal information that \\(X\\) could possibly pick up, the front-door exploits the outgoing information from \\(X\\) to derive a causal estimator."
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#the-limits-of-the-back-door-a-quick-example",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#the-limits-of-the-back-door-a-quick-example",
    "title": "Causality: The front-door criterion",
    "section": "The limits of the back-door: a quick example",
    "text": "The limits of the back-door: a quick example\nLet’s assume the following DAG, which is a darling of Pearl’s work. Does smoking cause cancer?\n\nexample <- dagify(x ~ u,\n                  m ~ x,\n                  y ~ u + m,\n                  labels = c(\"x\" = \"Smoking\",\n                             \"y\" = \"Cancer\",\n                             \"m\" = \"Tar\",\n                             \"u\" = \"Genotype\"),\n                  latent = \"u\",\n                  exposure = \"x\",\n                  outcome = \"y\")\n\n\n\n\n\n\nGiven that we cannot measure genotype, we cannot use the back-door criterion to stop \\(Smoking\\) from picking up the causal effect of \\(Genotype\\). Therefore, one can not use the back-door criterion to ascertain which portion of the observed association between smoking and cancer is spurious (because it is attributable to their common cause, Genotype) and what portion is genuinely causative."
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#consecutive-applications-of-the-back-door-criterion",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#consecutive-applications-of-the-back-door-criterion",
    "title": "Causality: The front-door criterion",
    "section": "Consecutive applications of the back-door criterion",
    "text": "Consecutive applications of the back-door criterion\nHowever, we notice that we can use the back-door criterion to estimate two partial effects: \\(X \\rightarrow M\\) and \\(M \\rightarrow Y\\). By chaining these two partial effects, we can obtain the overall effect \\(X \\rightarrow Y\\).\nThe intuition for the chaining is thus: intervening on the levels of tar in the lungs lead to different probabilities of cancer: \\(P(Y = y | \\text{do(M = m)})\\). However, the levels of tar are themselves determined by how much someone smokes: \\(P(M= m| \\text{do(X = x)})\\). Therefore, by intervening on smoking to determine the levels of tar we can estimate the causal effect of smoking.\nWe intervene on smoking and check the respective effect for each value of tar:\n\\[\nP(Y \\mid d o(X))=\\sum_{M} P(Y \\mid M, d o(X)) \\times P(M \\mid d o(X))\n\\]\nBecause smoking blocks all the back-door paths from tar into cancer, we can replace the conditioning expression by an intervention expression in the first term.\\(P(Y \\mid M, d o(X))=P(Y \\mid d o(M), d o(X))\\). Given that intervening on smoking, once we have intervened on tar has no effect on cancer, we can also write \\(P(Y \\mid M, d o(X))=P(Y \\mid d o(M), d o(X))=P(Y \\mid d o(M))\\).\nGiven that smoking blocks all backdoor paths into tar, we can estimate \\(P(Y \\mid d o(M))\\) using the back-door adjustment:\n\\[\nP(Y \\mid d o(M))=\\sum_{X} P(Y \\mid X, M) \\times P(X)\n\\]\nTherefore, we can re-write \\(P(Y \\mid d o(X))\\) thus:\n\\[\nP(Y \\mid d o(X)) = \\sum_{M} P(M | do(X)) \\sum_{X'} P(Y \\mid X', M) \\times P(X')\n\\]\nConsidering that there are no backdoor paths from smoking to tar, we can write \\(P(M | do(X)) = P(M | X)\\). Therefore, we can re-write our entire expression for \\(P(Y \\mid d o(X))\\) in terms of pre-intervention probabilities:\n\\[\nP(Y \\mid d o(X)) = \\sum_{M} P(M | X) \\sum_{X'} P(Y \\mid X', M) \\times P(X')\n\\]\nThis is the front-door formula."
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#empirical-estimation",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#empirical-estimation",
    "title": "Causality: The front-door criterion",
    "section": "Empirical estimation",
    "text": "Empirical estimation\nConceptually,\n\nthe FDC [Front-door Criterion] approach works by first estimating the effect of X on M, and then estimating the effect of M on Y holding X constant. Both of these effects are unbiased because nothing confounds the effect of X on M and X blocks the only back-door path between M on Y. Multiplying these effects by one another yields the FDC estimand.\n\nTherefore, in a regression setting we can estimate the causal effect using the Average Treatment Effect (ATE) via the FDC thus. Formulate two linear regressions:\n[ M_{i}=+X_{i}+{i} ] and [ Y{i}=+M_{i}+X_{i}+v_{i} ]\nOur estimate of the ATE is given by:\n\\[\nATE = E[Y|do(X)] = \\delta \\times \\gamma\n\\]"
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#when-can-we-use-the-front-door-criterion",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#when-can-we-use-the-front-door-criterion",
    "title": "Causality: The front-door criterion",
    "section": "When can we use the Front-Door criterion?",
    "text": "When can we use the Front-Door criterion?\nWe’ve given an intuitive proof of the Front-door criterion and given an empirical estimation technique. But what exactly have we presupposed that allowed us to do all of this? In other words, what are the fundamental assumptions behind the criterion?\nA set of variables ( Z ) is said to satisfy the front-door criterion relative to an ordered pair of variables ( (X, Y) ) if:\n\nZ intercepts all directed paths from ( X ) to ( Y ).\nThere is no backdoor path from ( X ) to ( Z )\nAll backdoor paths from Z to ( Y ) are blocked by X.\n\nWhen these conditions are met, we can use the Front-Door criterion to estimate the causal effect of \\(X\\)."
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#a-monte-carlo-experiment",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#a-monte-carlo-experiment",
    "title": "Causality: The front-door criterion",
    "section": "A Monte-Carlo experiment",
    "text": "A Monte-Carlo experiment\nLet’s work a Monte-Carlo experiment to show the power of the backdoor criterion. Consider the following DAG:\n\ndag <- downloadGraph(\"dagitty.net/m331\")\nggdag(dag) +\n  labs(title = \"We only need to measure W to estimate the effect of X on Y\")\n\n\n\n\nAssume that only \\(X, Y\\), and one additional variable can be measured. Which variable would would allow the identification of the causal effect \\(X\\) on \\(Y\\)? The answer is all in the front-door criterion! We only need to measure \\(W\\) to be able to estimate the effect. Notice that:\n\n\\(W\\) intercepts all the direct paths from \\(X\\) into \\(Y\\).\nThere is no backdoor path from \\(X\\) into \\(W\\).\nAll back-door paths from \\(W\\) into \\(Y\\) are blocked\n\nTherefore, we can use the front-door criterion.\nLet’s use a Monte-Carlo simulation to confirm the answer.\n\nn <- 500\nb <- rnorm(n)\nc <- rnorm(n)\nz <- -2*b +2*c + rnorm(n)\na <- 3*b + rnorm(n)\nd <- -3*c + rnorm(n)\nx <- 4+a +2*z + rnorm(n)\nw <- -2*x + rnorm(n)\ny <- z + 2*w + d + rnorm(n)\n\ndata <- data.frame(b, c, z, a, d, x, w, y)\n\nIn this simulated dataset, the causal effect of unit of \\(X\\) on \\(Y\\) is -4. Let’s recuperate this effect by using the back-door criterion to find out for which variables we must control.\n\nggdag_adjustment_set(dag, outcome = \"Y\", exposure = \"X\")\n\n\n\n\nLet’s take the first adjustment set.\n\nmodel_backdoor <- stan_glm(y ~ x + a + z, data = data, refresh = 0)\nmodel_backdoor %>% \n  spread_draws(x) %>% \n  ggplot(aes(x)) +\n  stat_halfeye(alpha = 0.6) + \n  hrbrthemes::theme_ipsum_rc(grid = \"y\") +\n  geom_vline(aes(xintercept = -4), linetype = 2, color = \"red\") +\n  annotate(\"text\", x = -4.08, y = 0.7, label = \"True causal effect\", color = \"red\", \n           family = theme_get()$text[[\"family\"]]) +\n  labs(title = \"Causal inference from Model y ~ x + a + z\",\n       subtitle = \"We deconfound our estimates by conditioning on a and z\")\n\n\n\n\nGiven our DAG, a testable implication is that we must arrive at the same answer by using the front-door criterion. Remember that it is just a consecutive use of the back-door criterion that translates into two regressions. Therefore, we can use a multi-variable model thus:\n\nmodel_frontdoor <- ulam(\n  alist(\n    c(Y, W) ~ multi_normal(c(muY, muW), Rho, Sigma),\n    muY <- alphaY + delta*W,\n    muW <- alphaW + gamma*X, \n    gq> ate <- gamma * delta, # calculate ate directly in stan\n    c(alphaY, alphaW) ~ normal(0, 0.2),\n    c(gamma, delta) ~ normal(0, 0.5),\n    Rho ~ lkj_corr(2),\n    Sigma ~ exponential(1)\n  ),\n  data = list(Y = data$y, X = data$x, W = data$w), chains = 4, cores = 4, iter = 3000\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 3 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 4 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 13.5 seconds.\nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 finished in 13.6 seconds.\nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 14.2 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 14.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 13.9 seconds.\nTotal execution time: 14.7 seconds.\n\nprecis(model_frontdoor)\n\n              mean          sd        5.5%      94.5%    n_eff     Rhat4\nalphaW  0.01317289 0.055819588 -0.07701513  0.1010920 4808.342 0.9996915\nalphaY  0.10889162 0.131415392 -0.10501164  0.3210728 4608.519 1.0008755\ndelta   2.00223411 0.011824792  1.98296890  2.0211506 5199.217 1.0003623\ngamma  -1.99645002 0.008484629 -2.01018000 -1.9829200 4639.059 0.9998826\nate    -3.99735509 0.028352648 -4.04329220 -3.9517884 5258.228 0.9998829\n\n\nAnd indeed, we arrive at the same answer!"
  },
  {
    "objectID": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#references",
    "href": "posts/causality/2020-07-30-causality-the-front-door-criterion.html#references",
    "title": "Causality: The front-door criterion",
    "section": "References",
    "text": "References\nBesides Chapter 3 of Pearl’s Causality, I found this terrific paper (PDF) by Bellemare and Bloem."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html",
    "title": "Causality: Invariance under Interventions",
    "section": "",
    "text": "In the last post we saw how two causal models can yield the same testable implications and thus cannot be distinguished from data alone. That is, we cannot gain causal understanding from data alone. Does that mean that we cannot ever gain causal understanding? Far from it; it just means that we must have a causal model.\nThus, causal effects cannot be estimated from the data itself without a causal story. In this blogpost, I’ll show how exactly the combination between causal models and observational data can lead us into estimating causal effects. In short, causal effects can be estimated by leveraging the invariant information that the pre-intervention distribution can provide. Doing so, we connect pre-intervention probabilities with the post-intervention probabilities that define the causal effect."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#defining-the-causal-effect-with-the-do-operator",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#defining-the-causal-effect-with-the-do-operator",
    "title": "Causality: Invariance under Interventions",
    "section": "Defining the causal effect with the do-operator",
    "text": "Defining the causal effect with the do-operator\nFundamentally, we cannot gain causal understanding with data because the data we see could have been generated by many a causal models. That is, the associations we see, \\(P(Y | X)\\), can be the result of many interactions; some of them causal and some purely observational. We can say that any statistically meaningful association is the result of a causal relationship somewhere in the system, but not necessarily of the causal effect of interest, \\(X \\rightarrow Y\\).\nTo disentangle this confusion, then, let’s define a causal effect. Following Pearl, we will take an interventionist position and say that a variable \\(X\\) has a causal influence on \\(Y\\) if intervening to change \\(X\\) leads to changes in \\(Y\\). Intervening on \\(X\\) means lifting \\(X\\) from whatever mechanism previously defined its value and now set it to a particular value \\(X=x\\) in an exogenous way.\nThus, the causal effect is defined as a function from the values \\(X\\) can take to the space of probability distributions on \\(Y\\). For example, if \\(X := x\\), then we arrive at the interventional distribution \\(P(Y| \\text{do}(x))\\): the population distribution of \\(Y\\) if everyone in the population had their \\(X\\) value fixed at \\(x\\).\nThe \\(\\text{do}\\) operator defines the exogenous process through which we have intervened to set the value of \\(X := x\\). Finally, we derive \\(P(Y| \\text{do}(x))\\) for every possible \\(x\\) and test whether the distribution changes as we change the value \\(X\\) takes.\nTherefore, to study the causal effect of \\(X\\) is to change the system by determining the value of \\(X\\) outside of it and seeing how the effects cascade thorough the system. However, before we change a system we must define it. How to represent the system? With a Causal Graph!"
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#causal-graphs",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#causal-graphs",
    "title": "Causality: Invariance under Interventions",
    "section": "Causal Graphs",
    "text": "Causal Graphs\nThe question, then, becomes: how can we simulate the effects of intervening in the causal system?. First, however, we must define the system in question.\nLet each node represent one of the variables of interest. We will draw an arrow from \\(X\\) to \\(Y\\) if there is a direct causal effect from \\(X\\) to \\(Y\\) for at least one individual. Alternatively, the lack of an arrow means that there’s no causal effect for any individual in the population. We will assume that the system is adequately written if all common causes of any pair of variables on the graph are themselves on the graph. Finally, we’ll say that a variable is always a cause of its descendants.\nWe will link Causal Graphs to Bayesian graphs by assuming that each variable, conditional on its parents, is independent of any variable for which it is not a cause (i.e., all its predecessors). In turn, this will imply that the Graph defines the same recursive decomposition of the joint distribution as a Bayesian Graph:\n\\[\nP\\left(x_{1}, \\ldots, x_{n}\\right)=\\prod_{j} P\\left(x_{j} \\mid pa_j\\right)\n\\]\nThereby, we can derive, using the d-separation criterion, testable implications of our causal models.\nTo make things more concrete, let’s work with the following fork: let’s say that a new treatment is developed to reduce cholesterol. However, women take the treatment more/less than men and have higher/lower levels of cholesterol. How to compute the causal effect of the treatment on cholesterol?"
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#interventions-eliminating-incoming-arrows",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#interventions-eliminating-incoming-arrows",
    "title": "Causality: Invariance under Interventions",
    "section": "Interventions: Eliminating incoming arrows",
    "text": "Interventions: Eliminating incoming arrows\nIntervening on \\(X\\) such that \\(\\text{do(X = 1)}\\) amounts to curtailing the previous mechanism that defined \\(X\\). In Graph lingo: eliminate the incoming arrows into \\(X\\): gender no longer cause \\(X\\). Therefore, we eliminate the arrow from Gender into treatment. Thus, an intervention is equivalent to eliminating arrows in a Causal Graph. Let’s label this new graph \\(G_m\\)"
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#invariant-probabilities-under-intervention",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#invariant-probabilities-under-intervention",
    "title": "Causality: Invariance under Interventions",
    "section": "Invariant probabilities under intervention",
    "text": "Invariant probabilities under intervention\nThe mutilated graph is still a Causal Graph. Thus, it implies a particular decomposition of the joint probability (\\(P_m\\)) of it’s own. With respect to this post-intervention distribution, we can define the causal effect: \\(P(Y=y|\\text{do}(X=x)) := P_m (Y=y|X=x)\\). However, this new post-intervention distribution \\(P_m\\) is not totally disconnected from the pre-intervention distribution (\\(P\\)) that we can study with observational data.\nThere are two invariant qualities that are the same in the pre-intervention and post-intervention distribution:\n\nOur intervention is atomic: there are no side effects that alter the way non-descendants of \\(X\\) are determined. Thus, \\(P_m(Z=z| X=x) = P(Z=z)\\).\nThe conditional probability \\(Y\\) is invariant, because the mechanism by which Y responds to \\(X\\) and \\(Z\\) remains the same, regardless of whether \\(X\\) changes spontaneously or by deliberate manipulation. Thus; \\(P_m(Y| X=x, Z = z) = P(Y|X=x, Z=z)\\).\n\n\nConnecting pre-intervention probabilities with post-treament\nTherefore, using probability laws and our independence assumption between \\(X\\) and \\(Z\\) in the mutilated graph, we define the causal effect in terms of post-intervention distribution thus:\n\\[\n\\begin{array}{l}\nP(Y = y | do(X=x)) := P_m (Y=y|X=x) \\\\\n=\\sum_{z} P_{m}(Y=y \\mid X=x, Z=z) P_{m}(Z=z \\mid X=x) \\\\\n=\\sum_{z} P_{m}(Y=y \\mid X=x, Z=z) P_{m}(Z=z)\n\\end{array}\n\\] Luckily, all the terms invariant: both terms can be connected to the original pre-intervention probability distribution:\n\\[\nP(Y=y \\mid d o(X=x))=\\sum_{z} P(Y=y \\mid X=x, Z=z) P(Z=z)\n\\] Therefore, we arrive at a definition of the causal effect in terms of the pre-treatment distribution. Thus, we can estimate the causal effect from observational studies without the need of actually carrying out the intervention."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#the-adjustment-formula",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#the-adjustment-formula",
    "title": "Causality: Invariance under Interventions",
    "section": "The Adjustment Formula",
    "text": "The Adjustment Formula\nMore generally, we define the causal effect in terms of pre-intervention probability thus. Given a graph \\(G\\) in which a set of variables \\(pa\\) are designated as the parents of \\(X\\), the causal effect of \\(X\\) on \\(Y\\) is given by:\n\\[\nP(Y=y|\\text{do}(X=x)) = \\sum_{z} P(Y=y | X=x, P A=z) P(pa=z)\n\\] Therefore, we can conclude why it is necessary to have a causal story to be able to estimate the causal effect: to identify the parents of \\(X\\) and adjust for them: first condition \\(P(Y=y| X =x)\\) on \\(PA\\) and then average the result, weighted the prior probability of \\(pa = z\\)."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#an-example",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#an-example",
    "title": "Causality: Invariance under Interventions",
    "section": "An example",
    "text": "An example\nLet’s follow our thought experiment with our previous graph. In the experiment, we observe both men and women who decide whether they take the drug or not. The results are the following:\n\n\n\n\n\n\n  \n  \n    \n      Recovered\n      N\n      Treatment\n      Gender\n    \n  \n  \n    81\n87\n1\nMale\n    234\n270\n0\nMale\n    192\n263\n1\nFemale\n    55\n80\n0\nFemale\n  \n  \n  \n\n\n\n\nWhen we study the data across genders, we find out that the patients who didn’t take the drug had a higher rate of recovery:\n\n\n\n\n\nHowever, once we separate the data by gender, the opposite picture arises:\n\n\n\n\n\nWe have a case of Simpson’s Paradox! Let’s use the causal knowledge embedded in our graph to estimate the true causal effect of the treatment. Given that Gender is the only parent of Treatment, we will adjust for it:\n[ P(Y=1 d o(X=1))=+=0.832 ] while, similarly, [ P(Y=1 d o(X=0))=+=0.7818 ] Thus, comparing the effect of drug-taking ( (X=1) ) to the effect of nontaking ( (X=0), ) we obtain [ A C E=P(Y=1 d o(X=1))-P(Y=1 d o(X=0))=0.832-0.7818=0.0502 ]\nHowever, if Gender had not been a parent of Treatment (i.e., if both Genders decide to take the treatment equally), our Causal effect would be different because we would adjust for Gender in the first place."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#identifiable",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#identifiable",
    "title": "Causality: Invariance under Interventions",
    "section": "Identifiable",
    "text": "Identifiable\nWe’ve estimated causal effects with a pretty simple strategy: adjust for the parents of the exposure and average those effects weighted by the probability of the parents.\nTherefore, according to our strategy, the causal effect will be identifiable whenever both \\(X, Y\\) and the parents of \\(X\\), \\(pa\\) are measured. Whenever measurements for some of them are missing, we must use other techniques to estimate the causal effect."
  },
  {
    "objectID": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#addendum-rct",
    "href": "posts/causality/2020-07-22-causality-invariance-under-interventions.html#addendum-rct",
    "title": "Causality: Invariance under Interventions",
    "section": "Addendum: RCT",
    "text": "Addendum: RCT\nRandomized Control Trials are sometimes referred to as the gold standard in causal inference. However, in our framework, they are nothing more than a different graph surgery. Whereas before we cut all the incoming arrows into treatment, now we replace all the incoming arrows with only with one arrow that signifies the randomization of the treatment:\n\n\n\n\n\nTherefore, now we must simply adjust by randomization to estimate the causal effect of treatment. Does that mean that they are not useful? No, they will always have the upper hand when we are uncertain about our causal model. If there is another parent of treatment that we are not accounting for, Randomization will offer a clean solution."
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "",
    "text": "Stats people know that correlation coefficients do not imply causal effects. Yet, very often, partial correlation coefficients from regressions with an ever growing set of ‘control variables’ are unequivocally interpreted as a step in the right direction toward estimating a causal effect. This mistaken intuition was aptly named by Richard McElreath, in his fantastic Stats course, as Causal Salad: people toss a bunch of control variables and hope to get a casual effect out of it.\nIn his fantastic course, Richard offers a clear and intuitive antidote for the Causal Salad: Graph Models for Causality. In these series of blogposts, I’ll explore the work of Judea Pearl, the father of the Graphical approach toward Causality. In particular, I’ll share what I learn from his book Causality: Models, Reasoning and Inference\nIn this blogpost, I’ll explore Bayesian Networks: the simplest of probability networks to represent a joint distribution and how we can derive testable implications from them using the d-separation criterion. Thorough the blopost, I’ll be using the excellent packages dagitty and ggdag."
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html#bayesian-networks",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html#bayesian-networks",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "Bayesian Networks",
    "text": "Bayesian Networks\nJoint probability distributions are tricky objects to represent: both in our heads and in our computers. They can imply an unworldly number of relationships. Probability theory gives us in the chain rule of probability a tool to decompose a joint probability distribution.\nSuppose we have a distribution ( P ) defined on ( n ) discrete variables, which we may order arbitrarily as ( X_{1}, X_{2}, , X_{n} . ) The chain rule of probability calculus always permits us to decompose ( P ) as a product of ( n ) conditional distributions:\n\\[\nP\\left(x_{1}, \\ldots, x_{n}\\right)=\\prod_{j} P\\left(x_{j} \\mid x_{1}, \\ldots, x_{j-1}\\right)\n\\]\nGiven structural knowledge about the problem at hand, we can simplify this decomposition given the conditional independence we posit in our model of the joint distribution. That is, it is not always necessary to condition on all the other variables: it suffices to control in a minimal set of variables to render other predecessor variables independent. For a given variable \\(x_j\\) let’s name this set of variables \\(pa_j\\): the parents of the variable \\(x_j\\).\nWe can then start constructing a graph (a Bayesian Network): each node will be a random variable; for any node, the arrows that enter into it represent the fact that conditional on its parents, the variable is conditionally independent of all other preceding variables. That is, at the \\(j\\)th stage of construction, we only draw an incoming arrow into \\(x_j\\) from its parents; conditional on its parents, all other predecessors are independent. Therefore, a missing arrow between any two nodes means that they are independent, once we know the values of their parents.\n\nAn example\nLet’s say we’re looking at the relationship between smoking and cardiac arrest. We might assume that smoking causes changes in cholesterol, which causes cardiac arrest. We start assuming that unhealthy lifestyle is the only ‘Parent’ for both smoking and weight:\n\n\n\n\n\nThen, we assume that the ‘Parents’ of cholesterol are Smoking and Weight:\n\n\n\n\n\nFinally, we assume that Cholesterol is the only ‘Parent’ of Cardiac Arrest\n\n\n\n\n\nTherefore, the arrows, by specifying what are the conditional independencies that hold in our model, tell us that what is the recursive decomposition of the joint probability distribution implied by the model. In this case, thus:\n\\[\nP\\left(x_{1}, x_2, x_3, x_4, x_{5}\\right) = p(x_1) p(x_2 | x_1) p (x_3 | x_1) p(x_4| x_2, x_3) p (x_5 | x_4)\n\\]\n\n\nProbability connections: Markov Compatibility\nWhenever a joint probability distribution \\(P\\) admits the decomposition posited by our graphical model \\(G\\), we say that \\(P\\) is Markov Relative to \\(G\\). That is, it means that \\(G\\) can explain the generation of the data represented by \\(P\\).\nOnce we know the probabilistic connection between our Graph and a probability distribution, we may be interested in deriving the testable implications of our graphical model \\(G\\). What other conditions does it imply about the probability distribution P?\nIn particular, in deriving which variables are independent and dependent, both marginally and conditional on other variables. Thanks to our graphical representation, we can derive an algorithm that returns all implied independencies that the model expects that will hold in the data: the d-separation criterion."
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html#d-separation-blocking-the-information-flows",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html#d-separation-blocking-the-information-flows",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "d-separation: blocking the information flows",
    "text": "d-separation: blocking the information flows\nDeriving testable implications from our assumptions is not as easy as it looks. Take the following example:\n\n\n\n\n\nCan we make \\(X\\) and \\(Y\\) conditionally dependent? The answer is not evident. To answer this question, we must understand how the statistical information moves through our graph."
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html#open-paths",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html#open-paths",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "Open paths",
    "text": "Open paths\nIn our Graph \\(G\\), the information moves across variables regardless of the direction of the arrows. If there is an open path between \\(X\\) and \\(Y\\), then, the information will flow and thus make the two variables dependent. The question, then, is what is an open path? The d-criterion answers precisely this question:\n( A ) path ( p ) is said to be blocked (or ( d ) -separated) by a set of nodes ( Z ) if and only if any of the following conditions hold:\n\nif ( p ) contains a chain ( i m j ) or a fork ( i m j ) such that the middle node ( m ) is in ( Z ).\nif ( p ) contains a collider ( i m j ) such that the middle node ( m ) is not in ( Z ) and such that no descendant of ( m ) is in ( Z ).\n\nLet’s examine why is that both conditions guarantee that the path is closed.\n\nOf chains and forks\nIn both chains and forks, the extreme variables are marginally dependent. However, once we adjust by the middle variable they become marginally independent. That is, the information between the extreme variables that was previously flowing stops flowing altogether. Why?\n\n\nChains\nLet’s look at an example of the chain where addictive behavior causes the person to smoke which causes Cancer. If we don’t adjust by any variable, the information flows freely: knowing whether some has an addictive behavior can tell us whether they smoke, and thus tells us something about the probability that they have cancer.\n\n\n\n\n\nHowever, if we adjust by a particular level of smoking (as in the above figure), knowing the addictive behavior of someone tells us nothing about the probability that they have cancer: the level of smoking says it all already. Thus, addictive behavior and cancer are d-separated by smoking.\n\n\nForks\nWith forks, the story is extremely similar: the middle variable is a common cause for both of the extreme variables. Thus, they are marginally dependent. For example, let’s say that addictive behavior causes both smoking and drinking coffee.\nIf we don’t know whether someone has an addictive behavior or not, the fact that they drink lots of coffee tells us that they are likely to have an addicting behavior and thus are more likely to smoke.\n\n\n\n\n\nHowever, once we adjust by a specific value of addictive behavior (as in the above figure), drinking coffee does not say anything about smoking, the value of addictive behavior has said it all already: they are independent conditional on addictive behavior.\nIt is only in the absence of knowing addictive behavior that they capture information from one another. That is, they are only dependent when we don’t adjust by addictive behavior.\nTherefore, whenever a path has either a fork or a chain, we can block that path by conditioning on the middle variable.\n\n\nOf Colliders\nWhereas before we spoke of open paths that were closed once we adjust, a collider starts being a closed path an only is opened once we adjust by the middle variable. The middle variable is a common consequence of two marginally independent causes: if we adjust by the consequence, we render the common causes dependent.\nTake the following example: being a hollywood actor results from either being attractive or being talented. Being attractive and being talented are independent; however, once we know whether someone is a hollywood actor, knowing one of the characteristics tells us about the other. e.g., if the actor is not talented, it tells us that the actor, to be in hollywood, then must be attractive.\n\n\n\n\n\nTherefore, if a path has a collider it will remained closed unless we adjust by the middle variable or one of its descendants.\n\n\nThe probabilistic implications of the d-separation criterion\nOnce we have understood our d-separation criterion, we can connect this graphical analysis with a counterpart probability implication:\nIf ( X ) and ( Y ) are ( d ) -separated by ( Z ) in a DAG ( G, ) then ( X ) is independent of ( Y ) conditional on ( Z ) in every distribution compatible with ( G ). Conversely, if ( X ) and ( Y ) are not ( d ) -separated by ( Z ) in a DAG ( G ), then ( X ) and ( Y ) are dependent conditional on ( Z )\n\n\nPutting the d-separation criterion to the test\nArmed with the d-separation criterion, we can ask ourselves, given our earlier that I reproduce below, how can we make \\(X\\) and \\(Y\\) conditionally dependent?\n\n\n\n\n\nThe answer, of course, is the d-separation criterion: can we open the path from \\(X\\) to \\(Y\\) by conditioning on some variable? Armed with what we just learned, the answer is now clear: \\(z_1\\) is a collider that lies in the path. To open it, we must condition on it. Therefore, \\(X\\) and \\(Y\\) are dependent conditional on \\(z_1\\). We can easily check our logic with dagitty::dconnected:\n\ndconnected(example, \"X\", \"Y\", \"Z1\")\n\n[1] TRUE\n\n\nWe can repeat this exercise for every pair of variables and check whether they are conditionally (or marginally) independent given some set of controlling variables. The set of variables that are conditionally independent, then, constitute the set of testable implications of our model. Therefore, we can check whether our Graphical model is consistent with a given dataset by comparing the implied conditional independence with the observed conditional independence in our data.\nFor the above model, then, the implied conditional independencies are:\n\nimpliedConditionalIndependencies(example)\n\nX _||_ Y\nX _||_ Z2\nX _||_ Z3\nY _||_ Z1 | Z2\nY _||_ Z1 | Z3\nY _||_ Z2 | Z3\nZ1 _||_ Z3 | Z2"
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html#can-we-distinguish-models-from-data-alone",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html#can-we-distinguish-models-from-data-alone",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "Can we distinguish models from data alone?",
    "text": "Can we distinguish models from data alone?\nWe finally have an strategy to test whether our model implications are in accordance with our observed values. However, it stands to reason that our model is not unique in this regard: there are other models that imply the same conditional independencies. We say, thus, that the two graphical models \\(G_1\\) and \\(G_2\\) are observationally equivalent when they imply the same conditional independencies. The set of all the models with indistinguishable implications is called an equivalence class.\nThat is, if two models are observationally equivalent, we cannot use data alone to distinguish from them. We must use our structural knowledge about the problem at hand to decide which model is the right one. Therefore, Observational equivalence places a limit on our ability to infer directionality from probabilities alone.\nFor simple models, the limitations are draconian. Let’s take as an example the fork we just analyzed where addictive behavior is a fork between drinking coffee and smoking:\n\n\n\n\n\nLet’s check what are this model’s testable implications:\n\nimpliedConditionalIndependencies(dagify(x ~ f,\n       y ~ f,\n       labels = c(\"x\" = \"Coffee\", \n                  \"y\" = \"Smoking\", \"f\" = \"Addictive \\nBehavior\")))\n\nx _||_ y | f\n\n\nNot surprisingly, the only conditional implication implied is that drinking coffee is independent on smoking once we condition on the addictive behavior. That is, that coffee is d-separated from smoking by addictive behavior."
  },
  {
    "objectID": "posts/causality/2020-07-18-causality-bayesian-networks.html#identifying-models",
    "href": "posts/causality/2020-07-18-causality-bayesian-networks.html#identifying-models",
    "title": "Causality: Bayesian Networks and Probability Distributions",
    "section": "Identifying models",
    "text": "Identifying models\nSuppose for a moment that the arrows in our Graph are now endowed with causal meaning: there’s an arrow from \\(X\\) to \\(Y\\) if \\(X\\) causes \\(Y\\). For the moment, it will suffice your intuitive understanding of what this means.\nThe Causal Graph will still have all the characteristics of a simple Bayesian Graph. Then, we can ask: What other causal models have only this testable implication? That is, with only observational data, can we distinguish between causal (interventional) models?\n\n\n\n\n\nThus, from data-alone we cannot infer the directionality of any of three posited causal relationships. That is, data alone cannot settle the issue of whether the appropriate model is a fork or a chain that begins at either coffee or smoking. As Pearl says, data are fundamentally dumb: if we rely only in data to inform our models, we are extremely limited on what we can learn from them. Therefore, we must extend our theory beyond conditional probabilities.\nThat is, we cannot predict the consequences of intervening in one of the variables with only observational data. That is, we cannot gain causal understanding with only observational data: we must assume a causal model to predict the the consequences of any intervention (i.e., the causal effects)."
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html",
    "title": "Causality: Mediation Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggdag)\nextrafont::loadfonts(device=\"win\")\ntheme_set(theme_dag(base_family = \"Roboto Condensed\"))"
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html#motivation",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html#motivation",
    "title": "Causality: Mediation Analysis",
    "section": "Motivation",
    "text": "Motivation\nKids are the prototypical question makers; they never stop asking questions. Just after you have answered a Why? question, they ask yet another Why? This is the problem of mediation analysis: if you answer that X causes Y, how does exactly the causal mechanism work? Is the causal effect direct or mediated through yet another variable M? Mediation analysis aims to disentangle the direct effect (which does not pass through the mediator) from the indirect effect (the part that passes through the mediator).\nJudah Pearl has formulated an answer to the mediation problem by using counterfactuals. By giving precise counterfactual interpretations to both the Natural Direct Effects (NDE) and the Natural Indirect Effects (NIE), we can use the machinery of Causal Inference to solve the mediation problem.\nIn this post, we’ll study the counterfactual definition and identification criteria behind direct and indirect effects. Finally, we’ll solve a numerical example to put what we have learnt into practice.\nAll quotes come from Chapter 9 of Pearl’s Causality and Chapter 4 of his primer."
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html#counterfactual-definitions",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html#counterfactual-definitions",
    "title": "Causality: Mediation Analysis",
    "section": "Counterfactual Definitions",
    "text": "Counterfactual Definitions\nWe will use the following canonical Structural Model for a mediation problem to define the following direct and indirect effects.\n\\[\nt=f_{T}\\left(u_{T}\\right) \\quad m=f_{M}\\left(t, u_{M}\\right) \\quad y=f_{Y}\\left(t, m, u_{Y}\\right)\n\\] Let \\(T\\) be a binary treatment.\n\nControl freak\nSo far, we have studied the total causal effect of \\(X\\) on \\(Y\\): \\(P(Y|do(X))\\). “The term “direct effect” is meant to quantify an effect that is not mediated by other variables in the model or, more accurately, the sensitivity of \\(Y\\) to changes in \\(X\\) while all other factors in the analysis are held fixed”. Notice that holding variables fixed implies an intervention that cannot always be mimicked by conditioning.\nWe will label this effect the Controlled Direct Effect (CDE). In counterfactual terms, it is defined thus:\n[\n\\[\\begin{aligned}\n\\operatorname{CDE}(m) &=E\\left[Y_{1, m}-Y_{0, m}\\right] \\\\\n&=E[Y \\mid d o(T=1, M=m)]-E[Y \\mid d o(T=0, M=m)]\n\\end{aligned}\\]\n]\n\nCDE measures the expected increase in ( Y ) as the treatment changes from ( T=0 ) to ( T=1, ) while the mediator is set to a specified level ( M=m ) uniformly over the entire population.\n\nHowever, intervening on the mediator is an over-kill. We need to be more intelligent.\n\n\nNatural: Let it flow\nA less stringent intervention is defined by studying the expected increase in ( Y ) as the treatment changes from ( T=0 ) to ( T=1, ), “while the mediator is set to whatever value it would have attained (for each individual) prior to the change, that is, under \\(T = 0\\)”. We will label this the Natural Direct Effect (NDE). In counterfactual terms:\n\\[\nN D E=E\\left[Y_{1, M_{0}}-Y_{0, M_{0}}\\right]\n\\] Whereas the CDE is made out of do-expressions, the NDE is defined in terms of nested counterfactuals. Therefore, according to Pearl’s Ladder of Causation and Bareinboim’s Causal Hierarchy Theorem, NDE requires a more elaborate causal knowledge to be identified than the CDE. That is, whereas the CDE could be estimated using experimental evidence, the NDE, in principle, cannot be estimated using only experimental evidence.\n\nWhat about indirect effects?\nOnce we have defined a direct effect, the natural thing to do, in order to tackle the mediation problem, is to also define an indirect effect. The comparison of the two terms will allow us to answer the mediation problem.\nNotice that we must define the Natural Indirect Effect (NIE) such that it measures “the portion of the effect that can be explained by mediation alone. Thus, it must disable the capacity of \\(Y\\) to respond to \\(X\\)”. To do so, we will define the NIE thus:\n\nNIE measures the expected increase in Y when the treatment is held constant, at \\(T = 0\\), and \\(M\\) changes to whatever value it would have attained (for each individual) under \\(T=1\\).\n\nIn counterfactual language:\n\\[\nN I E=E\\left[Y_{0, M_{1}}-Y_{0, M_{0}}\\right]\n\\] Just like with the NDE, we are faced with nested counterfactuals that cannot always be estimated using experimental evidence.\n\n\n\nResponse fractions\nTo answer the mediation question, it is useful to state the direct and indirect effects in terms of the total effect.\nWhat percentage is due to the direct effect of \\(X\\)? The ratio \\(NDE/TE\\) “measures the fraction of the response that is transmitted directly, with \\(M\\) frozen.”\nWhat percentage is due to the mediator variable, that is, is due to the indirect effect of \\(X\\)?\n\n\\(NIE∕TE\\) measures the fraction of the response that may be transmitted through \\(M\\), with \\(Y\\) blinded to \\(X\\). Consequently, the difference \\((TE − NDE)∕TE\\) measures the fraction of the response that is necessarily due to M."
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html#identification",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html#identification",
    "title": "Causality: Mediation Analysis",
    "section": "Identification",
    "text": "Identification\nBecause both the NDE and the NIE are defined with nested counterfactuals, they imply a contradiction between two different and clashing causal worlds that can only be resolved through the invariant information across worlds. However, not all structural causal models (SCM) imply enough restrictions such that this invariant information is enough to estimate the nested counterfactuals with a combination of observational and experimental evidence.\nWhat type of causal models yield NDE (and NIE) that are identifiable? In this paper, Pearl says that every model where there is a set \\(w\\) of measured covariates such that:\n\nNo member of \\(W\\) is a descendant of \\(T\\).\n\\(W\\) blocks all backdoor paths from \\(M\\) to \\(Y\\) not traversing \\(T\\). That is, \\(W\\) deconfounds the mediator-outcome relationship (holding \\(T\\) constant).\n\nThen, both effects (NDE and NIE) are identifiable with experimental evidence. The formula for the NDE becomes thus:\n\\[\n\\begin{aligned}\nN D E=\\sum_{m} \\sum_{w}\\left[E(Y \\mid d o(T=1, M=m)), W=w)-E(Y \\mid d o(T=0, M=m), W=w)\\right] \\\\\nP(M=m \\mid d o(T=0), W=w) P(W=w)\n\\end{aligned}\n\\]\nThe intuition is the following:\n\nThe natural direct effect is the weighted average of the controlled direct effect ( C D E(m), ) shown in the square brackets, using the no-treatment distribution ( P(M=m T=0) ) as a weighting function.\n\nFurthermore, if we require identification with observational data, we must have a causal model where, besides the former two assumptions, the following two assumptions also hold:\n\nThe ( W ) -specific effect of the treatment on the mediator is identifiable by some means. [ [P(m d o(t), w) ] ]\nThe ( W ) -specific joint effect of ( { ) treatment ( + ) mediator ( } ) on the outcome is identifiable by some means. [ [P(y d o(t, m), w) ] ]\n\nThen, the equation for the NDE (and the NIE) becomes:\n\\[\n\\begin{equation}\n\\begin{array}{c}\nN D E=\\sum_{m} \\sum_{w}[E(Y \\mid T=1, M=m, W=w)-E(Y \\mid T=0, M=m, W=w)] \\\\\nP(M=m \\mid T=0, W=w) P(W=w) \\\\\nN I E=\\sum_{m} \\sum_{w}[P(M=m \\mid T=1, W=w)-P(M=m \\mid T=0, W=w)] \\\\\nE(Y \\mid T=0, M=m, W=w) P(W=w)\n\\end{array}\n\\end{equation}\n\\]\nFinally, if there is no confounding in our causal model whatsoever, there’s no need fo conditioning on \\(W\\) and we arrive at the mediation formulas:\n\\[\nNDE = \\sum_{m}[E[Y \\mid T=1, M=m]-E[Y \\mid T=0, M=m]] P(M=m \\mid T=0)\n\\] Similarly, for the NIE the mediation formula is the following:\n\\[\nN I E=\\sum_{m} E[Y \\mid T=0, M=m][P(M=m \\mid T=1)-P(M=m \\mid T=0)]\n\\] In the following section, I’ll present four examples of causal models where the NDE and NIE may not be identifiable.\n\nExamples of Identification\n\nFirst example\nSuppose you have the following causal model. Are the NDE and NIE identifiable?\n\nfirst_example <- dagify(y ~ t + m,\n                        m ~ t)\nggdag(first_example) +\n  labs(title = \"Are the NDE and NIE identifiable?\",\n       subtitle = \"Given that there's no confounding, they are identifiable!\")\n\n\n\n\nYes, the effects are identifiable: there’s no confounding and we can use the mediator formulas.\n\n\nSecond example\nSuppose you have the following causal model. Are the NDE and NIE identifiable?\n\nsecond_example <- dagify(y ~ t + m + w,\n                        m ~ t + w,\n                        t ~ w)\nggdag(second_example) +\n  labs(title = \"Are the NDE and NIE identifiable?\",\n       subtitle = \"w confounds all three relationships. Adjusting for W, renders NDE and NIE identifiable\")\n\n\n\n\nYes, we can identify the NDE and the NIE. Although \\(W\\) confounds all three relationships, by adjusting by \\(W\\), we can deconfound them and estimate the NDE and NIE.\n\n\nThird example\nSuppose you have the following causal model where the dashed arc represents a common unobserved ancestor. Are the NDE and NIE identifiable?\n\nthird_example <- dagify(m ~ t,\n                        z ~ m,\n                        y ~ z + t,\n                        m ~~ y)\ntidy_dagitty(third_example, layout = \"nicely\", seed = 2) %>% \n  mutate(linetype = if_else(direction == \"->\", \"solid\", \"dashed\")) %>% \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend, edge_linetype = linetype)) +\n  geom_dag_edges(aes(end_cap = ggraph::circle(10, \"mm\"))) +\n  geom_dag_point() + \n  geom_dag_text() +\n  labs(title = \"Are the NDE and NIE identifiable?\",\n       subtitle = \"Although the causal effect M->Y is identifiable, we cannot deconfound the relationships and\n        thus cannot estimate neither the NDE nor the NIE\")\n\n\n\n\nWe cannot!\n\nUnfortunately, although the causal effect of ( {T, M} ) on ( Y, ) as well as the controlled direct effect ( C D E(m) ) are both identifiable (through the front-door estimator), condition (2 ) cannot be satisfied; no covariate can be measured that deconfounds the ( M Y ) relationship. The front-door estimator provides a consistent estimate of the population causal effect, ( P(Y=y d o(M=m)), ) while unconfoundedness, as defined before, requires independence of ( U_{M} ) and ( U_{Y}, ) which measurement of ( Z ) cannot induce.\n\nThis is yet another example of the Causal Hierarchy Theorem: experimental evidence is not enough to determine counterfactual information. In this case, causal effects are not enough to determine the nested counterfactuals that define the NDE.\n\n\nFour example\nSuppose you have the following causal model. Are the NDE and NIE identifiable?\n\nfourth_example <- dagify(y ~ t + m + w,\n                        m ~ t + w,\n                        w ~ t)\nggdag(fourth_example) +\n  labs(title = \"Are the NDE and NIE identifiable?\",\n       subtitle = \"Although W deconfounds M->Y, it is a descendant of T. Thus, we cannot identify NDE or NIE. \")\n\n\n\n\nNo, neither NDE nor NIE are identifiable. Given our first assumption, there’s the following “general pattern that prevents identification of natural effects in any non-parametric model”:\n\nWhenever a variable exists, be it measured or unmeasured, that is a descendant of T and an ancestor of both M and Y (W in our examples), NDE is not identifiable.\n\nHowever, the restriction does not apply to linear models where every counterfactual is identifiable once the parameters are identified. Sadly, “the increased identification power comes at increasing the danger of mis-specification”."
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html#numerical-example",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html#numerical-example",
    "title": "Causality: Mediation Analysis",
    "section": "Numerical example",
    "text": "Numerical example\nI’ll finish the post by giving the following numerical example to show how we can use what we’ve learnt to estimate natural effects and solve a mediation problem with data. This is the exercise 4.5.4 in Pearl’s primer.\n\nSuppose that a company is accused of gender discrimination. Let \\(T = 1\\) standing for male applicants, \\(M = 1\\) standing for highly qualified applicants, and \\(Y = 1\\) standing for hiring. (Find the proportion of the hiring disparity that is due to gender, and the proportion that could be explained by disparity in qualification alone.)\n\nThat is, there are two paths whereby there’s discrimination. Male applicants tend to get more easily hired and thus have more qualifications. However, male applicants may also be favored by the company just because they are male. We draw the following DAG:\n\ngender_discrimination <- dagify(m ~ t,\n                                y ~ m + t,\n                                labels = c(\"m\" = \"Qualifications\",\n                                           \"t\" = \"Gender\",\n                                           \"y\" = \"Hiring\"))\ngender_discrimination %>%  tidy_dagitty(layout = \"tree\") %>% \n   ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_edges() +\n    geom_dag_text(col = \"white\") +\n    geom_dag_point(alpha = 0.5) +\n    geom_dag_label_repel(aes(label = label), fill = \"dodgerblue4\",\n      col = \"white\", show.legend = FALSE, family = \"Roboto Condensed\")\n\n\n\n\nLet’s say that we collect the following data:\n\\[\n\\begin{array}{ccc}\n\\hline \\text { Gender } & \\text { Qualification } & \\text { Success Hiring } \\\\\nT & M & E(Y \\mid T=t, M=m) \\\\\n\\hline 1 & 1 & 0.80 \\\\\n1 & 0 & 0.40 \\\\\n0 & 1 & 0.30 \\\\\n0 & 0 & 0.20 \\\\\n\\hline\n\\end{array}\n\\]\n\\[\n\\begin{array}{cc}\n\\hline \\text { Gender } & \\text { Qualification } \\\\\nT & E(M \\mid T=t) \\\\\n\\hline 0 & 0.40 \\\\\n1 & 0.75 \\\\\n\\hline\n\\end{array}\n\\] Assuming that there’s no confounding, we use the mediator formulas thus:\n\\[\n\\begin{aligned}\nN D E=& \\sum_{m}[E[Y \\mid T=1, M=m]-E[Y \\mid T=0, M=m]] P(M=m \\mid T=0) \\\\\n=&[E[Y \\mid T=1, M=0]-E[Y \\mid T=0, M=0]] P(M=0 \\mid T=0) \\\\\n&+[E[Y \\mid T=1, M=1]-E[Y \\mid T=0, M=1]] P(M=1 \\mid T=0) \\\\\n=&(0.4-0.2)(1-0.4)+(0.8-0.3) 0.4 \\\\\n=& 0.32 \\\\\nN I E=& \\sum_{m} E[Y \\mid T=0, M=m][P(M=m \\mid T=1)-P(M=m \\mid T=0)] \\\\\n=& E[Y \\mid T=0, M=0][P(M=0 \\mid T=1)-P(M=0 \\mid T=0)] \\\\\n&+E[Y \\mid T=0, M=1][P(M=1 \\mid T=1)-P(M=1 \\mid T=0)] \\\\\n=&(0.75-0.4)(0.3-0.2) \\\\\n=& 0.035\n\\end{aligned}\n\\]\nTherefore, given that the direct effect is substantially larger than the indirect effect, we conclude that it is not the different qualifications in themselves, but the gender that is driving the hiring process in the company."
  },
  {
    "objectID": "posts/causality/2020-08-26-causality-mediation-analysis.html#conclusions",
    "href": "posts/causality/2020-08-26-causality-mediation-analysis.html#conclusions",
    "title": "Causality: Mediation Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nMediation analysis aims to disentangle the NDE (which does not pass through the mediator) from the NIE (the part that passes through the mediator). We’ve seen how the correct definition of these effects requires counterfactual thinking that cannot always be empirIcally identified with either experimental or observational evidence. After having stated the required assumptions for identifiability, we practiced recognizing such assumptions with several causal models. Finally, we practiced with a numerical example out of Pearl’s primer."
  },
  {
    "objectID": "posts/causality/2020-08-20-causality-probabilities-of-causation.html",
    "href": "posts/causality/2020-08-20-causality-probabilities-of-causation.html",
    "title": "Causality: Probabilities of Causation",
    "section": "",
    "text": "Questions of attribution are everywhere: i.e., did \\(X=x\\) cause \\(Y=y\\)? From legal battles to personal decision making, we are obsessed by them. Can we give a rigorous answer to the problem of attribution?\nOne alternative to solve the problem of attribution is to reason in the following manner: if there is no possible alternative causal process, which does not involve \\(X\\),that can cause \\(Y=y\\), then \\(X=x\\) is necessary to produce the effect in question. Therefore, the effect \\(Y=y\\) could not have happened without \\(X=x\\). In causal inference, this type of reasoning is studied by computing the Probability of Necessity (PN)\nIn the above alternative, however, the reasoning is tailored to a specific event under consideration. What if we are interested in studying a general tendency of a given effect? In this case, we are asking how sufficient is a cause, \\(X=x\\), for the production of the effect, \\(Y=y\\). We answer this question with the Probability of Sufficiency (PS)\nIn this blogpost, we will give counterfactual interpretations to both probabilities: \\(PN\\) and \\(PS\\). Thereby, we will be able to study them in a systematic fashion using the tools of causal inference. Although we will realize that they are not generally identifiable from a causal diagram and data alone, if we are willing to assume monotonicity, we will be able to estimate both \\(PN\\) and \\(PS\\) with a combination of experimental and observational data.\nFinally, we will work through some examples to put what we have learnt into practice. This blogpost follows the notation of Pearl’s Causality, Chapter 9 and Pearl’s Causal Inference in Statistics: A primer."
  },
  {
    "objectID": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#counterfactual-definitions",
    "href": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#counterfactual-definitions",
    "title": "Causality: Probabilities of Causation",
    "section": "Counterfactual definitions",
    "text": "Counterfactual definitions\nLet \\(X\\) and \\(Y\\) be two binary variables in causal model. In what follows, we will give counterfactual interpretations to both \\(PN\\) and \\(PS\\).\n\nProbability of Necessity\nThe Probability of Necessity (PN) stands for the probability that the event \\(Y=y\\) would not have occurred in the absence of event \\(X=X\\), given that \\(x\\) and \\(y\\) did in fact occur.\n\\[\nPN := P(Y_{x'} = false | X = true, Y = true)\n\\]\nTo gain some intuition, imagine Ms. Jones: a former cancer patient that underwent both a lumpectomy and irradiation. She speculates: do I owe my life to irradiation? We can study this question by figuring out how necessary was the lumpectomy for the remission to occur:\n\\[\nPN = P(Y_{\\text{no irradiation}} = \\text{no remission}| X = irradiation, Y = remission)\n\\]\nIf \\(PN\\) is high, then, yes, Ms. Jones owes her life to her decision of having irradiation.\n\n\nProbability of Sufficiency\nOn the other hand:\n\nThe Probability of Sufficiency (PS) measures the capacity of \\(x\\) to produce \\(y\\), and, since “production” implies a transition from absence to presence, we condition \\(Y_x\\) on situations where x and y are absent\n\nTherefore:\n\\[\nPS := P(Y_x = true | X = false, Y = false)\n\\]\nThe following example may clarify things. Imagine that, contrary to Ms. Jones above, Mrs. Smith had a lumpectomy alone and her tumor recurred. She speculates on her decision and concludes: I should have gone through irradiation. Is this regret warranted? We can quantify this by speaking about \\(PS\\):\n\\[\nPS = P(Y_{irradiation} = remission| X = \\text{no irradiation}, Y = \\text{no remission})\n\\] That is, \\(PS\\) computes the probability that remission would have occurred had Mrs. Smith gone through irradiation, given that she did not go and remission did not occur. Thus, it measures the degree to which the action not taken, \\(X=1\\), would have been sufficient for her recovery.\n\n\nCombining both probabilities\nWe can compute the probability that the cause is necessary and sufficient thus:\n\\[\nP N S:= P(y_{x}, y'_{x'}) =P(x, y) P N+P\\left(x^{\\prime}, y^{\\prime}\\right) P S\n\\]\nThat is, the contribution of \\(PN\\) is amplified or reduced by \\(P(x, y)\\) and likewise for the \\(PS\\)’s contribution."
  },
  {
    "objectID": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#idenitfiability",
    "href": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#idenitfiability",
    "title": "Causality: Probabilities of Causation",
    "section": "Idenitfiability",
    "text": "Idenitfiability\nIn the general case, when we have a causal diagram and observed (and experimental) data, neither \\(PN\\) nor \\(PS\\) are identifiable. This happens due to the relationship between the counterfactual’s antecedent and the fact that we are conditioning on \\(Y\\). If we wanted to model this relationship, we would need to know the functional relationship between \\(X, Pa_y\\) and \\(Y\\).\nIn practice, if we don’t know the functional relationship, we must at least assume monotonicity of \\(Y\\) relative to \\(X\\) to be able to identify them. Otherwise, we must content ourselves with theoretically sharp bounds on the probabilities of causation.\n\nWhat is Monotonicity?\nLet \\(u\\) be the unobserved background variables in a SCM, then, \\(Y\\) is monotonic relative to \\(X\\) if:\n\\[\nY_1(u) \\geq Y_0 (u)\n\\]\nThat is, exposure to treatment \\(X=1\\) always helps to bring about \\(Y=1\\).\n\n\nIdentifying the probabilities of causation\nIf we are willing to assume that Y is monotonic relative to X, then both \\(PN\\) and \\(PS\\) are identifiable when the causal effects \\(P(Y| do(X=1)\\) and \\(P(Y| do(X=0)\\) are identifiable. Whether this causal effect is identifiable because we have experimental evidence, or because we can identify them by using the Back-Door criterion or other graph-assisted identification strategy, it does not matter. Then, we can estimate \\(PN\\) with a combination of do-expressions and observational data thus:\n\\[\nPN = P(y'_{x'} | x, y) = \\frac{P(y) - P(y| do (x'))}{P(x, y)}\n\\]\nMoreover, if monotonicity does not hold, the above expression becomes a lower bound for \\(PN\\). The complete bound is the following:\n\\[\n\\max \\left\\{0, \\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{P(x, y)}\\right\\} \\leq P N \\leq \\min \\left\\{1, \\frac{P\\left(y^{\\prime} \\mid d o\\left(x^{\\prime}\\right)\\right)-P\\left(x^{\\prime}, y^{\\prime}\\right)}{P(x, y)}\\right\\}\n\\]\nEquivalently, \\(PS\\) can be estimated thus:\n\\[\nPS = P(y_x | x', y') = \\frac{P(y| do(x)) - P(y)}{P(x', y')}\n\\]\nWhich becomes the lower bound if we are not willing to assume monotonicity:\n\\[\n\\max \\left\\{\\begin{array}{c}\n0,\n\\frac{P\\left(y | do (x) \\right)-P(y)}{P\\left(x^{\\prime}, y^{\\prime}\\right)}\n\\end{array}\\right\\} \\leq P S \\leq \\min \\left\\{\\begin{array}{c}\n1,\n\\frac{P\\left(y | do(x) \\right)-P(x, y)}{P\\left(x^{\\prime}, y^{\\prime}\\right)}\n\\end{array}\\right\\}\n\\]\nLet’s use the estimators and the bounds in the following example."
  },
  {
    "objectID": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#a-first-example",
    "href": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#a-first-example",
    "title": "Causality: Probabilities of Causation",
    "section": "A first example",
    "text": "A first example\n\nA lawsuit is filed against the manufacturer of a drug X that was supposed to relieve back-pain. Was the drug a necessary cause for the the death of Mr. A?\n\nThe experimental data provide the estimates [\n\\[\\begin{aligned}\nP(y \\mid d o(x)) &=16 / 1000=0.016 \\\\\nP\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right) &=14 / 1000=0.014\n\\end{aligned}\\]\n] whereas the non-experimental data provide the estimates [ P(y)=30 / 2000=0.015 ] [\n\\[\\begin{array}{l}\nP(x, y)=2 / 2000=0.001 \\\\\nP(y \\mid x)=2 / 1000=0.002 \\\\\nP\\left(y \\mid x^{\\prime}\\right)=28 / 1000=0.028\n\\end{array}\\]\n]\nTherefore, assuming that the drug could only cause (but never prevent death), monotonicity holds:\n\\[\nPN = \\frac{0.015-0.014}{0.001} = 1\n\\]\n\nThe plaintiff was correct; barring sampling errors, the data provide us with 100% assurance that drug x was in fact responsible for the death of Mr A."
  },
  {
    "objectID": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#a-second-example",
    "href": "posts/causality/2020-08-20-causality-probabilities-of-causation.html#a-second-example",
    "title": "Causality: Probabilities of Causation",
    "section": "A second example",
    "text": "A second example\nRemember Ms. Jones? Is she right in attributing her recovery to the irradiation therapy. Suppose she gets her hands on the following data:\n\\[\n\\begin{aligned}\nP\\left(y^{\\prime}\\right) &=0.3 \\\\\nP\\left(x^{\\prime} \\mid y^{\\prime}\\right) &=0.7 \\\\\nP(y \\mid d o(x)) &=0.39 \\\\\nP\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right) &=0.14\n\\end{aligned}\n\\]\nWe can therefore start to bound \\(PN\\) to figure out whether irradiation was necessary for remission:\n\\[\n\\begin{aligned}\nP N & \\geq \\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{P(x, y)} \\\\\n&=\\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{P(y \\mid x) P(x)} \\\\\n&=\\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{\\left(1-\\frac{P\\left(x \\mid y^{\\prime}\\right) P\\left(y^{\\prime}\\right)}{P(x)}\\right) P(x)} \\\\\n&=\\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{P(x)-P\\left(x \\mid y^{\\prime}\\right) P\\left(y^{\\prime}\\right)}\n\\end{aligned}\n\\]\nWe don’t have data for \\(P(x)\\). However, given that we are interested in a lower bound, we can choose the parametrization that yields the smallest bound: \\(P(x) = 1\\). Therefore, the bound becomes:\n\\[\n\\begin{aligned}\nP N & \\geq \\frac{P(y)-P\\left(y \\mid d o\\left(x^{\\prime}\\right)\\right)}{P(x)-P\\left(x \\mid y^{\\prime}\\right) P\\left(y^{\\prime}\\right)} \\\\\n& \\geq \\frac{0.7-0.14}{1-(1-0.7) * 0.3} = \\\\\n& 0.62 > 0.5\n\\end{aligned}\n\\]\nTherefore, irradiation was more likely than not necessary for her remission."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html",
    "href": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html",
    "title": "Central Limit Theorem in Action",
    "section": "",
    "text": "I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In it, we have seen how the Law of Large Numbers for different estimators simply does not work fast enough (in Extremistan) to be used in real life. For example, we have seen how the distribution of the sample mean, PCA, sample correlation and \\(R^2\\) turn into pure noise when we are dealing with fat-tails.\nIn this post, I’ll try to show the same problem for the Central Limit Theorem (CLT). That is, when dealing with fat-tailed random variables with finite variance, the CLT takes way too long to warrant its use. The pre-asymptotic’s behavior, where we live and analyze data, will shows us how different Mediocristan is from Extremistan."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html#worsening-the-problem",
    "href": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html#worsening-the-problem",
    "title": "Central Limit Theorem in Action",
    "section": "Worsening the problem",
    "text": "Worsening the problem\nWe can check how this problems exacerbate once we increase the variance of the underlying normal. For example, here it is a small change where \\(sigma = 2\\) for the underlying normal.\n\nlognormal_large <- simulate_sample_means(function(x) rlnorm(n = x, sdlog = 2), 3000, ns) %>% \n  rename(lognormal = value) %>% \n  group_by(samples_sizes) %>% \n  mutate(z = (lognormal - exp(4/2) )/sqrt(( ( exp(4) - 1) * exp(4) ) / samples_sizes ) ) %>% \n  ungroup() %>% \n  mutate(samples_sizes = factor(samples_sizes, ordered = TRUE))\n\nlognormal_large %>% \n  ggplot(aes(z, fill = samples_sizes)) +\n  geom_density(aes(group = NA)) +\n  transition_states(samples_sizes) +\n  stat_function(fun = dnorm, aes(color = 'Normal'),\n                         args = list(mean = 0, sd = 1),\n                inherit.aes = FALSE) +\n  enter_fade() +\n  exit_shrink() +\n  view_follow() +\n  labs(subtitle = \"Sample size {closest_state}\",\n       title = \"Distribution of Sample Mean: Lognormal\",\n       caption = \"Theoretical distirbution is standard normal\",\n       y = \"Sample mean from Lognormal\")\n\n\n\n\nIndeed, now the sample mean’s distribution has a much longer right tail. That is, the skewness and kurtosis problems persists even at very large sample sizes.\n\nlognormal_large %>% \n  group_by(samples_sizes) %>% \n  summarise(mean = mean(z),\n            var = var(z),\n            skew = mean(z^3),\n            kurtosis = mean(z^4)) %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(mean, var, skew, kurtosis))\n\n\n\n\n\n  \n  \n    \n      samples_sizes\n      mean\n      var\n      skew\n      kurtosis\n    \n  \n  \n    2\n0.00\n0.56\n5.85\n80.45\n    5\n0.00\n0.81\n12.96\n296.29\n    10\n0.04\n1.73\n52.95\n2,398.74\n    20\n−0.03\n0.65\n6.83\n137.13\n    30\n0.03\n1.18\n20.64\n609.30\n    50\n−0.02\n0.61\n2.62\n22.20\n    100\n−0.01\n0.72\n3.49\n38.18\n    1000\n0.03\n1.59\n31.74\n1,236.59\n    10000\n0.01\n0.86\n0.78\n4.35\n  \n  \n  \n\n\n\n\nWhy does this happen? Because we have increased the importance of the tail in our calculations. Every once in a while we will get an extreme event: in the sample mean, this event can be averaged out. Here, even the square of the extreme event can be averaged out. However, this extreme event, when taking its cube or even its quartic value, cannot be so easily averaged out. To do so, we would need many, many more samples than we currently have."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html#more-forgiving-pareto",
    "href": "posts/fat-vs-thin-tails/2020-05-30-central-limit-theorem-in-action.html#more-forgiving-pareto",
    "title": "Central Limit Theorem in Action",
    "section": "More forgiving Pareto",
    "text": "More forgiving Pareto\nNotice that as the Pareto’s tail exponent increases, these problems are mitigated. For example, let’s take a Pareto with \\(\\alpha = 4.1\\) such that both skewness and kurtosis exist:\n\nalpha <- 4.1\n\nmean_pareto <- alpha / (alpha-1)\nvar_pareto <-  alpha / ((alpha-1)^2 * (alpha-2))\n\npareto_forgiving <- simulate_sample_means(rpareto, 3000, ns) %>% \n  rename(pareto = value) %>% \n  group_by(samples_sizes) %>% \n  mutate(z = (pareto - mean_pareto )/sqrt( var_pareto / samples_sizes ) ) %>% \n  ungroup() %>% \n  mutate(samples_sizes = factor(samples_sizes, ordered = TRUE))\n\npareto_forgiving %>% \n  ggplot(aes(z, fill = samples_sizes)) +\n  geom_density(aes(group = NA)) +\n  transition_states(samples_sizes) +\n  stat_function(fun = dnorm, aes(color = 'Normal'),\n                         args = list(mean = 0, sd = 1),\n                inherit.aes = FALSE) +\n  enter_fade() +\n  exit_shrink() +\n  view_follow() +\n  labs(subtitle = \"Sample size {closest_state}\",\n       title = \"Distribution of Sample Mean: Pareto (alpha = 4.1)\",\n       caption = \"Theoretical distirbution is standard normal\",\n       y = \"Sample mean from Pareto\")\n\n\n\n\nAs it can be seen, the problems with the tail diminish as we increase the tail exponent. We can also check the convergence of higher moments:\n\npareto_forgiving %>% \n  group_by(samples_sizes) %>% \n  summarise(mean = mean(z),\n            var = var(z),\n            skewness = mean(z^3),\n            kurtosis = mean(z^4)) %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(mean, var, skewness, kurtosis))\n\n\n\n\n\n  \n  \n    \n      samples_sizes\n      mean\n      var\n      skewness\n      kurtosis\n    \n  \n  \n    2\n0.01\n0.97\n3.26\n23.69\n    5\n−0.02\n0.88\n1.58\n8.07\n    10\n0.01\n0.93\n1.18\n5.27\n    20\n0.00\n0.95\n0.94\n4.28\n    30\n0.04\n1.04\n1.62\n11.86\n    50\n0.01\n0.97\n0.75\n4.64\n    100\n0.00\n0.96\n0.53\n3.34\n    1000\n0.00\n0.96\n0.12\n2.75\n    10000\n0.00\n1.00\n−0.05\n3.06"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-09-what-does-it-mean-to-fatten-the-tails.html",
    "href": "posts/fat-vs-thin-tails/2020-05-09-what-does-it-mean-to-fatten-the-tails.html",
    "title": "What does it mean to fatten the tails?",
    "section": "",
    "text": "First, let’s define what we mean by fatter tails.\n\n\nIntuitively, fat tails distribution are distributions for which their PDFs decay to zero very slowly. So slowly, that extreme values start gaining traction in the determination of the whole distribution. Thus, a distribution is fatter than another one if its PDF takes longer to decay to zero.\n\n\n\nThus, if we wanted to fatten the tails, the intuitive response is to add more mass at the tails such that the PDF takes more time to decay. However, the PDF must still sum up to 1. From where do we take the mass that we are going to put in the tails?\nNassim Taleb has a great chapter in his latest technical book wherein he explains a simple heuristic to fatten the tails by randomly switching from two different gaussians. Along the way, we learn to identify where the tails begin and what exactly is so difficult about Black Swan events.\n\n\n\nCreate a random variable \\(Fattened\\) such that:\n\nwith probability \\(p = \\dfrac{1}{2}\\), \\(X \\sim Normal(0, \\sigma \\sqrt{1-a})\\)\nwith probability \\(1-p =\\dfrac{1}{2}\\), \\(X \\sim Normal(0, \\sigma \\sqrt{1+a})\\)\n\nWith \\(0 \\leq a < 1\\).\nThat is, we create a new random variable with an stochastic standard deviation. Let’s simulate samples from this distribution using \\(\\sigma = 1\\) and \\(a = 0.8\\). We will compare this fattened samples from the samples of a \\(X \\sim Normal(0, 1)\\).\n\n# number of simulated samples\nnumber_samples <- 10000\n\n# simulate number_samples from an uniform\np_s <- runif(number_samples)\n\n# simulate fattened\nfatten_simulations <- function(p_s, a) {\n  \n  # create vector to store sims\n  sims <- vector(length = length(p_s))\n  # sample with probability p\n  p <- p_s < 1/2\n  \n  sims[p] <- rnorm(sum(p), sd = sqrt(1-a))\n  sims[!p] <- rnorm(sum(!p), sd = sqrt(1+a))\n  return(sims)\n}\n\n# simulated fattened\nfattened_normal <- fatten_simulations(p_s, 0.8)\n# simulate normal\nstandard_normal <- rnorm(number_samples)\n\n# plot\ndata.frame(n = seq(1, length.out = number_samples),\n           fattened_normal,\n           standard_normal) %>% \n  pivot_longer(-n, names_to = \"distribution\") %>% \n  ggplot(aes(value)) +\n  geom_density(aes(fill = distribution), alpha = 0.5) +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \" Fattening a distribution\",\n       subtitle = \"Tails and peak gain. Intermediate values lose.\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nTherefore, as we stochastize the variance of the distribution, 3 interesting facts arise:\n\nThe tails of the distribution grow fatter. The PDF at the tails decays to zero more slowly.\nThe peak of the PDF is higher: as Taleb writes, “fatter-tails manifest themselves with higher peaks”.\nIntermediate events are less likely. Note the yellow are above the fattened tail for intermediate values.\n\nThus, we can know answer our question: the probability that the tails gain is taken from the probability of the intermediate events. And then the intermediate events lose even more probability as moderate deviations also gain probability.\n\n\n\nWe have stochastized the variance of the distribution. Thus, the variance of the distribution has a variance. In particular, this variance of the variance grows with \\(a\\): the larger the \\(a\\), the bigger the difference between the two possible variances of the two different distributions from which we will sample. Taleb writes: “a is roughly the mean deviation of the stochastic volatility parameter”.\nTaleb shows this effect on the tails by analyzing the kurtosis of the transformed variable in terms of \\(a\\). The characteristic function is thus:\n\\[ \\phi(t, a) = \\dfrac{1}{2} e^{-\\dfrac{1}{2}(1+a)t^2\\sigma^2}   (1 + e^{at^2\\sigma^2})\\] The fourth derivative evaluated at zero:\n\\[ M(4) = 3 (a^2+1) \\sigma^4 \\]\nThus, the higher the \\(a\\), the larger the kurtosis. Thus, we can tinker with the following idea: if the kurtosis is convex to the scale of the distribution, are tail probabilities also convex to the scale of the distribution?\n\n\n\nThus, to find the size of the response to the scale of the distribution, let’s analyze tail probabilities as we grow the parameter \\(a\\) which controls the scale of the distribution. Thus, we find the sensitivies of the tail probabilities to perturbations in the variance and we can find out if tail probabilities are indeed convex to the scale of the distribution.\nLet’s run the experiments:\n\nstandard_normal <- data.frame(standard_normal)\n# create the different values for a\na_s <- seq(0.1, 0.9, by = 0.2)\nnames(a_s) <- unlist(map(a_s, ~ glue::glue(\"a = {.x}\")))\n\na_s %>%\n  # evaluate the function for different values of a\n  map_df(~ fatten_simulations(p_s, a = .x)) %>% \n  mutate(sim = seq(1, number_samples)) %>% \n  pivot_longer(-sim, names_to = \"a\") -> different_as\ndifferent_as %>% \n  # plot the distributions\n  ggplot(aes(value)) +\n  geom_density(aes(fill = a,\n                     color = a), alpha = 0.5) +\n  geom_density(data = standard_normal,\n               aes(standard_normal)) +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() +\n  facet_wrap(~a, ncol = 3) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"Fattening a distribution\",\n       subtitle = \"Higher a, higher stochastic volatility. Non linear fattening response.\",\n       caption = \"Standard normal in black line.\")\n\n\n\n\nAnd now with all curves in one plot:\n\ndifferent_as %>% \n   ggplot(aes(value)) +\n  geom_density(aes(fill = a), alpha = 0.5) +\n  geom_density(data = standard_normal,\n               aes(standard_normal)) +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Fattening a distribution\",\n       subtitle = \"Higher a, higher stochastic volatility. Non linear fattening response.\") \n\n\n\n\nThus, given the non-linear response of the tail probabilities to changes in \\(a\\), we can conclude, as Taleb does with derivatives, that tail probabilities are indeed convex to the scale of the distribution.\n\n\n\nWith this insight, we can respond the important question: where do the tails begin?. The tails are the points at the extremes where the probabilities are convex to the stochastic volatility. That is, where changes in the stochastic volatility generate non-linear changes in the probability. Thus, we arrive at a problem: probability estimation at the tails is unreliable. If we cannot reliably estimate the standard deviation, then the errors will propagate non-linearly into the estimation of tail probabilities.\nTherefore, Taleb concludes that the problem of Black Swans arises not only from the large size of the possible deviations, but also from the unreliability with which we can estimate the probability of these events."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html",
    "href": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html",
    "title": "Probability Calibration under fat-tails: useless",
    "section": "",
    "text": "Probability calibration refers to a manner of evaluating forecasts: the forecast frequency of an event should correspond to the correct frequency of the event happening in real life. Is this truly the mark of a good analysis? Under fat-tails, Nassim Taleb in his book answer with a categorical response NO!"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#probability-calibration-in-the-real-world",
    "href": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#probability-calibration-in-the-real-world",
    "title": "Probability Calibration under fat-tails: useless",
    "section": "Probability calibration in the real world",
    "text": "Probability calibration in the real world\nProbability calibration amounts, in the real world, to a binary payoff: a fixed sum is paid off if the event happens. If one wants to hedge the risk of a fat-tailed variable, the question is then: which lump sum?\n\nThe answer: there is no possible lump sum that can hedge the exposure to a fat-tailed variable. The reason is the same as to why single-point forecasts are useless:\n\nThere is no typical collapse or disaster, owing to the absence of characteristic scale\n\nTherefore, given that there is no characteristic scale for fat-tailed variables, one cannot know in advance the size of the collapse nor how much the lump sum of the binary payoff should be."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#monte-carlo-simulation",
    "href": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#monte-carlo-simulation",
    "title": "Probability Calibration under fat-tails: useless",
    "section": "Monte Carlo simulation",
    "text": "Monte Carlo simulation\nA quick Monte-Carlo Simulation should do the trick to understand why fat-tailed variables have no characteristic scale. Imagine you are exposed to certain losses. You take a lump-sum insurance. Let’s simulate the possible losses that you may incur if the exposure is a lognormal. For low-values of \\(\\sigma\\), the lognormal behaves as a Gaussian. For higher values, it behaves like a fat-tailed variable.\n\nLog normal, sigma = 0.2\nWith a log normal of sigma = 0.2, a lump-sum of 2 will absolutely cover any of the losses. The reason: the MDA is Gumbel that decays pretty rapidly. Therefore, the sample maxima is effectively bounded at large values from the mean.\n\n\n\n\n\nWhereas if we are exposed to a Pareto 80/20, there’s no lump sum that can covers us. The MDA is a Fréchet that decays as a power law:\n\n\n\n\n\nGiven this lack of characteristic scale, there should not be any prize for saying that a there will be a loss larger than \\(K\\). With fat-tailed variables, almost any larger value is likely. Indeed, fat-tailed variables are long-tailed variables and thus share the following property:\n\\[\n\\lim_{x \\to \\infty} \\Pr[X>x+t\\mid X>x] =1\n\\]\nThat is, reducing the variability of a fat-tailed random variable to a binary payoff makes no sense. Therefore, probability calibration with fat-tailed variables makes no sense."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#conclusion",
    "href": "posts/fat-vs-thin-tails/2020-06-24-probability-calibration-under-fat-tails-useless.html#conclusion",
    "title": "Probability Calibration under fat-tails: useless",
    "section": "Conclusion",
    "text": "Conclusion\n\nYou do not eat forecasts, most business have severly skewed payoffs, so being calibrated in probability is meaningless."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html",
    "href": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html",
    "title": "Gini Index under Fat-Tails",
    "section": "",
    "text": "I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In this blogpost, I’ll follow Taleb’s exposition of the Gini Index under fat-tails in Chapter 13 of his book.\nIntuitively, if we use the “empirical distribution” to estimate the Gini Index, under fat-tails, we underestimate the tail of the distribution and thus underestimate the Gini index. This is yet another example of how we fool ourselves when we are using the “empirical” distribution. Instead, Taleb recommends first understanding the tail behavior of the “Gini Index” by estimating the tail index of the distribution with Maxmimum Likelihood and then using the functional form of the maximum likelihood estimator for the Gini Index."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#what-is-the-gini-index",
    "href": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#what-is-the-gini-index",
    "title": "Gini Index under Fat-Tails",
    "section": "What is the Gini Index?",
    "text": "What is the Gini Index?\nThe Gini index is a measure of concentration commonly used in the income and wealth inequalities discussions. The stochastic representation of the Gini \\(g\\) is:\n\\[\ng=\\frac{1}{2} \\frac{\\mathbb{E}\\left(\\left|X^{\\prime}-X^{\\prime \\prime}\\right|\\right)}{\\mu} \\in[0,1]\n\\]\nwhere ( X^{} ) and ( X^{} ) are i.i.d. copies of a random variable ( X ) with c.d.f. ( F(x) (2020-06-26-gini-index-under-fat-tails_files/figure-html/lognormal-1.png){width=768} ::: :::\n\nFat-tails: Non-parametric estimator\nHowever, when the data-generating process of \\(X\\) is in the MDA of the Fréchet (i.e., it’s a fat-tailed variable), the non-parametric estimator of the Gini Index loses its properties of normality. Indeed, the limiting distribution of the non-parametric index becomes a skewed-to-the-right \\(\\alpha\\)-stable law. Thus, the non-parametric estimate underestimates the true Gini Index.\nThis can be checked with Monte-Carlo simulations. I’ll perform \\(10^4\\) Monte-Carlo experiments: in each of them, I’ll generate a 1000 samples from a Pareto with \\(\\alpha = 1.16\\). Then, I’ll calculate the Gini estimate using the non-parametric estimator.\nGiven this Pareto, the “true” Gini is thus:\n\\[\ng = \\dfrac{1}{2\\alpha -1} = 0.7575758\n\\]\n\nrpareto <- function(n) {\n    alpha <- 1.16\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\ncrossing(experiment = 1:10^4,\n         sample_size = 1000) %>% \n  mutate(data = map(sample_size, ~ rpareto(.)),\n         gini = map_dbl(data, ~ gini(.))) -> gini_pareto\n\ngini_pareto %>% \n  ggplot(aes(gini)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  geom_vline(aes(xintercept = 0.7575758), color = \"red\", linetype = 2) +\n  annotate(\"text\", x = 0.78, y = 400, label = \"True gini\", color = \"red\", \n           family = theme_get()$text[[\"family\"]]) +\n  labs(title = \"Non parametric Gini estimator\",\n       subtitle = \"Under fat-tails, non parametric estimator is skewed to the right. Thus, downward bias\",\n       caption = \"Data generating process is Pareto with alpha = 1.16\")\n\n\n\n\nTherefore, under fat-tails, the non-parametric Gini estimator will approach its true value more slowly, and from below."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#the-maximum-likelihood-alternative",
    "href": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#the-maximum-likelihood-alternative",
    "title": "Gini Index under Fat-Tails",
    "section": "The Maximum Likelihood alternative",
    "text": "The Maximum Likelihood alternative\nA better alternative when working with fat-tails, it’s to first estimate the tail and then derive your quantity of interest. Indeed, one does not need too much data to derive the properties of the tail. With a Pareto, for example, the Maximum Likelihood estimator for the tail exponent follows an inverse Gamma distribution that rapidly converges to a Gaussian tightly around the true \\(\\alpha\\). Therefore, one can reliably estimate the tail exponent of the Pareto and thus understand the properties of the distribution with relatively few data.\nThe ML estimator for the tail exponent of a Pareto is thus:\n\\[\n\\widehat \\alpha = \\frac{n}{\\sum _i  \\ln (x_i) }\n\\] Then, we can derive our Maximum Likelihood estimate for the Gini Index:\n\\[\ng = \\dfrac{1}{2\\widehat \\alpha -1}\n\\]\nIndeed, Taleb shows that this estimator for the Gini Index is not just asymptotically normal, but also asymptotically efficient. We can test for these using our Monte-Carlo simulations. For each of our simulated datasets, we can derive our Maximum Likelihood estimate and then derive our Maximum Likelihood estimate for the Gini Index.\n\nestimate_alpha_ml <- function(observations) {\n  alpha <- length(observations)/sum(log(observations))\n  if (alpha < 1) {\n    alpha <- 1.0005 \n  }\n  alpha\n}\n\ngini_pareto %>% \n  mutate(alpha_ml = map_dbl(data, ~ estimate_alpha_ml(.)),\n         gini_ml = 1/(2*alpha_ml - 1)) -> gini_pareto\n\ngini_pareto %>%\n  rename(nonparametric = gini,\n         maximum_likelihood = gini_ml) %>% \n  pivot_longer(c(nonparametric, maximum_likelihood), names_to = \"estimator\", values_to = \"gini\") %>% \n  ggplot(aes(gini, fill = estimator)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", alpha = 0.7,\n                 position = \"identity\") +\n  geom_vline(aes(xintercept = 0.7575758), color = \"red\", linetype = 2) +\n  annotate(\"text\", x = 0.78, y = 1000, label = \"True gini\", color = \"red\", \n           family = theme_get()$text[[\"family\"]]) +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Comparison of Gini estimators: Non-parametric vs Maximum Likelihood\",\n       subtitle = \"Under fat-tails, unlike the non parametric estimator, Max Likelihood estimate is still asymptotically normal\",\n       caption = \"Data generating process is a Pareto with alpha = 1.16\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#conclusion",
    "href": "posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html#conclusion",
    "title": "Gini Index under Fat-Tails",
    "section": "Conclusion",
    "text": "Conclusion\nWhen the underlying distribution is fat-tailed, which is always in the case of income or wealth, the non-parametric estimator for the Gini index is skewed to the right and thus underestimates the true Gini index. In this case, it is a much statistically sound strategy to first estimate the tail behavior of the distribution with Maximum Likelihood and then estimate the Gini Index with its plug-in estimator."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "",
    "text": "For fat-tailed random variables, the statistical properties are determined by a few observations in the tail. In Nassim Taleb’s words, “the tail wags the dog”. Therefore, it is vital to study the distribution of these few observations. A logical question to ask, then, is: is there a limiting distribution for the sample maxima as the number of samples grows? This is precisely what the Fisher Tippet Theorem states: the limiting distribution of (a normalized) sample maxima is the Generalized Extreme distribution (GED).\nIn this blogpost, I’ll run Monte-Carlo simulations to show the Fisher-Tippet Theorem and its consequences."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#the-fisher-tippet-theorem",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#the-fisher-tippet-theorem",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "The Fisher-Tippet Theorem",
    "text": "The Fisher-Tippet Theorem\n\nHow to model the values at the tail? Model the sample maxima\n\nYou have \\(n\\) \\(i.i.d.\\) samples from a distribution: \\(X_1, X_2, \\cdots, X_n\\). Define, for a given \\(n\\), the sample maximum \\(M_n = Max(X_1, X_2, \\cdots, X_n)\\). The theorem states that if there are normalizing constants \\(a_n\\) and \\(b_n\\) such that the distribution of the sample maximum is not degenerate, then the limiting distribution of the normalized sample maxima is the GED. In math:\n\\[  \\dfrac{(M_n - b_n)}{a_n} \\xrightarrow[]d{} GED(\\xi) \\] So, the question remains: What is the \\(GED(\\xi)\\)?"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#getting-to-know-the-generalized-extreme-distribution",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#getting-to-know-the-generalized-extreme-distribution",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "Getting to know the Generalized Extreme Distribution",
    "text": "Getting to know the Generalized Extreme Distribution\nThe Generalized Extreme Distribution (GED) is really a way to describe with one shape parameter \\(\\xi\\) three different distributions.\n\nWhen \\(\\xi > 0\\), the distribution is a Fréchet.\nWhen \\(\\xi < 0\\), the distribution is a Weibull.\nWhen \\(\\xi = 0\\), the distribution is a Gumbel.\n\nLet’s see how different these distributions really are through some Monte-Carlo simulations. I’ll generate 10^4 samples for each type of the distributions\n\nlabels <- c(glue::glue(\"Fréchet, xi = 0.5\"),\n            \"Weibull, xi = -1\",\n            \"Gumbel, xi = 0\")\nn <- 10^4\ndata.frame(frechet = rgev(n, shape = 0.5),\n           weibull = rgev(n, shape = -1),\n           gumbel  = rgev(n, shape = 0),\n           sim = 1:10^4) -> exploring_ged\n\n\n\n\n\n\nIndeed, as can be seen from the plot, the GED encodes very, very different distributions. Therefore, the limiting distribution of the sample maxima can be very different. Most importantly, the Fréchet has a much longer and fatter tail than any of the distribution. Indeed, the Fréchet’s tail decreases just as a power law with tail exponent \\(\\alpha = \\xi^{-1}\\). That is, the Fréchet is fat-tailed and the higher the \\(\\xi\\), the fatter the tail of the Fréchet.\nThe question is: what type of distributions have their sample maximum distributed as a Fréchet?"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#any-power-law-tail-leads-to-the-fréchet",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#any-power-law-tail-leads-to-the-fréchet",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "Any power law tail leads to the Fréchet",
    "text": "Any power law tail leads to the Fréchet\nUnsurprisingly, fat-tailed distributions have fat-tailed distributed (normalized) sample maxima: i.e., their sample maxima have as a limiting distribution the Fréchet. In math slang, the the Maximum Domain of Attraction of any fat-tailed variable is the Fréchet, which itself is fat-tailed.\nWith Monte-Carlo simulations, we can see this. We will produce 10,000 experiments: each experiment has \\(n\\) random samples from a Pareto with \\(\\alpha = 2\\). For each experiment, we get the maximum of the sample and we “normalize” it with \\(a_n = n^{1/\\alpha}\\) and \\(b_n = 0\\). Then, we will see that these normalized sample maxima follow a Fréchet distribution with the same \\(\\alpha\\) as the original Pareto random variable.\nWe will repeat these experiments for \\(n = 10, 100, 1000\\). The results can be seen in the following plot:\n\nalpha <- 2\nrpareto <- function(n) {\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\ncrossing(experiment = 1:10000, n = c(10, 50, 100, 1000, 10000)) %>%\n  mutate(samples = map(n, ~ rpareto(.)), # sample\n         max_in_sample = map_dbl(samples, ~ max(.))) %>%  # get the maximum\n  rowwise() %>% \n  mutate(max_in_sample_normalized = max_in_sample / n^(1/alpha)) -> sample_maximum_pareto # normalize\n\n\n\n\n\n\nTo reiterate the result: we get sample maxima from power law distributions. These sample maxima themselves also follow a power law distribution with the same tail exponent as the tail exponent of the original random variable."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#light-tails-upper-bounded",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#light-tails-upper-bounded",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "Light tails upper Bounded",
    "text": "Light tails upper Bounded\nFor light-tailed random variables with an upper bound, their sample maxima will have as a limiting distribution the Weibull distribution That is, the \\(GED(\\xi < 0)\\). Take for example, the standard continuous uniform on \\([0, 1]\\). We normalize the sample maxima with \\(a_n = 1/n\\) and \\(b_n = 1\\).\nWe repeat the same Monte-Carlo experiments as we did before. The results can be seen in the following plot:\n\ncrossing(experiment = 1:10000, n = c(10, 50, 100, 1000, 10000)) %>%\n  mutate(samples = map(n, ~ runif(.)), # sample\n         max_in_sample = map_dbl(samples, ~ max(.))) %>%  # get the maximum\n  rowwise() %>% \n  mutate(max_in_sample_normalized = (max_in_sample-1) / (1/n) ) -> sample_maximum_uniform # normalize\n\n\nsubtitle <- TeX(\"Maximum Domain of Attraction of Standard Uniform is GED with $\\\\xi = -1$. QQ-plot comparing sample maxima and Weibull with $\\\\alpha = 1$\")\nxtitle <- TeX(\"Theoretical quantiles from a Weibull $\\\\alpha = 1$\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#thin-tailed-variables-exponential-tail",
    "href": "posts/fat-vs-thin-tails/2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html#thin-tailed-variables-exponential-tail",
    "title": "Fisher Tippet Th: a “CLT” for the sample maxima",
    "section": "Thin-tailed variables: exponential tail",
    "text": "Thin-tailed variables: exponential tail\nFor thin-tailed variables, their sample maximum has as a limiting distribution the Gumbel case. That is, \\(GED(\\xi = 0)\\). Take for example the Standard Normal (\\(CDF = \\Phi(x)\\)). Their sample maximum will be normalized with \\(b_n = \\Phi^{-1}(1-1/n)\\) and \\(a_n = 1/b_n\\).\nWe repeat the same Monte-Carlo experiments as we did before. The results can be seen in the following plot:\n\ncrossing(experiment = 1:10000, n = c(10, 50, 100, 1000, 10000)) %>%\n  mutate(samples = map(n, ~ rnorm(.)), # sample\n         max_in_sample = map_dbl(samples, ~ max(.))) %>%  # get the maximum\n  rowwise() %>% \n  mutate(max_in_sample_normalized = (max_in_sample-qnorm(1-1/n, mean =0, sd = 1)) / (1/qnorm(1-1/n, mean =0, sd = 1)) ) -> sample_maximum_normal # normalize\n\n\n\n\n\n\nNotice that convergence takes longer for the sample maximum coming from a normal than the convergence of sample maxima coming from other types of distributions. Thus, Nassim Taleb, in his latest technical book: Statistical Consequences of Fat Tails, recommends working with the exact distribution of the sample maxima, instead of working with the asymptotic approximation (i.e., the Gumbel)."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-04-27-spurious-pca-under-thick-tails.html",
    "href": "posts/fat-vs-thin-tails/2020-04-27-spurious-pca-under-thick-tails.html",
    "title": "Spurious PCA under Thick Tails",
    "section": "",
    "text": "PCA is a dimensionality reduction technique. It seeks to project the data onto a lower dimensional hyperplane such that as much of the original data variance is preserved. The underlying idea is that the vectors creating these lower dimensional hyperplanes reflect a latent structure in the data. However, what happens when there is no structure at all?\nIn his most recently published technical book, Taleb examines this question under two different regimes: Mediocristan and Extremistan. That is, he examines what PCA ends up doing when the data is totally random (i.e., come from i.i.d. random variables); where the randomness can come from a thin-tail, like a Gaussian, or from a thick-taile r.v.\nIn this post, I’ll try to replicate his findings using a small Monte Carlo experiment in R.\n\n\nWhen the data comes from uncorrelated random variables, there’s absolutely no structure in the data. That is, no axis of variation should explain the data more than any other axis. Thus, Taleb writes:\n\nIn the simulation, the data that has absolutely no structure: principal components (PCs) should be all equal…\n\nHowever, he continues to say that this will only happen if we have enough data:\n\nBut if there is not enough data there is an illusion of what the structure is. As we increase the data (the n variables), the structure becomes ﬂat\n\n\n\n\n\n\n\npca <- function(df) {\n  df %>% \n    prcomp(retx = FALSE, rank. = 30) -> df\n  df$sdev[1:30] %>% \n    tibble(component = seq(1, 30), 'var_explained' = .) %>%\n    mutate(var_explained = var_explained^2) -> pca\n  pca\n}\n\nLet’s try to replicate this behavior in R. I’ll create 30 i.i.d. gaussians. First, I’ll sample 100 observations. Secondly, I’ll sample 1,000 observations. For each of these two simulated datasets, I’ll perform PCA and store the relative importance assigned to each Principle component.\n\ngaussian_small <- rerun(30, rnorm(100)) %>% \n  bind_cols()\n\ngaussian_large <- rerun(30, rnorm(10^6)) %>% \n  bind_cols()\n\npca(gaussian_small) %>% \n  left_join(pca(gaussian_large), by = c(\"component\"), suffix = c(\"_Small Sample\", \"_Large Sample\")) %>% \n  pivot_longer(cols = starts_with(\"var\"), names_prefix = \"var_explained_\") %>% \n  rename(\"sample\" = name, \n         \"variance\" = value) -> pcas_gaussian\n\nNow, let’s plot the results and check the different importances assigned to each of the principle components:\n\npcas_gaussian %>%\n  ggplot(aes(x = component, y = variance, fill = sample)) +\n    geom_col(color = \"black\", alpha = 0.8) +\n  facet_wrap(~sample) +\n  theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  guides(fill = FALSE) +\n  labs(y = \"Variance\",\n       x = \"PCA Component\",\n       title = \"PCA Ranking on uncorrelated randomness from Mediocristan\",\n       subtitle = \"Large sample washs away spurious correlation\",\n       caption = \"There's absolutely no structure in the data. That is, no axis of variation should explain the data more than any other axis\") \n\n\n\nggsave(\"mediocristan.jpg\")\n\nGreat! I got the same results as Taleb did. In his own words, the results are explained thus:\n\nWe can see the “ﬂatter” PCA structure with the Gaussian as n increases (the difference between PCAs shrinks).\n\nThus, with reasonably enough data, the spurious correlation between the total variance in the data and the different axes of variations identified by the algorithm wash away pretty quickly. Let’s see what happens under thick-tails.\n\n\n\nJust as before, I’ll create 30 random variables. With the difference being that this time the random variables will be Stabled Distributions with \\(\\alpha = \\dfrac{3}{2}, \\beta = 1, \\mu = 0, \\sigma = 1\\). Just as before, I’ll create two datasets: one with 100 observations and other one with 1,000. For each of these, I’ll perform PCA and store the relative importance given to the identified PCAs.\n\nstable_small <- rerun(30, rstable(100, alpha = 3/2, beta = 1, gamma = 1, delta = 0)) %>% \n  bind_cols()\n\nstable_large <- rerun(30, rstable(10^6, alpha = 3/2, beta = 1, gamma = 1, delta = 0)) %>% \n  bind_cols() \n\npca(stable_small) %>% \n  left_join(pca(stable_large), by = c(\"component\"), suffix = c(\"_Small Sample\", \"_Large Sample\")) %>% \n  pivot_longer(cols = starts_with(\"var\"), names_prefix = \"var_explained_\") %>% \n  rename(\"sample\" = name, \n         \"variance\" = value) -> pcas_stable\n\nLet’s plot the results:\n\npcas_stable %>% \n  ggplot(aes(x = component, y = variance, fill = sample)) +\n    geom_col(color = \"black\", alpha = 0.8) +\n  facet_wrap(~sample) +\n  theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  guides(fill = FALSE) +\n  labs(x = \"PCA component\",\n       y = \"Variance\",\n       title = \"PCA Ranking on uncorrelated randomness from Extremistan\",\n       subtitle = \"Spurious correlations do not wash away so easily\",\n       caption = \"There's absolutely no structure in the data. That is, no axis of variation should explain the data more than any other axis\")\n\n\n\nggsave(\"extremistan.jpg\")\n\nWheras before in Mediocristan the algorithm correctly identified that no axis of variation could possibly explain more than any other axis the total variation in the data, now the algorithm is fooled by the randomness coming from the thick-tail variables. Indeed, Taleb writes:\n\nThe small sample effect causes the ordered PCAs to show a declining slope. We have zero correlation on the matrix. For a thick tailed distribution (the lower section), we need a lot more data for the spurious correlation to wash out i.e., dimension reduction does not work with thick tails."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-04-17-fat-vs-thin-does-lln-work.html",
    "href": "posts/fat-vs-thin-tails/2020-04-17-fat-vs-thin-does-lln-work.html",
    "title": "Fat vs Thin: does LLN work?",
    "section": "",
    "text": "Statistical estimation is based on the LLN and CLT. The CLT states that the sampling distribution will look like a normal. The LLN that the variance of the normal will decrease as our sampling size increases.\n\nOr so does Nassim Nicholas Taleb says in his recently published technical book, wherein he explains how common practice statistical methodology breaks down under the Extremistan regime. That is, under fat tails.\nIn this post, I’ll try my best to understand why is he right. The key idea, I believe, when extremely rare things can happen, they alone determine the distribution. Thus, one cannot hope to understand it with normal samples.\n\n\nTaleb begins by giving an intuitive explanation. Imagine that you are working with a sample. You observe a large deviation from the values that you have. Is this deviation caused by one big observation or multiple medium deviations.\n\nAssume a large deviation X. - In Mediocristan, the probability of sampling higher than X twice in a row is greater than sampling higher than 2X once. - In Extremistan, the probability of sampling higher than 2X once is greater than the probability of sampling higher than X twice in a row.\n\nLet’s try to replicate what he did in R. He proposes to analyze the ratio of observing exceeding deviations in terms of sigmas for a normal. For example, the ratio of the Survival of two 2-sigma events to the Survival one 4-sigma event. Then, he plots this ratio for different values of \\(\\sigma\\). Let’s do it in R.\n\nx <- seq(0, 3.45, 0.05)\n\ntwo_x <- x*2\n\nratio <-((1 -pnorm(x))^2)/(1-pnorm(two_x))\n\ntibble(ratio) %>% \n  cbind(x) %>% \n  rename('k' = x) %>% \n  mutate(k = as.double(k)) %>% \n  arrange(k) %>% \n  ggplot(aes(x = k, y = ratio)) +\n    geom_point(color = 'dodgerblue4') +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"k in terms of sigma\",\n       title = \"Mediocristan\", \n       subtitle = \"Ratio of S(X)^2/S(2X) from a Standard Normal\")\n\n\n\n\nThus, it is almost impossible to fathom a 4-sigma event with a standard normal. It is much more likely that our deviation comes from two 2 sigma events. Thus, the distribution can be inferred disregarding the extreme. However, not all distributions work the same way. Let’s see how the LLN works differently for different types of random variables.\n\n\n\nWhat are the consequences of these differences? Imagine the law of large numbers. We are sampling to estimate the mean. With a standard normal, even if we get a deviation, it will be a mild one and it will probably come from some consecutive observations. Thus, with enough observations, the weighted contribution to the mean of these observations won’t derail our estimates.\nHowever, in Extremistan this does not play out; at least, not quick enough. Given the size of the deviations we get every once in a while, they alone will determine our estimates; making the rest of data meaningless.\nLet’s check this with some simulations.\n\n\nIn Mediocristan, given the tamed randomness that we are dealing with, we do not expect large deviations to play a significant role. As we said before, we will only have medium deviations from the mean that are smooth over pretty quickly as their weighted contribution to the average drops. That is, the LLN is with a relatively small number of observations.\n\nsamples <- 10000\n\nthin <-rnorm(samples, sd = 20)\n\nfat <- rlnorm(samples, sdlog = 5,meanlog = 0)\n\ncumulative_mean <- function(numbers) {\n    x <- seq(1, length(numbers))\n    cum_mean <- cumsum(numbers)/x \n    cum_mean\n}\n\n\nthin_cum_mean <- cumulative_mean(thin)\n\nthin_cum_mean %>%\n  tibble(running_mean = .) %>% \n  add_rownames(var = 'number_samples') %>% \n  mutate(number_samples = as.double(number_samples)) %>% \n  arrange(number_samples) %>% \n  ggplot(aes(x = number_samples, y = running_mean)) +\n    geom_line(color = 'dodgerblue4') +\n    geom_hline(yintercept = 0, linetype = 2, color = 'red') +\n  hrbrthemes::theme_ipsum_rc(grid = 'Y') +\n  scale_x_continuous(labels = scales::comma) +\n  labs(x = \"# of samples\",\n       title = \"High variance (20) Gaussian\", \n       subtitle = \"Sample is helpful. Red line is mean.\")\n\n\n\n\n\n\n\nHowever, in Extremistan we are on an entirely different regime. As large deviations determine the whole distribution, most of the information we get about the distribution is pure noise that won’t be helpful to estimate the mean. Indeed, Taleb writes:\n\nThe mean of a distribution will rarely correspond to the sample mean; it will have a persistent small sample effect (downward or upward)…\n\n\nsamples <- 1000\n\nfat <- 1/(runif(samples))^(1/1.13)\n\nfat_cum_mean <- cumulative_mean(fat)\n\nmean <- 1.13/0.13\n\nfat_cum_mean %>% \n  tibble(running_mean = .) %>% \n  add_rownames(var = 'number_samples') %>% \n  mutate(number_samples = as.double(number_samples)) %>% \n  arrange(number_samples) %>% \n  ggplot(aes(x = number_samples, y = running_mean)) +\n    geom_line(color = 'dodgerblue4') +\n  hrbrthemes::theme_ipsum_rc(grid = 'Y') +\n  scale_y_continuous(labels = scales::comma) +\n  geom_hline(yintercept = mean, linetype = 2, color = 'red') +\n  labs(title = 'Extremistan, Pareto 80/20',\n       subtitle = 'Sample mean is uninformative. Red line is mean')\n\n\n\n\n\n\n\n\nIn the same way, when a distribution is dominated by its extremes, it will take much, much longer for the sampling distribution to compress than it would with your average Gaussian.\n\n\n\ngaussian_mean <- function(sample_size) {\n  \n  samples <- rnorm(sample_size)\n  return(mean(samples))\n}\n\nmean_30 <- purrr::rerun(1000, gaussian_mean(30)) %>% \n  unlist()\n\nmean_1 <- rnorm(1000)\n\ntibble(iteration = seq(1, 1000, 1), mean_30, mean_1) %>% \n  pivot_longer(cols = starts_with(\"mean\")) %>%  \n  ggplot(aes(x = value, fill = name)) +\n  geom_density(alpha = 0.6) +\n  hrbrthemes::theme_ipsum_rc() +\n  hrbrthemes::scale_fill_ft() +\n  labs(subtitle = \"Distribution compress very quickly\",\n       title = \"Sample distribution for the mean of Gaussian\")\n\n\n\n\n\n\n\n\npareto_mean <- function(sample_size) {\n\n  fat <- 1/(runif(sample_size))^(1/1.13)\n  mean(fat)\n}\n\nmean_300 <- purrr::rerun(1000, pareto_mean(300)) %>% \n  unlist()\n\nmean_500 <- purrr::rerun(1000, pareto_mean(500)) %>% \n  unlist()\n\ntibble(iteration = seq(1, 1000, 1), mean_300, mean_500) %>% \n  pivot_longer(cols = starts_with(\"mean\")) %>%  \n  ggplot(aes(x = value, fill = name)) +\n  geom_density(alpha = 0.6) +\n  scale_x_continuous(limits = c(0, 30)) +\n  hrbrthemes::theme_ipsum_rc() +\n  hrbrthemes::scale_fill_ft() +\n  labs(title = \"Pare 80/20 mean for 300 and 500 obs\",\n       subtitle = \"The sampling distribution for the mean does not compress\")\n\n\n\n\n\n\n\n\nWhen a distribution is determined by a rare event, then, by definition, you will almost never observe its true properties from a sample. Also, the intuition that we gain from studying the Gaussian dulls our senses: it conditions us to a type of tame randomness that goes in the total opposite direction."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "",
    "text": "With fat-tailed random variables, as Nassim Taleb says, the tail wags the dogs. That is, “the tails (the rare events) play a disproportionately large role in determining the properties”. Following the presentation given by Taleb in his latest technical book: Statistical Consequences of Fat Tails, I’ll show:"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#why-does-the-empirical-distribution-fool-us",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#why-does-the-empirical-distribution-fool-us",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "Why does the empirical distribution fool us?",
    "text": "Why does the empirical distribution fool us?\nThe tails play a disproportionate role in defining the theoretical moments for fat-tailed distributions. However, if we are working with the non-parametric “empirical distribution”, we are effectively cutting the tail at our sample maximum. The rest of the tail, the possible values larger than our sample maximum, are taken out of the equation when estimating any moment through the “empirical” distribution. This hidden contribution to the theoretical mean that does not appear in the sample, however, is precisely the most important to define the theoretical moment that we are trying to estimate. Thus, our estimates with the “empirical” distribution will be terrible.\nInstead of using the “empirical distribution”, what one should attempt is an intelligent extrapolation to take into consideration future maxima and their influence in our estimate. This can be done, in the case of a Pareto distribution, by estimating the tail exponent \\(\\alpha\\) and then plug-in our estimated alpha to estimate the mean."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#visualizing-the-invisible-tail",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#visualizing-the-invisible-tail",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "Visualizing the invisible tail",
    "text": "Visualizing the invisible tail\nThe tails contribute the most for any theoretical moment of any fat-tailed variable. However, when we work with the “empirical” distribution, we are ignoring the contribution of the tail beyond our sample maximum. Graphically, Taleb shows it thus:\n\n\n\nHidden contribution to the p-moment\n\n\nTaleb also shows how this ignorance of the tail is most worrisome the fatter the distribution:\n\n\n\nThe fatter, the worse is the mistake of the empirical"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#estimating-the-tail-first-then-the-mean",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#estimating-the-tail-first-then-the-mean",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "Estimating the tail first, then the mean",
    "text": "Estimating the tail first, then the mean\nBy definition the tail exponent tells us information about the tail. Specifically, about the survival’s function rate of decay. Therefore:\n\nThe tail exponent \\(\\alpha\\) captures, by extrapolation, the low probability deviation not seen in the data, but that plays a disproportionately large share in determining the mean.\n\nThus, once one has taken into account the hidden tail’s influence with the estimated \\(\\widehat \\alpha\\), we can produce a less unreliable estimate of the mean (or other higher moments). However, care must be taken: with a Pareto, the mean is hardly what matters. What is really important here is the idea of first figuring out the properties of the fat-tailed distribution and then trying to estimate things."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#maximum-likelihood",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#maximum-likelihood",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nFor a Pareto with known minimum observation 1, things are pretty straightforward. As it is often the case in statistics, the answer is maximum likelihood. Just posit a likelihood for your data, take the log, differentiate w.r.t \\(\\alpha\\) and you have your estimate:\n\\[ L(\\alpha) = \\prod_{i=1}^n \\alpha \\frac {1} {x_i^{\\alpha+1}} = \\alpha^n \\prod_{i=1}^n \\frac {1}{x_i^{\\alpha+1}}. \\]\n\\[ \\ell(\\alpha) = n \\ln \\alpha  - (\\alpha + 1) \\sum_{i=1} ^n \\ln x_i. \\] \\[ \\widehat \\alpha = \\frac{n}{\\sum _i  \\ln (x_i) }\\] Luckily, this maximum likelihood estimate for \\(\\alpha\\) works reasonably well with relatively small amounts of data. Why? Because \\(\\widehat \\alpha\\) follows an Inverse gamma distribution with shape parameter equal to \\(n\\) and scale parameter equal to \\(\\alpha n\\). Although biased, the distribution of the estimator rapidly converges to a normal distribution tightly around the true \\(\\alpha\\). Therefore, one can reliably estimate the tail exponent of the Pareto and thus understand the properties of the distribution with relatively few data.\nOnce we have an estimate for \\(\\widehat \\alpha\\), our estimate for the mean will be \\(\\dfrac{\\widehat \\alpha}{ \\widehat \\alpha - 1 }\\). This is the plug-in estimator for the mean.\n\nMaximum likelihood in practice\nTo demonstrate the superiority of the maximum likelihood and plug in estimator approach to the sample mean of an empirical distribution, I’ll simulate 10^5 Monte-Carlo experiments. For each experiment, I’ll sample \\(n\\) observations from a Pareto with \\(\\alpha = 1.2\\) and theoretical mean \\(\\dfrac{1.2}{1.2 - 1} = 6\\). Then, I’ll produce the maximum likelihood estimate for the tail exponent and an estimate of the mean using our plug-in estimator. At the same time, I’ll produce the regular sample mean for each experiment considering the “empirical distribution”. Finally, I’ll compare the resulting distribution of both the sample mean and the mean from the plug-in estimator.\nI’ll repeat this for both \\(n = 100, 1000\\)\n\nalpha <- 1.2\nrpareto <- function(n) {\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\nestimate_alpha_ml <- function(observations) {\n  alpha <- length(observations)/sum(log(observations))\n  if (alpha < 1) {\n    alpha <- 1.0005 \n  }\n  alpha\n}\n\ncrossing(experiment = 1:10^5, sample_size = c(100, 1000)) %>% \n  mutate(data = map(sample_size, ~ rpareto(.)),\n         mean_sample = map_dbl(data, ~ mean(.)),\n         alpha_ml = map_dbl(data, ~ estimate_alpha_ml(.)),\n         mean_ml = alpha_ml / (alpha_ml - 1) ) -> simulations_result"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#maximum-likelihoods-alpha-distribution",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#maximum-likelihoods-alpha-distribution",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "Maximum likelihood’s alpha distribution",
    "text": "Maximum likelihood’s alpha distribution\nFrom relatively few observations, we can reliably estimate the tail exponent of the distribution.\nThis goes against the usual comment that with fat-tailed variables we need more and more observations; the information about the properties of the distribution is already there with some data. Let’s check our Monte-Carlo distribution for our maximum likelihood alpha estimates for both values of \\(n = 100\\) and \\(n= 1000\\)"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#from-ml-estimator-for-alpha-to-plug-in-mean",
    "href": "posts/fat-vs-thin-tails/2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution.html#from-ml-estimator-for-alpha-to-plug-in-mean",
    "title": "How to not get fooled by the “Empirical Distribution”",
    "section": "From ML estimator for alpha to plug-in mean",
    "text": "From ML estimator for alpha to plug-in mean\nOnce we have convinced ourselves that we can reliably estimate \\(\\alpha\\), we can then use this alpha estimate to estimate the mean of the distribution. However, one must be cautious. To prepare the reader, there are going to be crazy large observations that will confuse both methods at some Monte-Carlo experiments: this is just the nature of fat-tails and the precise reason why forecasting just a single variable is so dangerous. Therefore, the mean, or any other single estimate, cannot possibly prepare us for the enormous variation that a fat-tailed variable encodes. Thus, these problems haunt even our Maximum Likelihood estimator for the mean; just less than they haunt our estimate for the mean when we use the “empirical distribution”.\nThese problems show themselves in the form of a large mean for the distribution of the estimates according to each method. Alongside these means, the median and other percentiles of the distributions for both type of estimation methods and both \\(n\\)’s appear in the table below:\n\n\n\n\n\n\n  \n  \n    \n      method\n      n\n      mean\n      percentile_25\n      median\n      percentile_75\n      maximum_value\n    \n  \n  \n    Empirical Distribution\n100\n5.74\n3.34\n3.99\n5.12\n21,870.11\n    Maximum Likelihood\n100\n67.14\n4.47\n5.93\n8.95\n29,762.42\n    Empirical Distribution\n1000\n7.66\n4.16\n4.64\n5.42\n189,816.61\n    Maximum Likelihood\n1000\n6.16\n5.42\n6.00\n6.71\n47.60\n  \n  \n  \n\n\n\n\nFor what it’s worth, the medians of the maximum likelihood method’s distributions demonstrate its superiority versus using the mean of the empirical distribution. Once we zoom in on the majority of the sample, the histogram also shows the superiority of the maximum likelihood over the empirical distribution way of estimating the mean using the sample mean:\n\n\n\n\n\nJust remember, superiority over the “empirical distribution” is not that big of a compliment."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html",
    "href": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html",
    "title": "Standard Deviation and Fat Tails",
    "section": "",
    "text": "In this post, I’ll continue to explore with Monte-Carlo simulations the ideas in Nassim Taleb’s latest book: Statistical Consequences of Fat Tails. In other posts, I have look at the persistent small sample effect that plagues the mean estimates under fat tails as a consequence of the loooong pre-asymptotics of the law of large numbers. Also, how this in turn plagues other statistical techniques such as PCA. This time, I’ll explore what Taleb finds out by turning his attention to the much misunderstood standard deviation."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#game-plan",
    "href": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#game-plan",
    "title": "Standard Deviation and Fat Tails",
    "section": "Game Plan",
    "text": "Game Plan\nFirst, I’ll begin by exploring in what sense, under a Gaussian, the Standard Deviation (SD) is more “efficient estimator” than the Mean Absolute Deviation (MAD).\nSecondly, in order to gauge the problems that start to arise with standard deviation (SD) as a measure of the scale of the distribution, Taleb uses the same trick as before: stochastize the volatility by switching between 2 normals with same means but different variances. The larger the difference between the variances, the fatter the tail of the resulting random variable. I tried to play with this idea in this post. Therefore, by analyzing the relationship between SD and MAD as we increase this difference, we get a type of derivative: as we increase the fat-tailedness, the difference between Standard Deviation (SD) and Mean Absolute Deviation (MAD) grows."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#efficiency-in-mediocristan",
    "href": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#efficiency-in-mediocristan",
    "title": "Standard Deviation and Fat Tails",
    "section": "Efficiency in Mediocristan",
    "text": "Efficiency in Mediocristan\nWhen deciding between estimators, statistics textbooks end up talking about two asymptotic properties of the estimator: consistency and efficiency. Consistency concerns itself with the asymptotic accuracy of an estimator and efficiency with the asymptotic variance of the estimator. That is, consistency means that the estimator with lots of observations has a distribution tightly centered around the parameter. Whereas efficiency is something more esoteric but it means that the asymptotic variance is the lowest it can possibly be.\n\nEfficiency under the Poisson\nTo get some intuition around why we should care about the variance of an estimator, let’s play with \\(X \\sim Poisson(\\lambda)\\). Note that \\(E(X) = Var(X) = \\lambda\\). We could use either the estimate for the mean or the variance to estimate \\(\\lambda\\). Which is better?\n\nlambda_estimates <- function(size) {\n  sample <- rpois(size, lambda = 1)\n  mean_estimate <- mean(sample)\n  variance_estimate <- var(sample)\n  data.frame(sample_size = size, mean_estimate, variance_estimate) \n}\n\nmontecarlo_lambda <- function(size) {\n  rerun(1000, lambda_estimates(size)) %>% \n    bind_rows() %>% \n    pivot_longer(-sample_size, names_to = \"estimator\", values_to = \"estimate\")\n}\n\nc(10, 100, 500) %>% \n  map_df(montecarlo_lambda) %>% \n  mutate(sample_size = glue::glue(\"sample size: {sample_size}\")) -> lambda_est \nlambda_est %>% \n  ggplot(aes(x = estimate, y = sample_size)) +\n  geom_density_ridges(aes(fill = estimator), alpha = 0.5, color = \"black\") +\n  hrbrthemes::theme_ipsum_rc(grid = \"X\") +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"\", \n       title = \"Estimating Poisson's lambda = 1\",\n       subtitle = \"Mean estimate has lower variance\",\n       caption = \"1000 monte carlo simulations for each sample size\") +\n  scale_fill_viridis_d()\n\n\n\n\nThus, even though both estimators put a lot of their mass close to the true value, the Mean estimate has a lower variance than the Variance estimate of \\(\\lambda\\). And therefore is a safer bet to estimate the \\(\\lambda\\) parameter with the mean. Indeed, there’s a mathematical bound for the variance of an estimator of \\(\\lambda\\): \\(\\dfrac{\\lambda}{n}\\) which only the mean estimator satisties.\nThis is an important results: the variance estimate is too sensitive to outliers and we should prefer a natural weighting of the observations. However, with a Poisson and its thin tails, this is no big deal.\nSo far, we have talked about the intuition. If one wanted to compare the two estimator’s asymptotic efficiency, then one would compare the following ratio:\n\\[ \\dfrac{(Var(S^2)}{Var(\\bar{X})} \\]\nWhich is the asymptotic Relative efficiency of the two estimators. That is, which of them has a lower variance as the number of available observations becomes increasingly large.\n\nmontecarlo_lambda_efficiency <- function(size) {\n  rerun(1000, lambda_estimates(size)) %>% \n    bind_rows() %>% \n    pivot_longer(-sample_size, names_to = \"estimator\", values_to = \"estimate\") %>% \n    group_by(sample_size, estimator) %>% \n    summarise(variance_estimator = var(estimate))\n}\n\nc(100, 500, 1000, 5000, 10000) %>% \n  map_df(montecarlo_lambda_efficiency) -> lambda_efficiency\n\nlambda_efficiency %>% \n  pivot_wider(sample_size, names_from = estimator, values_from = variance_estimator) %>% \n  mutate(are = variance_estimate/mean_estimate) %>% \n  ggplot(aes(x = sample_size, y = are)) +\n    geom_point() +\n    geom_line() +\n    scale_x_log10() +\n    expand_limits(x = 100, y = 0) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") + \n  labs(x = \"sample size (log scale)\",\n       y = \"Asymptotic Relative Efficiency\",\n       title = \"ARE Mean and Variance to estimate Poisson's lambda\",\n       subtitle = \"Mean estimate is ~ 3x more efficient\")\n\n\n\n\n\n\nEfficiency under The Gaussian\nNow let’s start playing with a Gaussian with known mean 0. Imagine you have to estimate the scale of the distribution, whatever that may mean. We have two candidates: the SD or the MAD. Which one should we prefer? Just as with the Poisson, let’s play with with some Monte-Carlo simulations and calculate the ARE:\n\\[  \\dfrac{(\\dfrac{Var(SD^2)}{E(SD)^2})}{(\\dfrac{Var({MAD})}{E({MAD})^2})} \\]\nNote that SD and MAD are estimating different measures of the scale of the distribution Therefore, instead of comparing their variances, we compare their coefficients of variation.\n\ngaussian_estimates <- function(size) {\n  sample <- rnorm(size)\n  standard_deviation <- sqrt(mean(sample^2))\n  mad <- sum(abs(sample))/size \n  data.frame(sample_size = size, standard_deviation, mad)\n}\n\nmontecarlo_sigma <- function(size) {\n  rerun(1000, gaussian_estimates(size)) %>% \n    bind_rows() %>% \n    pivot_longer(-sample_size, names_to = \"estimator\", values_to = \"estimate\") %>% \n    group_by(sample_size, estimator) %>% \n    summarise(variance_estimator = var(estimate)/mean(estimate)^2)\n}\n\nc(10, 100, 500, 1000, 10000, 20000, 50000 ) %>% \n  map_df(montecarlo_sigma) -> gaussian_est\n\ngaussian_est %>% \n  pivot_wider(sample_size, names_from = estimator, values_from = variance_estimator) %>% \n  mutate(are = standard_deviation/mad) %>% \n  ggplot(aes(sample_size, are)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept = 0.875), linetype = 2, color = \"red\") +\n   scale_x_log10() +\n    expand_limits(x = 100) +\n  scale_y_continuous(limits = c(0, 1)) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Asymptotic Relative Efficiency SD/MAD under Mediocristan\",\n       subtitle = \"Variance of SD is ~ 0.875 that of MAD\",\n       y = \"Asymptotic Relative Efficiency\",\n       x = \"Sample size (log scale)\",\n       caption = \"Red is theoretical ARE = 0.875\") \n\n\n\n\nGreat, as our results replicate what Taleb did with math.\nThus, in a strict sense, when we are dealing with samples of Gaussian, it is more efficient to use SD than MAD. However, what happens when we slip away into fatter-tails?"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#what-happens-to-sds-efficiency-as-we-fatten-the-tails",
    "href": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#what-happens-to-sds-efficiency-as-we-fatten-the-tails",
    "title": "Standard Deviation and Fat Tails",
    "section": "What happens to SD’s efficiency as we fatten the tails?",
    "text": "What happens to SD’s efficiency as we fatten the tails?\nTaleb introduces an incredible heuristic ( which I already examined in this post ) that works perfectly to get the intuition of fattening a distribution. In Taleb’s words: “The equivalent of a functional derivative that provides good grasp for local sensitivities”\nThe heuristic is a simple switching between two gaussians with different standard deviations. Thus, we stochastize the variance of the distrbution. With probability \\(1-p\\), we switch to a \\(Normal(0, \\sigma)\\); with probability \\(p\\), we switch to \\(Normal(0, \\sigma \\sqrt{(1+a)})\\).\nThat is, if \\(p\\) is small, there’s a tiny probability that we switch to a gaussian with a higher standard deviation and that will likely generate outliers in comparison to the rest of the distribution. The higher \\(a\\), the larger the scale of the distribution. Thus, given that the tail probabilities are convex to the scale of the distribution, the higher \\(a\\), the more we fatten the distribution. Let’s sample from this distribution to have a quick look:\n\n# simulate fattened\nfatten_simulations <- function(samples, a, p, sigma  = 1)  {\n  \n  # create vector to store sims\n  sims <- vector(length = samples)\n  # sample with probability p\n  p_location <- rbernoulli(samples, p = p)\n  \n  sims[p_location] <- rnorm(sum(p_location), sd = sigma *sqrt(1+a))\n  sims[!p_location] <- rnorm(sum(!p_location), sd = sigma)\n  return(sims)\n}\n\nsamples <- 10000\n# simulated fattened\na_s <- seq(0, 100, length.out = 5)\nnames(a_s) <- unlist(map(a_s, ~ glue::glue(\"a = {.x/5}\")))\n\na_s%>% \n  map_df(~ fatten_simulations(samples, a = .x, p = 0.01, sigma = 5)) %>% \n  mutate(simulation = 1:samples) %>% \n  pivot_longer(-simulation, names_to = \"a_s\") %>% \n  mutate(a_s = fct_inorder(a_s)) -> fattend_different_as\nfattend_different_as %>% \n  ggplot(aes(sample = value, color = a_s)) +\n  stat_qq(alpha = 0.4) +\n  stat_qq_line() +\n  scale_color_viridis_d(direction = -1) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"QQ-plot for different stochastic volatilties\",\n       subtitle = \"Tail values are convex to the scale of the distribution. Higher a, larger scale\",\n       color = \"\",\n       caption = \"Theoretical distribution is the normal. parameter a as a multiple of sigma\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nTherefore, if we calculate the ARE for different values of \\(a\\), we are calculating its sensibility as we move close to fatter distributions. Let’s check whether SD keeps being more efficient than MAD:\n\nfatten_estimates <- function(size, a, p, sigma) {\n  sample <- fatten_simulations(size, a, p, sigma = sigma) \n  standard_deviation <- sqrt(mean(sample^2))\n  mad <- mean(abs(sample)) \n  data.frame(sample_size = size, standard_deviation, mad)\n}\n\nmontecarlo_sigma_fattened <- function(size, a, p, sigma) {\n  rerun(3000, fatten_estimates(size, a, p, sigma)) %>% \n    bind_rows() %>% \n    pivot_longer(-sample_size, names_to = \"estimator\", values_to = \"estimate\") %>% \n    group_by(estimator) %>% \n    summarise(variance_estimator = var(estimate)/mean(estimate)^2) %>% \n    mutate(a = a)\n}\n\na_s <- seq(0, 100, length.out = 40)\nnames(a_s) <- unlist(map(a_s, ~ glue::glue(\"a = {.x}\")))\na_s%>% \n  map_df(~montecarlo_sigma_fattened (size = 1000, a = .x, p = 0.01, sigma = 5)) ->fattened_are\nfattened_are %>% \n  pivot_wider(a, names_from = estimator, values_from = variance_estimator) %>% \n  mutate(are = standard_deviation/mad,\n         a = a / 5) %>% \n  ggplot(aes(a, are)) +\n  geom_point() +\n  expand_limits(x = 0, y = 0)  +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(x = \"parameter a as a multiple of sigma\",\n       title = \"Asymptotic Relative Efficiency of SD vs MAD\",\n       subtitle = \"Small outliers (~ 2 SD upwards) eat away the efficiency of SD\",\n       y = \"Asymptotic Relative Efficiency\")\n\n\n\n\nPer Taleb:\n\nA minute presence of outliers makes MAD more “efﬁcient” than STD. Small “outliers” of 5 standard deviations cause MAD to be ﬁve times more efﬁcient.\n\nTherefore, as we move towards distribution with fatter tails, we move to a place where standard deviation is worse than useless: it is dangerous. The problem compounds as the tails are precisely convex to the scale of the distribution.\nThus, to estimate the tails with SD is to play with fire: we are verly likely to make mistakes that will compound at the part of the distribution that matters the most. As Taleb says, this is also one of the misunderstandings of the Black Swan: the problem is not only that the extreme plays a large role; but also that we can very easily produce bogus estimates of the probabilities at the tail.\n\nWhy?\nThe problem arises from the weighting that SD performs. The larger the observation, the even larger weight will have on the Standard Deviation. Whereas MAD uses a more natural weighting, where the weight is equal to each observation. This becomes evident if we examine the cumulative SD and cumulative MAD, where every once in a while we get an extreme value.\nTo simulate this, let’s sample from a Cauchy distribution where there is not even a defined first moment. In sample, however, we can calculate anything:\n\nsamples <- rcauchy(10000)\ncumstd <- sqrt(cumvar(samples))\ncummad <- cummean(abs(samples - cummean(samples)))\n\ndata.frame(obs = 1:10000, cumstd, cummad) %>% \n  filter(obs > 2) %>% \n  pivot_longer(-obs) %>% \n  ggplot(aes(obs, value)) +\n    geom_line(aes(color = name)) +\n    hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(color = \"\",\n       title = \"The SD is way too sensible to outliers\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#conclusion",
    "href": "posts/fat-vs-thin-tails/2020-05-13-standard-deviation-and-fat-tails.html#conclusion",
    "title": "Standard Deviation and Fat Tails",
    "section": "Conclusion",
    "text": "Conclusion\nSD is only optimal in Mediocristan. When we start moving toward Extremistan, SD blows up due to being a sum of squares. Even relatively small deviations wash away the efficiency of SD over MAD. Thus, when dealing with fat-tails, SD should not be used as it can lead to very imprecise estimates of the probability at the tails. Thus, SD is not an appropriate measure of the scale of the distribution for fat tails. It is much better to use the MAD. That is, MAD is a better estimator of \\(E(|X-\\mu|)\\) than SD as an estimator of \\(\\sigma\\)."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html",
    "title": "Correlation is not Correlation",
    "section": "",
    "text": "To the usual phrase of correlation is not causation, Nassim Taleb often answers: correlation is not correlation. First, just like the mean and PCA, the sample correlation coefficient has persistent small sample effects when variables from Extremistan are involved. These topics are analyized in his latest book: Statistical Consequences of Fat Tails. I’ll explore this with some Monte-Carlo simulations from both Mediocristan and Extremsitan.\nSecondly, however, as Taleb says, even if we are in Mediocristan, the correlation coefficient is commonly misued and/or misunderstood. Commonly misused because people take non-random subsamples and expect the same correlation. Thereby arriving at some paradoxes: Berkson’s paradox (Collider Bias) and Simpson’s Paradox. I’ll take non random subsamples from a underlying random sample to show how you can get both paradoxes. Also, it is commonly misused because Correlation should not be used when there’s a non-linear relationship between the variables; otherwise, you get deceiving results. Commonly misunderstood because the amount of information it conveys is not linear. That is, a correlation of 0.9 conveys much, much more information than 0.7. Taleb analyzes these topics in a terrific, short paper called Fooled by Correlation: Common Misinterpretations in Social “Science”"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#sample-correlation-in-mediocristan",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#sample-correlation-in-mediocristan",
    "title": "Correlation is not Correlation",
    "section": "Sample Correlation in Mediocristan",
    "text": "Sample Correlation in Mediocristan\nLet’s do some Monte-Carlo simulations in Mediocristan. I’ll be sampling from two independent gaussians; that is, we know the “true” correlation is 0 and then we will compare it with the sample-correlation.\n\ncorrelation_mediocristan <- function(n, rho = 0) {\n  # sample from multivariate normal\n  data <- rnorm2d(n = n, rho = rho)\n  \n  # compute sample correlation\n  sample_correlation <- cor(data)[1,2]\n  sample_correlation\n}\n\nrerun(10000, correlation_mediocristan(20, 0)) %>% \n  unlist() -> mediocristan_correlations_20_obs\n\nrerun(10000, correlation_mediocristan(1000, 0)) %>% \n  unlist() -> mediocristan_correlations_1000_obs\n\ndata.frame(sim = 1:10000, small_sample = mediocristan_correlations_20_obs,\n           large_sample = mediocristan_correlations_1000_obs) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"sample_correlation\") %>% \n  mutate(sample = if_else(sample == \"large_sample\", \"B: Sample with 1,000 observations\", \n                          \"A: Sample with 20 observations\")) -> gaussian_corr\n\nNow we plot them:\n\ngaussian_corr %>% \n  ggplot(aes(sim, sample_correlation)) +\n  geom_col(aes(fill = sample)) +\n  facet_wrap(~sample) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  labs(title = \"Sample correlations across different simulations\",\n       subtitle = \"Sample correlation quickly converges. Variables are independent\",\n       y = \"sample correlation\")\n\n\n\n\n\ngaussian_corr %>% \n  ggplot(aes(sample_correlation, fill = sample)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", alpha = 0.5) +\n  facet_wrap(~sample) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Sample correlations in Mediocristan\",\n       subtitle = \"Sample correlation quickly converges. Variables are independent\",\n       x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n\n\n\n\nTherefore, we can use our sample correlation coefficients in Mediocristan as they quickly converge. That is, with randomness coming from Mediocristan, noise quickly washes out with relatively small sample size"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#sample-correlations-from-extremistan",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#sample-correlations-from-extremistan",
    "title": "Correlation is not Correlation",
    "section": "Sample correlations from Extremistan",
    "text": "Sample correlations from Extremistan\nTo examine the slow convergence of the sample correlation coefficient, Taleb proposes a bivariate t-student distribution with exponent 2/3. Notice that the mean and variance are undefined. Yet, there is a finite correlation. Let’s replicate the same experiment as we did in mediocristan:\n\ncorrelation_bivariate_t <- function(n, rho = 0) {\n  # sample from multivariate normal\n  data <- rt2d(n, rho = rho, nu = 2/3)\n  \n  # compute sample correlation\n  sample_correlation <- cor(data)[1,2]\n  sample_correlation\n}\n\nrerun(10000, correlation_bivariate_t(20, rho = 0)) %>% \n  unlist() -> bivariate_t_20\n\nrerun(10000, correlation_bivariate_t(1000, rho = 0)) %>% \n  unlist() -> bivariate_t_thousand\n\ndata.frame(sim = 1:10000, bivariate_t_20, bivariate_t_thousand) %>% \n  pivot_longer(-sim, names_to = \"sample_size\", values_to = \"sample_correlation\") %>% \n  mutate(sample_size = if_else(sample_size == \"bivariate_t_20\", \n                               \"A: Sample with 20 observations\", \"B: Sample with 1,000 observations\")) -> t_corr \n\nNow, let’s plot the results:\n\nt_corr %>% \n  ggplot(aes(sim, sample_correlation)) +\n  geom_col(aes(fill = sample_size)) +\n  facet_wrap(~sample_size) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  labs(title = \"Sample correlations across different simulations\",\n       subtitle = \"Sample correlation is just as erratic, regardless of sample size. True correlation is zero. \",\n       y = \"sample correlation\")\n\n\n\n\n\nt_corr %>% \n  ggplot(aes(sample_correlation, fill = sample_size)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  facet_wrap(~sample_size) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Sample correlations from Extremistan\",\n       subtitle = \"Sample correlation suffers from small sample effect. True correlation is zero.\",\n         x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n\n\n\n\nAs Taleb writes: “finite correlation doesn’t mean low [estimator’s] variance: it exists, but may not be useful for statistical purposes owing to the noise and slow convergence.”\nFinally, let’s experiment what happens if we make our samples 10k big:\n\nrerun(10000, correlation_bivariate_t(10000, rho = 0)) %>% \n  unlist() -> bivariate_t_10k\n\nLet’s plot the results:\n\ndata.frame(sample_size_10k = bivariate_t_10k) %>% \n  ggplot(aes(sample_size_10k)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Distribution of Sample correlations from Extremistan\",\n       subtitle = \"Sample (n = 10k) correlation suffers from small sample effect. True correlation is zero.\",\n         x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n\n\n\n\nThat is, the sample correlations suffers from persistent small sample effect. Thus, there’s to much noise to use the sample correlation for fat-tailed distributions."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#quadrants-correlation",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#quadrants-correlation",
    "title": "Correlation is not Correlation",
    "section": "Quadrant’s Correlation",
    "text": "Quadrant’s Correlation\nTo get some intuition about the problem, let’s sample \\(10^6\\) from a multivariate normal with correlation of \\(0.75\\). Then, we will group the data into geometrically intuitive non-random partitions: into quadrants. Then, we will calculate the correlation between the observations within each quadrant.\n\nn <- 10^6\nrho <- 3/4 \ndata <- rnorm2d(n = n, rho = rho) \n\ndata.frame(data) %>% \n  rename(x = X1, y = X2) %>% \n  mutate(quadrant = case_when(\n    x > 0 & y > 0 ~ \"II\",\n    x < 0 & y > 0 ~ \"I\",\n    x < 0 & y < 0 ~ \"IV\",\n    x > 0 & y < 0 ~ \"III\" \n    )) %>% \n  group_by(quadrant) %>% \n  summarise(correlation = cor(x, y)) %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(correlation))\n\n\n\n\n\n  \n  \n    \n      quadrant\n      correlation\n    \n  \n  \n    I\n0.19\n    II\n0.53\n    III\n0.18\n    IV\n0.53\n  \n  \n  \n\n\n\n\nTherefore, we should not expect to have the same correlation in non-random subsamples of the data as we have in the whole sample."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#berksons-paradox",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#berksons-paradox",
    "title": "Correlation is not Correlation",
    "section": "Berkson’s paradox",
    "text": "Berkson’s paradox\nIn Berkson’s paradox, there appears to be a negative correlation between 2 variables in a subsample; when in fact there is no correlation between them considering the whole. The bias is introduced by sub-sampling our observations based on another variable that the 2 variables in question jointly determine. In DAG slang, it is collider bias.\nImagine, then, that we have samples from two gaussian variables that are independent.\n\nn <- 10^4\nrho <- 0 \ndata <- rnorm2d(n = n, rho = rho) \n\ndata.frame(data) %>% \n  rename(x = X1, y = X2) -> data_berkson\n\ndata_berkson %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(linetype = 2, color = \"red\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Variables are uncorrelated in the whole sample\")\n\n\n\n\nHowever, they jointly determine another variable \\(Z\\) thus: \\(Z\\) is zero except when either \\(X\\) or \\(Y\\) are greater than \\(0\\), in which case we have \\(Z = 1\\). Then, sampling conditioning on \\(Z = 1\\) is non a random subsample. Therefore, as we know that Correlation is subadditive, we know that we do not expect the same correlation as in the whole sample.\n\ndata_berkson %>% \n  mutate(z = if_else(x > 0 | y > 0, 1, 0))-> data_berkson_z\n\ndata_berkson_z %>% \n  ggplot(aes(x, y, color = factor(z))) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2) +\n  hrbrthemes::theme_ipsum_rc() +\n  scale_color_viridis_d(begin = 1, end = 0) +\n  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"Berkson's paradox: Negative relation between independent vars\",\n       subtitle = \"Correlation does weird things on non-random sub samples.\",\n       caption = \"Red line is the whole sample trend\")\n\n\n\n\nIntuitively, if we know that \\(X\\) is negative, due to our conditioning on \\(Z\\), then it must be the case that \\(Y\\) is positive. Therefore, conditioning on \\(Z\\) creates a negative correlation between two independent variables.\nTherefore, if we condition on \\(Z=1\\), a non-random subsample (thereby opening a collider), we get a negative correlation from two independent variables: Berkson’s paradox. Again, we should not expect the global correlation to stay constant under non-random sub samples."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#simpsons-paradox",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#simpsons-paradox",
    "title": "Correlation is not Correlation",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nTo consider Simpson’s paradox, let’s consider 3 random variables. \\(X\\) is going to determine the value of \\(Z\\). \\(X\\) and \\(Z\\) are jointly going to determine the value \\(Y\\)\nOn the whole sample, \\(X\\) and \\(Y\\) are positively related\n\nn <- 10000\nx <- rnorm(n)\nz <- rnorm(n, 1.2*x) \ny <- rnorm(n, -0.8*x + z)\n\ndata.frame(x, y, z) %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(se = FALSE, linetype = 2) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"X and Y are positively related in the whole sample\")\n\n\n\n\nHowever, once we condition on \\(Z\\), that is, take non-random sub-samples according to the values of \\(Z\\), we will see a reversal of the sign of the correlation between \\(X\\) and \\(Y\\).\n\ndata.frame(x, y, z) %>% \n  mutate(z = cut(z, 10)) %>% \n  ggplot(aes(x, y, color = z)) +\n  geom_point(alpha = 0.1)  +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = \"red\") +\n  hrbrthemes::theme_ipsum_rc() + \n  theme(legend.position = \"none\") +\n  scale_color_viridis_d() + \n  labs(title = \"Simpson's Paradox: Reversal of correlation sign\",\n       subtitle = \"Correlation does weird things on non-random sub samples.\",\n       caption = \"Red line is whole sample trend\")\n\n\n\n\nWe are doing something extremely similar to what linear regression does. Once we adjust for \\(Z\\), what’s left is the negative effect of \\(X\\) on \\(Y\\). Again, we should not expect the global correlation to stay constant under non-random sub-samples."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#correlation-under-non-linearities",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#correlation-under-non-linearities",
    "title": "Correlation is not Correlation",
    "section": "Correlation under non linearities",
    "text": "Correlation under non linearities\nSometimes, the association between two variables depends on the levels of the variables themselves. That is, there is a non-linear relationship between the two variables. In these types of situations it’s a mistake to use total correlation. Taleb proposes the following experiment, which he calls “Dead Man Bias”:\n\nYou administer IQ tests to 10k people, then give them a “performance test” for anything, any task. 2000 of them are dead. Dead people score 0 on IQ and 0 on performance. The rest have the IQ uncorrelated to the performance to the performance. What is the spurious correlation IQ/Performance?\n\nLet’s perform this thought experiment:\n\nn <- 10000\np_dead <- 0.2\n\ndead <- rbernoulli(n, p_dead)\n\nperformance <- runif(n)\niq <- runif(n)\n\nperformance[dead] <- 0\niq[dead] <- 0\n\nLet’s plot our results:\n\ndata.frame(iq, performance, dead) %>% \n  ggplot(aes(iq, performance, color = dead)) +\n  geom_point(alpha = 0.1) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"You'll never guess a positive relationship\",\n       subtitle = \"Until you notice the dark spot at the origin\") +\n  scale_color_viridis_d(begin = 1, end = 0) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nLooking at the data, you’ll never believe there’s a positive correlation. However, the dark spot in the origin changes the whole history. Let’s calculate the correlation coefficient:\n\ncor(iq, performance)\n\n[1] 0.3849505\n\n\nThat is, iq (in this thought experiment, at least) is not a predictor of good performance. However, it does matter: the dark spot (the dead people) fools correlation into thinking that if \\(iq=0\\) means \\(performance=0\\), then the relationship must hold for the entire range of observations. That is, the extremely non-linear relationship between iq and performance fools the correlation coefficient.\nMoral of the story, never use correlation when there’s an indication of non-linear relationship between the variables."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#entropy-and-mutual-information",
    "href": "posts/fat-vs-thin-tails/2020-05-22-correlation-is-not-correlation.html#entropy-and-mutual-information",
    "title": "Correlation is not Correlation",
    "section": "Entropy and Mutual Information",
    "text": "Entropy and Mutual Information\nFrom information theory, we get a measure of the inherent uncertainty in a distribution. Information Entropy, which is just: \\(-E[log(p_i)]\\). Also, there is divergence: the extra uncertainity that is induced when we approximate the distribution p with the distribution q: \\(E[log(\\dfrac{p_i}{q_i})]\\).\nIn the same way, there is the concept of Mutual Information: the extra uncertainity that is induced when we approximate a joint probability distribution with the product of their marginal distributions. If we don’t increase the uncertainty, then we do not lose information by modelling them independently. Therefore, it is a measure of dependence: the higher, the more dependence between the variables.\nFor the Gaussian case, Taleb shows that the Mutual information is:\n\\[ MI = -\\dfrac{1}{2} log (1 - \\rho^2) \\]\nTherefore, the information about the association between two variables that Correlation conveys does not scale linearly with the correlation coefficient. Indeed, MI has a convex response to \\(\\rho\\):\n\nrho <- seq(-1, 1, length.out = 100)\nmutual_information = -1/2* log(1 - rho^2)\n\ndata.frame(rho, mutual_information) %>%\n  mutate(association = if_else(rho < 0, \"negative\", \"positive\")) %>% \n  ggplot(aes(rho, mutual_information, fill = association)) +\n  geom_area(alpha = 0.5) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(legend.position = \"none\") +\n  labs(subtitle = \"Information conveyed by correlation does not scale linearly\",\n       title = \"Mutual information as a function of correlation\",\n       caption = \"Gaussian case\", \n       y = \"Mutual Information\",\n       x = \"correlation coefficient\")\n\n\n\n\nFor example, the change in the information conveyed from a correlation of \\(0\\) to \\(0.5\\) is much, much smaller than a change from \\(0.7\\) to \\(0.99\\). Therefore, correlation coefficient should not be interpreted linearly."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html",
    "href": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html",
    "title": "LLN for higher p Moments",
    "section": "",
    "text": "I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In it, we have seen how the Law of Large Numbers for different estimators simply does not work fast enough (in Extremistan) to be used in real life. For example, we have seen how the distribution of the sample mean, PCA, sample correlation and \\(R^2\\) turn into pure noise when we are dealing with fat-tails. Also, we have seen how the slow convergence of the CLT for fat-tails: thus making the normal approximation for most samples sizes an unreliable on.\nIn this post, I’ll explore the slow workings of the Law of Large numbers for higher moments. Similar to what we did for the sample mean, we will check whether adding observations reduce the variability of our estimate or whether it causes occasional jumps that lead us to believe that large sub-samples will produce different averages, indicating that the theoretical moment does not exist. This problem will only be worsened with higher moments: higher moments are convex transforms of the original random variable. Thus, they are even more fatter than our original variable and our estimates will be more likely to suffer from large discontinuous jumps that will derail convergence."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#fast-convergence-mediocristan",
    "href": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#fast-convergence-mediocristan",
    "title": "LLN for higher p Moments",
    "section": "Fast-Convergence: Mediocristan",
    "text": "Fast-Convergence: Mediocristan\n\ngaussian <- rnorm(500) \ndata.frame(gaussian) %>% \n  ggplot(aes(sample = gaussian)) +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Gaussian\",\n       subtitle = \"Fast convergence to 0\")\n\n\n\n\nFor a Gaussian, all the moments exist and therefore the ratio of the max-to-sum for all the moments converge to \\(0\\). Notice that after 50 observations, the ratio stabilizes and adding more observations always results in the ratio converging to 0. However, note that just as we saw with the CLT, higher moments take longer to converge. Notice that this is not a feature of the sample, but a feature of the fast convergence of the LLN for the Gaussian. To see this, we can check the convergence for 10 different gaussian samples:\n\nrerun(10, rnorm(500)) %>% bind_cols() %>% \n  mutate(sim =1:500) %>% \n  pivot_longer(-sim, names_to = \"simulation\") %>%  \n  mutate(simulation = str_extract(simulation, \"\\\\d+\")) %>% \n  ggplot(aes(sample = value))  +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  facet_wrap(~simulation, nrow = 4) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Gaussian\",\n       subtitle = \"Fast convergence to 0\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#lognormal",
    "href": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#lognormal",
    "title": "LLN for higher p Moments",
    "section": "Lognormal",
    "text": "Lognormal\nNotice that all the moments also exist for a lognormal. However, the convergence is much slower: the extreme events observations cause sudden jumps that derail the convergence. Much more data is needed. Here we have an order of magnitude more.\n\nlognormal <- rlnorm(5000, sd = 4) \ndata.frame(lognormal) %>% \n  ggplot(aes(sample = lognormal)) +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Lognormal (sd = 4)\",\n       subtitle = \"Slow convergence and even slower for higher moments\")\n\n\n\n\nCrucially, the higher moments are even more fat-tailed. Thus, they are much more susceptible to discontinuous jumps and therefore have slower convergence.\nNotice that this behavior is not a property of our specific sample but of the distribution itself.\n\nrerun(10, rlnorm(500, sd = 4)) %>% bind_cols() %>% \n  mutate(sim =1:500) %>% \n  pivot_longer(-sim, names_to = \"simulation\") %>%  \n  mutate(simulation = str_extract(simulation, \"\\\\d+\")) %>% \n  ggplot(aes(sample = value))  +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  facet_wrap(~simulation) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Lognormal\",\n       subtitle = \"Slow convergence for higher moments\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#pareto-distribution",
    "href": "posts/fat-vs-thin-tails/2020-06-02-lln-for-higher-p-moments.html#pareto-distribution",
    "title": "LLN for higher p Moments",
    "section": "Pareto Distribution",
    "text": "Pareto Distribution\nNow it’s time to analyze a distribution that has no finite theoretical moments. For example, a Pareto with tail exponent \\(\\alpha = 1.16\\), equivalent to the 80/20 rule, only has finite mean. All other higher moments are infinite. If you want an intuitive explanation of the tail exponent, check this other blogpost\n\nalpha <- 1.16\nrpareto <- function(n) {\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\npareto <- rpareto(5000)\ndata.frame(pareto) %>% \n  ggplot(aes(sample = pareto)) +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Pareto (alpha = 1.16)\",\n       subtitle = \"Only the mean converges\")\n\n\n\n\nCompare it for a Pareto with $ = 2.32$:\n\nalpha <- 2.1\nrpareto <- function(n) {\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\npareto <- rpareto(5000)\ndata.frame(pareto) %>% \n  ggplot(aes(sample = pareto)) +\n  stat_max_sum_ratio_plot(p = 4) +\n  geom_hline(aes(yintercept = 0),\n             linetype = 2) +\n  transition_reveal(stat(x)) +\n  scale_color_viridis_d() +\n  labs(title = \"Max to Sum ratio for a Pareto (alpha = 2.1)\",\n       subtitle = \"The two first moments exist, yet only one converges. Second moment has alpha = 1.05\")\n\n\n\n\nIndeed, the first two moments exist and should both converge to zero. However, the second moment has fatter tails and thus takes longer to converge."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "",
    "text": "In this blogpost, I’ll answer the question, following Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails, when can we use GARCH (and firends) models? As an example, also following Taleb, I’ll check the resulting conditions with the S&P500."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#what-are-the-obstacles",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#what-are-the-obstacles",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "What are the obstacles?",
    "text": "What are the obstacles?\nThe Generalized Autoregressive Conditional Heteroskedasticity (GARCH) family of models attempt to model a given time series by exploiting “volatility” clustering (i.e., for some periods volatility is consistently high, for other periods is consistently low). When is this warranted?\n\nThe theoretical variance must exist.\nWe must be able to reliably estimate the variance with the amount of data we have.\n\nOf course, neither of the conditions may be satisfied with fat-tailed data. For example, the theoretical mean may exist but the sample mean estimator may suffer from persistent small-sample effect: the Law of Large Numbers (LLN) just works way too slowly. Thus, we cannot reliably estimate the mean.\nThe exact same thing happens for the variance, just that the problem compounds: higher moments are more fat-tailed than the original random variable and thus the LLN works even more slowly. That is, the theoretical variance may exist but it may be impossible to estimate it with the amount of data we have. A heuristic to know the reliability of the sample variance is the existence of the third and fourth moment. As Taleb says:\n\nIn a situation of infinite third moment, the observed second moment will fail to be informative as it will almost never converge to its value\n\nThat is, belonging to the class of Power Law tails with (\\(\\alpha \\leq 4\\)) cancels the use of the sample variance in any method"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#are-the-conditions-satistified-for-the-sp-500",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#are-the-conditions-satistified-for-the-sp-500",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "Are the conditions satistified for the S&P 500?",
    "text": "Are the conditions satistified for the S&P 500?\nTherefore, we could possibly attempt to model the S&P 500’s returns if:\n\nThe theoretical variance exists\nThe LLN works fast enough for the variance: i.e., the fourth moment is finite.\n\nBoth conditions relate to the fat-tailedness of the returns. If they are fat-tailed, how fat-tailed are them? Could we justify positing an \\(\\alpha > 4\\)? Following Taleb’s explanation, I’ll conduct a battery of tests to diagnose exactly how fat-tailed the returns are and what may be the tail exponent of their tail.\nI’ll use all the available data from 1948 to the present day\n\nc(\"^GSPC\") %>% \n  tq_get(from = \"1948-01-09\") -> sp_data"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#zipf-plot",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#zipf-plot",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "Zipf Plot",
    "text": "Zipf Plot\nThe Zipf-plot, or the log-log plot, is a great way to diagnose if the tails of a distribution decay as a Power Law. For Power Laws, we expect a linear decay at the tail and a variation of several order of magnitudes in the x-axis. The zipf plot for the absolute daily returns of the S&P is thus:\n\n\n\n\n\nIndeed, we see a linear decay of the tail and a variation of several orders of magnitude in the x-axis. Both signs of fat-tailedness for the day returns."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#mean-excess-plots",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#mean-excess-plots",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "Mean excess plots",
    "text": "Mean excess plots\nMean excess plots are also a test of fat-tailedness. For a given threshold \\(v\\), they estimate:\n\\[ E[X - v | X > v] \\]\nThat is, how the conditional expectation increases as the threshold increases. For a power-law, we expect a linear increase:"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#max-to-sum-ratios",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#max-to-sum-ratios",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "Max-to-Sum Ratios",
    "text": "Max-to-Sum Ratios\nMax-to-Sum ratio plots are a way to check the existence of higher moments. If a given moment exists, the ratio of the max-to-sum should eventually converge to zero:\n\n\n\n\n\nIndeed, neither the third nor the fourth moment seem to converge. No Gaussian variable would ever behave like this: a single discontinuous jump in the ratio allows us to deny it. Indeed, the returns clearly follow a power-law in the tail that curbs any convergence for both the skeweness and the kurtosis. Thus, we can say that the tail follows a power law around \\(\\alpha \\approx 3\\)."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#conclusion",
    "href": "posts/fat-vs-thin-tails/2020-06-14-when-are-garch-and-friends-models-warranted.html#conclusion",
    "title": "When are GARCH (and friends) models warranted?",
    "section": "Conclusion",
    "text": "Conclusion\nThe tails of the daily returns behave like a power law. Thus, the tails wag the dog, as Taleb says. Given the data, only the first two moments seem to converge. Neither the third nor fourth converge. Thus, we posit a power law of around 3 to describe the tails of the daily returns. This power law is so fat-tailed, that it is just not possible to use the sample variance estimate with the amount of available data. Therefore, the use of any model that tries to estimate the variance of the daily returns is not warranted."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html",
    "href": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html",
    "title": "Forecasting elections? Taleb says no",
    "section": "",
    "text": "With US elections around the corner, news outlets are constantly deploying new models to try to predict who will win the next presidential elections. Ever since Trump’s ‘surprising’ win in ’16, these (pre-election) polls based models have come under scrutiny. Indeed, there is a huge amount of uncertainty that these models do not seem to capture well enough.\nTaleb’s and Dhruv Madeka’s point is the following: whilst forecasting an uncertain election, one cannot invoke probabilistic thinking unless one imposes severe constraints on how the forecast will move up to election day. If one’s forecast is having large swings across time, then, one’s forecast is not coherent with the basic tenet of subjective probability as espoused by De Finetti.\nIn this blogpost, I’ll explain what is the basic tenet of subjective probability according to De Finetti and explain how Taleb expands on this definition to verify what are the consequences that every probabilistic forecast mus satisfy to be considered coherent forecasts. At the end, we will arrive at the surprising result that uncertain and highly volatile elections must result in very stable 50/50 forecasts."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html#de-finettis-argument",
    "href": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html#de-finettis-argument",
    "title": "Forecasting elections? Taleb says no",
    "section": "De Finetti’s argument",
    "text": "De Finetti’s argument\nDe Finetti (an italian writing in French) argued that any subjective probability statement defines a bet that one would be willing to accept. That is, subjective probability statements must be backed up by skin in the game. In his own words in La prévision: ses lois logiques, ses sources subjectives (PDF)\n\nIl s’agit simplement de préciser mathématiquement l’idée banale et évidente que le degré de probabilité attribué par un individu à un événement donné est révélé par les conditions dans lesquelles il serait disposé de parier sur cet événement\n\n\nSupposons qu’un individu soit obligé d’évaluer le prix p pour lequel il serait disposé d’échanger la possession d’une somme quelconque S (positive ou négative) subordonnée à l’arrivée d’un événement donné, E, avec la possession de la somme p S ; nous dirons par définition que ce nombre p est la mesure du degré de probabilité attribué par l’individu considéré à l’événement E, ou, plus simplement, que p est la probabilité de E.\n\nIndeed, from this simple statement, all the axioms of probability follow:\n\nde telle façon que toute la théorie des proba- bilités puisse se déduire immédiatement d’une condition très naturelle ayant une signification évidente\n\nTo be a coherent statement, this bet must preclude the possibility of arbitrage. That is, an opposite bettor cannot create a risk-free series of bets such that he always makes a profit.\n\nCeci posé, lorsqu’un individu a évalué les probabilités de certains événements, deux cas peuvent se présenter : ou bien il est possible de parier avec lui en s’assurant de gagner à coup sûr, ou bien cette possibilité n’existe pas. Dans le premier cas on doit dire évidemment que l’évaluation de la probabilité donnée par cet individu contient une incohérence, une contradiction intrinsèque; dans l’autre cas, nous dirons que l’individu est cohérent.\n\nTherefore, the only coherent probabilistic statements are the ones that imply a bet that precludes arbitrage.\n\nC’est précisément cette condition de cohérence qui constitue le seul principe d’où l’on puisse déduire tout le calcul des probabilités : ce calcul apparaît alors comme l’ensemble des règles auxquelles l’évaluation subjective des probabilités de divers événements par un même individu doive être assujettie si l’on ne veut pas qu’il y ait entre elles une contradiction fondamentale."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html#taleb-expands-de-finettis-argument",
    "href": "posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html#taleb-expands-de-finettis-argument",
    "title": "Forecasting elections? Taleb says no",
    "section": "Taleb expands De Finetti’s argument",
    "text": "Taleb expands De Finetti’s argument\nWhilst De Finetti worked with one forecast, Taleb expands this coherence argument into a series of supposedly probabilistic statements that create a series of forecast leading up to the election. Intuitively, highly uncertain elections are uncertain because in whatever time is left, a lot of events can happen that can decide an election.\nGiven a forecast, how should this forecast evolve through time? Should it be swayed by every piece of evidence into forecasting a clear winner, even months ahead of the election? Or should it remain skeptical up to last minute? Using De Finetti’s skin in the game argument, Taleb and Madeka argue that the problem is equivalent to that of pricing a binary option on the election date.\nImposing arbitrage boundaries lead us to the conclusion that the more volatile the election, our forecast must be around 50/50 and very stable (low volatility), despite what the current evidence may incline one to believe.\n\nOtherwise, the future revised estimates with large swings open up the possibility of inter-temporal arbitrage by buying and selling from the “assessor” that is forecasting. From the comic of the Black Swan Man:\n\nTherefore, a series of forecasts are only coherent if they preclude the possibility of arbitrage. And in the case of highly uncertain elections, the only way to make this happen is to forecast a very stable 50/50 that is only updated at the very last minute."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html",
    "href": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html",
    "title": "Understanding the tail exponent",
    "section": "",
    "text": "Power Laws are ubiquitous to describe fat tails, a topic that I’ve been trying to wrap my head around for the last couple of weeks. However, up until now, I haven’t had a visceral understanding of what exactly is the function of their main parameter: the tail exponent \\(\\alpha\\). This blogpost is my attempt at gaining understanding. To do so, I will be replicating some of the plots and derivations from two sources:\n\nNassim Taleb in his latest book: Statistical Consequences of Fat Tails\nPasquale Cirillo and his excelents YouTube lectures on Quantitative Risk Management\n\nMy main goals are:\n\nTo understand how sensitive the dominance of the tail event is to the \\(\\alpha\\) and how convex transformations influence this association.\nTo understand how can we “see” the \\(\\alpha\\) in data.\n\n\n\nFirst, let’s be clear about what what are Power Laws: the random variables such that their survival function (\\(S(x) = 1 - P(X>x)\\)) can be described thus:\n\\[ S(x) \\approx \\dfrac{1}{x^{\\alpha}}\\] That is, the lower the value of \\(\\alpha\\), the larger the survival function for any value of \\(x\\) and thus the tail will take more time to decay. Therefore, the lower the \\(\\alpha\\), the fatter the tail. Indeed, the survival function is convex to the value of \\(\\alpha\\).\n\n\n\nTo gain perspective on what this means, Taleb has a clever trick to check the probability concentration effects. Thus, the share of the \\(p\\)th quantile for a given tail is:\n\\[ p^{\\dfrac{\\alpha-1}{\\alpha}}\\]\nThus, if what we are modelling is wealth, for \\(p=0.2\\), we are modelling what percentage of the wealth is owned by the top 20%. Let’s plot this as a function of \\(\\alpha\\):\n\nalpha = seq(1, 4, length.out = 100)\n\np = 0.2\n\ntop_20 <- p^((alpha-1)/alpha)\ntibble(top_20, alpha) %>% \n  ggplot(aes(alpha, top_20)) +\n  geom_point(alpha = 0.8, color = \"dodgerblue4\") +\n  expand_limits(y = 0) +\n  labs(title = \"Share of the 20th percentile as a function of alpha\",\n       subtitle = \"Dominance of the tail is convex to alpha\",\n       y = \"top 20% share\") +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\nThat is, we can see that the degree of dominance by the tail events is convex to the value of \\(\\alpha\\). Thus, the larger the \\(alpha\\), the fatter the tail, and the larger dominance of the phenomena by the tail events.\nPerhap’s even more worringly, this behavior is exacerbated by convex transformations to the original random variable. For example, if we instead are interested in the square of the wealth to compute the variance, this square of the wealth is going to be even more dominated by the tail-events. In fact, if \\(X\\) has tail exponent \\(\\alpha\\), \\(X^2\\) has tail exponent \\(\\alpha/2\\)\nLet’s plot the share of the top 1% percent\n\nalpha = seq(2, 4, length.out = 100)\nalpha_wealth_squared <- alpha/2\n\n\np = 0.01\n\ntop_1_convex_transform <- p^((alpha_wealth_squared-1)/alpha_wealth_squared)\n\n\ntop_1 <- p^((alpha-1)/alpha)\n\ntibble(top_1, alpha, top_1_convex_transform) %>% \n  pivot_longer(-alpha, names_to = \"distribution\", values_to = \"top_1_share\") %>% \n  ggplot(aes(alpha, top_1_share, color = distribution)) +\n  geom_point(alpha = 0.8) +\n  expand_limits(y = 0) +\n  labs(title = \"Share of the top 1% as a function of alpha\",\n       subtitle = \"Convex transformations only make matters worse\",\n       y = \"top 1 % share\") +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nIn conclusion, the contribution of the tails is convex to the alpha. Convex transformations of the original random variable only heighten this contribution"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#non-scalable-mediocristan",
    "href": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#non-scalable-mediocristan",
    "title": "Understanding the tail exponent",
    "section": "Non-scalable: Mediocristan",
    "text": "Non-scalable: Mediocristan\nIntuitively, when we are considering thin-tailed distributions, the rate of decay of the tails is constantly decreasing. For example, take the Survival function of the Gaussian. The farther we go from the mean, the larger the difference between \\(S(k)\\) and \\(S(2k)\\). That is, the ratio \\(\\dfrac{S(k)}{S(2k)}\\) increases as \\(k\\) grows larger.\n\ntibble(k = seq(1, 4, length.out = 10)) %>% \n  mutate(ratio = (1-pnorm(k))/(1-pnorm(2*k))) %>% \n  gt() %>% \n  fmt_number(columns = vars(ratio, k)) %>% \n  tab_style(\n    style = cell_text(size = px(15), weight = \"bold\"),\n    locations = cells_body(vars(ratio))\n  ) %>% \n   tab_style(\n    style = cell_text(\n      size = px(11),\n      color = \"#999\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_labels(everything())\n  ) %>% \n  cols_width(vars(ratio) ~ px(175))\n\n\n\n\n\n  \n    \n    \n  \n  \n  \n    \n      k\n      ratio\n    \n  \n  \n    1.00\n6.97\n    1.33\n23.81\n    1.67\n111.38\n    2.00\n718.32\n    2.33\n6,412.62\n    2.67\n79,446.99\n    3.00\n1,368,249.40\n    3.33\n32,792,803.68\n    3.67\n1,094,641,005.07\n    4.00\n47,544,864,306.00\n  \n  \n  \n\n\n\n\nThe ratio’s growth is astounding. In a very real sense, the difference between the \\(S(k)\\) and \\(S(2k)\\) depends on where we are on the distribution. Also, realize that the numerator of the slope of the log of the survival function is approximately the inverse of this ratio\n\\[ \\dfrac{\\Delta log(S(x))}{\\Delta log(x)} \\approx \\dfrac{log(S(2x)) - log(S(x))}{\\Delta log(x)} = \\dfrac{log(\\dfrac{S(2x)}{S(x)})}{\\Delta log(x)}\\] Therefore, as \\(\\lim_{x \\to \\infty } \\dfrac{S(2x)}{S(x)} = 0\\) for the gaussian (and faster than the logarithm), the slope of the log of the Survival function goes towards negative infinity (\\(\\lim_{z \\to 0} log (z) = -\\infty\\)) as we move away from the mean. Indeed, a common plot in the study of fat-tailed distributions is this plot precisely: log of the survival function vs log(x). It is called a zipf plot.\nTo incorporate this kind of plot in the ggplot2 workflow, I implemented a small package called ggtails:\n\nlibrary(ggtails)\n\nLet’s simulate from a truncated normal and check that the slope of the log of the Survival function goes towards negative infinity:\n\nrtruncnorm <- function(N, mean = 0, sd = 1, a = 0, b = Inf) {\n  # truncnormal with inverse transform sampling\n  if (a > b) stop('Error: Truncation range is empty')\n  U <- runif(N, pnorm(a, mean, sd), pnorm(b, mean, sd))\n  qnorm(U, mean, sd)\n}\n\ntruncated_normal <- rtruncnorm(1000)\n\ntibble(truncated_normal) %>% \n  ggplot(aes(sample = truncated_normal)) +\n  stat_zipf(alpha = 0.2) +\n  scale_y_log10() +\n  scale_x_log10() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Zipf plot for a truncated normal\",\n       x = \"log(x)\",\n       y = \"log(Survival)\",\n       subtitle = \"The slope approaches negative infinity\")\n\n\n\n\nIndeed, the slope of the log of the survival function approaches negative infinity. That is, the behaviour of the Survival function depends on where we are: every time we move away from the mean, the Survival function decreases more and more. In a sense, our knowledge of the tail is not scalable, it changes radically as we move in the tail."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#scalable-extremistan",
    "href": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#scalable-extremistan",
    "title": "Understanding the tail exponent",
    "section": "Scalable: Extremistan",
    "text": "Scalable: Extremistan\nLet’s replicate our analysis of the scalability of our knowledge of the Survival function with a Pareto with \\(\\alpha = 2\\). That is, let’s compare the ratio \\(\\dfrac{S(k)}{S(2k)\\) as \\(k\\) grows larger.\n\nsurvival_pareto <- function(x, alpha) {\n  ifelse(x>=1, (1/x)^alpha, 1)\n}\n\ntibble(k = seq(1, 10, length.out = 10)) %>% \n  mutate(ratio = (survival_pareto(k, alpha = 2))/(survival_pareto(2*k, alpha = 2))) %>% \n  gt() %>% \n  fmt_number(columns = vars(ratio, k)) %>% \n  tab_style(\n    style = cell_text(size = px(15), weight = \"bold\"),\n    locations = cells_body(vars(ratio))\n  ) %>% \n   tab_style(\n    style = cell_text(\n      size = px(11),\n      color = \"#999\",\n      transform = \"uppercase\"\n    ),\n    locations = cells_column_labels(everything())\n  ) %>% \n  cols_width(vars(ratio) ~ px(175))\n\n\n\n\n\n  \n    \n    \n  \n  \n  \n    \n      k\n      ratio\n    \n  \n  \n    1.00\n4.00\n    2.00\n4.00\n    3.00\n4.00\n    4.00\n4.00\n    5.00\n4.00\n    6.00\n4.00\n    7.00\n4.00\n    8.00\n4.00\n    9.00\n4.00\n    10.00\n4.00\n  \n  \n  \n\n\n\n\nSurprisingly, the change of the Survival function does not depend on where we are on the distribution. Indeed, our knowledge of the tail is scalable:\n\\[ S(k) = 4 S(2k)\\]\nIn fact, this is a general property of the Pareto distributions.\nRemember that with the Gaussian, the slope of the log of the Survival function approaches negative infinity. With the result above, we can see that the slope of the Pareto will be constant. Thus:\n\\[ \\dfrac{log(\\dfrac{S(2k))}{S(k)})}{\\Delta log(x)} = \\dfrac{log(\\dfrac{2k^{-\\alpha}}{k^{-\\alpha}})}{\\Delta log(x)} = -\\alpha \\dfrac{\\Delta log(x)}{\\Delta log(x)} = -\\alpha \\]\nThat is, the slope of the log of the Survival is constant and is the negative of the tail exponent. Let’s plot the Zipf plot for the Pareto:\n\npareto_samples <- (1/runif(1000)^(1/2)) #inverse transform sampling\n\ntibble(pareto_samples) %>% \n  ggplot(aes(sample = pareto_samples)) +\n  stat_zipf(alpha = 0.2) +\n  geom_abline(slope = -2, linetype = 2, color = \"deeppink4\") +\n  scale_y_log10() +\n  scale_x_log10() +\n  hrbrthemes::theme_ipsum_rc(grid = \"y\") +\n  labs(title = \"Zipf plot for a Pareto with alpha = 2\",\n       x = \"log(x)\",\n       y = \"log(Survival)\",\n       subtitle = \"The slope of the line is -2\")\n\n\n\n\nTo conclude, the \\(\\alpha\\) can be understood as an approximation the slope of Survival Function. The lower the alpha, the more slowly the Survival function decays and thus the fatter the tail\nTherefore, thin tails can be understood as having an \\(\\alpha = +\\infty\\). To better understand what this entails, let’s plot a Zipfplot for samples from different distributions:\n\ntruncated_normal <- rtruncnorm(10000)\nlognormal <- rlnorm(10000)\nexponential <- rexp(10000)\npareto <- 1/(runif(10000))^(1/1.16) # inverse transform sampling\npareto_squared <- 1/(runif(10000))^(1/0.58)\n\ntibble(sim = 1:10000, truncated_normal, lognormal, exponential, pareto, pareto_squared) %>% \n  pivot_longer(-sim, names_to = \"random_variable\", values_to = \"value\") %>% \n  ggplot(aes(sample = value, color = random_variable)) +\n  stat_zipf(alpha = 0.3) +\n  geom_abline(slope = -1.16, linetype = 2, color = \"deeppink4\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Zipf plot for different distributions\",\n       subtitle = \"For fat-tails, the tail dominates, specially under convex transformations\",\n       x = \"Log(x)\",\n       y = \"Log(Survival(x))\") +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\nTherefore, we can we see how quickly different distributions decay. Whereas the Pareto keeps decaying at a constant pace. This is what makes its tails so dangerous, specially under convex transformations."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#mean-excess-plots",
    "href": "posts/fat-vs-thin-tails/2020-05-19-understanding-the-tail-exponent.html#mean-excess-plots",
    "title": "Understanding the tail exponent",
    "section": "Mean Excess plots",
    "text": "Mean Excess plots\nTo finally hone this point home, we can check the mean excess. For a given threshold \\(v\\), the Mean Excess for a random variable \\(X\\) is:\n\\[ E[X - v | X > v] \\]\nThis is just a weighted sum. Even though the average term keeps increasing as we increase the threshold \\(v\\), for a gaussian the weights decrease so much that the Mean Excess decreases as we increase \\(v\\).\n\ngaussian <- rnorm(10000)\n\ntibble(gaussian) %>% \n  ggplot(aes(sample = gaussian)) +\n  stat_mean_excess(alpha = 0.1)  +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title =\"Mean excess for a Gaussian\",\n       subtitle = \"Converges to naught\",\n       x = \"Mean Excess\")\n\n\n\n\nWhereas for a Pareto r.v., the weighted sum increases. Although the weights decrease, they do not decrease fast enough to counteract the increasing average term that we are summing over. In fact, for a Pareto, the mean excess will be linearly increasing and its slope will have a negative derivative with respect to \\(\\alpha\\).\n\npareto <- 1/(runif(10000))^(1/1.16) # inverse transform sampling\n\ntibble(pareto) %>% \n  ggplot(aes(sample = pareto)) +\n  stat_mean_excess(alpha = 0.1) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Mean Excess Function for a Pareto\",\n       subtitle = \"Linearly increasing Mean Excess\",\n       x = \"mean_excess\")\n\n\n\n\nFinally, a comparison of the mean excess across functions can really help us to digest what is the big difference between fat-tails and other distributions:\n\ntibble(sim = 1:10000, gaussian,  lognormal, exponential, pareto, pareto_squared) %>% \n  pivot_longer(-sim, names_to = \"random_variable\", values_to = \"value\") %>% \n  ggplot(aes(sample = value)) +\n  stat_mean_excess(aes(color = random_variable),alpha = 0.3) +\n  facet_wrap(~random_variable, scales = \"free\", nrow = 3) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Mean Excess across different functions\",\n       color = \"\",\n       subtitle = \"Tails can still be dominant all the way\") +\n  scale_color_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html",
    "title": "Tail Risk of diseases in R",
    "section": "",
    "text": "Pasquale Cirillo and Nassim Taleb published a short, interesting and important paper on the Tail Risk of contagious diseases. In short, the distribution of fatalities is strongly fat-tailed: thus rendering any forecast, whether is pointwise or a distributional forecast, useless and dangerous. The distributional evidence is there: the lack of a characteristic scale makes our uncertainty at any point in the pandemic maximal: we can only say that it can always get worse.\nRead the paper! It’s short and packed of ideas. In this blogpost, I’ll reproduce the main plots, the main model that uses Extreme Value Theory to model the tail of the distribution of casualties and, finally, I will re-implement the model in a Bayesian framework using Stan."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#the-data",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#the-data",
    "title": "Tail Risk of diseases in R",
    "section": "The Data",
    "text": "The Data\nTaleb and Cirillo collected data for 72 events with more than 1,000 estimated victims.\n\n\n\nData summary\n\n\nName\ndata\n\n\nNumber of rows\n72\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1\n9\n34\n0\n69\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nstart_year\n0\n1\n1619.08\n517.26\n-429\n1595.00\n1813.0\n1916.50\n2019\n▁▁▁▁▇\n\n\nend_year\n0\n1\n1613.35\n520.65\n-426\n1593.00\n1813.5\n1923.75\n2020\n▁▁▁▁▇\n\n\nlower\n0\n1\n2660.74\n9915.03\n1\n10.00\n75.5\n850.00\n75000\n▇▁▁▁▁\n\n\navg_est\n0\n1\n4877.66\n19132.36\n1\n10.00\n82.0\n850.00\n137500\n▇▁▁▁▁\n\n\nupper_est\n0\n1\n7094.56\n28705.00\n1\n10.00\n88.0\n850.00\n200000\n▇▁▁▁▁\n\n\nrescaled_avg_est\n0\n1\n84874.62\n409099.85\n2\n41.75\n738.0\n6112.25\n2678283\n▇▁▁▁▁\n\n\npopulation\n0\n1\n2037.94\n2434.28\n50\n554.00\n990.0\n1817.25\n7643\n▇▂▁▁▂\n\n\n\n\n\nThe estimates for the number of casualties are in the thousands. The avg_est is the estimate for the number of casualities that we are going to be working with."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#the-plots",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#the-plots",
    "title": "Tail Risk of diseases in R",
    "section": "The plots",
    "text": "The plots\nTaleb and Cirillo begin the paper with a graphical analysis of the properties of the data. In it, they show that the data present all of the traits of fat-tailed random variables. To reproduce most of the figures, I use an R package that I wrote: ggtails.\n\nMax-to-Sum ratio\nTaleb and Cirillo begin by examining the Max-to-Sum ratio plots. A consequence of the Law of Large Numbers (LLN) is the following:\n\\[\nE[X^p] < \\infty  \\iff R_n^p = \\dfrac{max(X_1^p, \\dots, X_n^p)}{\\sum_{i=1}^n X_i^p} \\to 0, \\ \\text{as} \\ n \\to \\infty\n\\] That is, the theoretical moment \\(p\\) exists if and only if the ratio of the partial max to the partial sum converges to \\(0\\). Neither of the fourth moments converges for neither of the fatalities’ estimates\n\ndata %>% \n  ggplot(aes(sample = avg_est)) +\n  ggtails::stat_max_sum_ratio_plot() +\n  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1)) +\n  scale_color_brewer(palette = 2, type = \"qual\") +\n  labs(title = \"Max-to-Sum ratio plot\",\n       subtitle = \"There's no convergence. No finite moment is likely to exist\")\n\n\n\n\nGiven that none of the moments converges, it is likely that we are dealing with such a fat-tailed random variable that all of the theoretical moments are undefined. Or if the theoretical moments exist, that the Law of Large Numbers works way too slowly for us to use it. In which case any method that relies on any sample moment estimator of the empirical distribution is useless.\n\n\nHistogram\nLet’s check what exactly is the range of values that Cirillo and Taleb have collected:\n\ndata %>% \n  ggplot(aes(avg_est*1000)) +\n  geom_histogram(binwidth = 10^8/80, fill = \"dodgerblue4\", color = \"black\", alpha = 0.5) +\n  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^8)) +\n  labs(x = TeX(\"Casualties (x $10^8$)\"),\n       title = \"Histogram of casualties\",\n       subtitle = \"The data encodes a huge array of variation. No characteristic scale, typical of fat-tails\") \n\n\n\n\nThe data contains an incredible arrange of variation. This is typical for fat-tailed random variables.\n\n\nThe Zipf plot\nFor a Pareto random variable, the slope of the log of the Survival function in log space decays linearly. With the Zipf plot, we compare the decay of the log of the empirical survival function with the linear decay that we expect with a Pareto.\n\ndata %>% \n  ggplot(aes(sample = log10(avg_est*1000))) +\n  ggtails::stat_zipf(color = \"dodgerblue4\", alpha = 0.7)  +\n  scale_x_log10(label=scientific_10) +\n  scale_y_log10() +\n  labs(title = \"Zipf plot of casualties\",\n       subtitle = \"There's the linear decay we expect with fat-tailed variables\",\n       x = \"log(x)\",\n       y = \"log(Survival)\")\n\n\n\n\nThe empirical survival function indeed decays slowly: it’s almost linear. Thus, giving us a hint that we are dealing with a fat-tailed random variable.\n\n\nMean Excess Plot\nFor a given threshold \\(v\\), the Mean Excess for a random variable \\(X\\) is:\n\\[\nE[X - v | X > v]\n\\] For a Pareto, we expect this mean excess to scale linearly with the threshold.\n\ndata %>% \n  ggplot(aes(sample = avg_est*1000)) +\n  ggtails::stat_mean_excess()-> p \ndat <- layer_data(p)\n\ndat %>%\n  filter(y < 3.95e07) %>% \n  ggplot() +\n  geom_point(aes(x, y), color = \"dodgerblue4\", alpha = 0.7) +\n  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^6)) +\n  scale_y_continuous(labels = function(x) scales::number(x, scale = 1/10^7)) +\n  expand_limits(y = 5e07) +\n  labs(title = \"Mean Excess Plot for casualties\",\n       subtitle = \"There's the linear slope that we expect with fat-tailed variables\",\n       caption = \"More volatile observations were excluded.\",\n       x = TeX(\"Threshold (x $10^6$)\"),\n       y = TeX(\" Mean Excess Funtion (x $10^7$)\"))\n\n\n\n\nGiven that the mean excess plot increases linearly, we are even more convinced that the number of casualties is indeed fat-tailed."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#fitting-the-tail",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#fitting-the-tail",
    "title": "Tail Risk of diseases in R",
    "section": "Fitting the tail",
    "text": "Fitting the tail\nThe graphical analysis tells us that we are likely dealing with a fat-tailed random variable: the survival function decays very slowly. Thus, the fat-tails make a) an extremely large array of possibilities relevant; b) thus, eliminating the possibility of a characteristic scale or “typical” catastrophe; c) and possibly making the theoretical moments undefined.\n\nWait a moment: infinite casualties?\nWe know that the number of casualties is bounded by the total population. Thus, the variable only has the appearance of an infinite mean given its upper bound. Graphically, by ignoring the upper bound, we are positing a continuous tail thus:\n\nThe difference, thus, is only relevant in the vicinity of the upper bound \\(H\\). One could thus keep modeling ignoring the upper bound without too many practical consequences. Nevertheless, it would be epistemologically wrong. To solve this problem, Taleb and Cirillo introduce a log transformation that eliminates the upper bound:\n\\[\nZ = \\varphi(Y)=L-H \\log \\left(\\frac{H-Y}{H-L}\\right)\n\\]\nTaleb and Cirillo call \\(Z\\) the dual observations. \\(Z\\) is the variable that we will model with Extreme Value theory.\n\nL <- 1\nH <- 7700000\ndata_to_model <- data %>% \n  mutate(dual = L -  H* log( (H-avg_est) / (H-L) ) )\n\n\n\nExtreme Value theory on the dual observations\nA logical question, then, is how fat-tailed is exactly the tail of casualties from contagious diseases? Extreme Value Theory offers an answer. Indeed, the Pickands–Balkema–de Haan theorem states that tail events (events larger than a large threshold) have as a limiting distribution a Generalized Pareto Distribution (GPD). In math, for large u, the conditional excess function is thus defined and approximated by a GPD:\n\\[\nG_{u}(z)=P(Z \\leq z \\mid Z>u)=\\frac{G(z)-G(u)}{1-G(u)} \\approx GPD(z; \\xi, \\beta, u)\n\\] Where \\(\\xi\\) is the crucial shape parameter that determines how slowly the tail decays. The larger \\(\\xi\\), the more slowly it decays. For example, the variance is only defined for \\(\\xi < 0.5\\). For \\(\\xi > 1\\), the theoretical mean is not defined.\nCrucially, we can approximate the tail of the original distribution \\(G(z)\\) with a \\(GPD\\) with the same shape parameter \\(\\xi\\). Finally, Taleb and Cirillo use \\(200,000\\) as a threshold \\(u\\). We can check both in the Mean Excess Plot and the Zipf plot that around this value we observe a power-law like behavior.\n\n\nMaximum Likelihood estimate\nWe can fit the GPD via maximum likelihood.\n\nfit <- evir::gpd(data_to_model$dual, threshold = 200)\nround(fit$par.ests, 2)\n\n     xi    beta \n   1.62 1174.74 \n\n\nWhich are the same estimates as Taleb and Cirillo. The standard errors are thus:\n\nround(fit$par.ses, 2)\n\n    xi   beta \n  0.52 534.44 \n\n\nJust as we saw with our graphical analysis, the variable is definitely fat-tailed: \\(\\xi > 0\\), which thanks to the relationship of the the Pickands–Balkema–de Haan with the Fisher-Tippet theorem, tells us that the Maximum Domain of Attraction is thus a Fréchet. From Taleb and Cirillo’s paper:\n\nAs expected \\(\\xi\\) > 1 once again supporting the idea of an infinite first moment… Looking at the standard error of \\(\\xi\\), one could also argue that, with more data from the upper tail, the first moment could possibly become finite. Yet there is no doubt about the non-existence of the second moment, and thus about the unreliability of the sample mean, which remains too volatile to be safely used.\n\nLet’s see if we can reproduce this conclusions in a bayesian framework."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#bayesian-model",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#bayesian-model",
    "title": "Tail Risk of diseases in R",
    "section": "Bayesian model",
    "text": "Bayesian model\nI’ll sample from the posterior using Stan’s incredible implementation of Hamiltonian Monte Carlo. Most of the heavy lifting in Stan has already been done by Aki Vehtari in a case study using the GPD.\nOur bayesian model will be thus defined:\n\\[\ny \\sim GPD(\\xi, \\beta, u = 200) \\\\\n\\xi \\sim Normal(1, 1) \\\\\n\\beta \\sim Normal(1000, 300)\n\\] The prior for \\(\\xi\\) is weakly informative and yet still opens the opportunity for the data to move our posterior towards a finite mean and finite variance.\n\nSimulating fake data\nTo verify that our code is correctly working, we’ll simulate data and assess parameter recovery.\n\nexpose_stan_functions(\"gpd.stan\")\n\nfake_data <- replicate(1e3, gpareto_rng(ymin = 200, 0.8, 1000))\nds<-list(ymin=200, N=1e3, y=fake_data)\nfit_gpd <- stan(file='gpd.stan', data=ds, refresh=0,\n                     chains=4, seed=100, cores = 4)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.3 seconds.\nChain 2 finished in 1.3 seconds.\nChain 3 finished in 1.3 seconds.\nChain 4 finished in 1.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.3 seconds.\nTotal execution time: 1.8 seconds.\n\nprint(fit_gpd, pars = c(\"xi\", \"beta\"))\n\nInference for Stan model: gpd-202211241219-1-288048.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n        mean se_mean    sd   2.5%     25%     50%     75%   97.5% n_eff Rhat\nxi      0.78    0.00  0.06   0.67    0.74    0.78    0.82    0.90   942    1\nbeta 1111.34    2.24 65.43 983.84 1068.06 1112.49 1153.74 1241.58   856    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:38 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe credible intervals are in line with the true parameters. Graphically:\n\nposterior <- as.matrix(fit_gpd, pars = c(\"xi\", \"beta\"))\ntrue <- c(0.8, 1000)\nmcmc_recover_hist(posterior, true) +\n  labs(title = \"Parameter recovery\",\n       subtitle = \"We successfully recover the parameters\")\n\n\n\n\nWe can thus reasonably recover our parameter of interest \\(\\xi\\) with our current model. Therefore, we can follow along and fit our model to the real data.\n\n\nFitting the model\nUsing the dual observations, we end up with \\(25/72\\) observations, around \\(34.7\\)% of the total number of observations\n\ndata_to_model %>% \n  filter(dual > 200) %>% \n  pull(dual) -> dual_observations\nsummary(dual_observations)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    200     800    1500   14040    7504  138742 \n\n\nFitting the model is just as simple as fitting it to fake data:\n\nds<-list(ymin=200, N=25, y=dual_observations)\nfit_gpd <- stan(file='gpd.stan', data=ds,\n                     chains=4, seed=100, cores = 4, iter = 5000)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 1 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 2 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 3 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 4 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 4 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 1 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 2 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 3 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n\nprint(fit_gpd, pars = c(\"xi\", \"beta\"))\n\nInference for Stan model: gpd-202211241219-1-6efc8a.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat\nxi      1.69    0.01   0.43   0.99   1.39    1.65    1.95    2.66  5847    1\nbeta 1077.46    3.32 246.93 624.37 902.09 1069.18 1240.92 1579.44  5519    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:51 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nFor know, the \\(\\widehat R\\) values look OK, which indicates that there’s no much disagreement between the Markov Chains. The credible intervals rule out the possibility that \\(\\xi < 0\\).\nWe can intuitively interrogate the posterior for more precise questions in a Bayesian settings. For example, what is the probability that the theoretical mean is undefined, i.e., that \\(\\xi > 1\\):\n\n\n\n\n\nTherefore, we conclude that it is very likely that the theoretical mean is undefined and we rule out definitely the possibility of a finite second moment. Thus, we reproduce the paper’s conclusions.\nGiven the fat-tailedness of the data and the resulting lack of characteristic scale from the huge array of variability that it’s possible, there’s no possibility of forecasting what we may face with either a pointwise prediction or a distributional forecast.\n\n\nPosterior predictive checks\nIf the model fits well to the data, the replicated data under the model should look similar to the observed data. We can easily generate data from our model because our model is generative: we draw simulated values from the posterior predictive distribution of replicated data.\nHowever, with fat-tailed data: what exactly does it mean for our replicated data to track our observed data. For example, what about the replicated maxima? These definitely should include the observed maxima: but also much larger values. That is the whole point of being in the MDA of a Fréchet.\n\nyrep <- extract(fit_gpd)$yrep \nppc_stat(log(dual_observations), log(yrep), stat = \"max\") +\n  labs(title = \"Posterior predictive check (log scale)\",\n       subtitle = \"Maxima across replicated datasets track observed maximum and beyond, just like it should\",\n       x = \"log(maxima)\")\n\n\n\n\nAnd this is exactly what our model does: a difference of 30 in a log-scale is huge. We can just as well expect maxima as large as observed, and even much larger.\n\n\nConvergence diagnostics\nOne of the main benefits of Stan’s implementation of HMC is that it yells at you when things have gone wrong. We can thus check a variety of diagnostics to check for convergence. Above, we examined that the \\(\\widehat R\\) values look OK. We can also check trace plots:\n\nposterior <- as.array(fit_gpd)\ncolor_scheme_set(\"viridis\")\nmcmc_trace(posterior, pars = c(\"xi\", \"beta\")) +\n  labs(title = \"Traceplots\",\n       subtitle = \"Traceplots are stationary, well mixed and the chains converge\")\n\n\n\n\nWe can check HMC specific diagnostics:\n\ncheck_divergences(fit_gpd)\ncheck_treedepth(fit_gpd)"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#stressing-the-data",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#stressing-the-data",
    "title": "Tail Risk of diseases in R",
    "section": "Stressing the data",
    "text": "Stressing the data\nTaleb and Cirillo are well aware that they are not working with the more precise of data. Thus, they ‘stress’ the data and check whether the results still hold. Given that we are working with fat-tailed data, the tail wags the dog: data problems can only modify the results if they are in the tail.\n\nMeasurement error\nTo account for measurement error, Taleb and Cirillo recreate 10,000 of their datasets, but this time where each observation is allowed to vary between 80% and 120% of its recorded values. They find that their results are robust to this modification.\nIn a bayesian setting, we can very easily extend our model to account for uncertainty around the true data. Indeed, in a bayesian model there’s no fundamental difference between a parameter and an observation: one is observed and the other is not. Thus, we formulate the true casualties being measured as missing data.\nTherefore, we specify that the measurement comes a normal with unknown mean, the true number of casualties, and that their standard deviation is 20% of the observed value.\n\\[\ny \\sim Normal(y_{true}, y*0.2)\n\\] We fit the model and let Bayes do the rest:\n\nds<-list(ymin=200, N=25, y=dual_observations, noise = 0.2*dual_observations)\nfit_gpd_m <- stan(file='gpd_measurementerror.stan', data=ds,\n                     chains=4, seed=100, cores = 4, iter = 5000)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 1 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 2 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 2 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 3 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 4 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 4 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 1 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 2 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n\nprint(fit_gpd_m, pars = c(\"xi\", \"beta\"))\n\nInference for Stan model: gpd_measurementerror-202211241219-1-98c1f4.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat\nxi      1.66    0.00   0.43   0.95   1.35    1.62    1.92    2.61 13151    1\nbeta 1056.99    1.99 248.82 583.47 887.32 1049.45 1219.83 1562.54 15706    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:57 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe model sampled remarkably well. We can thus check whether our inferences still hold:"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#influential-observations",
    "href": "posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html#influential-observations",
    "title": "Tail Risk of diseases in R",
    "section": "Influential observations",
    "text": "Influential observations\nTaleb and Cirillo also stress their data by recreating the dataset 10,000 times and then eliminating from 1 to 7 of the observations with a jacknife resampling procedure. Thus, they confirm that no single observation is driving the inference.\nIn a bayesian setting, we can check for influential observations by comparing the full-data predictive posterior distribution to the Leave-one-out predictive posterior for each left out point. That is: we compare \\(p(y_i | y)\\) with \\(p(y_i, | y_{-i})\\).\nWe can quickly estimate \\(p(\\theta_i, | y_{-i})\\) for each \\(i\\) by just sampling from \\(p(\\theta | y)\\) using Pareto Smoothed Importance Sampling LOO. By examining the distribution of the importance weights, we can compare how different the two distributions are. If the weights are too fat-tailed, and the variance is infinite for the \\(j\\)th observation, then the \\(j\\)th observation is highly influential observation determining our posterior distribution.\n\nloglik <- extract(fit_gpd)$log_lixi\nloopsis <- loo::loo(loglik, loo::relative_eff(exp(loglik)))\nplot(loopsis)\n\n\n\n\nIndeed, no single observation is driving our inference."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html",
    "href": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html",
    "title": "Extreme Value Theory for Time Series",
    "section": "",
    "text": "The Fisher-Tippet theorem (a type of CLT for the tail events) rests on the assumption that the observed values are independent and identically distributed. However, in any non trivial example, time series will reflect an underlying structure that will create dependence among the observations. Indeed, tail events tend to occur in clusters. Does this mean that we cannot use the Extreme Value Theory (EVT) to model the maxima of a time series?\nThe answer? Not necessarily. We can only use EVT if the maxima of the time series behave like they are independent. In this blogpost, I’ll give:\nNote that thorought the blogpost I assume that the time series is stationary."
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#the-d-condtions",
    "href": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#the-d-condtions",
    "title": "Extreme Value Theory for Time Series",
    "section": "The D-Condtions",
    "text": "The D-Condtions\nThe D-conditions limit the dependence structure between the maxima of a time series, thereby allowing us to use EVT. There are two of them:\n\nThe \\(D\\) condition limits the long-range dependence between the maxima of a time series. That is, separate the time series into two intervals: the \\(D\\) condition states that the maxima of the two separated intervals are approximately independent.\nThe \\(D'\\) condition limits the local dependence structure between the maxima of a time series. That is, separate the time series into small blocks. Count as an exceedance an observation that exceeds a given large threshold. The \\(D'\\) condition postulates that the probability of observing more than one exceedance in a block is negligible.\n\nIf both \\(D\\) and \\(D'\\) are satisfied, then the dependence between far-apart maxima and local maxima is largely limited. Therefore, we can generalize the Fisher-Tippet Theorem to work with these type of time series. However, how would one check these conditions with real data?"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#record-breaking-observations",
    "href": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#record-breaking-observations",
    "title": "Extreme Value Theory for Time Series",
    "section": "Record breaking observations",
    "text": "Record breaking observations\nWe can compare the number of record-breaking observations in our time series with the expected number of record-breaking observations for independent observations. If they are roughly similar, we can conclude that both the \\(D\\) conditions hold for our time series. Let’s begin defining how we count the number of record-breaking of observations:\n\\[\nN_n=1+\\sum_{k=2}^n1_{X_k>M_{k-1}}, \\quad n\\geq2\n\\] Then, it can be shown that if the observations are independent, the expected number of record-breaking observations is:\n\\[\nE[N_n]=\\sum_{k=1}^n \\frac{1}{k} \\approx \\log n +\\gamma\n\\] Where \\(\\gamma\\) is Euler’s constant. Therefore, for independent observations, the number of record-breaking observations grows very slowly. We can also check the variance of the number of record-breaking observations for independent observations:\n\\[\nvar(N_n)= \\sum_{k=1}^n \\left(\\frac{1}{k} - \\frac{1}{k^2} \\right)\n\\] Let’s try to get some intuition for how these formulas bound the number of record breaking observations when independence holds.\n\nA Monte-Carlo proof\nTo prove this statement, I’ll perform a Monte-Carlo experiment with \\(10^4\\) different independent time series with marginal distribution Cauchy. For each time series, I’ll simulate 1,000 observations. Then, we can compare the Monte Carlo distribution of the number of observed record-breaking observations with the expected number.\n\n\n\n\n\n\n\nSP500 tail returns: independent?\nWe can divide the returns of the S&P500 in positive and negative returns. Can we model the tail returns for either of them with EVT? As we’ve seen, the answer depends on the \\(D\\) conditions: are the maxima too clustered? are far-apart maxima related? If the answer to both questions is no, then we can use EVT.\nTo test it out, we will compare the number of record breaking returns (both positive and negative) with the expected number of record breaking returns if the returns where independent. In this analysis, I use all the data from 1948 up to the present day and I’ll model the log returns:"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#generalizing-the-fisher-tippet-theorem",
    "href": "posts/fat-vs-thin-tails/2020-06-17-extreme-value-theory-for-time-series.html#generalizing-the-fisher-tippet-theorem",
    "title": "Extreme Value Theory for Time Series",
    "section": "Generalizing the Fisher-Tippet Theorem",
    "text": "Generalizing the Fisher-Tippet Theorem\nIf both \\(D\\) conditions hold, we can generalize the Fisher-Tippet Theorem. Crucially, the maxima of the time series will still converge to one of the \\(GEV_{\\xi}\\) distributions. However, it will converge to a transformed version of the \\(GEV\\) thus:\nLet ( {X_{i}} ) be a dependent time series and let ( {{i}} ) be independent variables with the same marginal distribution. Set ( M{n}={X_{1}, , X_{n}} ) and ( {n}={{1}, , {n}} . ) If the \\(D\\) conditions hold, then: [ {({n}-b_{n}) / a_{n} z} (z), n ] if and only if [ {(M_{n}-b_{n}) / a_{n} z} G(z) ] where ( G(z)={GEV_{}}^{}(z) ) for some constant ( ) which is called the extremal index of the time series.\n\nConsequences\n\nThe \\(\\theta\\) is a measure of the clustering of the maxima. The lower the theta, the more clustered are the maxima.\nIf the observations are independent, \\(\\theta = 1\\). Thus, the extremal index is a measure of dependence between the data. The smaller the extremal index, the more dependent are the maxima of the time series. In particular:\n\n\\[\nP(M_n < x) \\approx F(x)^{n\\theta}\n\\]\nTherefore, we can consider these maxima arising from a dependent time series as equivalently arising from \\(n\\theta\\) independent observations with the same marginal distribution.\n\nUsing the independence assumption leads us to underestimate the quantiles of the possible maxima. Indeed, for a large probability p:\n\n\\[\nF^{-1}(p) \\approx GEV^{-1}\\left(p^{n \\theta}\\right)>GEV^{-1}\\left(p^{n}\\right)\n\\] Indeed, when considering the dependence of the data, the VaR risk (for say 99%) measure will decrease. The probability of none of the extreme events ever happening decreases. However, as we will see, when it rains, it pours.\n\nCrucially, both \\({GEV_{\\xi}}^{\\theta}(z), {GEV_{\\xi}}(z)\\) share the same shape (\\(xi\\)) parameter: thus, they share the same tail behavior. Indeed, by raising the distribution to the power of \\(\\theta\\), the parameters of the distribution change thus:\n\n\\[\n\\tilde{\\mu}=\\mu-\\frac{\\sigma}{\\xi}\\left(1-\\theta^{-\\xi}\\right), \\quad \\tilde{\\sigma}=\\sigma \\theta^{\\xi}, \\quad \\tilde{\\xi}=\\xi\n\\]\nThat is, when \\(\\xi > 0\\) and the MDA of the distribution is the Fréchet, the location and scale parameters change thus:\n\n\n\n\n\nWhy does the mean increase so much when the maxima are \\(\\theta\\) is small? The answer is in the dependence of the maxima: when \\(\\theta\\) is small, the maxima are dependent. Thus, when one of them happens, all the other events also tend to happen. When it rains, it pours.\nTo show this, I’ll simulate observations from an Autoregressive(1) Cauchy Sequence:\n\\[\nX_t = \\rho X_{t-1} + (1-\\rho) Z_t \\\\\nZ_t \\sim Cauchy\n\\]\nThe larger \\(\\rho\\), the more dependent the data. Indeed, it can be shown1 that \\(\\theta = 1 - \\rho\\). Therefore,"
  },
  {
    "objectID": "posts/fat-vs-thin-tails/2020-05-26-r-squared-and-fat-tails.html",
    "href": "posts/fat-vs-thin-tails/2020-05-26-r-squared-and-fat-tails.html",
    "title": "R-squared and fat tails",
    "section": "",
    "text": "This post continues to explore how common statistical methods are unreliable and dangerous when we are dealing with fat-tails. So far, we have seen how the distribution of the sample mean, PCA and sample correlation turn into pure noise when we are dealing with fat-tails. In this post, I’ll show the same for \\(R^2\\) (i.e., coefficient of determination). Remember, it is a random variable that we are estimating and thefore has its own distribution.\nIn short, the goal is to justify with simulations Nassim Taleb’s conclusion in his latest technical book regarding R-squared:\n\nWhen a fat tailed random variable is regresed against a thin tailed one, the coefficient of determination \\(R^2\\) will be biased higher, and requires a much larger sample size to converge (if it ever does)\n\n\n\nI’ll follow the same gameplan as usual: explore with Monte-Carlo the distribution of our estimator in both Mediocristan and Extremistan.\n\n\n\nAssume the usual scenario in a Gaussian regression: Gaussian errors.\n\n# simulate\nn <- 10^6\nx <- rnorm(n)\ny <- rnorm(n, mean = 0.2 + 1 * x)\n\nLet’s plot (some of ) the data:\n\ndata.frame(x, y)[sample(n, 10^4), ] %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Gaussian Regression\")\n\n\n\n\n\nglue::glue(\"The correlation coefficient is: {round(cor(x,y), 2)}\")\n\nThe correlation coefficient is: 0.71\n\n\nThen, the \\(R^2\\) should be the squared of this: \\(0.50\\)\n\nfit <- lm(y ~ 1 + x, data = data.frame(x, y))\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ 1 + x, data = data.frame(x, y))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8839 -0.6735  0.0003  0.6743  4.7843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.1994577  0.0009997   199.5   <2e-16 ***\nx           0.9994888  0.0009987  1000.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9997 on 999998 degrees of freedom\nMultiple R-squared:  0.5004,    Adjusted R-squared:  0.5004 \nF-statistic: 1.002e+06 on 1 and 999998 DF,  p-value: < 2.2e-16\n\n\n\nbroom::glance(fit)$r.squared\n\n[1] 0.5004092\n\n\nWhich indeed it is1. Let’s create a Monte-Carlo function to simulate smaller samples and check the convergence of the \\(R^2\\).\n\nsimulate_R_two <- function(n = 30) {\n  x <- rnorm(n)\n  y <- rnorm(n, mean = 0.2 + 1 * x )\n  fit <- lm(y ~ 1 + x, data = data.frame(x, y))\n  r2 <- broom::glance(fit)$r.squared\n  data.frame(r_squared = r2)\n}\n\nrerun(1000, simulate_R_two()) %>% \n  bind_rows() -> r_squareds_30\n\nrerun(1000, simulate_R_two(n = 100)) %>% \n  bind_rows() -> r_squareds_100\n\nrerun(1000, simulate_R_two(n = 1000)) %>% \n  bind_rows() -> r_squareds_1000\n\nLet’s plot the results\n\ndata.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %>% \n  rename(sample_30 = r_squared,\n         sample_100 = r_squared.1,\n         sample_1000 = r_squared.2) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"r_squared\") %>% \n  mutate(sample = str_extract(sample, \"\\\\d+\"),\n         sample = glue::glue(\"{sample} obs per sample\"),\n         sample = factor(sample)) %>% \n  ggplot(aes(r_squared, fill = sample)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0.5), linetype = 2, color = \"red\") +\n  facet_wrap(~sample) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"none\") +\n  labs(caption = \"Binwidth is 0.05\",\n       title = \"Mediocristan: Distribution of R-squared values\",\n       subtitle = \"Gaussian Regression. True R^2 shown as red line.\",\n       x = \"R squared\")\n\n\n\n\nTherefore, when we are dealing with randomness coming from Mediocristan, we can reliably use our estimates of the R-squared. They converge at a good pace toward the true vlue.\n\n\n\nNow, let’s swtich pace and sample from Extremistan. Imagine then, our same simulation as before. However, instead of our noise coming from a Gaussian, our noise will come from a Pareto with tail exponent of \\(1.5\\) (theoretical mean exists but higher moments do not). Let’s simulate:\n\nn <- 10^5\n\nx <- rnorm(n)\n\npareto_errors <- (1/runif(n)^(1/1.5))\n\ny <- 0.2 + 10*x + pareto_errors\n\nBefore we plot, let’s think through what exactly is \\(R^2\\): it defines the proportion of the total variance of our outcome variable that is explained by our model. However, when the errors are Pareto distributed, our outcome variable is also Pareto distributed (with the same tail exponent). Therefore, the outcome variable won’t have a theoretical variance. That is, it will have an infinite variance. As you can imagine, no matter what variance the model explains, it is going to be tiny in comparison to the total variance. Thus, we arrive at the following: the true \\(R^2\\) is zero. That is: \\(E[R^2] = 0\\)\n\ndata.frame(x, y) %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Pareto Regression\")\n\n\n\n\n\nfit <- lm(y ~ 1 + x, data = data.frame(x, y))\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ 1 + x, data = data.frame(x, y))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n    -4     -2     -2     -1  39405 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.5633     0.3988   8.935   <2e-16 ***\nx            10.4619     0.3993  26.204   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 126.1 on 99998 degrees of freedom\nMultiple R-squared:  0.00682,   Adjusted R-squared:  0.00681 \nF-statistic: 686.6 on 1 and 99998 DF,  p-value: < 2.2e-16\n\n\nEven with \\(10^5\\) observations, we are way off mark here. This is the same problem as we had with the other estimators. There isn’t enough data. As Taleb says:\n\n\\(R^2\\) … is a stochastic variable that will be extremely sample dependent, and only stabilize for large n, perhaps even astronomically large n\n\nTo show this, let’s create with Monte-Carlo simulations the distribution of the sample R-squared:\n\nsimulate_R_two <- function(n = 30) {\n  x <- rnorm(n)\n  pareto_errors <- (1/runif(n)^(1/1.5))\n  y <- 0.2 + 10*x + pareto_errors\n  fit <- lm(y ~ 1 + x, data = data.frame(x, y))\n  r2 <- broom::glance(fit)$r.squared\n  data.frame(r_squared = r2)\n}\n\nrerun(1000, simulate_R_two()) %>% \n  bind_rows() -> r_squareds_30\n\nrerun(1000, simulate_R_two(n = 100)) %>% \n  bind_rows() -> r_squareds_100\n\nrerun(1000, simulate_R_two(n = 1000)) %>% \n  bind_rows() -> r_squareds_1000\n\nLet’s plot our results:\n\ndata.frame(sim = 1:1000, r_squareds_30, r_squareds_100, r_squareds_1000) %>% \n  rename(sample_30 = r_squared,\n         sample_100 = r_squared.1,\n         sample_1000 = r_squared.2) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"r_squared\") %>% \n  mutate(sample = str_extract(sample, \"\\\\d+\"),\n         sample = glue::glue(\"{sample} obs per sample\"),\n         sample = factor(sample)) %>% \n  ggplot(aes(r_squared, fill = sample)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0), linetype = 2, color = \"red\") +\n  facet_wrap(~sample) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  theme(legend.position = \"none\") +\n  labs(caption = \"Binwidth is 0.05\",\n       title = \"Extremistan: Distribution of R-squared values\",\n       subtitle = \"Pareto (infinite variance) Regression. True R-squared is zero\",\n       x = \"R squared\")\n\n\n\n\n\n\n\nAs Taleb reminds us, \\(R^2\\) is a stochastic variable. When the variance of our outcome variable approaches infinity, the \\(E[R^2] \\to 0\\). However, to get this result in sample we must get a good estimate of the variance of our outcome variable in the first place. As we have seen, the Law of Large Numbers is way too slow to be useful when dealing with fat-tailed variables. Therefore, to get a good estimate of \\(R^2\\) we will need an astronomically large sample size; otherwise, we will be estimating noise.\nTo conclude, \\(R^2\\) should not be used when we are dealing in Extremistan. Whatever we estimate, it’s going to be pure noise. Even when the variance is not undefined, it will still be biased upwards."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "",
    "text": "Statistical Rethinking is a fabulous course on Bayesian Statistics (and much more). By following simulations in the book, I recently tried to understand why pooling is the process and shrinkage is the result. In this post, I’ll try to do the same for a model where we pool across intercepts and slopes. That is, we will posit a multivariate common distribution for both intercept and slopes to impose adaptive regularization on our predictions."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#the-joint-distribution",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#the-joint-distribution",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "The joint distribution",
    "text": "The joint distribution\nRemember that “priors are not ontology, but epistemology”. Therefore, we never include the priors on our simulations. Let’s begin by setting up the parameters of the common joint distribution of intercepts and slopes.\n\n\n\nWe then construct the var-cov matrix:\n\n\n      [,1]  [,2]\n[1,]  1.00 -0.35\n[2,] -0.35  0.25"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#simulate-for-each-cafe",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#simulate-for-each-cafe",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "Simulate for each cafe",
    "text": "Simulate for each cafe\n\n\n num [1:20, 1:2] 4.22 2.01 4.57 3.34 1.7 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nThat is, for each of the cafés we have one intercept and one slope. Let’s check how our simulated intercepts and slopes fit in the overall joint distribution:\n\n\n\n\n\nThe points that lie in the farthest ellipses are the “outliers”. Note that most of the samples surround the center of the distribution."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#simulate-observations",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#simulate-observations",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "Simulate observations",
    "text": "Simulate observations\nOnce we have an \\(\\alpha, \\beta\\) for each café, we can simulate the waiting times for each of them:\n\n\n# A tibble: 20 × 2\n# Groups:   cafe_ids [20]\n   cafe_ids     n\n      <int> <int>\n 1        1     6\n 2        2    10\n 3        3    16\n 4        4    20\n 5        5     6\n 6        6    10\n 7        7    16\n 8        8    20\n 9        9     6\n10       10    10\n11       11    16\n12       12    20\n13       13     6\n14       14    10\n15       15    16\n16       16    20\n17       17     6\n18       18    10\n19       19    16\n20       20    20\n\n\nNow, we are ready to simulate:\n\n\nRows: 260\nColumns: 3\nRowwise: cafe_ids\n$ cafe_ids  <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, …\n$ afternoon <int> 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n$ wait      <dbl> 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.543…\n\n\nIn Richard’s words:\n\nExactly the sort of data that is well-suited to a varying slopes model. There are multiple clusters in the data. These are the cafés. And each cluster is observed under differened conditions. So it’s possible to estimate both an individual intercept for each cluster, as well as an individua slope."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#fixed-effects-maximal-overfit",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#fixed-effects-maximal-overfit",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "Fixed effects: Maximal Overfit",
    "text": "Fixed effects: Maximal Overfit\nWhen we do not allow any pooling at all, we maximally overfit. The prediction for both afternoon and morning, for each café, will be the observed mean at each of the times."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#partial-pooling-multilevel-model",
    "href": "posts/bayesian-statistics/2020-06-01-understanding-pooling-across-intercepts-and-slopes.html#partial-pooling-multilevel-model",
    "title": "Understanding Pooling across Intercepts and Slopes",
    "section": "Partial Pooling: Multilevel model",
    "text": "Partial Pooling: Multilevel model\nInstead of ignoring the rest of the cafes when doing our predictions, let’s pool the information across cafes and across parameter types. To do so, let’s finish setting up the model that we had at the beginning:\n\\[ W_i \\sim Normal(\\mu, \\sigma) \\]\n\\[ \\mu_i = \\alpha_{ café_{[i]} } + \\beta_{ café_{[i]} } A_i \\]\n\\[ \\begin{bmatrix}\n           \\alpha_{café_j} \\\\\n           \\beta_{cafe_j}\n         \\end{bmatrix}  =  MVNormal ( \\begin{bmatrix}\n           \\alpha \\\\\n           \\beta\n         \\end{bmatrix} , S)\\]\nThe only thing left is to posit priors for the parameters. First, we write the var-cov matrix thus:\n\\[ S = \\begin{bmatrix}\n  \\sigma_{\\alpha} \\  0 \\\\\n  0 \\ \\sigma_{\\beta}\n\\end{bmatrix} R \\begin{bmatrix}\n  \\sigma_{\\alpha} \\  0 \\\\\n  0 \\ \\sigma_{\\beta}\n\\end{bmatrix} \\]\nWe do this to set up the prior for the correlation matrix thus:\n\\[ R \\sim \\text{LKJcorr}(2) \\]\nThe rest will be traditional priors. Let’s focus for a second on the LKJcorr.\n\nLKJcorr interlude\nFor our purposes, what we need to do is to understand how our prior changes with the parameter of the distribution \\(\\eta\\):\n\n\nNULL\n\n\n\n\nFit the model\nFinally, we are ready to fit our partial pooling model.\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 10000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 10000 [  1%]  (Warmup) \nChain 2 Iteration:    1 / 10000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 10000 [  1%]  (Warmup) \nChain 3 Iteration:    1 / 10000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 10000 [  1%]  (Warmup) \nChain 4 Iteration:    1 / 10000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 10000 [  1%]  (Warmup) \nChain 1 Iteration:  200 / 10000 [  2%]  (Warmup) \nChain 1 Iteration:  300 / 10000 [  3%]  (Warmup) \nChain 2 Iteration:  200 / 10000 [  2%]  (Warmup) \nChain 2 Iteration:  300 / 10000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 10000 [  2%]  (Warmup) \nChain 3 Iteration:  300 / 10000 [  3%]  (Warmup) \nChain 4 Iteration:  200 / 10000 [  2%]  (Warmup) \nChain 4 Iteration:  300 / 10000 [  3%]  (Warmup) \nChain 1 Iteration:  400 / 10000 [  4%]  (Warmup) \nChain 2 Iteration:  400 / 10000 [  4%]  (Warmup) \nChain 1 Iteration:  500 / 10000 [  5%]  (Warmup) \nChain 2 Iteration:  500 / 10000 [  5%]  (Warmup) \nChain 3 Iteration:  400 / 10000 [  4%]  (Warmup) \nChain 3 Iteration:  500 / 10000 [  5%]  (Warmup) \nChain 4 Iteration:  400 / 10000 [  4%]  (Warmup) \nChain 4 Iteration:  500 / 10000 [  5%]  (Warmup) \nChain 1 Iteration:  600 / 10000 [  6%]  (Warmup) \nChain 1 Iteration:  700 / 10000 [  7%]  (Warmup) \nChain 2 Iteration:  600 / 10000 [  6%]  (Warmup) \nChain 3 Iteration:  600 / 10000 [  6%]  (Warmup) \nChain 4 Iteration:  600 / 10000 [  6%]  (Warmup) \nChain 1 Iteration:  800 / 10000 [  8%]  (Warmup) \nChain 2 Iteration:  700 / 10000 [  7%]  (Warmup) \nChain 2 Iteration:  800 / 10000 [  8%]  (Warmup) \nChain 3 Iteration:  700 / 10000 [  7%]  (Warmup) \nChain 3 Iteration:  800 / 10000 [  8%]  (Warmup) \nChain 4 Iteration:  700 / 10000 [  7%]  (Warmup) \nChain 1 Iteration:  900 / 10000 [  9%]  (Warmup) \nChain 2 Iteration:  900 / 10000 [  9%]  (Warmup) \nChain 3 Iteration:  900 / 10000 [  9%]  (Warmup) \nChain 4 Iteration:  800 / 10000 [  8%]  (Warmup) \nChain 1 Iteration: 1000 / 10000 [ 10%]  (Warmup) \nChain 1 Iteration: 1100 / 10000 [ 11%]  (Warmup) \nChain 2 Iteration: 1000 / 10000 [ 10%]  (Warmup) \nChain 3 Iteration: 1000 / 10000 [ 10%]  (Warmup) \nChain 4 Iteration:  900 / 10000 [  9%]  (Warmup) \nChain 4 Iteration: 1000 / 10000 [ 10%]  (Warmup) \nChain 1 Iteration: 1200 / 10000 [ 12%]  (Warmup) \nChain 2 Iteration: 1100 / 10000 [ 11%]  (Warmup) \nChain 3 Iteration: 1100 / 10000 [ 11%]  (Warmup) \nChain 3 Iteration: 1200 / 10000 [ 12%]  (Warmup) \nChain 4 Iteration: 1100 / 10000 [ 11%]  (Warmup) \nChain 1 Iteration: 1300 / 10000 [ 13%]  (Warmup) \nChain 2 Iteration: 1200 / 10000 [ 12%]  (Warmup) \nChain 2 Iteration: 1300 / 10000 [ 13%]  (Warmup) \nChain 3 Iteration: 1300 / 10000 [ 13%]  (Warmup) \nChain 4 Iteration: 1200 / 10000 [ 12%]  (Warmup) \nChain 1 Iteration: 1400 / 10000 [ 14%]  (Warmup) \nChain 1 Iteration: 1500 / 10000 [ 15%]  (Warmup) \nChain 2 Iteration: 1400 / 10000 [ 14%]  (Warmup) \nChain 3 Iteration: 1400 / 10000 [ 14%]  (Warmup) \nChain 4 Iteration: 1300 / 10000 [ 13%]  (Warmup) \nChain 4 Iteration: 1400 / 10000 [ 14%]  (Warmup) \nChain 1 Iteration: 1600 / 10000 [ 16%]  (Warmup) \nChain 2 Iteration: 1500 / 10000 [ 15%]  (Warmup) \nChain 2 Iteration: 1600 / 10000 [ 16%]  (Warmup) \nChain 3 Iteration: 1500 / 10000 [ 15%]  (Warmup) \nChain 3 Iteration: 1600 / 10000 [ 16%]  (Warmup) \nChain 4 Iteration: 1500 / 10000 [ 15%]  (Warmup) \nChain 1 Iteration: 1700 / 10000 [ 17%]  (Warmup) \nChain 1 Iteration: 1800 / 10000 [ 18%]  (Warmup) \nChain 2 Iteration: 1700 / 10000 [ 17%]  (Warmup) \nChain 3 Iteration: 1700 / 10000 [ 17%]  (Warmup) \nChain 4 Iteration: 1600 / 10000 [ 16%]  (Warmup) \nChain 1 Iteration: 1900 / 10000 [ 19%]  (Warmup) \nChain 2 Iteration: 1800 / 10000 [ 18%]  (Warmup) \nChain 3 Iteration: 1800 / 10000 [ 18%]  (Warmup) \nChain 4 Iteration: 1700 / 10000 [ 17%]  (Warmup) \nChain 4 Iteration: 1800 / 10000 [ 18%]  (Warmup) \nChain 1 Iteration: 2000 / 10000 [ 20%]  (Warmup) \nChain 2 Iteration: 1900 / 10000 [ 19%]  (Warmup) \nChain 3 Iteration: 1900 / 10000 [ 19%]  (Warmup) \nChain 4 Iteration: 1900 / 10000 [ 19%]  (Warmup) \nChain 1 Iteration: 2100 / 10000 [ 21%]  (Warmup) \nChain 2 Iteration: 2000 / 10000 [ 20%]  (Warmup) \nChain 3 Iteration: 2000 / 10000 [ 20%]  (Warmup) \nChain 3 Iteration: 2100 / 10000 [ 21%]  (Warmup) \nChain 4 Iteration: 2000 / 10000 [ 20%]  (Warmup) \nChain 1 Iteration: 2200 / 10000 [ 22%]  (Warmup) \nChain 1 Iteration: 2300 / 10000 [ 23%]  (Warmup) \nChain 2 Iteration: 2100 / 10000 [ 21%]  (Warmup) \nChain 2 Iteration: 2200 / 10000 [ 22%]  (Warmup) \nChain 3 Iteration: 2200 / 10000 [ 22%]  (Warmup) \nChain 4 Iteration: 2100 / 10000 [ 21%]  (Warmup) \nChain 4 Iteration: 2200 / 10000 [ 22%]  (Warmup) \nChain 1 Iteration: 2400 / 10000 [ 24%]  (Warmup) \nChain 2 Iteration: 2300 / 10000 [ 23%]  (Warmup) \nChain 3 Iteration: 2300 / 10000 [ 23%]  (Warmup) \nChain 4 Iteration: 2300 / 10000 [ 23%]  (Warmup) \nChain 1 Iteration: 2500 / 10000 [ 25%]  (Warmup) \nChain 2 Iteration: 2400 / 10000 [ 24%]  (Warmup) \nChain 3 Iteration: 2400 / 10000 [ 24%]  (Warmup) \nChain 4 Iteration: 2400 / 10000 [ 24%]  (Warmup) \nChain 1 Iteration: 2600 / 10000 [ 26%]  (Warmup) \nChain 1 Iteration: 2700 / 10000 [ 27%]  (Warmup) \nChain 2 Iteration: 2500 / 10000 [ 25%]  (Warmup) \nChain 3 Iteration: 2500 / 10000 [ 25%]  (Warmup) \nChain 4 Iteration: 2500 / 10000 [ 25%]  (Warmup) \nChain 4 Iteration: 2600 / 10000 [ 26%]  (Warmup) \nChain 1 Iteration: 2800 / 10000 [ 28%]  (Warmup) \nChain 2 Iteration: 2600 / 10000 [ 26%]  (Warmup) \nChain 2 Iteration: 2700 / 10000 [ 27%]  (Warmup) \nChain 3 Iteration: 2600 / 10000 [ 26%]  (Warmup) \nChain 4 Iteration: 2700 / 10000 [ 27%]  (Warmup) \nChain 1 Iteration: 2900 / 10000 [ 29%]  (Warmup) \nChain 2 Iteration: 2800 / 10000 [ 28%]  (Warmup) \nChain 3 Iteration: 2700 / 10000 [ 27%]  (Warmup) \nChain 3 Iteration: 2800 / 10000 [ 28%]  (Warmup) \nChain 4 Iteration: 2800 / 10000 [ 28%]  (Warmup) \nChain 1 Iteration: 3000 / 10000 [ 30%]  (Warmup) \nChain 2 Iteration: 2900 / 10000 [ 29%]  (Warmup) \nChain 3 Iteration: 2900 / 10000 [ 29%]  (Warmup) \nChain 4 Iteration: 2900 / 10000 [ 29%]  (Warmup) \nChain 4 Iteration: 3000 / 10000 [ 30%]  (Warmup) \nChain 1 Iteration: 3100 / 10000 [ 31%]  (Warmup) \nChain 1 Iteration: 3200 / 10000 [ 32%]  (Warmup) \nChain 2 Iteration: 3000 / 10000 [ 30%]  (Warmup) \nChain 2 Iteration: 3100 / 10000 [ 31%]  (Warmup) \nChain 3 Iteration: 3000 / 10000 [ 30%]  (Warmup) \nChain 3 Iteration: 3100 / 10000 [ 31%]  (Warmup) \nChain 4 Iteration: 3100 / 10000 [ 31%]  (Warmup) \nChain 1 Iteration: 3300 / 10000 [ 33%]  (Warmup) \nChain 2 Iteration: 3200 / 10000 [ 32%]  (Warmup) \nChain 3 Iteration: 3200 / 10000 [ 32%]  (Warmup) \nChain 4 Iteration: 3200 / 10000 [ 32%]  (Warmup) \nChain 4 Iteration: 3300 / 10000 [ 33%]  (Warmup) \nChain 1 Iteration: 3400 / 10000 [ 34%]  (Warmup) \nChain 1 Iteration: 3500 / 10000 [ 35%]  (Warmup) \nChain 2 Iteration: 3300 / 10000 [ 33%]  (Warmup) \nChain 2 Iteration: 3400 / 10000 [ 34%]  (Warmup) \nChain 3 Iteration: 3300 / 10000 [ 33%]  (Warmup) \nChain 3 Iteration: 3400 / 10000 [ 34%]  (Warmup) \nChain 4 Iteration: 3400 / 10000 [ 34%]  (Warmup) \nChain 1 Iteration: 3600 / 10000 [ 36%]  (Warmup) \nChain 2 Iteration: 3500 / 10000 [ 35%]  (Warmup) \nChain 3 Iteration: 3500 / 10000 [ 35%]  (Warmup) \nChain 4 Iteration: 3500 / 10000 [ 35%]  (Warmup) \nChain 1 Iteration: 3700 / 10000 [ 37%]  (Warmup) \nChain 2 Iteration: 3600 / 10000 [ 36%]  (Warmup) \nChain 3 Iteration: 3600 / 10000 [ 36%]  (Warmup) \nChain 4 Iteration: 3600 / 10000 [ 36%]  (Warmup) \nChain 1 Iteration: 3800 / 10000 [ 38%]  (Warmup) \nChain 1 Iteration: 3900 / 10000 [ 39%]  (Warmup) \nChain 2 Iteration: 3700 / 10000 [ 37%]  (Warmup) \nChain 3 Iteration: 3700 / 10000 [ 37%]  (Warmup) \nChain 4 Iteration: 3700 / 10000 [ 37%]  (Warmup) \nChain 1 Iteration: 4000 / 10000 [ 40%]  (Warmup) \nChain 2 Iteration: 3800 / 10000 [ 38%]  (Warmup) \nChain 2 Iteration: 3900 / 10000 [ 39%]  (Warmup) \nChain 3 Iteration: 3800 / 10000 [ 38%]  (Warmup) \nChain 3 Iteration: 3900 / 10000 [ 39%]  (Warmup) \nChain 4 Iteration: 3800 / 10000 [ 38%]  (Warmup) \nChain 4 Iteration: 3900 / 10000 [ 39%]  (Warmup) \nChain 1 Iteration: 4100 / 10000 [ 41%]  (Warmup) \nChain 2 Iteration: 4000 / 10000 [ 40%]  (Warmup) \nChain 3 Iteration: 4000 / 10000 [ 40%]  (Warmup) \nChain 4 Iteration: 4000 / 10000 [ 40%]  (Warmup) \nChain 1 Iteration: 4200 / 10000 [ 42%]  (Warmup) \nChain 1 Iteration: 4300 / 10000 [ 43%]  (Warmup) \nChain 2 Iteration: 4100 / 10000 [ 41%]  (Warmup) \nChain 2 Iteration: 4200 / 10000 [ 42%]  (Warmup) \nChain 3 Iteration: 4100 / 10000 [ 41%]  (Warmup) \nChain 3 Iteration: 4200 / 10000 [ 42%]  (Warmup) \nChain 4 Iteration: 4100 / 10000 [ 41%]  (Warmup) \nChain 4 Iteration: 4200 / 10000 [ 42%]  (Warmup) \nChain 1 Iteration: 4400 / 10000 [ 44%]  (Warmup) \nChain 2 Iteration: 4300 / 10000 [ 43%]  (Warmup) \nChain 3 Iteration: 4300 / 10000 [ 43%]  (Warmup) \nChain 4 Iteration: 4300 / 10000 [ 43%]  (Warmup) \nChain 1 Iteration: 4500 / 10000 [ 45%]  (Warmup) \nChain 2 Iteration: 4400 / 10000 [ 44%]  (Warmup) \nChain 2 Iteration: 4500 / 10000 [ 45%]  (Warmup) \nChain 3 Iteration: 4400 / 10000 [ 44%]  (Warmup) \nChain 3 Iteration: 4500 / 10000 [ 45%]  (Warmup) \nChain 4 Iteration: 4400 / 10000 [ 44%]  (Warmup) \nChain 4 Iteration: 4500 / 10000 [ 45%]  (Warmup) \nChain 1 Iteration: 4600 / 10000 [ 46%]  (Warmup) \nChain 1 Iteration: 4700 / 10000 [ 47%]  (Warmup) \nChain 2 Iteration: 4600 / 10000 [ 46%]  (Warmup) \nChain 3 Iteration: 4600 / 10000 [ 46%]  (Warmup) \nChain 4 Iteration: 4600 / 10000 [ 46%]  (Warmup) \nChain 1 Iteration: 4800 / 10000 [ 48%]  (Warmup) \nChain 2 Iteration: 4700 / 10000 [ 47%]  (Warmup) \nChain 3 Iteration: 4700 / 10000 [ 47%]  (Warmup) \nChain 4 Iteration: 4700 / 10000 [ 47%]  (Warmup) \nChain 4 Iteration: 4800 / 10000 [ 48%]  (Warmup) \nChain 1 Iteration: 4900 / 10000 [ 49%]  (Warmup) \nChain 2 Iteration: 4800 / 10000 [ 48%]  (Warmup) \nChain 2 Iteration: 4900 / 10000 [ 49%]  (Warmup) \nChain 3 Iteration: 4800 / 10000 [ 48%]  (Warmup) \nChain 3 Iteration: 4900 / 10000 [ 49%]  (Warmup) \nChain 4 Iteration: 4900 / 10000 [ 49%]  (Warmup) \nChain 1 Iteration: 5000 / 10000 [ 50%]  (Warmup) \nChain 1 Iteration: 5001 / 10000 [ 50%]  (Sampling) \nChain 1 Iteration: 5100 / 10000 [ 51%]  (Sampling) \nChain 2 Iteration: 5000 / 10000 [ 50%]  (Warmup) \nChain 2 Iteration: 5001 / 10000 [ 50%]  (Sampling) \nChain 3 Iteration: 5000 / 10000 [ 50%]  (Warmup) \nChain 3 Iteration: 5001 / 10000 [ 50%]  (Sampling) \nChain 3 Iteration: 5100 / 10000 [ 51%]  (Sampling) \nChain 4 Iteration: 5000 / 10000 [ 50%]  (Warmup) \nChain 4 Iteration: 5001 / 10000 [ 50%]  (Sampling) \nChain 4 Iteration: 5100 / 10000 [ 51%]  (Sampling) \nChain 1 Iteration: 5200 / 10000 [ 52%]  (Sampling) \nChain 2 Iteration: 5100 / 10000 [ 51%]  (Sampling) \nChain 2 Iteration: 5200 / 10000 [ 52%]  (Sampling) \nChain 3 Iteration: 5200 / 10000 [ 52%]  (Sampling) \nChain 4 Iteration: 5200 / 10000 [ 52%]  (Sampling) \nChain 1 Iteration: 5300 / 10000 [ 53%]  (Sampling) \nChain 1 Iteration: 5400 / 10000 [ 54%]  (Sampling) \nChain 2 Iteration: 5300 / 10000 [ 53%]  (Sampling) \nChain 2 Iteration: 5400 / 10000 [ 54%]  (Sampling) \nChain 3 Iteration: 5300 / 10000 [ 53%]  (Sampling) \nChain 3 Iteration: 5400 / 10000 [ 54%]  (Sampling) \nChain 4 Iteration: 5300 / 10000 [ 53%]  (Sampling) \nChain 4 Iteration: 5400 / 10000 [ 54%]  (Sampling) \nChain 1 Iteration: 5500 / 10000 [ 55%]  (Sampling) \nChain 2 Iteration: 5500 / 10000 [ 55%]  (Sampling) \nChain 3 Iteration: 5500 / 10000 [ 55%]  (Sampling) \nChain 4 Iteration: 5500 / 10000 [ 55%]  (Sampling) \nChain 1 Iteration: 5600 / 10000 [ 56%]  (Sampling) \nChain 2 Iteration: 5600 / 10000 [ 56%]  (Sampling) \nChain 2 Iteration: 5700 / 10000 [ 57%]  (Sampling) \nChain 3 Iteration: 5600 / 10000 [ 56%]  (Sampling) \nChain 3 Iteration: 5700 / 10000 [ 57%]  (Sampling) \nChain 4 Iteration: 5600 / 10000 [ 56%]  (Sampling) \nChain 4 Iteration: 5700 / 10000 [ 57%]  (Sampling) \nChain 1 Iteration: 5700 / 10000 [ 57%]  (Sampling) \nChain 2 Iteration: 5800 / 10000 [ 58%]  (Sampling) \nChain 3 Iteration: 5800 / 10000 [ 58%]  (Sampling) \nChain 4 Iteration: 5800 / 10000 [ 58%]  (Sampling) \nChain 1 Iteration: 5800 / 10000 [ 58%]  (Sampling) \nChain 2 Iteration: 5900 / 10000 [ 59%]  (Sampling) \nChain 3 Iteration: 5900 / 10000 [ 59%]  (Sampling) \nChain 4 Iteration: 5900 / 10000 [ 59%]  (Sampling) \nChain 1 Iteration: 5900 / 10000 [ 59%]  (Sampling) \nChain 3 Iteration: 6000 / 10000 [ 60%]  (Sampling) \nChain 4 Iteration: 6000 / 10000 [ 60%]  (Sampling) \nChain 1 Iteration: 6000 / 10000 [ 60%]  (Sampling) \nChain 2 Iteration: 6000 / 10000 [ 60%]  (Sampling) \nChain 2 Iteration: 6100 / 10000 [ 61%]  (Sampling) \nChain 3 Iteration: 6100 / 10000 [ 61%]  (Sampling) \nChain 4 Iteration: 6100 / 10000 [ 61%]  (Sampling) \nChain 1 Iteration: 6100 / 10000 [ 61%]  (Sampling) \nChain 2 Iteration: 6200 / 10000 [ 62%]  (Sampling) \nChain 3 Iteration: 6200 / 10000 [ 62%]  (Sampling) \nChain 3 Iteration: 6300 / 10000 [ 63%]  (Sampling) \nChain 4 Iteration: 6200 / 10000 [ 62%]  (Sampling) \nChain 4 Iteration: 6300 / 10000 [ 63%]  (Sampling) \nChain 1 Iteration: 6200 / 10000 [ 62%]  (Sampling) \nChain 2 Iteration: 6300 / 10000 [ 63%]  (Sampling) \nChain 3 Iteration: 6400 / 10000 [ 64%]  (Sampling) \nChain 4 Iteration: 6400 / 10000 [ 64%]  (Sampling) \nChain 1 Iteration: 6300 / 10000 [ 63%]  (Sampling) \nChain 2 Iteration: 6400 / 10000 [ 64%]  (Sampling) \nChain 1 Iteration: 6400 / 10000 [ 64%]  (Sampling) \nChain 2 Iteration: 6500 / 10000 [ 65%]  (Sampling) \nChain 3 Iteration: 6500 / 10000 [ 65%]  (Sampling) \nChain 4 Iteration: 6500 / 10000 [ 65%]  (Sampling) \nChain 1 Iteration: 6500 / 10000 [ 65%]  (Sampling) \nChain 2 Iteration: 6600 / 10000 [ 66%]  (Sampling) \nChain 2 Iteration: 6700 / 10000 [ 67%]  (Sampling) \nChain 3 Iteration: 6600 / 10000 [ 66%]  (Sampling) \nChain 3 Iteration: 6700 / 10000 [ 67%]  (Sampling) \nChain 4 Iteration: 6600 / 10000 [ 66%]  (Sampling) \nChain 4 Iteration: 6700 / 10000 [ 67%]  (Sampling) \nChain 1 Iteration: 6600 / 10000 [ 66%]  (Sampling) \nChain 2 Iteration: 6800 / 10000 [ 68%]  (Sampling) \nChain 3 Iteration: 6800 / 10000 [ 68%]  (Sampling) \nChain 4 Iteration: 6800 / 10000 [ 68%]  (Sampling) \nChain 1 Iteration: 6700 / 10000 [ 67%]  (Sampling) \nChain 1 Iteration: 6800 / 10000 [ 68%]  (Sampling) \nChain 2 Iteration: 6900 / 10000 [ 69%]  (Sampling) \nChain 3 Iteration: 6900 / 10000 [ 69%]  (Sampling) \nChain 3 Iteration: 7000 / 10000 [ 70%]  (Sampling) \nChain 4 Iteration: 6900 / 10000 [ 69%]  (Sampling) \nChain 1 Iteration: 6900 / 10000 [ 69%]  (Sampling) \nChain 2 Iteration: 7000 / 10000 [ 70%]  (Sampling) \nChain 2 Iteration: 7100 / 10000 [ 71%]  (Sampling) \nChain 3 Iteration: 7100 / 10000 [ 71%]  (Sampling) \nChain 4 Iteration: 7000 / 10000 [ 70%]  (Sampling) \nChain 4 Iteration: 7100 / 10000 [ 71%]  (Sampling) \nChain 1 Iteration: 7000 / 10000 [ 70%]  (Sampling) \nChain 1 Iteration: 7100 / 10000 [ 71%]  (Sampling) \nChain 2 Iteration: 7200 / 10000 [ 72%]  (Sampling) \nChain 3 Iteration: 7200 / 10000 [ 72%]  (Sampling) \nChain 3 Iteration: 7300 / 10000 [ 73%]  (Sampling) \nChain 4 Iteration: 7200 / 10000 [ 72%]  (Sampling) \nChain 1 Iteration: 7200 / 10000 [ 72%]  (Sampling) \nChain 2 Iteration: 7300 / 10000 [ 73%]  (Sampling) \nChain 2 Iteration: 7400 / 10000 [ 74%]  (Sampling) \nChain 3 Iteration: 7400 / 10000 [ 74%]  (Sampling) \nChain 4 Iteration: 7300 / 10000 [ 73%]  (Sampling) \nChain 1 Iteration: 7300 / 10000 [ 73%]  (Sampling) \nChain 2 Iteration: 7500 / 10000 [ 75%]  (Sampling) \nChain 3 Iteration: 7500 / 10000 [ 75%]  (Sampling) \nChain 4 Iteration: 7400 / 10000 [ 74%]  (Sampling) \nChain 4 Iteration: 7500 / 10000 [ 75%]  (Sampling) \nChain 1 Iteration: 7400 / 10000 [ 74%]  (Sampling) \nChain 2 Iteration: 7600 / 10000 [ 76%]  (Sampling) \nChain 3 Iteration: 7600 / 10000 [ 76%]  (Sampling) \nChain 4 Iteration: 7600 / 10000 [ 76%]  (Sampling) \nChain 1 Iteration: 7500 / 10000 [ 75%]  (Sampling) \nChain 2 Iteration: 7700 / 10000 [ 77%]  (Sampling) \nChain 2 Iteration: 7800 / 10000 [ 78%]  (Sampling) \nChain 3 Iteration: 7700 / 10000 [ 77%]  (Sampling) \nChain 3 Iteration: 7800 / 10000 [ 78%]  (Sampling) \nChain 4 Iteration: 7700 / 10000 [ 77%]  (Sampling) \nChain 1 Iteration: 7600 / 10000 [ 76%]  (Sampling) \nChain 2 Iteration: 7900 / 10000 [ 79%]  (Sampling) \nChain 3 Iteration: 7900 / 10000 [ 79%]  (Sampling) \nChain 4 Iteration: 7800 / 10000 [ 78%]  (Sampling) \nChain 1 Iteration: 7700 / 10000 [ 77%]  (Sampling) \nChain 1 Iteration: 7800 / 10000 [ 78%]  (Sampling) \nChain 2 Iteration: 8000 / 10000 [ 80%]  (Sampling) \nChain 3 Iteration: 8000 / 10000 [ 80%]  (Sampling) \nChain 4 Iteration: 7900 / 10000 [ 79%]  (Sampling) \nChain 4 Iteration: 8000 / 10000 [ 80%]  (Sampling) \nChain 1 Iteration: 7900 / 10000 [ 79%]  (Sampling) \nChain 2 Iteration: 8100 / 10000 [ 81%]  (Sampling) \nChain 3 Iteration: 8100 / 10000 [ 81%]  (Sampling) \nChain 3 Iteration: 8200 / 10000 [ 82%]  (Sampling) \nChain 4 Iteration: 8100 / 10000 [ 81%]  (Sampling) \nChain 1 Iteration: 8000 / 10000 [ 80%]  (Sampling) \nChain 2 Iteration: 8200 / 10000 [ 82%]  (Sampling) \nChain 2 Iteration: 8300 / 10000 [ 83%]  (Sampling) \nChain 3 Iteration: 8300 / 10000 [ 83%]  (Sampling) \nChain 4 Iteration: 8200 / 10000 [ 82%]  (Sampling) \nChain 1 Iteration: 8100 / 10000 [ 81%]  (Sampling) \nChain 2 Iteration: 8400 / 10000 [ 84%]  (Sampling) \nChain 3 Iteration: 8400 / 10000 [ 84%]  (Sampling) \nChain 4 Iteration: 8300 / 10000 [ 83%]  (Sampling) \nChain 4 Iteration: 8400 / 10000 [ 84%]  (Sampling) \nChain 1 Iteration: 8200 / 10000 [ 82%]  (Sampling) \nChain 1 Iteration: 8300 / 10000 [ 83%]  (Sampling) \nChain 2 Iteration: 8500 / 10000 [ 85%]  (Sampling) \nChain 2 Iteration: 8600 / 10000 [ 86%]  (Sampling) \nChain 3 Iteration: 8500 / 10000 [ 85%]  (Sampling) \nChain 3 Iteration: 8600 / 10000 [ 86%]  (Sampling) \nChain 4 Iteration: 8500 / 10000 [ 85%]  (Sampling) \nChain 1 Iteration: 8400 / 10000 [ 84%]  (Sampling) \nChain 2 Iteration: 8700 / 10000 [ 87%]  (Sampling) \nChain 3 Iteration: 8700 / 10000 [ 87%]  (Sampling) \nChain 4 Iteration: 8600 / 10000 [ 86%]  (Sampling) \nChain 1 Iteration: 8500 / 10000 [ 85%]  (Sampling) \nChain 2 Iteration: 8800 / 10000 [ 88%]  (Sampling) \nChain 3 Iteration: 8800 / 10000 [ 88%]  (Sampling) \nChain 3 Iteration: 8900 / 10000 [ 89%]  (Sampling) \nChain 4 Iteration: 8700 / 10000 [ 87%]  (Sampling) \nChain 4 Iteration: 8800 / 10000 [ 88%]  (Sampling) \nChain 1 Iteration: 8600 / 10000 [ 86%]  (Sampling) \nChain 2 Iteration: 8900 / 10000 [ 89%]  (Sampling) \nChain 2 Iteration: 9000 / 10000 [ 90%]  (Sampling) \nChain 3 Iteration: 9000 / 10000 [ 90%]  (Sampling) \nChain 4 Iteration: 8900 / 10000 [ 89%]  (Sampling) \nChain 1 Iteration: 8700 / 10000 [ 87%]  (Sampling) \nChain 2 Iteration: 9100 / 10000 [ 91%]  (Sampling) \nChain 3 Iteration: 9100 / 10000 [ 91%]  (Sampling) \nChain 4 Iteration: 9000 / 10000 [ 90%]  (Sampling) \nChain 1 Iteration: 8800 / 10000 [ 88%]  (Sampling) \nChain 2 Iteration: 9200 / 10000 [ 92%]  (Sampling) \nChain 3 Iteration: 9200 / 10000 [ 92%]  (Sampling) \nChain 4 Iteration: 9100 / 10000 [ 91%]  (Sampling) \nChain 1 Iteration: 8900 / 10000 [ 89%]  (Sampling) \nChain 2 Iteration: 9300 / 10000 [ 93%]  (Sampling) \nChain 3 Iteration: 9300 / 10000 [ 93%]  (Sampling) \nChain 4 Iteration: 9200 / 10000 [ 92%]  (Sampling) \nChain 1 Iteration: 9000 / 10000 [ 90%]  (Sampling) \nChain 2 Iteration: 9400 / 10000 [ 94%]  (Sampling) \nChain 3 Iteration: 9400 / 10000 [ 94%]  (Sampling) \nChain 4 Iteration: 9300 / 10000 [ 93%]  (Sampling) \nChain 1 Iteration: 9100 / 10000 [ 91%]  (Sampling) \nChain 2 Iteration: 9500 / 10000 [ 95%]  (Sampling) \nChain 3 Iteration: 9500 / 10000 [ 95%]  (Sampling) \nChain 4 Iteration: 9400 / 10000 [ 94%]  (Sampling) \nChain 4 Iteration: 9500 / 10000 [ 95%]  (Sampling) \nChain 1 Iteration: 9200 / 10000 [ 92%]  (Sampling) \nChain 2 Iteration: 9600 / 10000 [ 96%]  (Sampling) \nChain 2 Iteration: 9700 / 10000 [ 97%]  (Sampling) \nChain 3 Iteration: 9600 / 10000 [ 96%]  (Sampling) \nChain 3 Iteration: 9700 / 10000 [ 97%]  (Sampling) \nChain 4 Iteration: 9600 / 10000 [ 96%]  (Sampling) \nChain 1 Iteration: 9300 / 10000 [ 93%]  (Sampling) \nChain 2 Iteration: 9800 / 10000 [ 98%]  (Sampling) \nChain 3 Iteration: 9800 / 10000 [ 98%]  (Sampling) \nChain 4 Iteration: 9700 / 10000 [ 97%]  (Sampling) \nChain 1 Iteration: 9400 / 10000 [ 94%]  (Sampling) \nChain 1 Iteration: 9500 / 10000 [ 95%]  (Sampling) \nChain 2 Iteration: 9900 / 10000 [ 99%]  (Sampling) \nChain 3 Iteration: 9900 / 10000 [ 99%]  (Sampling) \nChain 3 Iteration: 10000 / 10000 [100%]  (Sampling) \nChain 4 Iteration: 9800 / 10000 [ 98%]  (Sampling) \nChain 4 Iteration: 9900 / 10000 [ 99%]  (Sampling) \nChain 1 Iteration: 9600 / 10000 [ 96%]  (Sampling) \nChain 2 Iteration: 10000 / 10000 [100%]  (Sampling) \nChain 4 Iteration: 10000 / 10000 [100%]  (Sampling) \nChain 2 finished in 8.3 seconds.\nChain 3 finished in 8.3 seconds.\nChain 4 finished in 8.3 seconds.\nChain 1 Iteration: 9700 / 10000 [ 97%]  (Sampling) \nChain 1 Iteration: 9800 / 10000 [ 98%]  (Sampling) \nChain 1 Iteration: 9900 / 10000 [ 99%]  (Sampling) \nChain 1 Iteration: 10000 / 10000 [100%]  (Sampling) \nChain 1 finished in 8.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 8.4 seconds.\nTotal execution time: 8.8 seconds.\n\n\nLet’s check our chains’ health:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur chains look healthy enough:\n\nThey mix well\nThey are stationary.\nDifferent chains converge to explore the same parameter space.\n\nLet’s look at the \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9998  \n 1st Qu.:0.9999  \n Median :0.9999  \n Mean   :0.9999  \n 3rd Qu.:1.0000  \n Max.   :1.0001  \n\n\nThe \\(\\hat{R}\\) values look OK. Let’s check our inferences for the joint common distribution:\n\n\n            mean         sd      5.5%      94.5%    n_eff     Rhat4\nsigma  0.4968487 0.02379535  0.460315  0.5360532 21274.46 0.9999182\na      3.6278289 0.22224191  3.272897  3.9801400 20564.28 1.0000185\nb     -1.0885792 0.14784582 -1.324067 -0.8492441 21529.59 0.9999577\n\n\nIt seems that we’ve been able to recover the common joint distribution’s parameters that adaptively regularizes our individual estimates for each café. For example, the marginal distribution of \\(a\\):\n\n\n\n\n\nNow for the posterior marginal distribution of b:\n\n\n\n\n\nFinally, we can plot the joint distribution with contours:\n\n\n\n\n\nAs we can see, our joint common distribution captures the negative correlation between intercepts and slopes.\n\n\nVisualizing the shrinkage\nWe’ve seen how and why it’s sensible to pool information across clusters and across parameter types. We’ve estimated with our multilevel model the common joint distribution. Now it’s time to visualize the shrinkage: how our estimates are pulled towards the estimated joint common distribution toward its mean.\nTo do, we must first average our estimates for each café over the posterior distribution. Then, we can compare with our fixed effects estimates from the beginning.\n\n\nRows: 40\nColumns: 4\n$ cafe_ids  <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10…\n$ method    <chr> \"partial_pooling\", \"fixed\", \"partial_pooling\", \"fixed\", \"par…\n$ intercept <dbl> 4.263780, 4.271750, 1.991360, 1.902532, 4.565624, 4.666991, …\n$ slope     <dbl> -0.9559145, -0.8844626, -0.7982419, -0.7224460, -2.0983092, …\n\n\nFinally, we can plot our points over our posterior joint common distribution to visualize how our estimates are pooled over towards the joint common distribution’s mean:\n\n\n\n\n\nAs we can see, the partial pooling estimates are always closer to the center of the distribution than the fixed effects estimates. This is the direct result of pooling the information with a joint common distribution that shrinks our estimates towards the grand mean. Not only that, shrinking in one dimension entails shrinking in the other dimension. This is the direct result of the pooling across parameter types."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-03-statistical-rethinking-week-3.html",
    "href": "posts/bayesian-statistics/2020-05-03-statistical-rethinking-week-3.html",
    "title": "Statistical Rethinking: Week 3",
    "section": "",
    "text": "Week 3 gave the most interesting discussion of multiple regression. Why isn’t it enough with univariate regression? It allows us to disentagle two types of mistakes:\n\nSpurious correlation between the predictor and independent variable.\nA masking relationship between two explanatory variables.\n\nIt also started to introduce DAGs and how they are an incredible tool for thinking before fitting. Specially, it managed to convince me the frequent strategy of tossing everything into a multiple regression and hoping for the ebst is a recipe for disaster.\n\n\n\n\n\nInvent your own example of a spurious correlation.\n\n\nx_real <- rnorm(1000)\n# spurious is correlated with real\nx_spur <- rnorm(1000, x_real)\n# outcome variable is only correlated with x_real\ny <- rnorm(1000, x_real)\ndata <- data.frame(y, x_real, x_spur)\n\nThus, when we analyze the relationship between x_spur and y, it may seem as if there is a relationship.\n\n# fit the model\nmodel_spurious <- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b* x_spur,\n    a ~ dnorm(0, 1),\n    b ~ dnorm(0, 1),\n    sigma ~ dunif(0, 2)\n  ),\n  data = data\n)\n\n# sample from posterior\nsamples_spurious <- extract.samples(model_spurious)\n# get samples for slope\ncoefficient_spurious <- samples_spurious$b\n\nprecis(model_spurious)\n\n             mean         sd        5.5%     94.5%\na     -0.01397705 0.03926831 -0.07673539 0.0487813\nb      0.48773043 0.02780489  0.44329284 0.5321680\nsigma  1.24242979 0.02778322  1.19802684 1.2868328\n\n\nWhereas when we fit a multiple regression with both coefficients, the relationship should dissapear:\n\n# fit the model\nmodel_with_real <- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + b* x_spur + c* x_real ,\n    a ~ dnorm(0, 1),\n    b ~ dnorm(0, 1),\n    c ~ dnorm(0, 1),\n    sigma ~ dunif(0, 2)\n  ),\n  data = data\n)\n\n# sample from posterior\nsamples_with_real <- extract.samples(model_with_real)\n\n# get samples for slope\ncoefficient_with_real <- samples_with_real$b\n\nprecis(model_with_real)\n\n              mean         sd        5.5%      94.5%\na     -0.003852203 0.03252227 -0.05582908 0.04812467\nb     -0.017186891 0.03297004 -0.06987938 0.03550560\nc      0.994250135 0.04646359  0.91999235 1.06850792\nsigma  1.028634063 0.02299992  0.99187575 1.06539238\n\n\nTo make the comparison more obvious, let’s plot a ggridge with the samples from the different models.\n\ndata.frame(controlling_by_real = coefficient_with_real,\n           not_controlling = coefficient_spurious,\n           sample = seq(1, 10000)) %>% \n  pivot_longer(-sample) %>% \n  ggplot(aes(x = value, fill = name)) +\n    geom_histogram(color = \"black\", alpha = 0.7,\n                   binwidth = 0.01) +\n    hrbrthemes::theme_ipsum_rc(grid = \"X\") +\n    theme(legend.position = \"bottom\") +\n    scale_fill_viridis_d() +\n    labs(fill = \"\",\n         title = \"Multiple regression identifies spurious correlation\",\n         subtitle = \"Controlling and not controlling for real relationship\",\n         x = \"Slope's value\",\n         caption = \"Samples from the posterior for 2 different models. \")\n\n\n\n\n\nInvent your own example of a masked relationship\n\nFirst, let’s simulate the data such that:\n\nThe outcome is correlated with both variables, but in opposite directions.\nBoth predictors are correlated.\n\n\nset.seed(42)\nx_one <- rnorm(1000)\n# relation between explanatory variables\nx_two <- rnorm(1000, x_one)\n# outcome is related to both\n# but in opposite directions\ny <- rnorm(1000, x_one - x_two)\ndata <- data.frame(y, x_one, x_two)\n\nNow, let’s fit two univariate models.\n\n# fit the model that masks x_one\nmodel_mask_one <- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + one* x_one,\n    a ~ dnorm(0, 1),\n    one ~ dnorm(0, 1),\n    sigma ~ dunif(0, 2)\n  ),\n  data = data\n)\n# fit the model that masks x_two\nmodel_mask_two <- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + two* x_two,\n    a ~ dnorm(0, 1),\n    two ~ dnorm(0, 1),\n    sigma ~ dunif(0, 2)\n  ),\n  data = data\n)\n\nThen, we can fit the multivariate model:\n\nmodel_unmasking <- quap(\n  alist(\n    y ~ dnorm(mu, sigma),\n    mu <- a + one* x_one + two*x_two,\n    a ~ dnorm(0, 1),\n    one ~ dnorm(0, 1),\n    two ~ dnorm(0, 1),\n    sigma ~ dunif(0, 2)\n  ),\n  data = data\n)\n\nLet’s visualize how our estimates have changed once we have included both variables:\n\none_masked <- extract.samples(model_mask_one)$one\none_unmasked <- extract.samples(model_unmasking)$one\n\ndata.frame(sample = 1:10000,\n           one_masked, \n           one_unmasked) %>% \n  pivot_longer(-sample) %>% \n  ggplot(aes(x = value, fill = name)) +\n    geom_histogram(color = \"black\", alpha = 0.7,\n                   binwidth = 0.01) +\n    hrbrthemes::theme_ipsum_rc(grid = \"X\") +\n    theme(legend.position = \"bottom\") +\n    scale_fill_viridis_d() +\n  labs(fill = \"\",\n       title = \"Multiple regression unmasks true relationship\",\n       caption = \"Samples from the posterior of different models.\",\n       x = \"Slope\")\n\n\n\n\nWhereas the univariate regression, due to the unobserved variable’s effect, cannot reliably estimate the coefficient, multiple regression does the unmasking. Once we control for the correlation between the explanatory variables, the positive relationship between the first and the outcome variable is revealed.\nFor the other variable, that is negatively correlated with the outcome, we expect the opposite effect:\n\ntwo_masked <- extract.samples(model_mask_two)$two\ntwo_unmasked <- extract.samples(model_unmasking)$two\n\ndata.frame(sample = 1:10000,\n           two_masked, \n           two_unmasked) %>% \n  pivot_longer(-sample) %>% \n  ggplot(aes(x = value, fill = name)) +\n    geom_histogram(color = \"black\", alpha = 0.7,\n                   binwidth = 0.01) +\n    hrbrthemes::theme_ipsum_rc(grid = \"X\") +\n    theme(legend.position = \"bottom\") +\n    scale_fill_viridis_d() +\n  labs(fill = \"\",\n       title = \"Multiple regression unmasks true relationship\",\n       caption = \"Samples from the posterior of different models.\",\n       x = \"Slope\")\n\n\n\n\nJust as expected, multiple regression helps us unmask the true relationship. Before, due to the correlation between one and two, we were underestimating the magnitude of the relationship. Once we include one in the regression, we can estimate the true effect."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-10-bda-week-8-bayesian-decision-analysis.html",
    "href": "posts/bayesian-statistics/2020-07-10-bda-week-8-bayesian-decision-analysis.html",
    "title": "BDA Week 8: Bayesian Decision Analysis",
    "section": "",
    "text": "Many if not most statistical analyses are performed for the ultimate goal of decision making. Bayesian Statistics has the advantage of direct use of probability to quantify the uncertainty around unobserved quantities of interest: whether those are parameters or predictions, we end up with a posterior distribution.\nIndeed, our posterior predictions interact uniquely for each possible action \\(d\\) we can take to create a unique distribution of our utility \\(U(x)|d\\). Then, it amounts to compare which of these distributions benefits of the most. For example, we can choose the action \\(d\\) with the highest expected utility. With the help of Mathematica, I go over the following toy problem:\n\nWidgets cost $2 each to manufacture and you can sell them for $3. Your forecast for the market for widgets is (approximately) normally distributed with mean 10,000 and standard deviation 5,000. How many widgets should you manufacture in order to maximize your expected net profit?\n\nWhich action, production number, leads to maximize our expected profit? Read this post to find out.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "",
    "text": "Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\nSo far in the course, we have seen how the main obstacle in the way of performing Bayesian Statistics is the computation of the posterior. Thus, we must ask ourselves: if we cannot fully compute the posterior, but we can evaluate an unnormalized version, how can we approximate the posterior distribution?\nIn this week, we started analyzing a promising alternative: Monte-Carlo Markov Chains (MCMC). In this blogpost, I’ll give a succinct overview of the most basic MCMC algorithm: the Metropolis Algorithm and quick example of it with some real data."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#a-monte-carlo-markov-chain",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#a-monte-carlo-markov-chain",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "A Monte-Carlo Markov Chain",
    "text": "A Monte-Carlo Markov Chain\nThe Metropolis Algorithm is just a random walk through parameter space. At each iteration of the algorithm, say \\(t\\), where we are depends only on where we were at \\(t-1\\). That is \\(P(\\theta_t | \\theta_{t-1}, \\theta_{t-2}, \\cdots, \\theta_{0}) = P(\\theta_t | \\theta_{t-1}\\). This is the Markov part. At each time step, then, we must define transition distribution: the probability of \\(P(\\theta_t | \\theta_{t-1})\\).\nThe Monte-Carlo part comes because we use these different samples \\((\\theta_{t-1}, \\theta_{t-2}, \\cdots, \\theta_{0})\\) to estimate the posterior distribution. We can only do this if at time step \\(T \\to \\infty\\), \\(P(\\theta_T) = P(\\theta_T | y)\\). That is, if the stationary distribution (the probability that we are at any given point in time \\(T\\)) is the target posterior distribution. The challenge then, is how to engineer each transition distribution such that the stationary distribution is the posterior. We will check how the Metropolis algorithm solves this problem with a numerical example.\nIf we can construct such a Markov Chain, then our Monte-Carlo estimates using these samples will be asymptotically consistent. However, two problems arise: first, there’s an auto-correlation in our samples from the Markov chains. Although the Central Limit Theorem still holds, our effective sample size for our Monte-Carlo estimates will be lower than our number of Markov Chain iterations. Secondly, we cannot know if we have run the Markov Chain long enough such that our samples are in proportion according to their stationary distribution: that is, we cannot know if the chains have converged toward the posterior distribution. We will check both problems with convergence diagnostics once we have worked out a numerical example of the Metropolis algorithm."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#the-metropolis-algorithm",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#the-metropolis-algorithm",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "The Metropolis Algorithm",
    "text": "The Metropolis Algorithm\nThe Metropolis algorithm is thus defined. A random walk through parameter space such that at each iteration of the Markov Chain, our samples are corrected such that they approximate our posterior distribution. In particular, begin at some point \\(\\theta_{0}\\). Then, we generate a proposed move by direct sampling from a proposal distribution: say a normal centered around \\(\\theta_{0}\\). The suggestion then is \\(\\theta^*\\). We will then decide if we move to \\(\\theta^*\\) by comparing the ratio of unnormalized posterior distribution densities at \\(\\theta^*\\) and \\(\\theta_{0}\\).\n\\[\nr = \\dfrac{q(\\theta^*|y)}{q(\\theta_0| y)}\n\\] Which, given that both are normalized by the same constant in the posterior distribution, is equivalent to comparing the posterior densities at both points:\n\\[\nr = \\dfrac{q(\\theta^*|y)}{q(\\theta_0| y)} = \\dfrac{q(\\theta^*|y) / \\int q(\\theta|y) d\\theta}{q(\\theta_0| y)/ \\int q(\\theta|y) d\\theta} =  \\dfrac{p(\\theta^*|y)}{p(\\theta_0 | y)}\n\\] Finally, we decide whether to move to \\(\\theta^*\\) by a Bernoulli trial with probability \\(min(r, 1)\\). That is:\n\nif the proposed jump increases the posterior (\\(p(\\theta^*|y) > p(\\theta_0|y)\\)), then our Markov Chain moves to \\(\\theta^*\\) and we set \\(\\theta_t = \\theta^*\\).\nif the proposed jump decreases the posterior (\\(p(\\theta^*|y) < p(\\theta_0|y)\\)), then our Markov Chain then we may or may not move to \\(\\theta^*\\). The probability that we do move decreases as the decreased density resulting from the jump increases.\n\nTherefore:\n\nThe Metropolis algorithm can thus be viewed as a stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density until it finds the mode and then only sometimes accepting steps that decrease the posterior density.\n\nThus, as long as the algorithm has run long enough to find the posterior mode, and the area around the mode is a good representation of the overall posterior, the Metropolis Algorithm will work."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#an-example-of-the-metropolis-algorithm",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#an-example-of-the-metropolis-algorithm",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "An example of the Metropolis Algorithm",
    "text": "An example of the Metropolis Algorithm\nThe data come from the excellent Bayesian course Statistical Rethinking. Which is the best statistics course that I’ve ever taken.\n\ndata(\"Howell1\")\nheights <- Howell1\nskimr::skim(heights)\n\n\nData summary\n\n\nName\nheights\n\n\nNumber of rows\n544\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1\n138.26\n27.60\n53.98\n125.10\n148.59\n157.48\n179.07\n▁▂▂▇▇\n\n\nweight\n0\n1\n35.61\n14.72\n4.25\n22.01\n40.06\n47.21\n62.99\n▃▂▃▇▂\n\n\nage\n0\n1\n29.34\n20.75\n0.00\n12.00\n27.00\n43.00\n88.00\n▇▆▅▂▁\n\n\nmale\n0\n1\n0.47\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\n\n\n\nWe will therefore model a very simple Gaussian probability model for the height:\n\\[\nheight_i \\sim Normal(\\mu, \\sigma) \\\\\n\\mu \\sim Normal(150, 20) \\\\\n\\sigma \\sim Normal(5, 10)\n\\]\nThus, for a given \\(\\mu, \\sigma\\), the model’s log-likelihood is thus:\n\nmodel_log_likelihood <- function(mu, sigma) {\n  sum(dnorm(heights$height, mu, sigma, log = TRUE))\n}\n\nThe unnormalized posterior is thus the log-likelihood plus the prior density log:\n\nlog_unnormalized_posterior <- function(mu, sigma) {\n  dnorm(mu, 150, 20, log = TRUE) + dnorm(sigma, 5, 10, log = TRUE) + model_log_likelihood(mu, sigma)\n}\n\nTherefore, if our proposal algorithm is a normal centered around the past iteration with scale of 5 for \\(\\mu\\) and scale of 2 for \\(\\sigma\\), the metropolis algorithm can be written thus:\n\ndensity_ratio <- function(alpha_propose, alpha_previous, sigma_propose, sigma_previous) {\n  exp(log_unnormalized_posterior(alpha_propose, sigma_propose) -  log_unnormalized_posterior(alpha_previous, sigma_previous))\n}\n\nsingle_metropolis <- function(total_iter = 10000) {\n    \n  alpha <- vector(length = total_iter)\n  sigma <- vector(length = total_iter)\n  alpha[1] <- runif(1, min = 100, 150) # initialize the chains at random points\n  sigma[1] <- runif(1, min = 10, 40) # initialize the chains at random points\n  for (i in 2:total_iter) {\n    \n    # sample proposal\n    alpha_propose <- rnorm(1, mean = alpha[i-1], sd = 5)\n    sigma_propose <- rnorm(1, mean = sigma[i-1], sd = 2)\n    # compare posterior at past and proposal\n    ratio <- density_ratio(alpha_propose, alpha[i-1], sigma_propose, sigma[i-1])\n    ratio <- min(1, ratio) \n    # check whether you move\n    bool_move <- rbernoulli(1, p = ratio)\n    \n    if (bool_move == 1) {\n      alpha[i] <- alpha_propose\n      sigma[i] <- sigma_propose\n    }\n    \n    else{\n      alpha[i] <- alpha[i-1]\n      sigma[i] <- sigma[i-1]\n    }\n    \n  }\n  list(alpha = alpha[5001:total_iter], sigma = sigma[5001:total_iter])\n}\n\nNotice that we do not use all of our iterations. In fact, we discard half of them. The reason? At the beginning of the chain, the probabilities have not converged to that of the stationary distribution. Thus, they are not correct samples from the posterior distribution. This beginning period serves to warm-up the Chains long enough until they find the stationary distribution and start yielding usable samples from the posterior.\nTo run multiple chains of the Metropolis algorithm:\n\nmultiple_metropolis <- function(chains = 4) {\n  alpha <- list()\n  sigma <- list()\n  for (chain in 1:chains) {\n    \n    result_chain <- single_metropolis()\n    alpha[[chain]] <- result_chain[[1]]\n    sigma[[chain]] <- result_chain[[2]]\n  }\n  list(alpha = alpha, sigma = sigma)\n}\n\nresults <- multiple_metropolis()\n\nOur results, then, can be summarised just as we work with Monte-Carlo samples from other methods. For example, the posterior mean can is thus:\n\ndo.call(cbind, results$alpha) %>% \n  data.frame(iter = 1:5000, .) %>% \n  pivot_longer(-iter, values_to = \"alpha\") %>% \n  select(-name) %>% \n  left_join(do.call(cbind, results$sigma) %>% \n    data.frame(iter = 1:5000, .) %>% \n    pivot_longer(-iter, values_to = \"sigma\") %>% \n    select(-name)) -> results_plot\n\nresults_plot %>% \n  select(-iter) %>% \n  summarise_all(mean)\n\n# A tibble: 1 × 2\n  alpha sigma\n  <dbl> <dbl>\n1  138.  27.5\n\n\nWe can visualize the samples from the posterior:\n\nresults_plot %>% \n  ggplot(aes(alpha, sigma)) +\n  geom_jitter(alpha = 0.1) +\n  labs(title = \"Samples from the posterior\",\n       subtitle = \"Posterior obtained with Metropolis algorithm\")"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#convergence-diagnostics",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#convergence-diagnostics",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "Convergence Diagnostics",
    "text": "Convergence Diagnostics\nIf we run multiple chains, we can check whether each chain converged to explore the same areas of the parameter space in the same proportions. If the chains are in not in agreement between each other, then, it’s a sure sign that the chains have yet to converge. We can visualize the path that each chain took through the parameter space with trace plots:\n\ndo.call(cbind, results$alpha) %>% \n  data.frame(., iter = 1:5000) %>% \n  pivot_longer(-iter, names_to = \"chain\", names_prefix = \"X\",\n               values_to = \"alpha\") %>% \n  ggplot(aes(iter, alpha, color = chain)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(title = \"Trace plot for alpha\",\n       subtitle = \"The chains have converged to explore the same areas\")\n\n\n\n\n\ndo.call(cbind, results$sigma) %>% \n  data.frame(., iter = 1:5000) %>% \n  pivot_longer(-iter, names_to = \"chain\", names_prefix = \"X\",\n               values_to = \"sigma\") %>% \n  ggplot(aes(iter, sigma, color = chain)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(title = \"Trace plot for sigma\",\n       subtitle = \"The chains have converged to explore the same areas\")\n\n\n\n\nA numerical convergence-diagnostic is \\(\\widehat R\\). It measures agreement between the chains by comparing the within chain variance \\(W\\) with the estimated variance using all of the available data \\(var(\\theta | y)\\). If all of the chains have converged, \\(W\\) and \\(var(\\theta | y)\\) should be equal. Thus, \\(\\widehat R\\), which is the squared root of their ratio should be 1:\n\\[\n\\widehat R = \\sqrt{\\dfrac{Var(\\theta | y)}{W}}\n\\] However, if the chains are in disagreement between each other because they have not converged, they will underestimate the total variance \\(Var(\\theta | y)\\). Why? Because they have yet to explore the full posterior scale. Thus, \\(\\widehat R\\) will be larger than 1. As the chains converge (as the number of iterations grows), we expect \\(\\widehat R\\) to converge to 1 from above.\n\niterations <- c(20, 50, 100, 5000)\nnames(iterations) <- c(20, 50, 100, 5000)\nmap_df(iterations, ~ rstan::Rhat(do.call(cbind, results$alpha)[1:.x])) %>% \n  pivot_longer(everything(), names_to = \"iterations\", values_to = \"Rhat\") %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(Rhat))\n\n\n\n\n\n  \n  \n    \n      iterations\n      Rhat\n    \n  \n  \n    20\n1.82\n    50\n1.16\n    100\n1.07\n    5000\n1.01\n  \n  \n  \n\n\n\n\nNow, for sigma:\n\niterations <- c(20, 50, 100, 5000)\nnames(iterations) <- c(20, 50, 100, 5000)\nmap_df(iterations, ~ rstan::Rhat(do.call(cbind, results$sigma)[1:.x])) %>% \n  pivot_longer(everything(), names_to = \"iterations\", values_to = \"Rhat\") %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(Rhat))\n\n\n\n\n\n  \n  \n    \n      iterations\n      Rhat\n    \n  \n  \n    20\n1.77\n    50\n1.04\n    100\n1.22\n    5000\n1.01"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#estimating-the-effective-samples-sizes",
    "href": "posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html#estimating-the-effective-samples-sizes",
    "title": "Bayesian Data Analysis: Week 5 -> Metropolis",
    "section": "Estimating the effective samples sizes",
    "text": "Estimating the effective samples sizes\nAs we said before, the sample size is not equal to the number of iterations times the number of chains. There’s an autocorrelation between the samples that we must take into account to find out how many equivalent independent samples from the posterior our iterations represent. To do so, we correct the number of total iterations by the sum of all autocorrelation lags \\(\\rho_t\\):\n\\[\ns_{eff} = \\dfrac{iterations * chains}{1 + 2\\sum^{\\infty} \\rho_t}\n\\]\nWhich can be estimated for alpha thus:\n\nrstan::ess_bulk(do.call(cbind, results$alpha))\n\n[1] 1469.648\n\n\nWe ran 5,000 iterations. Yet, we only have an effective sample size much smaller. Finally, for sigma:\n\nrstan::ess_bulk(do.call(cbind, results$sigma))\n\n[1] 1223.447"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-24-statistical-rethinking-week-7.html",
    "href": "posts/bayesian-statistics/2020-05-24-statistical-rethinking-week-7.html",
    "title": "Statistical Rethinking: Week 7",
    "section": "",
    "text": "Statistical Rethinking: Week 7\nThis week paid off. All the hard work of understanding link functions, HMC flavored Monte-Carlo, and GLM allowed to study more complex models. To keep using Richard’s metaphor: it allowed us to study monsters: models with different parts made out of different models. In particular, Zero Inflated Models and Ordered Categories.\n\n\nHomework\n\nIn the Trolley data—data(Trolley)—we saw how education level (modeled as an ordered category) is associated with responses. Is this association causal? One plausible confound is that education is also associated with age, through a causal process: People are older when they finish school than when they begin it. Reconsider the Trolley data in this light. Draw a DAG that represents hypothetical causal relationships among response, education, and age. Which statical model or models do you need to evaluate the causal influence of education on responses? Fit these models to the trolley data. What do you conclude about the causal relationships among these three variables?\n\nLet’s begin by drawing the DAG\n\n\n\n\n\nAccording to our assumptions, we cannot evaluate the causal effect of Education on response without first performing statistical adjustments on our models by including Age. Otherwise, Education will pick up some of the effect of Age on response, thereby biasing our estimates. That is, there is a backdoor leading back to Education. To close it, we must include Age in our estimates. Our computer can confirm our reasoning:\n\n\n{ Age }\n\n\n\n\nRows: 9,930\nColumns: 12\n$ case      <fct> cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbo…\n$ response  <int> 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, …\n$ order     <int> 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, …\n$ id        <fct> 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;4…\n$ age       <int> 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, …\n$ male      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ edu       <fct> Middle School, Middle School, Middle School, Middle School, …\n$ action    <int> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, …\n$ intention <int> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ contact   <int> 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ story     <fct> aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, …\n$ action2   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, …\n\n\nLet’s begin by preparing our variables:\n\nage. We will model it as a continuous parameter.\n\n\n\n\n\n\n\neducation. We will model it as an ordered category.\n\n\n\n[1] \"Bachelor's Degree\"    \"Elementary School\"    \"Graduate Degree\"     \n[4] \"High School Graduate\" \"Master's Degree\"      \"Middle School\"       \n[7] \"Some College\"         \"Some High School\"    \n\n\nHowever, R has read automatically order the factors in alphabetical order. Let’s order them in the order we need\n\n\n[1] \"Elementary School\"    \"Middle School\"        \"Some High School\"    \n[4] \"High School Graduate\" \"Some College\"         \"Bachelor's Degree\"   \n[7] \"Master's Degree\"      \"Graduate Degree\"     \n\n\nNow, we can turn them into integers and we’ll know they will be kept in the order we need them to be:\n\n\n  edu_int        edu_releveled                  edu    n\n1       1    Elementary School    Elementary School   60\n2       2        Middle School        Middle School  120\n3       3     Some High School     Some High School  420\n4       4 High School Graduate High School Graduate  870\n5       5         Some College         Some College 2460\n6       6    Bachelor's Degree    Bachelor's Degree 3540\n7       7      Master's Degree      Master's Degree 1410\n8       8      Graduate Degree      Graduate Degree 1050\n\n\nBefore we use the model, let’s verify that we haven’t missing values:\n\n\n[1] 9930\n\n\n\n\n[1] 9930\n\n\nThere are no missing obs to worry about. As a last step, let’s see what we would get if weren’t performing statistical adjustments by the type of questions nor by the education level of the respondents:\n\n\n\n\n\nLet’s code and fit the model:\n\n\nRunning MCMC with 5 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 5 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 5 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 5 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 5 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 5 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 5 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 5 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 5 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 5 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 5 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 1221.1 seconds.\nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 1227.9 seconds.\nChain 5 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 1237.8 seconds.\nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 5 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 5 finished in 1273.5 seconds.\nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 1281.1 seconds.\n\nAll 5 chains finished successfully.\nMean chain execution time: 1248.3 seconds.\nTotal execution time: 1281.4 seconds.\n\n\n\n\n\n\n\n\n\n\nThe chains look healthy enough; although the number of effective samples seems terribly slow for the total effect of education. Also, there’s the Stan warning message. Maybe one should try to run the model a bit longer. Note that the paths, during warmup exclusively, wander around in some chains on some presumably not typical set parts of the parameter space. However, this transient like behavior quickly stops and it is never present during sampling.\n\n\n                mean         sd        5.5%       94.5%    n_eff     Rhat4\nkappa[1] -2.40980142 0.07631585 -2.53447555 -2.29064580 1209.842 1.0005148\nkappa[2] -1.71487666 0.07426259 -1.83246110 -1.59697275 1196.452 1.0004060\nkappa[3] -1.12127199 0.07296840 -1.23552065 -1.00379345 1206.023 1.0003597\nkappa[4] -0.08792258 0.07183468 -0.19972794  0.02959908 1223.252 1.0004976\nkappa[5]  0.58314558 0.07241557  0.46958480  0.69982295 1243.306 1.0004691\nkappa[6]  1.48641206 0.07499928  1.36691670  1.60971435 1294.911 1.0003917\nbIC      -1.25482654 0.09641519 -1.40823995 -1.10109895 1592.040 1.0033685\nbIA      -0.45191163 0.07873589 -0.58032757 -0.32724275 1531.498 1.0020863\nbI       -0.27130468 0.05617661 -0.36041274 -0.18256063 1375.417 1.0045916\nbAge     -0.10612173 0.01960340 -0.13762106 -0.07446687 2881.420 0.9993960\nbE        0.29021678 0.07860667  0.17066931  0.41466910 1235.761 1.0002091\nbC       -0.32300800 0.06847975 -0.43346993 -0.21317939 1783.069 1.0027797\nbA       -0.45373700 0.05189077 -0.53737928 -0.36990367 1866.863 1.0013140\ndelta[1]  0.11883917 0.07300123  0.02770316  0.25202713 3703.875 0.9998998\ndelta[2]  0.13664603 0.07902661  0.03497736  0.27997437 3296.719 0.9997046\ndelta[3]  0.08227563 0.05221710  0.01903524  0.18013100 3530.285 0.9999000\ndelta[4]  0.05681726 0.04121216  0.01066274  0.12948807 2572.711 1.0002676\ndelta[5]  0.44516349 0.10474074  0.27986451  0.60637022 2219.672 0.9985959\ndelta[6]  0.07323684 0.04816609  0.01594991  0.16075218 2587.867 0.9995390\ndelta[7]  0.08702158 0.05633486  0.01825699  0.18928007 3123.487 0.9989435\n\n\nCompared to the model that had no adjustment by age, the Education coefficient has changed. Whereas before the coefficient was negative, here it is positive. Thus, we conclude: 1) Due to change in the coefficient, we believe that indeed there is a path between education and age. 2) Indicating that higher education leads to respond higher in the scale, indicating that they see the moral actions as more permissible. 3) Also, older people seem to respond lower in the scale and thus considering the actions less morally permisible.\nLet’s plot the predicted differences:\n\n\nList of 9\n $ kappa: num [1:2500, 1:6] -2.47 -2.38 -2.52 -2.42 -2.38 ...\n $ bIC  : num [1:2500(1d)] -1.13 -1.24 -1.17 -1.19 -1.16 ...\n $ bIA  : num [1:2500(1d)] -0.391 -0.615 -0.439 -0.41 -0.531 ...\n $ bI   : num [1:2500(1d)] -0.373 -0.21 -0.328 -0.238 -0.31 ...\n $ bAge : num [1:2500(1d)] -0.131 -0.115 -0.113 -0.105 -0.132 ...\n $ bE   : num [1:2500(1d)] 0.31 0.279 0.358 0.218 0.281 ...\n $ bC   : num [1:2500(1d)] -0.346 -0.343 -0.374 -0.39 -0.316 ...\n $ bA   : num [1:2500(1d)] -0.473 -0.43 -0.49 -0.492 -0.324 ...\n $ delta: num [1:2500, 1:7] 0.065 0.1331 0.0247 0.1584 0.0516 ...\n - attr(*, \"source\")= chr \"ulam posterior: 2500 samples from object\"\n\n\nLet’s add the zero to the delta:\n\n\n\nNow, let’s write some functions to work with the samples from the posterior:\n\n\n\n\n\n\n\n\nAs we gauged from the coefficient, we predict that, on average, older people respond lower lower on the response scale. Above, for an action with both intention and contact, the difference is very clear: older people respond more often with lower values of moral permissibility, whereas younger people respond more often with higher values of moral permissibility.\nLet’s check the difference across education levels:\n\n\n\n\n\nTo conclude the question:\n\nOlder people tend to assign lower levels of moral permissibility.\nMore educated people tend to assign higher levels of moral permisibility.\nAge and education level seem correlated. Thus, without accounting for age, education level will pick up some of the effects of age on responses.\n\n\nConsider one more variable in the Trolley data: Gender. Suppose that gender might influence education as well as response directly. Draw the DAG now that includes response, education, age, and gender. Using only the DAG, is it possible that the inferences from Problem 1 are confounded by gender? If so,define any additional models you need to infer the causal influence of education on response. What do you conclude?\n\n\n\n\n\n\nYes, according to our current DAG, inferences from problem 1 are confounded. By including Education in our statistical adjustments, we are conditioning on a collider and thereby opening a backdoor: Education will pick up the effect of gender on response. Therefore, if we wish to infer the influence of education on response, we must perform a statistical adjustment with Gender.\n\n\n{ Age, Gender }\n\n\nWe will include gender with an index variable:\n\n\n\nLet’s first try to gauge what we would get if we weren’t performing statistical adjustment by other covariates:\n\n\n\n\n\nNow, let’s fit the model:\n\n\nRunning MCMC with 5 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 5 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 5 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 5 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 5 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 5 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 5 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 5 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 5 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 5 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 5 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 5 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 5 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 5 finished in 2405.7 seconds.\nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 2421.8 seconds.\nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 2500.8 seconds.\nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 2515.9 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 2541.7 seconds.\n\nAll 5 chains finished successfully.\nMean chain execution time: 2477.2 seconds.\nTotal execution time: 2542.1 seconds.\n\n\nLet’s check the traceplots of our chains:\n\n\n\n\n\n\n\n\n\n\n                  mean         sd        5.5%       94.5%     n_eff     Rhat4\nbIC        -1.26222698 0.09563927 -1.41074770 -1.11361000 1474.4962 1.0029517\nbIA        -0.44224902 0.07749018 -0.56902660 -0.32144160 1379.1919 1.0011014\nbI         -0.28783297 0.05548265 -0.37528620 -0.19835261 1184.6262 1.0019602\nbAge       -0.07248551 0.02178785 -0.10676978 -0.03744601 1256.6172 0.9998830\nbE          0.04806116 0.15049454 -0.20515563  0.26088915  633.9650 1.0010820\nbC         -0.34144399 0.06682613 -0.44986452 -0.23195024 1486.2202 1.0018834\nbA         -0.47192368 0.05198801 -0.55570593 -0.38783936 1430.7327 1.0002398\nbGender[1]  0.25767990 0.19853134 -0.05838659  0.57397892  873.4987 1.0006516\nbGender[2]  0.82169908 0.19885860  0.50228403  1.13746555  813.2591 1.0004183\ndelta[1]    0.14271785 0.09071624  0.02994543  0.30946166 2765.3540 0.9993168\ndelta[2]    0.14430729 0.08713521  0.03617260  0.30389157 2700.6726 0.9995506\ndelta[3]    0.13050518 0.08342718  0.03009101  0.28879109 1870.8427 1.0001615\ndelta[4]    0.12275620 0.08961734  0.02284280  0.29555093 1355.8506 0.9997193\ndelta[5]    0.21254299 0.15103173  0.02559998  0.48118750  671.0591 1.0024846\ndelta[6]    0.11936824 0.08260875  0.02197871  0.27901172 1602.1946 1.0031488\ndelta[7]    0.12780225 0.08041071  0.02754787  0.27423736 3116.7440 0.9997262\n\n\nIt seems that the coefficient for men is considerably higher than the coefficient for women. That is, on the relative scale, women are more likely to give lower moral permissibility to the different situations. Interestingly, the coefficient for Education has greatly reduced, and now it encompasses zero right in the middle. Therefore, by performing statistical adjustment with gender, the Education coefficient has greatly decreased.\nIndeed, let’s check the difference across education levels.\nLet’s modify our functions to simulate from our model now including a gender intercept:\n\n\n\nLet’s extract samples:\n\n\n\nAnd finally simulate:\n\n\n\n\n\nAs expected, now that we are performing a statistical adjustment by Gender, the influence of education level on response has greatly reduced. In fact, it has reduced so much that our model predicts barely any difference response levels across educations. Let’s do the same plot but for men:\n\n\n\n\n\nIt seems that Gender is driving the variation in responses in the sample. To see this, let’s simulate the differnce by gender for the most highly educated and older people in the sample:\n\n\n\n\n\nLet’s plot our simulations for yet another situation, this time a situation with both action, intent and contact:\n\n\n\n\n\nTherefore, we can conclude that, among the covariates studied, the greatest variation across responses is found on gender. Regardless of education level, women, on average, regard the different acts as much less morally permissible than men.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-03-statistical-rethinking-week-9.html",
    "href": "posts/bayesian-statistics/2020-06-03-statistical-rethinking-week-9.html",
    "title": "Statistical Rethinking: Week 9",
    "section": "",
    "text": "Week 9 was all about fitting models with multivariate distributions in them. For example, a multivariate likelihood helps us use an instrumental variable to estimate the true causal effect of a predictor. But also as an adaptive prior for some of the predictors. In both cases, we found out that the benefit comes from modelling the resulting var-cov matrix. In the instrumental variable case, the resulting joint distribution for the residuals was the key to capture the statistical information of the confounding variable. In the adaptive prior case, it helps understand the relationship between different parameter types.\n\nHomework\n\n\n1st question\nRevisit the Bangladesh fertility data,data(bangladesh). Fit a model with both varying intercepts by district_id and varying slopes of urban (as a 0/1 indicator variable) by district_id. You are still predicting use.contraception. Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, in each district, with urban women on one axis and rural on the other, might also help.\n\ndata(\"bangladesh\")\n\n# Fix the district id\nbangladesh %>% \n  mutate(district_id = as.integer( as.factor(district) ) ) -> bangladesh\nglimpse(bangladesh)\n\nRows: 1,934\nColumns: 7\n$ woman             <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ district          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ use.contraception <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ living.children   <int> 4, 1, 3, 4, 1, 1, 4, 4, 2, 4, 1, 1, 2, 4, 4, 4, 1, 4…\n$ age.centered      <dbl> 18.4400, -5.5599, 1.4400, 8.4400, -13.5590, -11.5600…\n$ urban             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ district_id       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nLet’s fit the varying effects models for each district to have its average contraception use its own the differential between urban and rural areas.\n\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban\n)\n\n\nmodel_varying <- ulam(\n  alist(\n    contraception ~ binomial(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban,\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  data = data_varying,\n  chains = 4, cores = 4,\n  iter = 2000\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 25.4 seconds.\nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 29.4 seconds.\nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 30.7 seconds.\nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 32.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 29.5 seconds.\nTotal execution time: 33.0 seconds.\n\n\nLet’s check our chains’ health:\n\ntraceplot_ulam(model_varying)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe chains look healthy enough. They are:\n\nThey are stationary\nThey mix well across the parameter space.\nDifferent chains converge to explore the same parameter space.\n\nLet’s check the \\(\\hat{R}\\) values:\n\nresults <- precis(model_varying, depth = 3)\nresults %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9995  \n Median :0.9999  \n Mean   :1.0001  \n 3rd Qu.:1.0002  \n Max.   :1.0051  \n NA's   :2       \n\n\nThe \\(\\hat{R}\\) look OK, indicating that the Markov chains are in close agreement with each other. Let’s check the parameters:\n\nprecis(model_varying, depth = 2, pars = c(\"sigma\", \"a\", \"b\"))\n\n               mean         sd       5.5%      94.5%     n_eff    Rhat4\nsigma[1]  0.5732478 0.09736082  0.4268930  0.7362033 1117.7376 1.000141\nsigma[2]  0.7767925 0.20201103  0.4672885  1.1105892  503.5447 1.005126\na        -0.7055373 0.10293818 -0.8725771 -0.5428108 2976.0407 1.000136\nb         0.6969825 0.16603119  0.4444045  0.9658206 2375.1974 1.000239\n\n\nThe contraceptive use is not that likely, thus the negative (in log-odds scale) average value in the adaptive prior for \\(a\\). The positive value for \\(b\\), on the other hand, indicates that the average distribution of slopes is positive. That is, women in urban areas are, on average, more likely to use contraception. Finally, the variances. Both indicate quite a bit of variation in the multivariate population for intercepts and slopes.\n\nprecis(model_varying, pars = \"Rho\", depth = 3)\n\n               mean        sd      5.5%      94.5%    n_eff    Rhat4\nRho[1,1]  1.0000000 0.0000000  1.000000  1.0000000      NaN      NaN\nRho[1,2] -0.6585362 0.1612475 -0.868516 -0.3679604 710.9425 1.003216\nRho[2,1] -0.6585362 0.1612475 -0.868516 -0.3679604 710.9425 1.003216\nRho[2,2]  1.0000000 0.0000000  1.000000  1.0000000      NaN      NaN\n\n\nThere’s a negative correlation between the parameter types: i.e., for districts with higher contraceptive usage overall, the correlation informs us that we should predict a lower than average differential in the use of contraceptives between rural and urban areas.\nWe can follow Richard’s advice and plot both types of parameters for each district. We can even overlay the ellipses that determine the levels of the multivariate adaptive prior:\n\nsamples <- extract.samples(model_varying)\n\nMu_est <- c(mean(samples$a), mean(samples$b))\nrho_est <- mean(samples$Rho[,1,2])\nsa_est <- mean(samples$sigma[,1])\nsb_est <- mean(samples$sigma[, 2])\ncov_ab <- sa_est*sb_est*rho_est\nSigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol = 2)\n\ncontour_level <- function(level) {\n  ellipse::ellipse(Sigma_est, centre = Mu_est, level = level) %>% \n    data.frame() %>% \n    mutate(level = level)\n} \n\npurrr::map(c(0.1, 0.3, 0.5, 0.8, 0.99), contour_level) %>% \n  bind_rows() -> data_elipses\ndata_elipses %>% \n  ggplot(aes(x, y)) +\n  geom_path(aes(group = level), linetype = 2) +\n  geom_point(data = data.frame(x = Mu_est[1]), y = Mu_est[2], color = \"red\")\n\n\n\n\nFinally, we can plot the points:\n\nsamples$alpha %>% \n  as_tibble() %>% \n  pivot_longer(everything(), names_to = \"district_id_\", names_prefix = \"V\", values_to = \"alpha\") %>% \n  bind_cols(samples$beta %>% \n  as_tibble() %>% \n  pivot_longer(everything(), names_to = \"district_id\", names_prefix = \"V\", values_to = \"beta\")) %>% \n  group_by(district_id) %>% \n  median_qi(alpha, beta) %>% \n  select(district_id, alpha, beta) %>% \n  ggplot(aes(alpha, beta)) +\n  geom_point(alpha = 0.6) +\n  geom_path(data = data_elipses,\n            inherit.aes = F,\n            mapping = aes(x, y, group = level), linetype = 2, color = \"dodgerblue4\") +\n  geom_point(data = data.frame(x = Mu_est[1]), y = Mu_est[2], color = \"red\",\n             inherit.aes = FALSE,\n             mapping = aes(x, y)) +\n  labs(title = \"Negative correlation between intercepts and slopes per district\",\n       subtitle = \"Districts with higher overall use have lower differentials between urban and rural\",\n       x = expression(alpha),\n       y = expression(beta))\n\n\n\n\n\n\n2nd question\nNow consider the predictor variables age.centered and living.children, also contained in data(bangladesh). Suppose that age influences contraceptive use (changing attitudes) and number of children (older people have had more time to have kids). Number of children may also directly influence contraceptive use. Draw a DAG that reflects these hypothetical relationships. Then build models needed to evaluate the DAG. You will need at least two models. Retain district and urban, as in Problem 1. What do you conclude about the causal influence of age and children?\n\ndag <- dagitty::dagitty(\" dag {\n                        Age -> N_children\n                        Age -> contraception\n                        N_children -> contraception\n                        }\")\ndrawdag(dag)\n\n\n\n\nConditional on this DAG, the total causal effect of Age on contraception is mediated (pipe) with Number of Children. Thus, to get the total effect we must not control by number of children.\nLet’s fit this model:\n\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban,\n  age = bangladesh$age.centered, \n  kids = bangladesh$living.children\n)\nmodel_only_age <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban + gamma*age,\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 50.3 seconds.\nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 52.1 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 52.6 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 54.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 52.4 seconds.\nTotal execution time: 54.7 seconds.\n\n\nLet’s check our chains’ health:\n\ntraceplot_ulam(model_only_age)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe chains look healthy enough. They are:\n\nThey are stationary\nThey mix well across the parameter space.\nDifferent chains converge to explore the same parameter space.\n\nLet’s check the \\(\\hat{R}\\) values:\n\nprecis(model_only_age, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9996  \n Median :1.0000  \n Mean   :1.0002  \n 3rd Qu.:1.0005  \n Max.   :1.0069  \n NA's   :2       \n\n\nThe \\(\\hat{R}\\) look OK, indicating that the Markov chains are in close agreement with each other. Let’s check the parameters:\n\nprecis(model_only_age, depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\"))\n\n                 mean          sd          5.5%       94.5%     n_eff     Rhat4\na        -0.711537292 0.102268885 -0.8787672400 -0.55198898 2905.2135 1.0005300\nb         0.695464373 0.173304390  0.4243350000  0.97019381 2109.0982 1.0006393\ngamma     0.009474581 0.005552167  0.0005521033  0.01849995 8484.1344 0.9992501\nsigma[1]  0.584962697 0.100125045  0.4371533400  0.75428655 1260.7598 1.0016182\nsigma[2]  0.802377308 0.196794320  0.5076205600  1.13047220  586.1314 1.0069074\nRho[1,1]  1.000000000 0.000000000  1.0000000000  1.00000000       NaN       NaN\nRho[1,2] -0.650102143 0.167089307 -0.8636559950 -0.33474679  864.0654 1.0037881\nRho[2,1] -0.650102143 0.167089307 -0.8636559950 -0.33474679  864.0654 1.0037881\nRho[2,2]  1.000000000 0.000000000  1.0000000000  1.00000000       NaN       NaN\n\n\nThe distribution of intercepts and slopes looks completely unchanged. For the \\(\\gamma\\), our estimated effect has much of its probability mass around zero and 0.02. Therefore, we conclude that the total causal effect of age on the use of contraception is small. For example, let’s take the woman from the first district and predict our expected probability that they use contraception, across both urban and rural areas, as function of age:\n\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(age, district_id = 1) %>% \n  add_fitted_draws(model_only_age) %>% \n  ggplot(aes(age, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of age\",\n       subtitle = \"Age has a positive small effect. No statistical adjustment by # of kids \",\n       y = \"predicted prob\")\n\n\n\n\nNow for the model that takes into account the number of children each woman has:\n\nmodel_age_kids <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban + gamma*age + delta*kids,\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    delta ~ normal(0, 1),\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 72.7 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 73.5 seconds.\nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 75.1 seconds.\nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 78.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 75.0 seconds.\nTotal execution time: 78.9 seconds.\n\n\nLet’s look at our \\(\\hat{R}\\):\n\nprecis(model_age_kids, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9997  \n Median :1.0000  \n Mean   :1.0002  \n 3rd Qu.:1.0005  \n Max.   :1.0041  \n NA's   :2       \n\n\nThe \\(\\hat{R}\\) look OK, indicating agreement between chains. Let’s check our posterior’s parameters:\n\nprecis(model_age_kids,  depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\", \"delta\"))\n\n                mean          sd        5.5%       94.5%     n_eff     Rhat4\na        -1.83023872 0.189105439 -2.12943000 -1.53019765  737.8404 1.0010395\nb         0.74380971 0.170902543  0.47651030  1.01466715 1834.3857 1.0011111\ngamma    -0.02977116 0.007806217 -0.04238032 -0.01738009 1240.2477 0.9999487\nsigma[1]  0.60960022 0.103413091  0.45610425  0.77975195 1147.9766 1.0014546\nsigma[2]  0.77524901 0.204278550  0.46024833  1.10468865  480.3989 1.0041223\nRho[1,1]  1.00000000 0.000000000  1.00000000  1.00000000       NaN       NaN\nRho[1,2] -0.63624957 0.172037893 -0.85805022 -0.31101017  569.2614 1.0039501\nRho[2,1] -0.63624957 0.172037893 -0.85805022 -0.31101017  569.2614 1.0039501\nRho[2,2]  1.00000000 0.000000000  1.00000000  1.00000000       NaN       NaN\ndelta     0.41420075 0.056842441  0.32393682  0.50519928  715.3344 1.0002757\n\n\nOur population distribution for slopes and parameters has shifted: the average probability of using contraception, for a woman with 1 kids, is much lower. That can be explained as our parameters for the number of children, \\(\\delta\\), is clearly positive with an 87% compatibility interval between (0.33, 0.50) in the log-odds. Notice also that the effect of age has changed signs and it’s mass is around (-0.04, -0.02) in the log odds scale. That is, older women, adjusting by the number of children they have, are less likely to use contraception.\nLet’s plot the effect of having children for the women of the district 20 of average age:\n\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(kids, district_id = 20, age = 0) %>% \n  add_fitted_draws(model_age_kids) %>% \n  ggplot(aes(kids, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of # of kids\",\n       subtitle = \"Women with more kids are more likely to use contraception\")\n\n\n\n\nNow, for age:\n\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(age, district_id = 1, kids = 1) %>% \n  add_fitted_draws(model_age_kids) %>% \n  ggplot(aes(age, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of age\",\n       subtitle = \"Age has a negative effect. Statistically adjusting by # of kids\",\n       y = \"predicted prob\")\n\n\n\n\nGoing back to our DAG, our findings are in accordance with it. The total causal effect of age is less than the direct causal effect due to the pipe that goes through number of kids. That is, older women have lower probabilities to use contraception once we statistically adjust by the number of kids they have. However, older women also tend to have more children and the direct effect of having more children is to be less likely to use contraception. Therefore, the mixed signal that we get from the total effect.\n\n\n3rd question\nModify any models from Problem 2 that contained that children variable and model the variable now as a monotonic ordered category, like education from the week we did ordered categories. Education in that example had 8 categories. Children here will have fewer (no one in the sample had 8 children). So modify the code appropriately. What do you conclude about the causal influence of each additional child on use of contraception?\nAlmost inadvertently, in our previous model we assumed that the additional effect of each kid in the log odds of using contraception was constant. By modelling as an ordered category, we let the data decide whether it should be so.\n\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban,\n  age = bangladesh$age.centered, \n  kids = as.integer(bangladesh$living.children),\n  alpha = rep(2, 3)\n)\n\nmodel_age_kids_ord <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alp[district_id] + beta[district_id] * urban + gamma*age + bks*sum(delta_j[1:kids]),\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    bks ~ normal(0, 1),\n    # adaptive priors\n    c(alp, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2),\n    vector[4]: delta_j <<- append_row(0, delta),\n    simplex[3]: delta ~ dirichlet(alpha)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 97.2 seconds.\nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 107.4 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 107.9 seconds.\nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 113.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 106.5 seconds.\nTotal execution time: 113.8 seconds.\n\n\nLet’s look at our \\(\\hat{R}\\):\n\nprecis(model_age_kids_ord, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9998  \n Median :1.0004  \n Mean   :1.0006  \n 3rd Qu.:1.0010  \n Max.   :1.0081  \n NA's   :2       \n\n\nThe \\(\\hat{R}\\) values look OK, indicating that the chains are in close agreement with each other. Let’s check our parameters:\n\nprecis(model_age_kids_ord,  depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\", \"bks\"))\n\n                mean          sd        5.5%       94.5%     n_eff    Rhat4\na        -1.65414138 0.150164278 -1.89677880 -1.41186295 1077.8983 1.001821\nb         0.75640661 0.166677397  0.49969545  1.02912495 1975.9601 1.000855\ngamma    -0.02849433 0.007341003 -0.04022939 -0.01663006 2461.7104 1.000734\nsigma[1]  0.60865051 0.103430477  0.45507541  0.78141514 1311.5317 1.002836\nsigma[2]  0.77329681 0.212034191  0.44747856  1.11943225  483.0631 1.008150\nRho[1,1]  1.00000000 0.000000000  1.00000000  1.00000000       NaN      NaN\nRho[1,2] -0.64433118 0.169482917 -0.86035707 -0.33008590  860.8983 1.006834\nRho[2,1] -0.64433118 0.169482917 -0.86035707 -0.33008590  860.8983 1.006834\nRho[2,2]  1.00000000 0.000000000  1.00000000  1.00000000       NaN      NaN\nbks       1.37803430 0.160614910  1.12189780  1.62950030 1038.0832 1.002649\n\n\nThe overall effect of the children variable, when a woman has 4 children, has the same sign and roughly the same magnitude as previous inferences. Let’s look at the effect splitted by the number of children:\n\nprecis(model_age_kids_ord, depth = 3, pars = \"delta\")\n\n               mean         sd       5.5%     94.5%    n_eff     Rhat4\ndelta[1] 0.73293447 0.08141250 0.59898722 0.8600923 4670.253 0.9994883\ndelta[2] 0.16762106 0.07829638 0.05269849 0.2999460 5027.078 0.9994653\ndelta[3] 0.09944448 0.05476397 0.02512521 0.1978564 5644.457 0.9993787\n\n\nRemember that these are percentages of the total effect. That is, around 73% of the total effect comes from having the second child. Therefore, we conclude that most of the effect that having children increases the chances of using contraception comes from having a second child.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html",
    "href": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html",
    "title": "Bayesian Data Analysis: Week 4 -> Importance Sampling",
    "section": "",
    "text": "Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\nIn this blogpost, I’ll go over one of the main topics of Week 4: Importance Sampling; I’ll also solve a couple of the exercises for Chapter 10 of the book. Week 4 was all about preparing the way for MCMC: if we cannot fully compute the posterior, but we can evaluate an unnormalized version, how can we approximate the posterior distribution?"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html#importance-sampling",
    "href": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html#importance-sampling",
    "title": "Bayesian Data Analysis: Week 4 -> Importance Sampling",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nLet \\(q(\\theta|y)\\) be the unnormalized posterior density that we can compute at some values for theta. Then, the posterior expectation is:\n\\[\nE(\\theta|y) = \\dfrac{\\int \\theta q(\\theta|y) d\\theta}{\\int q(\\theta|y) d\\theta}\n\\] However, the denominator is often intractable. Importance sampling, then, reduces to consider another density \\(g(\\theta)\\) from which we can draw direct samples. Then, if we multiply and divide by \\(g(\\theta)\\) in both numerator and denominator:\n\\[\n\\frac{\\int[\\theta q(\\theta \\mid y) / g(\\theta)] g(\\theta) d \\theta}{\\int[q(\\theta \\mid y) / g(\\theta)] g(\\theta) d \\theta}\n\\] Which can be considered expectations with respect to \\(g(\\theta)\\). Therefore, we can estimate both numerator and denominator by direct sampling from \\(g(\\theta)\\):\n\\[\n\\frac{\\frac{1}{S} \\sum_{s=1}^{S} \\left(\\theta^{s}\\right) w\\left(\\theta^{s}\\right)}{\\frac{1}{S} \\sum_{s=1}^{S} w\\left(\\theta^{s}\\right)}\n\\]\nWhere \\(w\\left(\\theta^{s}\\right)\\) are called the importance weights and are defined thus:\n\\[\nw\\left(\\theta^{s}\\right)=\\frac{q\\left(\\theta^{s} \\mid y\\right)}{g\\left(\\theta^{s}\\right)}\n\\]\nTherefore, importance sampling is sampling from an approximation to the posterior and then correcting the importance that each sample has in the computation of a specific expectation. We correct by the ratio of the unnormalized posterior density to the density of the approximation.\nImportance sampling will give precise estimates of the expectation if the weights are roughly uniform. However, if the importance weights vary substantially, the method will yield unsatisfactory estimates. Indeed:\n\nThe worst possible scenario occurs when the importance ratios are small with high probability but with a low probability are huge, which happens, for example, if ( q ) has wide tails compared to ( g, ) as a function of ( )"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html#putting-it-into-practice",
    "href": "posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html#putting-it-into-practice",
    "title": "Bayesian Data Analysis: Week 4 -> Importance Sampling",
    "section": "Putting it into practice",
    "text": "Putting it into practice\nHere I’ll solve exercise 6 and exercise 7\n\nExercise 6\nImportance sampling when the importance weights are well behaved: consider a univariate posterior distribution, ( p(y), ) which we wish to approximate and then calculate moments of, using importance sampling from an unnormalized density, ( g() ). Suppose the posterior distribution is normal, and the approximation is ( t_{3} ) with mode and curvature matched to the posterior density.\nThe ( t_{3} ) has a variance of 3. Therefore, we sample from a normal with standard deviation of 3.\n\n\nDraw a sample of size ( S=100 ) from the approximate density and compute the importance ratios. Plot a histogram of the log importance ratios.\n\n\nFirst, we draw samples from the approximate density ( t_{3} )\n\napproximate_samples <- rt(100, df = 3)\n\nThen, we compute the density \\(g(\\theta)\\) at these points:\n\napproximate_density <- dt(approximate_samples, df = 3, log = TRUE)\n\nFinally, we compute the density at the sampled points:\n\nunnormalized_posterior <- dnorm(approximate_samples, log = TRUE, sd = sqrt(3))\n\nThe log importance weights are then:\n\nlog_imp_weights = unnormalized_posterior - approximate_density\ndata.frame(log_imp_weights) %>% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n\n\n\n\n\n\nEstimate ( (y) ) and ( (y) ) using importance sampling. Compare to the true values.\n\n\nFirst, we exponentiate and normalize the weights:\n\nnorm_weights <- exp(log_imp_weights)/sum(exp(log_imp_weights))\n\nFinally, we compute ( (y) ) simply as the sum of the product of the samples and these weights:\n\nmean_estimate <- sum(approximate_samples*norm_weights)\nglue::glue(\"Mean estimate: {round(mean_estimate, 2)}\")\n\nMean estimate: -0.12\n\n\nAnd ( (y) ):\n\nvariance_estimate <- sum(approximate_samples^2*norm_weights) + mean_estimate^2\nglue::glue(\"Variance estimate: {round(variance_estimate, 2)}\")\n\nVariance estimate: 3.41\n\n\nWhich makes for a Monte-Carlo standard error for the mean of:\n\nvariance_estimate/100\n\n[1] 0.03412438\n\n\n\n\nRepeat (a) and (b) for ( S=10,000 )\n\n\n\napproximate_samples <- rt(10000, df = 3)\napproximate_density <- dt(approximate_samples, df = 3, log = TRUE)\nunnormalized_posterior <- dnorm(approximate_samples, log = TRUE, sd = sqrt(3))\n\nlog_imp_weights = unnormalized_posterior - approximate_density\ndata.frame(log_imp_weights) %>% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a normal with a t distribution\")\n\n\n\n\nFinally, the computations:\n\nnorm_weights <- exp(log_imp_weights)/sum(exp(log_imp_weights))\nmean_estimate <- sum(approximate_samples*norm_weights)\nglue::glue(\"Mean estimate: {round(mean_estimate, 3)}\")\n\nMean estimate: 0.031\n\n\n\nvariance_estimate <- sum(approximate_samples^2*norm_weights) + mean_estimate^2\nglue::glue(\"Variance estimate: {round(variance_estimate, 2)}\")\n\nVariance estimate: 2.88\n\n\nWhich makes for a Monte-Carlo standard error for the mean of:\n\nvariance_estimate/10000\n\n[1] 0.0002883421\n\n\n\n\nUsing the sample obtained in (c), compute an estimate of effective sample size using (10.4) on page 266\n\n\nThe effective sample size is:\n\\[\nS_{\\mathrm{eff}}=\\frac{1}{\\sum_{s=1}^{S}\\left(\\tilde{w}\\left(\\theta^{s}\\right)\\right)^{2}}\n\\] where \\(\\tilde w\\) are the normalized weights.\n\ns_eff <- 1/sum(norm_weights^2)\nglue::glue(\"Effective sample size: {round(s_eff,0)}\")\n\nEffective sample size: 8246\n\n\nGiven that most of the weights are very small, we can have a reasonably efficient approximation of the posterior using importance sampling\n\n\nExercise 7\nImportance sampling when the importance weights are too variable: repeat the previous exercise, but with a ( t_{3} ) posterior distribution and a normal approximation. Explain why the estimates of ( (y) ) are systematically too low.\n\napproximate_samples <- rnorm(100, sd = sqrt(3))\napproximate_density <- dnorm(approximate_samples,  log = TRUE, sd = sqrt(3))\nunnormalized_posterior <- dt(approximate_samples, df = 3, log = TRUE) \n\nlog_imp_weights = unnormalized_posterior - approximate_density\ndata.frame(log_imp_weights) %>% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n       subtitle = \"Approximating a t-distribution with a normal\")\n\n\n\n\n\nnorm_weights <- exp(log_imp_weights)/sum(exp(log_imp_weights))\nmean_estimate <- sum(approximate_samples*norm_weights)\nglue::glue(\"Mean estimate: {round(mean_estimate, 3)}\")\n\nMean estimate: -0.115\n\n\n\nvariance_estimate <- sum(approximate_samples^2*norm_weights) + mean_estimate^2\nglue::glue(\"Variance estimate: {round(variance_estimate, 2)}\")\n\nVariance estimate: 2.64\n\n\n\ns_eff <- 1/sum(norm_weights^2)\nglue::glue(\"Effective sample size: {round(s_eff,0)}\")\n\nEffective sample size: 86\n\n\nEven if we increase the sample size, the problems still remain:\n\napproximate_samples <- rnorm(10000, sd = sqrt(3))\napproximate_density <- dnorm(approximate_samples,  log = TRUE, sd = sqrt(3))\nunnormalized_posterior <- dtnew(approximate_samples, df = 3, log = TRUE) \n\nlog_imp_weights = unnormalized_posterior - approximate_density\ndata.frame(log_imp_weights) %>% \n  ggplot(aes(exp(log_imp_weights))) +\n  geom_density(fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Importance weights\",\n      subtitle = \"Approximating a t-distribution with a normal\")\n\n\n\n\n\nnorm_weights <- exp(log_imp_weights)/sum(exp(log_imp_weights))\nmean_estimate <- sum(approximate_samples*norm_weights)\nglue::glue(\"Mean estimate: {round(mean_estimate, 3)}\")\n\nMean estimate: 0.03\n\n\n\nvariance_estimate <- sum(approximate_samples^2*norm_weights) + mean_estimate^2\nglue::glue(\"Variance estimate: {round(variance_estimate, 2)}\")\n\nVariance estimate: 2.11\n\n\nNow, the posterior will have fatter tails than the normal approximation. Therefore, we will have to correct our normal samples with large importance weights, highlighting the inadequacy of our samples generated from the normal to approximate the tails of the posterior. Thus, we cannot adequately account for the tail effect in neither the mean nor the variance. However, the effect is worsened for the variance, as the L2 norm amplifies the consequences of inadequately sampling from the tail."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-15-statistical-rethinking-week-5-hmc-samples.html",
    "href": "posts/bayesian-statistics/2020-05-15-statistical-rethinking-week-5-hmc-samples.html",
    "title": "Statistical Rethinking Week 5 -> HMC samples",
    "section": "",
    "text": "Statistical Rethinking: Week 5\nAfter a quick tour around interactions, this week was a quick introduction to MCMC samplers and how they are the engine that powers current Bayesian modelling. We looked at Metropolis, Gibbs and finally HMC. Not only HMC is more efficient, but it also let us know when it fails. Let’s tackle the homework with these new tools:\n\n\nHomework 5\n\nProblem Week 1\n\n\n            judge  n\n1 Daniele Meulder 20\n2  Francis Schott 20\n3    Jamal Rayyis 20\n4 Jean-M Cardebat 20\n5        John Foy 20\n6    Linda Murphy 20\n7 Olivier Gergaud 20\n8  Robert Hodgson 20\n9    Tyler Colman 20\n\n\nWe have 9 judges and each of them gave 20 reviews. Let’s check the scores\n\n\n\n\n\n0 should be a meaningful metric. As well as 1. Let’s perform feature scaling on score:\n\n\n\n\n\nLet’s analyze our predictor variables: judge and wine\n\n\n  judge.amer   n\n1          0  80\n2          1 100\n\n\n\n\n   wine n\n1    A1 9\n2    A2 9\n3    B1 9\n4    B2 9\n5    C1 9\n6    C2 9\n7    D1 9\n8    D2 9\n9    E1 9\n10   E2 9\n11   F1 9\n12   F2 9\n13   G1 9\n14   G2 9\n15   H1 9\n16   H2 9\n17   I1 9\n18   I2 9\n19   J1 9\n20   J2 9\n\n\nLet’s create an index variable:\n\n\n  judge judge.amer   n\n1     1          0  80\n2     2          1 100\n\n\n\n\n   wine_fct wine n\n1         1   A1 9\n2         2   A2 9\n3         3   B1 9\n4         4   B2 9\n5         5   C1 9\n6         6   C2 9\n7         7   D1 9\n8         8   D2 9\n9         9   E1 9\n10       10   E2 9\n11       11   F1 9\n12       12   F2 9\n13       13   G1 9\n14       14   G2 9\n15       15   H1 9\n16       16   H2 9\n17       17   I1 9\n18       18   I2 9\n19       19   J1 9\n20       20   J2 9\n\n\n\n\n\n\nConsider only variation among judges and wines\n\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n\n\nLet’s check how the sampling of the posterior went:\n\n\n\n\n\n\n\n\nOur chains have all the characteristics of healthy chains:\n\nThey are stationary.\nThey mix well the the entire parameter space.\nThey converge to explore the same parameter space across chains.\n\n\n\n           mean         sd       5.5%     94.5%    n_eff     Rhat4\nj[1]  0.1812679 0.04184097 0.11536425 0.2466991 2681.346 0.9985223\nj[2]  0.2837266 0.04060739 0.21792262 0.3494357 2899.418 1.0001070\nj[3]  0.2813452 0.04079548 0.21543446 0.3445295 2030.262 0.9990988\nj[4]  0.1266579 0.04195269 0.05924000 0.1945397 2379.647 0.9985487\nj[5]  0.4039546 0.04183995 0.33724045 0.4717385 2603.052 0.9984313\nj[6]  0.3365812 0.04205265 0.26763338 0.4031346 2840.250 0.9989044\nj[7]  0.2655935 0.04304167 0.19877817 0.3340447 2494.531 1.0002036\nj[8]  0.1027778 0.04143975 0.03733354 0.1692216 2595.286 0.9994269\nj[9]  0.1675230 0.04240294 0.09861235 0.2367892 2540.246 0.9985044\nw[1]  0.4068375 0.05576358 0.31675751 0.4962600 3606.090 0.9992622\nw[2]  0.4001387 0.05307854 0.31754251 0.4874240 3270.758 1.0001830\nw[3]  0.4294509 0.05501064 0.34096957 0.5193709 3839.314 0.9993493\nw[4]  0.4767516 0.05393619 0.39104134 0.5604556 2963.826 0.9988180\nw[5]  0.3625362 0.05507341 0.27333534 0.4511795 3170.747 0.9998158\nw[6]  0.3192989 0.05798840 0.22911316 0.4113976 4700.174 0.9987453\nw[7]  0.4324590 0.05396901 0.34503395 0.5191273 3180.325 1.0000889\nw[8]  0.4299744 0.05557086 0.34025961 0.5180390 3402.303 0.9987850\nw[9]  0.3966763 0.05431820 0.30998893 0.4810641 3682.290 0.9984534\nw[10] 0.4038158 0.05637373 0.31703852 0.4963275 3372.165 0.9988019\nw[11] 0.3807681 0.05667858 0.28855148 0.4698498 3777.542 0.9987768\nw[12] 0.3785841 0.05678800 0.28678797 0.4700092 3388.091 0.9987300\nw[13] 0.3657092 0.05677917 0.27569986 0.4570325 3358.784 0.9991056\nw[14] 0.3837913 0.05656656 0.29291662 0.4726256 3036.489 0.9984837\nw[15] 0.3461885 0.05485679 0.25859807 0.4339357 3575.390 0.9984682\nw[16] 0.3496063 0.05351562 0.26491040 0.4349948 3079.376 0.9987447\nw[17] 0.3571750 0.05774181 0.26448740 0.4515752 2918.020 0.9993020\nw[18] 0.2374186 0.05732765 0.14861334 0.3269997 4018.920 0.9991466\nw[19] 0.3554900 0.05523885 0.26811755 0.4450294 3130.919 0.9983058\nw[20] 0.4487768 0.05523754 0.35953171 0.5372723 3765.401 0.9982562\nsigma 0.1878011 0.01141724 0.17077951 0.2075262 3011.966 0.9994656\n\n\nThe Rhat is ok for all the parameters.\nThe table is just not informative. Let’s do some ggridges for both judges and wines.\n\n\n\n\n\nIt seems that John Foy was, on average, according to our data and assumptions, gave the highest scores across wines. Whilst Robert Hodgson was the one who gave the least favorable scores.\n\n\n\n\n\nIt seems that we expect, on average, wines B2 and J2 to be scored the highest across all judges. Whilst the lowest we expect it to be I2.\n\nNow, consider three features of the wines and judges:\n\n\nflight\nwine.amer\njudge.amer\n\n\n\n\n\n\n  flight flight_int  n\n1    red          1 90\n2  white          2 90\n\n\nThere are equal number of wines\n\n\n  wine.amer wine.amer_int   n\n1         0             1  72\n2         1             2 108\n\n\nThere are more american wines.\n\n\n  judge.amer judge.amer_int   n\n1          0              1  80\n2          1              2 100\n\n\nThere are more american judges’ scores.\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n\n\n\n\n\nThese chains look healthy. They:\n\nAre stationary.\nMix well.\nAll chains converge.\n\n\n\n           mean         sd       5.5%     94.5%    n_eff     Rhat4\nf[1]  0.1964321 0.05921141 0.10264248 0.2932756 1929.489 1.0017991\nf[2]  0.1972408 0.05864249 0.10323917 0.2898972 1930.435 1.0015680\nwa[1] 0.2209778 0.06044152 0.12594084 0.3167652 1905.601 0.9995583\nwa[2] 0.1760510 0.06016986 0.08176468 0.2715712 1894.694 0.9998729\nja[1] 0.1751097 0.06005995 0.07914266 0.2699528 1761.885 1.0008554\nja[2] 0.2211422 0.06001029 0.12355457 0.3164412 1712.893 1.0004454\nsigma 0.2146800 0.01172746 0.19687684 0.2346205 2371.869 0.9998254\n\n\nLet’s visualize the differences between flights:\n\n\n\n\n\nOn expectation, there does not seem to be a difference between the red’s score and the white’s score.\n\n\n\n\n\nAmerican judges, on average, tend to give higher scores.\n\n\n\n\n\nNon-American wines tend to get higher scores on average.\n\nNow consider two way intercations among the three features\n\n\n\n\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 finished in 1.9 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 2.0 seconds.\nChain 3 finished in 2.0 seconds.\nChain 4 finished in 2.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 2.1 seconds.\n\n\n\n\n\nThese chains look healthy, i.e., they:\n\nThey are stationary\nThey mix well over the parameter space.\nDifferent chains converge to explore the same ridges of parameter space.\n\n\n\n             mean         sd        5.5%       94.5%    n_eff     Rhat4\na      0.35108556 0.04081151  0.28627780  0.41637738 1729.482 1.0000419\nw      0.15192587 0.04763595  0.07572766  0.23084453 1833.197 1.0002522\nj      0.26885958 0.04838545  0.19295458  0.34522238 1993.536 1.0001668\nf      0.17341672 0.05018627  0.09319243  0.25358759 2102.432 1.0013111\nwj    -0.16473790 0.05206359 -0.24481138 -0.08426990 2395.754 1.0006508\nwf    -0.06783489 0.05247324 -0.15139847  0.01594699 2271.637 1.0019762\njf    -0.13253690 0.05238871 -0.21463390 -0.04946512 2615.125 1.0007479\nsigma  0.22859152 0.01338763  0.20795889  0.24996585 2700.983 0.9999136\n\n\nThe Rhat is ok for all the parameters. However, now that we have interactions, it is not easy nor intuitive to analyze parameters on their own scale. We must compare them on the outcome scale. Let’s create predictions for all eight possible values\n\n\n# A tibble: 8 × 3\n     wa    ja flight\n  <dbl> <dbl>  <dbl>\n1     1     1      1\n2     1     1      0\n3     1     0      1\n4     1     0      0\n5     0     1      1\n6     0     1      0\n7     0     0      1\n8     0     0      0\n\n\nThen, we can calculate our expected predictions for each of the cases:\n\n\n\n\n\n\n\n\nWhites tend, on average, to be higher scored. Also, non American Judges tend to be harsher than their american counterparts, regardless of the origin of the wine. The worst rated wine, on average, are the red international wine.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-09-statistical-rethinking-week-10.html",
    "href": "posts/bayesian-statistics/2020-06-09-statistical-rethinking-week-10.html",
    "title": "Statistical Rethinking Week 10",
    "section": "",
    "text": "This is the final week of the best Statistics course out there. It showed the benefits of being ruthless with conditional probabilities: replace everything you don’t know with a distribution conditioned on what you do know. Bayes will do the rest. This holds for both measurement error and missing data."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-09-statistical-rethinking-week-10.html#solution",
    "href": "posts/bayesian-statistics/2020-06-09-statistical-rethinking-week-10.html#solution",
    "title": "Statistical Rethinking Week 10",
    "section": "Solution",
    "text": "Solution\nTo handle measurement error we will state that the observed values for both M and B come from a distribution centered around true unknown values but with known measurement error (the one that Richard just simulated).\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 finished in 2.0 seconds.\nChain 2 finished in 1.9 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 1.9 seconds.\nChain 4 finished in 1.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.9 seconds.\nTotal execution time: 2.2 seconds.\n\n\nLet’s check our posterior:\n\n\n           mean         sd      5.5%     94.5%    n_eff     Rhat4\na     0.4191687 0.05866025 0.3280848 0.5165294 1898.249 0.9985912\nb     0.7905614 0.01447971 0.7676140 0.8135265 1925.384 0.9988059\nsigma 0.2638313 0.01772510 0.2367339 0.2921449 2423.656 0.9993535\n\n\nOur inferences have hardly changed now that we have taken into account the measurement error.\n\n\n                                WAIC           SE        dWAIC          dSE\nmodel_no_measurement   -8.694090e+02 3.775739e+01 0.000000e+00           NA\nmodel_with_measurement  1.751128e+13 8.911135e+12 1.751128e+13 8.935717e+12\n                              pWAIC weight\nmodel_no_measurement   2.620076e+00      1\nmodel_with_measurement 8.755447e+12      0\n\n\nLet’s plot the predicted relationship:\n\n\n\n\n\n\n\n\nThe regression line is mainly informed by the small animals because they are more numerous and have smaller measurement errors (by assumption). Therefore, the terrible fit for the largest animals in the sample: there’s few of them and they have the largest measurement error."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-04-28-statistical-rethinking-week-2.html",
    "href": "posts/bayesian-statistics/2020-04-28-statistical-rethinking-week-2.html",
    "title": "Statistical Rethinking: Week 2",
    "section": "",
    "text": "library(rethinking)\nlibrary(tidyverse)\nlibrary(ggridges)\nextrafont::loadfonts(device=\"win\") \nset.seed(24)"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-04-28-statistical-rethinking-week-2.html#homework",
    "href": "posts/bayesian-statistics/2020-04-28-statistical-rethinking-week-2.html#homework",
    "title": "Statistical Rethinking: Week 2",
    "section": "Homework",
    "text": "Homework\n\nThe weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% compatibility intervals for each of these individuals.\n\n\ntibble(individual = seq(1, 5), weight = c(45, 40, 65, 31, 53),\n       expected_height = NA, left_interval = NA, right_interval = NA) -> to_predict\nto_predict\n\n# A tibble: 5 × 5\n  individual weight expected_height left_interval right_interval\n       <int>  <dbl> <lgl>           <lgl>         <lgl>         \n1          1     45 NA              NA            NA            \n2          2     40 NA              NA            NA            \n3          3     65 NA              NA            NA            \n4          4     31 NA              NA            NA            \n5          5     53 NA              NA            NA            \n\n\n\nThe model\nWe will fit a linear regression between the weight and the height. Thus, we will predict the aforementioned individuals. The model will have a normal likelihood:\n\\[ height_i \\sim Normal(\\mu_i, \\sigma) \\] \\[ \\mu_i = \\alpha + \\beta (weight_i - \\bar{weight}) \\] And the following priors:\n\\[ \\alpha ~ Normal(178, 20) \\]\n\\[ \\beta \\sim LogNormal(0, 1) \\]\n\\[ \\sigma \\sim Uniform(0, 50) \\]\nWhich translates in code thus:\n\n# only keep the adults in the sample\ndata <- Howell1 %>% filter(age >= 18)\n\n# mean to later on center weight at 0\nxbar <- mean(data$weight)\n\n# fit the model\nmodel_linear <- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b*(weight - xbar),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = data\n)\n\nThe model fitted is:\n\nprecis(model_linear)\n\n            mean         sd        5.5%      94.5%\na     154.601367 0.27030759 154.1693633 155.033371\nb       0.903281 0.04192362   0.8362789   0.970283\nsigma   5.071880 0.19115465   4.7663775   5.377382\n\n\nThat is, a multivatiate normal with the above means and the following var-cov matrix:\n\nvcov(model_linear)\n\n                  a             b         sigma\na      7.306619e-02 -4.243897e-08  6.157797e-05\nb     -4.243897e-08  1.757590e-03 -2.518310e-05\nsigma  6.157797e-05 -2.518310e-05  3.654010e-02\n\n\nNow, let’s compute the expected height, according to our model, for the aforementioned individuals:\n\nmu <- sim(model_linear, data = to_predict, n = 10000)\n\nNow, we have samples from the posterior with all the required parameters (\\(\\alpha, \\beta, \\sigma\\)) to predict our model’s expected height for each of the individuals. Let’s fill the table:\n\nto_predict$expected_height <- apply(mu, 2, mean)\nwhole_interval <- apply(mu, 2, PI, prob = 0.89)\nto_predict$left_interval <- whole_interval[1,]\nto_predict$right_interval <- whole_interval[2,]\nround(to_predict, 2)\n\n# A tibble: 5 × 5\n  individual weight expected_height left_interval right_interval\n       <dbl>  <dbl>           <dbl>         <dbl>          <dbl>\n1          1     45            155.          147.           163.\n2          2     40            150.          142.           158.\n3          3     65            173.          164.           181.\n4          4     31            142.          134.           150.\n5          5     53            162.          154.           170.\n\n\nLet’s visualize our predictions\n\ncomputed_with_samples <- data.frame(mu, index = seq(1, length(mu))) \ncolnames(computed_with_samples) <- c(45, 40, 65, 31, 53, \"index\")\n\ncomputed_with_samples%>% \n  pivot_longer(-index) %>% \n  rename(\"individual\" = name,\n         \"prediction\" = value) %>% \n  ggplot(aes(y = individual, x = prediction)) +\n    geom_density_ridges(scale = 0.8, fill = \"dodgerblue4\", alpha = 0.6) +\n  scale_y_discrete(expand = c(0, 0)) +     # will generally have to set the `expand` option\n  scale_x_continuous(expand = c(0, 0)) +   # for both axes to remove unneeded padding\n  coord_cartesian(clip = \"off\") + # to avoid clipping of the very top of the top ridgeline\n  hrbrthemes::theme_ipsum_rc(grid = \"x\") +\n  labs(title = \"Predictions averaged over posterior\",\n       subtitle = \"Not suprisingly, we predict higher heights for heavier people\",\n       x = \"Predicted height\",\n       y = \"Weight\")\n\n\n\nggsave(\"ridges.png\")\n\n\nModel the relationship between height and the natural logarithm of weight using the entire data.\n\n\ndata <- Howell1\n\nThus, the model we will be working with will be:\nWe will fit a linear regression between the weight and the height. Thus, we will predict the aforementioned individuals. The model will have a normal likelihood:\n\\[ height_i \\sim Normal(\\mu_i, \\sigma) \\]\n\\[ \\mu_i = \\alpha + \\beta (log(weight_i) - log(\\bar{weight_i})) \\] And the following priors:\n\\[ \\alpha ~ Normal(178, 20) \\]\n\\[ \\beta \\sim LogNormal(0, 1) \\]\n\\[ \\sigma \\sim Uniform(0, 50) \\]\n\ndata %>% \n  mutate(log_weight = log(weight)) -> data_with_log\n\nx_bar <- mean(data_with_log$log_weight)\n\nlog_model <- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b*(log_weight ),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = data_with_log\n)\n\nLet’s check the mean of the parameters of the posterior:\n\nprecis(log_model)\n\n            mean        sd       5.5%      94.5%\na     -22.874419 1.3343063 -25.006898 -20.741940\nb      46.817810 0.3823284  46.206776  47.428845\nsigma   5.137147 0.1558892   4.888006   5.386288\n\n\nAnd the var-cov:\n\nvcov(log_model)\n\n                 a            b        sigma\na      1.780373247 -0.503145565  0.008933974\nb     -0.503145565  0.146175019 -0.002528771\nsigma  0.008933974 -0.002528771  0.024301445\n\n\nNow, let’s plot our predictions for the range of weights that we have.\n\nweights <- log(1:max(data$weight))\n\n\npredictions_from_posterior <- sim(log_model, data.frame(log_weight = weights), 10000)\n\n\nmu <- apply(predictions_from_posterior, 2, mean)\ninterval <- apply(predictions_from_posterior, 2, PI, prob = 0.75)\nleft_interval <- interval[1,]\nright_interval <- interval[2,]\n\n\ntibble(weights = exp(weights),\n       mu,\n       left_interval,\n       right_interval) %>% \n  ggplot(aes(weights, mu)) +\n    geom_line() +\n    geom_ribbon(aes(ymin = left_interval, ymax = right_interval), alpha = 0.3) +\n  geom_point(data = data_with_log, aes( x = weight, y = height), alpha = 0.3,\n             color = \"dodgerblue4\")  +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Model predictions in original space\",\n       subtitle = \"Shaded area represents 75% Percentage Interval\",\n       y = \"height\")\n\n\n\n\n\nPlot the prior predictive distribution for the polynomial regression model of height\n\nSo, let’s suppose that we are planning to fit the following model to the data. First, we would work with standardize weights.\n\\[ height_i \\sim Normal(\\mu_i, \\sigma) \\]\n\\[ \\mu_i = \\alpha + \\beta_1 * weight_s + \\beta_2 weight_s^2 \\] And the following priors:\n\\[ \\alpha ~ Normal(178, 20) \\]\n\\[ \\beta_1 \\sim LogNormal(0, 1) \\]\n\\[ \\beta_2 \\sim dnorm(0, 1) \\]\n\\[ \\sigma \\sim Uniform(0, 50) \\]\nWhat predictions do the priors we set are implying? To find out, let’s sample from them:\n\nN <- 1000\na <- rnorm(N, 178, 20)\nb1 <- rlnorm(N, 0, 1)\nb2 <- rnorm(N, 0, 1)\n\nNow, let’s plot them:\n\nplot_priors <- function(N, a, b1, b2) {\n  weights <- seq(-2, 2, length.out = 50)\n  data.frame(simulation = seq(1, N), intercept = a, first = b1, second = b2) %>% \n    mutate(prediction = pmap(list(intercept, first, second), function(first, second, third) first[1] + weights*second + weights^2*third)) %>% \n    unnest() %>% \n    mutate(weight = rep(weights, N)) %>% \n    ggplot(aes(x = weight, y = prediction, group = simulation)) +\n      geom_line(alpha = 0.1) +\n      ylim(c(0, 250)) +\n      hrbrthemes::theme_ipsum_rc() +\n    labs(y = \"Predicted height\",\n         title = \"Implied predictions by prior\",\n         x = \"weight z-score\")\n}\nplot_priors(N, a, b1, b2)\n\n\n\n\nThe curves have hardly any bent at all, which is what we would like to see knowing the polynomial relationship between height and weight. Let’s try to put an uniform negative prior on the \\(b_2\\) coefficient:\n\nnegative_b2 <- runif(N, min = -10, max = 0)\n\n\nplot_priors(N, a, b1, negative_b2)\n\n\n\n\nHowever, the curves start to bend down to quickly. Let’s change the prior on \\(b_1\\) too:\n\nb1_larger <- rlnorm(N, 4, 0.5)\n\n\nplot_priors(N, a, b1_larger, negative_b2) +\n  ylim(c(0, 300)) +\n  labs(subtitle = \"Bend the implied predictions\")\n\n\n\nggsave(\"prior.png\")\n\nWe bent them! However, we created a prior that implies impossible predictions. This is very hard to set as the parameters should be set jointly. However, we are not considering any correlation between the samples."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-22-bayesian-data-analysis-week-2.html",
    "href": "posts/bayesian-statistics/2020-06-22-bayesian-data-analysis-week-2.html",
    "title": "Bayesian Data Analysis: Week 2",
    "section": "",
    "text": "Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\nIn this series of blogposts, I’ll go over the homeworks that Aki has kindly made available online. Hopefully, I’ll go over the whole book and learn a thing or two. If you are interested in also learning bayesian statistics, I wholeheartedly recommend Statistical Rethinking first: the best and most interesting statistics course ever created."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-22-bayesian-data-analysis-week-2.html#week-2",
    "href": "posts/bayesian-statistics/2020-06-22-bayesian-data-analysis-week-2.html#week-2",
    "title": "Bayesian Data Analysis: Week 2",
    "section": "Week 2",
    "text": "Week 2\nAlgae status is monitored in 274 sites at Finnish lakes and rivers. The observations for the 2008 algae status at each site are presented in file algae. The data can also be accessed from the aaltobda ( R ) package as follows:\nSo that you can test the correctness of your code implementations, we provide some results for the following test data. It is also possible to check the functions you need to implement with markmyassignment. ( > ) algae_test ( <-c(0,1,1,0,0,0) ) Note! This data is only for the tests, you need to change to the full data algae when reporting your results.\nLet ( ) be the probability of a monitoring site having detectable blue-green algae levels and ( y ) the observations in algae. Use a binomial likelihood for the observations ( y ) and a Beta (2,10) prior for ( ) to formulate a Bayesian model. Here it is not necessary to derive the posterior distribution for ( ) as it has already been done in the book. Also, it is not necessary to write out the distributions; it is sufficient to use label-parameter format, e.g. Beta ( (, ) ) Your task is to formulate a Bayesian model and answer questions based on it:\n\nQuestion A\n\nformulate (1) model likelihood ( p(y | ),(2) ) the prior ( p(), ) and (3) the resulting posterior ( p(| y) . ) Report the posterior in the format Beta ( (, ), ) where you replace ’s with the correct numerical values.\n\n\\[\ny \\sim Binomial(n, \\pi) \\\\\n\\pi \\sim Beta(2, 10)\n\\]\nGiven that the Beta distribution is the conjugate of the binomial, the posterior is also a binomial. In particular:\n\\[\n\\pi | y \\sim Beta(2 + y, 10 + n - y)\n\\] We must then, find both \\(n\\) (number of trials) and \\(y\\) (number of successes):\n\ndata(\"algae\")\nglue::glue(\"n = {length(algae)}\")\n\nn = 274\n\nglue::glue(\"y = {sum(algae == 1)}\")\n\ny = 44\n\n\nTherefore:\n\\[\n\\pi | y \\sim Beta(46, 240)\n\\] ### Question B\n\nWhat can you say about the value of the unknown ( ) according to the observations and your prior knowledge? Summarize your results with a point estimate (i.e. ( E(| y) ) ) and a ( 90 % ) posterior interval. Note! Posterior intervals are also called credible intervals and are different from confidence intervals.\n\nThe posterior mean:\n\\[\nE(\\theta | y) = 46 / 286 = 0.16\n\\] For a posterior interval:\n\nconf.level <- 0.9\nupper <- qbeta(conf.level + (1-conf.level)/2, 46, 240)\nlower <- qbeta((1-conf.level)/2, 46, 240)\nglue::glue(\"The 90% posterior interval is {round(lower, 2)}, {round(upper, 2)}\")\n\nThe 90% posterior interval is 0.13, 0.2\n\n\n\n\nQuestion C\n\nWhat is the probability that the proportion of monitoring sites with detectable algae levels ( ) is smaller than ( _{0}=0.2 ) that is known from historical records?\n\nGiven our posterior:\n\nprob <- pbeta(0.2, 46, 240)\nglue::glue(\"The probability that the proportion of monitoring sites with detectable algae levels is smaller than 0.2 is {round(prob, 2)}\")\n\nThe probability that the proportion of monitoring sites with detectable algae levels is smaller than 0.2 is 0.96\n\n\n\n\nQuestion d\n\nWhat assumptions are required in order to use this kind of a model with this type of data?\n\nWe are assuming that the presence of detectable algae levels are conditionally independent give \\(\\pi\\), with the probability of presence equal to \\(\\pi\\) for all cases.\n\n\nQuestion e\n\nMake prior sensitivity analysis by testing a couple of different reasonable priors and plot the different posteriors. Summarize the results by one or two sentences.\n\n\nn <- 274\ny = 44\n\nget_posterior <- function(prior_alpha, prior_beta) {\n  alpha <- prior_alpha + y\n  beta <- prior_beta + n - y\n  rbeta(10000, alpha, beta)\n}\nset.seed(25)\nlabels_alpha <- unlist(map(seq(2, 50, length.out = 4), ~ glue::glue(\"alpha = {.}\")))\nlabels_beta <- unlist(map(seq(8, 200, length.out = 4), ~ glue::glue(\"beta = {.}\")))\nlabels_prior_observations <- unlist(map(seq(2, 50, length.out = 4) + seq(8, 200, length.out = 4) - 2,\n                                    ~ glue::glue(\"prior obs = {.}\")))\n\ntibble(alpha = seq(2, 50, length.out = 4),\n         beta = seq(8, 250, length.out = 4)) %>% \n  mutate(samples_posterior = map2(alpha, beta, ~ get_posterior(.x, .y))) %>% \n  unnest(samples_posterior) %>% \n  mutate(prior_observations = alpha + beta - 2,\n         prior_observations = factor(prior_observations, labels = labels_prior_observations),\n        alpha = factor(alpha, labels = labels_alpha),\n         beta = factor(beta, labels = labels_beta)) %>% \n  ggplot(aes(samples_posterior)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.5) +\n  geom_vline(aes(xintercept = 0.16), linetype = 2, color = \"red\") +\n  facet_wrap(beta~alpha + prior_observations) +\n  theme(axis.title.y = element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank()) +\n  labs(title = \"Prior Sensitivity Analysis\",\n       subtitle = \"Original posterior mean shown in red. Subsequent priors are increasingly concentrated around 0.2,\n      yet the posterior hardly moves to 0.2\")\n\n\n\n\nThe posterior is very robust. Even after drastically increasing the number of prior observations, the prior’s mean (0.2) is still a very unlikely event in the eyes of the posterior."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-11-statistical-rethinking-week-4.html",
    "href": "posts/bayesian-statistics/2020-05-11-statistical-rethinking-week-4.html",
    "title": "Statistical Rethinking: Week 4",
    "section": "",
    "text": "This week was a marathon of content. Richard introduced beautifully the trade-off between overfitting and underfitting and prescribed two complimentary methods to help us navigate this trade-off:\n\nRegularizing priors\nInformation criteria and Cross-Validation estimates of the risk of overfitting.\n\nRegularizing priors reduces the risk of overfitting of any model by introducing skepticisim into the priors. Whereas information criteria and Cross-Validation help us to estimate whether we have overfitted or not.\n\n\n\n\n\n\n\n\nFirst, compute the entropy of each island’s bird distribution:\n\n\n\n\n\n\nIsland 1, having an uniform probability of each bird, encodes the highest amount of uncertainity of all the three islands. Both island 2 and 3, having 0.8 and 0.7, respectively, in only one type of bird, enconde much lower levels of uncertainty in their distributions.\n\nSecond, use each island’s bird distribution to predict the other two.\n\n\n\n\n\n\nTwo facts arise:\n\nPredicting “Island 1” is relatively easy. It already encodes so much uncertainty that the extra uncertainty induced by using any other distribution is relatively low. Nevertheless, it is better to use “Island 3”, which encodes the highest uncertainty.\nTo predict either “Island 2” or “Island 3”, it is markedly better to use the distribution with the highest entropy: “Island 1”. It encodes so much uncertainty in itself, that, when we see the true distribution, it is hardly surprised: it expects almost anything.\n\n\n\n\n\n\nRecall the marriage age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or LOO, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?\n\nLet’s load the data:\n\n\n\nBefore fitting the model, let’s remember our assumed DAG.\n\n\n\n\n\nThus, Age and Happiness are independent. However, if we condition on Marriage, we open a collider which will make our statistical model to identify and information flow between age and happiness.\n\n\n          WAIC       SE    dWAIC      dSE    pWAIC     weight\nm6.9  2713.797 37.55914   0.0000       NA 3.647937 1.0000e+00\nm6.10 3102.072 27.75233 388.2746 35.46971 2.423316 4.8667e-85\n\n\nConsidering our DAG, the model that correctly identifies the causal relationship between happiness and age is m6.10. Whereas m6.9 opens a collider and thus its coefficient estimates are confounded.\nNevertheless, by examing the difference in WAIC (the smaller the better), we estimate that the model that will predict happiness better out of sample is m6.9: the confounded model. This only highlights the difference between the different goals of prediction and causal inference. Why? Because the information that flows once we open the collider, although causally incorrectly attributed to age, is valuable information that can be used to predict happiness.\n\nReconsider the urban fox analysis from last week’s homework. Use WAIC or LOO based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:\n\n\navgfood + groupsize + area\navgfood + groupsize\ngroupsize + area\navgfood\narea\n\nLet’s draw the data and remember our DAG:\n\n\n\n\n\n\navgfood + groupsize + area\n\nGiven our DAG, using area and avgfood simultaneously blocks the pipe that goes from area to weight. However, it correctly estimates the effect of groupsize on weight.\n\n\n\n\navgfood + groupsize\n\nGiven this model, we will estimate the effect of groupsize on weight correctly.\n\n\n\n\ngroupsize + area\n\nGiven our DAG, controlling by groupsize and including avgfood should be equivalent as including avgfood.\n\n\n\n\navgfood\n\nGiven our DAG, this model estimates correctly the total effect of avgfood on weight.\n\n\n\n\narea\n\n\n\n\nThis model correctly estimates the total effect of area on weight.\nNow, it’s time to compare the out-of-sample prediction accuracy of the different models:\n\n\n            WAIC       SE    dWAIC      dSE    pWAIC      weight\nmodel_1 322.8945 16.27362  0.00000       NA 4.655545 0.457482115\nmodel_2 323.8989 16.08777  1.00444 3.570389 3.730356 0.276861647\nmodel_3 324.0148 15.85169  1.12030 2.962192 3.790548 0.261278688\nmodel_4 333.4444 13.78855 10.54991 7.181399 2.426279 0.002341478\nmodel_5 333.7239 13.79447 10.82943 7.230715 2.650636 0.002036072\n\n\nGiven our DAG, if we include groupsize, the information flowing from either area or avgfood should be the same in the 3 models. Thus, it makes sense that we get equivalent predictions with either way.\nFinally, given that the total effect of avgfood and area are the same, the information flows from either of those in the models 4 and 5 should also be the same. Thus, we get equivalent predictions for both models."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html",
    "href": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html",
    "title": "BDA Week 9: Large Sample Theory for the Posterior",
    "section": "",
    "text": "As Richard McElreath says in his fantastic Statistics course, Frequentist statistics is more a framework to evaluate estimators than a framework for deriving them. Therefore, we can use frequentist tools to evaluate the posterior. In particular, what happens to the posterior as more and more data arrive from the same sampling distribution?\nIn this blogpost, I’ll follow chapter 4 of Bayesian Data Analysis and the material in week 9 of Aki Vehtari’s course to study the Posterior Distribution under the framework of Large Sample Theory."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#asymptotic-normality-and-consistency",
    "href": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#asymptotic-normality-and-consistency",
    "title": "BDA Week 9: Large Sample Theory for the Posterior",
    "section": "Asymptotic Normality and Consistency",
    "text": "Asymptotic Normality and Consistency\nSuppose then that the true data distribution is \\(f(y)\\). If the true data distribution is included in the parametric family, then, for some \\(\\theta_0\\) we have \\(f(y) = \\pi(y |\\theta_0)\\). Asymptotic consistency then, guarantees that as the sample size increases, our posterior distribution will converge to a point mass at the true parameter value \\(\\theta_0\\). Both the median, the mean and the mode of the posterior are consistent estimators for \\(\\theta_0\\).\nIndeed, asymptotic normality, in its turn, guarantees that the limiting distribution of the posterior can be approximated with a Gaussian centered at the mode \\(\\widehat \\theta\\) of the posterior:\n\\[\n\\pi(\\theta | y) \\approx N(\\widehat \\theta, [\\frac{d^2}{d\\theta^2} \\log \\pi(\\theta | y)]_{\\theta = \\widehat \\theta}^{-1} )\n\\]\nNaturally, we can then approximate the posterior by finding the mode by finding the maximum of the posterior and then estimating the curvature at that point. As more and more data arrives, the approximation will be more and more precise."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#big-data-and-the-normal-approximation",
    "href": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#big-data-and-the-normal-approximation",
    "title": "BDA Week 9: Large Sample Theory for the Posterior",
    "section": "Big Data and the Normal Approximation",
    "text": "Big Data and the Normal Approximation\nIf more and more data makes the normal approximation better and better, does that mean that the era of Big Data will usher a new era where the posterior will be easily and reliably approximated with a Gaussian?\nNot quite: as we have more and more data, so we have more and more questions that we can possibly ask. We can then create more complex models with more parameters to try to answer these more complicated questions. In this scenario, as more and more data arrives, the posterior distribution will not converge to the Gaussian approximation in the expanding parameter space that reflects the increasing complexity of our model. The reason? The curse of dimensionality\nThe more dimensions we have, ceteris paribus, due to the concentration of measure, the mode and the area around the mode will be farther and farther way from the typical set. Thus, the Gaussian approximation that concentrates most of the mass around the mode will be a poor substitute of the typical set and thus a poor approximation for the posterior distribution."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#normal-approximation-for-the-marginals",
    "href": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#normal-approximation-for-the-marginals",
    "title": "BDA Week 9: Large Sample Theory for the Posterior",
    "section": "Normal Approximation for the Marginals",
    "text": "Normal Approximation for the Marginals\nNevertheless, even if we cannot approximate the joint posterior distribution with the Gaussian approximation, the normal approximation is not that faulty if we instead focus on the marginal posterior. The reason? Determining the marginal distribution of a component of \\(\\theta\\) is equivalent to averaging over all the other components of \\(\\theta\\), and averaging a family of distributions generally brings them closer to normality, by the same logic that underlies the central limit theorem.\nThis fact explains why we see so many approximately Gaussian marginals in practice and why can sometimes summarize them with a point estimate and a standard error."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#unbiasedness-and-hierarchical-models",
    "href": "posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html#unbiasedness-and-hierarchical-models",
    "title": "BDA Week 9: Large Sample Theory for the Posterior",
    "section": "Unbiasedness and Hierarchical Models",
    "text": "Unbiasedness and Hierarchical Models\nFrequentist methods place great emphasis on unbiasedness: \\(E[\\widehat \\theta] = \\theta_0\\). However, when parameters are related and partial knowledge of some of the parameter is clearly relevant to the estimation of others, the emphasis on unbiasedness is clearly misplaced: it leads to a naive point in the bias-variance trade-off.\nIndeed, if we have a familiy of parameters \\(\\theta_j\\) that are related, a Bayesian would fit a hierarchical model by positing a common distribution from which these parameters are sampled. This procedure leads to pooling of the information and thus to shrink the individual parameters \\(\\theta_j\\) toward each other; thereby biasing the individual estimates \\(\\theta_j\\) but reducing their variance.\nThus, this highlights the problem that it is often not possible to estimate several parameters at once in an even approximately unbiased manner: unbiased \\(\\theta_j\\) leads to ignoring relevant information about their common distribution, which in turn leads to a biased estimate of the variance of the \\(\\theta_j\\)."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "",
    "text": "In the last couple of weeks, We’ve seen how the most difficult part of Bayesian Statistics is computing the posterior distribution. In particular, in the last week, we’ve studied the Metropolis Algorithm. In this blogpost, I’ll study why Metropolis does not scale well enough to high dimensions and give an intuitive explanation of our best alternative: Hamiltonian Monte Carlo (HMC).\nThis blogpost is my personal digestion of the excellent content that Michael Betancourt has put out there to explain HMC. I particularly enjoyed his YouTube lecture and his blogpost about Probabilistic computation."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#metropolis-algorithm",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#metropolis-algorithm",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\nAs we’ve seen, the Metropolis algorithm is just a random walk through parameter space where the proposed jumps are corrected by a comparison of posterior densities to determine whether to move or not. That is, we can see the Metropolis algorithm as:\n\nA stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density until it finds the mode and then only sometimes accepting steps that decrease the posterior density.\n\nThus, as long as the algorithm has run long enough to find the posterior mode, and the area around the mode is a good representation of the overall posterior, the Metropolis Algorithm will work. Sadly, this assumption only holds for lower dimensions"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#an-expectation-in-low-dimensions",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#an-expectation-in-low-dimensions",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "An Expectation in low dimensions",
    "text": "An Expectation in low dimensions\nFundamentally, we sample from the posterior in order to compute Monte-Carlo estimators. That is, we want to approximate expectations:\n\\[\n\\mathbb{E}_{\\pi}[f]=\\int_{Q} \\mathrm{d} q \\pi(q) f(q)\n\\]\nWith the Metropolis algorithm, we are saying that we can approximate these expectation by computing at values near the mode of \\(\\pi(q)\\) and occasionally exploring lower density parts. Which is a pretty good approximation of most unimodal low dimensional probability distributions. However, this entirely ignores the contribution of \\(\\mathrm{d} q\\), the differential volume, to the integral."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#an-expectation-in-high-dimensions",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#an-expectation-in-high-dimensions",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "An Expectation in high dimensions",
    "text": "An Expectation in high dimensions\nHowever, as the number of dimensions grows, so does the importance of \\(\\mathrm{d} q\\), the differential volume, to the integral. Therefore, in higher dimensions, ignoring the differential volume when computing the expectation guarantees biased samples. Why does this happen? This is due to a phenomenon called concentration of measure.\nIntuitively, an integral is nothing more than a weighted sum of parts of the distribution with height equal to function heights. However, as dimensions grows, the contribution of any single part of the distribution to the integral decreases very rapidly. Why? Because, the relative differential volume of this single part of the distribution decreases as we increase the number of dimensions. Michael’s Betancourt box-experiment is very illuminating.\n\nCorner Madness\n\nLet’s begin with a simple example - a square box representing the ambient space. In each dimension we will partition the box into three smaller boxes, with the central box corresponding to the neighborhood around the mode and the outer two side boxes corresponding to the rest of the space.\n\nIn one dimension, the central box encapsulates one third of the total volume. In two dimensions, there are nine boxes, thus the central box represents only 1/9 of the volume. In three dimensions, we must fill the corners of the corners with more boxes, thus creating 27 boxes. The central box now only represents 1/27 of the total volume. We can keep increasing dimensions with the aid of Monte-Carlo samples.\n\nThe relative importance of the central box is its probability with respect to a uniform distribution over the enclosing box, which we can estimate by sampling from that uniform distribution and computing the Monte Carlo estimator of the expectation value of the corresponding inclusion function\n\n\\[\nP(CentralBox) \\approx \\dfrac{N_{centralBox}}{N}\n\\]\n\nN <- 10000\nDs <- 1:10\nprob_means <- 0 * Ds\n\nfor (D in Ds) {\n  is_central_samples <- rep(1, N)\n  for (n in 1:N) {\n    # Sample a new point one dimension at a time\n    for (d in 1:D) {\n      q_d <- runif(1, -3, 3)\n      \n    # If the sampled component is not contained \n    # within the central interval then set the \n    # inclusion function to zero\n      if (-1 > q_d || q_d > 1)\n        is_central_samples[n] <- 0\n    }\n  }\n  # Estimate the relative volume as a probability\n  prob_means[D] <- mean(is_central_samples)\n}\n\ndata.frame(dimensions = Ds, prob_central_box = prob_means) %>% \n  ggplot(aes(dimensions, prob_central_box)) +\n  geom_step(color = \"dodgerblue4\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_x_continuous(breaks = 1:10) +\n  labs(y = \"Probability of central box\",\n       subtitle = \"As dimensions grow, volume of a single box decreases rapidly\",\n       title = \"Central Box Volume in high dimensions\")\n\n\n\n\n\n\nAll that matters: the typical set\nThus, the volume of a single neighborhood of the distribution decreases very rapidly as the number of dimensions grow. In our expectation, thus, the mode of the density \\(\\pi(q)\\) represents a neighborhood of the distribution with less and less volume and thus less and less probability mass. Indeed, in higher dimensions, our earlier intuition of a a probability distribution as concentrating around the mode and quickly decreasing is terribly wrong.\nIndeed, the probability of mass of any neighborhood of the distribution really depends on the interplay between its volume and the density. Without both of them, the probability mass at that neighborhood the distribution vanishes. Indeed, Michael Betancourt’s has a great plot showing where the volume and the target density interact to create large probability mass:\n\n\nThe neighborhood immediately around the mode features large densities, but in more than a few dimensions the small volume of that neighborhood prevents it from having much contribution to any expectation. On the other hand, the complimentary neighborhood far away from the mode features a much larger volume, but the vanishing densities lead to similarly negligible contributions expectations. The only significant contributions come from the neighborhood between these two extremes known as the typical set\n\nTherefore, to calculate an expectation it suffices to sample from the typical set instead of the entire parameter space. The question, then, is what is exactly the shape of this typical set?\n\n\nA surface concentrating away from the mode\nAs the number of dimensions grows and grows, the increasing tension between probability density and volume yields a worse compromise, and those compromises are found at points further and further from the mode. This is known as concentration of measure. Thus:\n\nThe repulsion of the typical set away from the mode implies that the relative breadth of the typical set, how far its fuzziness diffuses, shrinks with increasing dimension. As we consider higher-dimensional spaces the typical set becomes ever more thin and gossamer, and all the harder to find and explore.\n\nThus, the typical set, as the number of dimensions grow, becomes a narrow ridge of probability that becomes very difficult to explore with guess and check algorithms like the Metropolis algorithm."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#metropolis-and-the-typical-set",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#metropolis-and-the-typical-set",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "Metropolis and the typical set",
    "text": "Metropolis and the typical set\nThe Metropolis Algorithm is not equipped to sample from the typical set. As we’ve seen, it’s a mode finding algorithm that explores the neighborhood around it. However, this neighborhood, as we’ve just seen, has barely any volume in high dimensions and thus have barely any probability mass. Indeed, given the narrowness of the typical set, almost all proposed jumps of the random walk will be outside the typical set and thus will be rejected.\nTherefore, the Metropolis Algorithm in high dimensions results in inaction: the Metropolis algorithm will end up without moving at all for a long time. Thus, being a waste of our computational resources.\nIn two dimensions, we can visualize a typical set as a thin donut: at any point within it, there’s only a narrow ridge of points next to it that also belong to the typical set. Any ad-hoc proposal distribution that guesses in which direction we should move will be biased to propose points outside the typical set that will be rejected once we compare the Metropolis acceptance probability.\nChi Feng has awesome interactive visualizations that represent exactly this conundrum:\n<<<<<<< HEAD  =======  >>>>>>> abe09e3cd313dfc12630c3063c969959af729a6a"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#crafting-a-proposal-distribution-for-the-typical-set",
    "href": "posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html#crafting-a-proposal-distribution-for-the-typical-set",
    "title": "BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo",
    "section": "Crafting a proposal distribution for the typical set",
    "text": "Crafting a proposal distribution for the typical set\nWe’ve identified that an intelligent proposal distribution will encode the geometry of the typical set. A natural way of encoding this geometry is with a vector field aligned with the typical set. Thus, instead of taking a random-walk through parameter space, we will simply follow the vector field for some time. Therefore:\n\nContinuing this process traces out a coherent trajectory through the typical set that efficiently moves us far away from the initial point to new, unexplored regions of the typical set as quickly as possible.\n\n<<<<<<< HEAD The question, then, is how to create these vector field? A first approximation is to exploit the differential structure of the posterior through the gradient. However, following the gradient pulls us away from the typical set and into the mode of the posterior. Here comes Physics to the rescue. This is an equivalent problem to the problem of placing a satellite in a stable orbit around a hypothetical planet. ======= The question, then, is how to create these vector field? A first approximation is to exploit the differential structure of the posterior thoruhg the gradient. However, following the gradient pulls us away from the typical set and into the mode of the posterior. Here comes Physics to the rescue. This is an equivalent problem to the problem of placing a satellite in a stable orbit around a hypothetical planet. >>>>>>> abe09e3cd313dfc12630c3063c969959af729a6a\nThe key to do it is to endow our walk through the typical set by with enough momentum (\\(p\\)) to counteract the gravitation attraction of the mode. Therefore:\n\nWe can twist the gradient vector field into a vector field aligned with the typical set if we expand our original probabilistic system with the introduction of auxiliary momentum parameters.\n\n\nAugmenting with the momenta\nWe augment our probabilistic system with the momenta \\(p\\) thus:\n\\[\n\\pi(q, p)=\\pi(p \\mid q) \\pi(q)\n\\]\n<<<<<<< HEAD Which allows us to ignore the momenta when necessary by marginalizing it. The joint density \\(\\pi(q, p)\\) defines a Hamiltonian\n\\[\nH(p, q) = - log \\pi(p,\\theta)\n\\]\nThus, given some point in the phase space \\((q_0, p_0)\\) that is in the typical set, the Hamiltonian defines how to generate a new sample such that we stay in the typical set."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-28-simulating-into-understanding-multilevel-models.html",
    "href": "posts/bayesian-statistics/2020-05-28-simulating-into-understanding-multilevel-models.html",
    "title": "Simulating into understanding Multilevel Models",
    "section": "",
    "text": "Pooling is the process and shrinkning is the result\n\nPooling and Shrinking are not easy concepts to understand. In the lectures, Richard, as always, does an excellent job of creating metaphors and examples to help us gain intuition around what Multilevel models do. Multilevel models are models mnesic models.\nImagine a cluster of observations: it can be different classrooms in a school. Pooling means using the information from other classrooms to inform our estimates for each classroom. A model with no pooling means that each classroom is the first classroom that we have ever seen, as other classrooms have no effect on our estimates. No pooling models are amnesic models.\nFinally, shrinking is the result of this pooling: our estimates for each classroom will be pulled towards the global mean across classrooms. But how do multilevel models do this?\n\n\nMultilevel models propose to model a family of parameters (the parameters for each classroom) as coming from a common statistical population parameters. For example, the family of varying intercepts for each classroom in the school. Then, as we learn each parameter for each classroom, we learn simultaneously the family of the parameters for all classrooms; both processes complement each other. Therefore, this distribution of the family of parameters will become an adaptive regularizer for our estimates: they will shrink the varying intercepts for each classroom to the estimated mean of the common distribution; the amount of shrinkage will be determined by the variation that we estimate for the distribution of the family of parameters. The more influenced parameters are going to be those that come from classrooms with small sample sizes.\nHowever, it is one thing to have some intuition and another one is to really understand something. When it comes to statistics, I am a big fan of simulation. Thankfully, Richard does precisely this in chapter 12. Let’s simulate a model to visualize both pooling and shrinking.\n\n\n\nWe simulate the number of students who passed some exam at different classrooms at one school. That is, each classroom has \\(S_i\\) students who passed the test, from a maximum of \\(N_i\\). The model then is the following:\n\\[ S_i \\sim Binomial(N_i, p_i) \\]\n\\[ logit(p_i) = \\alpha_{classroom_{[i]}} \\]\n\\[ \\alpha_j \\sim Normal(\\bar{\\alpha}, \\sigma)\\]\n\\[ \\bar{\\alpha} \\sim Normal(0, 1.5) \\]\n\\[ \\sigma \\sim Exponential(1) \\]\nThen, we posit a distribution for the average log-odds of passing the exam for each classroom: \\(\\alpha_j \\sim Normal(\\bar{\\alpha}, \\sigma)\\). That is, the prior for each intercept will be one distribution that we will simultaneously learn as we learn the individual parameters. Finally, we have hyper-priors: priors for the parameters of the distribution of intercepts (\\(\\bar{\\alpha}, \\sigma\\)).\n\n\n\nTo simulate this model, we will define the parameters of the distribution of intercepts. Then, for each classroom, we will simulate an average log-odds of passing the exam. Then, we will simulate the number of students at each classroom that passed the test.\nNotice that neither the hyper-priors nor the priors are part of our simulation. In Richard’s words:\n\nPriors are epistomology, not ontology.\n\nLet’s begin by setting the parameters of the population of intercepts:\n\na_bar <- 1.5\nsigma <- 1.5\nn_classrooms <- 60\n# students per classrom\nNi <- as.integer(rep(c(5, 10, 25, 35), each = 15))\n\nThen, we simulate the average log-odds of passing the exam for each of the classrooms\n\navg_lod_odds_per_classrom <- rnorm(n_classrooms, mean = a_bar, sd = sigma)\n\nThen, we have the following:\n\ndata_simulation <- data.frame(classroom = 1:n_classrooms, Ni = Ni, true_log_odds = avg_lod_odds_per_classrom)\nglimpse(data_simulation)\n\nRows: 60\nColumns: 3\n$ classroom     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Ni            <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10,…\n$ true_log_odds <dbl> 2.3245048, 0.2375944, 1.5494969, 2.2862246, -1.0914062, …\n\n\n\n\n\nPutting the logistic into the random binomial function, we can generate the number of students who passed the test for each classrom:\n\nRemember that the logistic is simply the inverse of the logit. Thus, by applying the logistic we go from the log-odds into the probability.\n\ndata_simulation %>% \n  mutate(number_passed_test = rbinom(n_classrooms, prob = logistic(true_log_odds), size = Ni)) -> data_simulation\nglimpse(data_simulation)\n\nRows: 60\nColumns: 4\n$ classroom          <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ Ni                 <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10…\n$ true_log_odds      <dbl> 2.3245048, 0.2375944, 1.5494969, 2.2862246, -1.0914…\n$ number_passed_test <int> 5, 5, 4, 5, 1, 4, 5, 5, 5, 1, 5, 5, 3, 5, 5, 8, 8, …\n\n\n\ndata_simulation %>% \n  ggplot(aes(classroom, number_passed_test, color = Ni)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  labs(title = \"Simulated students who passed the test per Classroom\",\n       color = \"Initial #\")\n\n\n\n\n\n\n\nPooling means using the information from other classrooms to inform our predictions of estimated probabilities of passing the exams at different classrooms. Therefore, no-pooling means treating each classroom as completely unrelated to others. That is, estimating that the variance of the population of parameters is infinite.\nTherefore, our estimate of the probability of passing the test at each classroom will just be the raw sample proportion at each classrom:\n\ndata_simulation %>% \n  mutate(estimated_probability_no_pooling = number_passed_test / Ni) -> data_simulation\nglimpse(data_simulation)\n\nRows: 60\nColumns: 5\n$ classroom                        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…\n$ Ni                               <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\n$ true_log_odds                    <dbl> 2.3245048, 0.2375944, 1.5494969, 2.28…\n$ number_passed_test               <int> 5, 5, 4, 5, 1, 4, 5, 5, 5, 1, 5, 5, 3…\n$ estimated_probability_no_pooling <dbl> 1.0, 1.0, 0.8, 1.0, 0.2, 0.8, 1.0, 1.…\n\n\n\n\n\nPartial pooling means to model explicitly the population of parameters. Then, with a mean and a standard deviation estimated, we can perform adaptive regularization, i.e., shrinkage to our predictions. To do so, we will fit a multilevel binomial model:\n\ndata_model <- list(Si = data_simulation$number_passed_test, Ni = data_simulation$Ni, classroom = data_simulation$classroom)\n\nmultilevel_model <- alist(\n  Si ~ dbinom(Ni, p),\n  logit(p) <- a_classroom[classroom], # each pond get its own average log odds of survival\n  a_classroom[classroom] ~ dnorm(a_bar, sigma),\n  a_bar ~ dnorm(0, 1.5),\n  sigma ~ dexp(1)\n)\n\nThen, we use HMC to sample from our posterior:\n\nmultilevel_fit <- ulam(multilevel_model, data = data_model, chains = 4, cores = 4)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.3 seconds.\n\n\nLet’s evaluate the validity of our Markov Chains:\n\ntraceplot_ulam(multilevel_fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe chains look healthy because:\n\nThey have good mixing across the parameter space.\nThey are stationary.\nDifferent chains converge to explore the same spaces.\n\nNow, let’s find out our estimated parameters:\n\nprecis(multilevel_fit, depth = 2) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n\n     Rhat4       \n Min.   :0.9982  \n 1st Qu.:0.9988  \n Median :0.9993  \n Mean   :0.9995  \n 3rd Qu.:0.9998  \n Max.   :1.0021  \n\n\nThe Rhat values look OK. That is, it seems that we sampled correctly from our posterior. Let’s use these samples from the posterior distribution to calculate our estimated log-odds of survival for each pond.\n\nposterior_samples <- extract.samples(multilevel_fit)\nglimpse(posterior_samples)\n\nList of 3\n $ a_classroom: num [1:2000, 1:60] 2.93 2.06 3.09 5.13 1.17 ...\n $ a_bar      : num [1:2000(1d)] 1.48 1.24 1.72 1.49 1.11 ...\n $ sigma      : num [1:2000(1d)] 1.62 1.8 1.42 2.04 1.45 ...\n - attr(*, \"source\")= chr \"ulam posterior: 2000 samples from object\"\n\n\nBefore we calculate our estimated log-odds, let’s check our estimates for the population of parameters from which each intercept comes:\n\ndata.frame(alpha_bar = posterior_samples$a_bar) %>% \n  ggplot(aes(alpha_bar)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  geom_vline(aes(xintercept = 1.5), linetype = 2, color = \"red\") +\n  labs(title = \"Posterior samples for population alpha\")\n\n\n\n\nIt seems that we’ve correctly caputred the mean of the population. Let’s check the standard deviation of the distribution:\n\ndata.frame(sigma = posterior_samples$sigma) %>% \n  ggplot(aes(sigma)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  geom_vline(aes(xintercept = 1.5), linetype = 2, color = \"red\") +\n  labs(title = \"Posterior samples for population s.d.\")\n\n\n\n\nOur estimates for the variation in the population could be better. Nevertheless, let’s check our estimated probability of survival for each pond:\n\nlogistic_own <- function(var) {\n  1/(1+exp(-var))\n}\n\nmatrix_estimated_probs <- logistic_own(posterior_samples$a_classroom)\n\nglimpse(matrix_estimated_probs)\n\n num [1:2000, 1:60] 0.949 0.886 0.956 0.994 0.763 ...\n\n\nWe have a matrix of 2000 rows (2000 simulations) and 60 columns (60 different ponds). Let’s take the average across samples. This will be our estimated probability for each classroom:\n\npartial_pooling_estimates <- apply(matrix_estimated_probs, 2, mean)\ndata.frame(estimated_probability_partial_pooling = partial_pooling_estimates) %>% \n  cbind(data_simulation) -> data_simulation\nglimpse(data_simulation)\n\nRows: 60\nColumns: 6\n$ estimated_probability_partial_pooling <dbl> 0.9116976, 0.9089778, 0.7913792,…\n$ classroom                             <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ Ni                                    <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ true_log_odds                         <dbl> 2.3245048, 0.2375944, 1.5494969,…\n$ number_passed_test                    <int> 5, 5, 4, 5, 1, 4, 5, 5, 5, 1, 5,…\n$ estimated_probability_no_pooling      <dbl> 1.0, 1.0, 0.8, 1.0, 0.2, 0.8, 1.…\n\n\nThen, we must convert our true log-odds into true probabilities:\n\ndata_simulation %>% \n  mutate(true_probabilities = inv_logit(true_log_odds)) -> data_simulation\nglimpse(data_simulation)\n\nRows: 60\nColumns: 7\n$ estimated_probability_partial_pooling <dbl> 0.9116976, 0.9089778, 0.7913792,…\n$ classroom                             <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1…\n$ Ni                                    <int> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ true_log_odds                         <dbl> 2.3245048, 0.2375944, 1.5494969,…\n$ number_passed_test                    <int> 5, 5, 4, 5, 1, 4, 5, 5, 5, 1, 5,…\n$ estimated_probability_no_pooling      <dbl> 1.0, 1.0, 0.8, 1.0, 0.2, 0.8, 1.…\n$ true_probabilities                    <dbl> 0.9108863, 0.5591207, 0.8248411,…\n\n\n\n\n\n\nRemember that pooling means sharing the information across classrooms. This is done by explicitly modeling the distribution of the average log odds of passing the exam across classrooms. That is, our estimated mean for the distribution of intercepts for each classroom will inform each of our predictions. Let’s calculate this estimated global mean across classrooms:\n\ndata.frame(alpha_bar = posterior_samples$a_bar) %>% \n  mutate(alpha_bar = inv_logit(alpha_bar)) %>% \n  summarise(mean(alpha_bar)) -> estimated_global_mean\nestimated_global_mean <- estimated_global_mean[1,1]\nglue::glue(\"The estimated global mean is: {round(estimated_global_mean, 2)}\")\n\nThe estimated global mean is: 0.81\n\n\nNow let’s plot how our classroom estimates relate to the estimated global mean:\n\ndata_simulation %>% \n  select(classroom, estimated_probability_partial_pooling, estimated_probability_no_pooling, Ni) %>% \n  pivot_longer(-c(classroom, Ni), names_to = \"method\", values_to = \"estimated_probability\") %>% \n  mutate(Ni = glue::glue(\"Sample size in classrooms: {Ni}\"),\n         Ni = factor(Ni, levels = c(\"Sample size in classrooms: 5\",\n                                    \"Sample size in classrooms: 10\",\n                                    \"Sample size in classrooms: 25\",\n                                    \"Sample size in classrooms: 35\"))) %>% \n  ggplot(aes(classroom, estimated_probability, color = method)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(aes(yintercept = estimated_global_mean), linetype = 2, color = \"red\") +\n  facet_wrap(~Ni, scales = \"free\") +\n  scale_color_viridis_d() +\n  scale_y_continuous(labels = scales::percent) +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Visualizing pooling and Shrinking in a Multilevel Model\",\n       subtitle = \"Global estimated mean informs predictions for each classroom. Estimates are shrunk toward the global estimated mean\",\n       caption = \"Global estimated mean shown in red.\")\n\n\n\n\nNow we can see that, with partial pooling, our estimates are informed by the estimated global mean. Therefore, we shrink whatever proportion we calculate for the specific classroom towards this overall mean. This can be seen by zooming in on the yellow points, the estimates from partial pooling, and noticing that they are always closer to the red line than the purple points (i.e., the sample classroom proportion). Notice that pooling results in more aggressive shrinkage for the classrooms where we have fewer data. As we will see, these classrooms’ predictions are exactly the ones that need to be shrunk the most.\n\n\n\nFinally, we can compare how well the different models did:\n\ndata_simulation %>% \n  mutate(no_pooling_error = abs(estimated_probability_no_pooling - true_probabilities),\n         partial_pooling_error = abs(estimated_probability_partial_pooling - true_probabilities)) %>% \n  select(classroom, no_pooling_error, partial_pooling_error, Ni) %>% \n  pivot_longer(-c(classroom, Ni), names_to = \"method\", values_to = \"error\") %>% \n  mutate(Ni = glue::glue(\"Sample size in classrooms: {Ni}\"),\n         Ni = factor(Ni, levels = c(\"Sample size in classrooms: 5\",\n                                    \"Sample size in classrooms: 10\",\n                                    \"Sample size in classrooms: 25\",\n                                    \"Sample size in classrooms: 35\"))) %>% \n  ggplot(aes(error, factor(classroom), color = method)) +\n    geom_point(alpha = 0.6) +\n    scale_color_viridis_d() +\n    facet_wrap(~Ni, scales = \"free_y\") +\n    hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n    theme(legend.position = \"bottom\") +\n    labs(title = \"Partial Pooling vs No-pooling: Benefits of shrinkage\",\n         subtitle = \"Partial Pooling shines with low sample sizes and outliers\",\n         y = \"classroom\")\n\n\n\n\nThis plot shows the prediction errors (comparing our estimated probability to the true probability) across classrooms. Therefore, lower values are better. Nota bene:\n\nPartial pooling results into shrinkage. This is most helpful for the classrooms where we have relatively fewer data (i.e., classrooms with sample size of 5 and 10). For these clasrooms, we complement the little data that we have with the information pooled from larger classrooms: that is, we shrink our estimates to the population mean that we’ve estimated. Whereas the model with no pooling just uses the information in the low sample ponds, resulting in overfitting that shows itself in the plot in the form of large prediction errors. The comparison between the two methods shows us how shrinkage helps us to reduce overfitting and thus predict better out of sample.\nThe amount of shrinkage depends on the amount of data available. When we have fewer data, we shrink a lot. When we have lots of data, we shrink a lot less. Therefore, we can see that, for the classrooms that have lots of data (i.e., sample size of 35), partial pooling results in an almost identical prediction as the method with no pooling.\nThe model with no pooling can sometimes beat the model with partial pooling. However, on average, the model with partial pooling performs much better."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html",
    "href": "posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html",
    "title": "Bayesian Data Analysis: Week 3 -> Fitting a Gaussian probability model",
    "section": "",
    "text": "Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\nInstead of going through the homeworks (due to the fear of ruining the fun for future students of Aki’s), I’ll go through some of the examples of the book as case studies. In this blogpost, I’ll (wrongly) estimate the speed of light from the measurements of Simon Newcomb’s 1882 experiments."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html#the-gaussian-probability-model",
    "href": "posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html#the-gaussian-probability-model",
    "title": "Bayesian Data Analysis: Week 3 -> Fitting a Gaussian probability model",
    "section": "The Gaussian Probability model",
    "text": "The Gaussian Probability model\nWhen fitting a Gaussian probability model, there are to parameters to estimate: \\(\\mu, \\sigma\\). Therefore, we arrive at a joint posterior distribution:\n\\[\ny | \\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2) \\\\\np(\\mu, \\sigma | y) \\propto p (y | \\mu, \\sigma) p(\\mu, \\sigma)\n\\] In this case, \\(\\sigma\\) is a nuisance parameter: we are only really interested in knowing \\(\\mu\\). The following we assume the non-informative prior:\n\\[\np(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1}\n\\]\n\nPosterior marginal of sigma\nWe can show that the marginal posterior distribution for \\(\\sigma\\) is:\n\\[\n\\sigma^2 | y \\sim Inv -\\chi^2 (n - 1, s^2)\n\\] Where \\(s^2\\) is the sample variance of the \\(y_i\\)’s.\n\n\nMarginal Conditional posterior for mu\nThen:\n\\[\n\\mu | \\sigma^2, y \\sim N(\\bar y, \\sigma^2 / n)\n\\] > The posterior distribution of \\(\\mu\\) can be regarded as a mixture of normal distributions, mixed over the scaled inverse \\(\\chi^2\\) distribution for the variance, \\(\\sigma^2\\).\n\n\nMarginal posterior for mu\n\\[\n\\dfrac{\\mu  - \\bar y}{s/\\sqrt{n}} | y \\sim t_{n-1}\n\\]\nWhich has a nice correspondence with the distribution used for the mean estimator in frequentist statistics in the case of small samples.\n## Fitting it with data\nThen, let’s read the measurements from Newcomb’s experiment:\n\nread_delim(\"http://www.stat.columbia.edu/~gelman/book/data/light.asc\", \n                     delim = \" \", skip = 3,col_names = F) %>% \n  pivot_longer(everything()) %>% \n  select(value) %>% \n  drop_na()-> measurements\n\nmeasurements %>% \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Newcomb's measurements for the speed of light\",\n       subtitle = \"There are clearly problems with the data\",\n       x = \"measurement\")\n\n\n\n\nThere are clearly some problems with the data. Garbage in, garbage out.\n\n\nMarginal for sigma\nWe therefore can derive the marginal distribution for \\(\\sigma\\):\n\nmeasurements%>% \n  skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n66\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvalue\n0\n1\n26.21\n10.75\n-44\n24\n27\n30.75\n40\n▁▁▁▂▇\n\n\n\n\n\nThus:\n\\[\n\\sigma^2 | y \\sim Inv -\\chi^2 (65, 10.7^2)\n\\]\nWhereas for mu we have :\n\\[\n\\dfrac{\\mu  - 26.2}{10.7/\\sqrt{66}} | y \\sim t_{n-1}\n\\] In code:\n\nrsinvchisq <- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)\n\nsigma_draw = rinvchisq(1000, 65, 10.7^2)\nmu_draw = rtnew(10000, df = 65, mean = 26.2, scale = 10.7/sqrt(66) )\n\nThe 95% credible interval for our \\(\\mu\\) is thus:\n\ninterval <- rethinking::PI(mu_draw, prob = 0.95)\nlower <- interval[[1]]\nupper <- interval[[2]]\nglue::glue(\"The 95% credible interval is thus: {round(lower, 1)}, {round(upper, 1)}\")\n\nThe 95% credible interval is thus: 23.6, 28.8\n\n\nA visualization may help:\n\ndata.frame(mu_draw) %>% \n  ggplot(aes(mu_draw)) +\n  geom_histogram(binwidth = 0.1, color = \"black\", fill = \"dodgerblue4\", alpha = 0.5) +\n  geom_vline(aes(xintercept = lower), color = \"red\", linetype = 2) +\n  geom_vline(aes(xintercept = upper), color = \"red\", linetype = 2) +\n  geom_vline(aes(xintercept = 33), color = \"black\", linetype = 1) +\n  labs(title = \"Speed of light: Posterior draws for mu\",\n       subtitle = \"Contemporary estimates for the speed of light in the experiment is 33. Garbage in, garbage...\")"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html",
    "href": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html",
    "title": "Bayesian Data Analysis: Week 3-> Exercises",
    "section": "",
    "text": "Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\nIn this blogpost, I’ll go over a couple of the selected exercises for week 3: exercise number 2 and exercise number 3."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#exercise-2",
    "href": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#exercise-2",
    "title": "Bayesian Data Analysis: Week 3-> Exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nComparison of two multinomial observations: on September ( 25,1988, ) the evening of a presidential campaign debate, ABC News conducted a survey of registered voters in the United States; 639 persons were polled before the debate, and 639 different persons were polled after. Assume the surveys are independent simple random samples from the population of registered voters. Model the data with two different multinomial distributions. For ( t=1,2, ) let ( _{t} ) be the proportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey ( t . ) Plot a histogram of the posterior density for ( {2}-{1} . ) What is the posterior probability that there was a shift toward Bush?\nThe results of the surveys are thus:\n\\[\n\\begin{array}{c|ccc|c}\n\\text { Survey } & \\text { Bush } & \\text { Dukakis } & \\text { No opinion/other } & \\text { Total } \\\\\n\\hline \\text { pre-debate } & 294 & 307 & 38 & 639 \\\\\n\\text { post-debate } & 288 & 332 & 19 & 639\n\\end{array}\n\\]\n\nSolution\nTherefore, for the pre-debate we posit a multinomial model. A multinomial model is nothing more than the extension of the binomial model to more than 2 categories. Here we have 3: Bush, Dukakis and other. For both models, we assume that the 639 observations are independent and exchangeable. The likelihood for each survey is thus:\n\\[\np(y \\mid \\theta) \\propto \\prod_{j=1}^{k} \\theta_{j}^{y_{j}}\n\\]\nWhere \\(\\theta_j\\) is the probability of choosing the \\(j\\) option. The conjugate prior for the distribution is a multivariate generalization of the beta distribution known as Dirichlet:\n\\[\np(\\theta \\mid \\beta) \\propto \\prod_{j=1}^{k} \\theta_{j}^{\\beta_{j}-1}\n\\]\nIf we set all \\(\\beta_j = 1\\), we get an uniform distribution on the possible distributions for the \\(\\theta\\)’s. That is, just as the beta distribution, the Dirichlet distribution is a distribution of distributions.\nThe resulting posterior distribution for the \\(\\theta_j\\)’s is a Dirichlet with parameters \\(\\beta_j + y_j\\). The question, then, is how to go from the \\(\\theta_j\\), the proportion that favors the option \\(j\\), to the requested \\(\\alpha_t\\):\n\nProportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey t.\n\nNote that given the inherent restriction on the Dirichlet, we can rewrite the distribution of the \\(\\theta_j\\)’s as \\((\\theta_1, \\theta_2, 1 - \\theta_1 - \\theta_2)\\). We can then perform a change of variables: \\((\\alpha, \\gamma) = (\\dfrac{\\theta_1}{\\theta_1 + \\theta_2}, \\theta_1 + \\theta_2)\\). Which it can be shown that \\(\\alpha\\) is then distributed thus:\n\\[\n\\alpha | y \\sim Beta(y_1 + \\beta_1, y_2 + \\beta_2)\n\\] ### Pre-Debate\nTherefore, setting an uniform prior (\\(\\beta_j = 1 \\ \\forall j\\)) on the possible distribution of the \\(\\theta_j\\)’s, the posterior distribution is:\n\\[\n(\\theta_{bush}, \\theta_{dukakis}, \\theta_{neither}) | y \\sim Dirichlet(295, 308, 39)\n\\] Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the pre-debate, that is, \\(\\alpha_1\\) is thus:\n\\[\n\\alpha_1 | y \\sim Beta(295, 308)\n\\] Which we can visualize thus:\n\nalpha1 = rbeta(10000, 295, 308)\ndata.frame(alpha1) -> simulations_alpha1\n\nsimulations_alpha1 %>% \n  ggplot(aes(alpha1)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  geom_vline(aes(xintercept = 0.5), linetype = 2, color = \"red\") +\n  labs(title = TeX(\"Posterior distribution for $\\\\alpha_1$\"),\n       subtitle = \" Proportion of voters who preferred Bush, out of those who had a preference \n       for either Bush or Dukakis at pre-debate\",\n       x = TeX(\"$\\\\alpha_1$\"))\n\n\n\n\nThat is, our posterior distribution points that at the pre-debate, there was already a majority of people (among the already decided) who favored Dukakis. Indeed:\n\npredebate <- pbeta(0.5, 295, 308)\nglue::glue(\"There's a {round(predebate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in\n           the pre-debate.\")\n\nThere's a 70% posterior probability that among decided voters Dukakis had a majority in\nthe pre-debate.\n\n\n\n\nPost-Debate\nTherefore, setting an uniform prior (\\(\\beta_j = 1 \\ \\forall j\\)) on the possible distribution of the \\(\\theta_j\\)’s, the posterior distribution is:\n\\[\n(\\theta_{bush}, \\theta_{dukakis}, \\theta_{neither}) | y \\sim Dirichlet(289, 333, 39)\n\\] Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the post-debate, that is, \\(\\alpha_2\\) is thus:\n\\[\n\\alpha_1 | y \\sim Beta(289, 333)\n\\]\nWhich we can visualize thus:\n\nalpha2 = rbeta(10000, 289, 333)\ndata.frame(alpha2) -> simulations_alpha2\n\nsimulations_alpha2 %>% \n  ggplot(aes(alpha2)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  geom_vline(aes(xintercept = 0.5), linetype = 2, color = \"red\") +\n  labs(title = TeX(\"Posterior distribution for $\\\\alpha_2$\"),\n       subtitle = \" Proportion of voters who preferred Bush, out of those who had a preference \n       for either Bush or Dukakis at post-debate\",\n       x = TeX(\"$\\\\alpha_2$\"))\n\n\n\n\nAfter the debate, Dukakis won an even larger majority among the decided voters:\n\npostdebeate <- pbeta(0.5, 289, 333)\nglue::glue(\"There's a {round(postdebeate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in\n           the pre-debate.\")\n\nThere's a 96% posterior probability that among decided voters Dukakis had a majority in\nthe pre-debate.\n\n\n\n\nA shift toward Bush?\nWe have the posterior probability for both \\(\\alpha_1\\) and \\(\\alpha_2\\). Sampling form these posteriors, we can then arrive at a posterior distribution for \\(\\alpha_2 - \\alpha_1\\)\n\ndifference <- alpha2 - alpha1\n\ndata.frame(difference) %>% \n  ggplot(aes(difference)) +\n  geom_vline(aes(xintercept = 0), color = \"red\", linetype = 2) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7)  +\n  labs(title = TeX(\"Posterior distribution for $\\\\alpha_2 - \\\\alpha_1$\"))\n\n\n\n\nThe posterior probability that there was a shift toward Bush is the probability that \\(\\alpha_2 - \\alpha_1 > 0\\)\n\nshift <- sum(difference > 0) / length(difference)\nglue::glue(\"The posterior probability that there was a shift toward Bush is thus {round(shift, 2)*100}%\")\n\nThe posterior probability that there was a shift toward Bush is thus 19%"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#exercise-3",
    "href": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#exercise-3",
    "title": "Bayesian Data Analysis: Week 3-> Exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nEstimation from two independent experiments: an experiment was performed on the effects of magnetic fields on the flow of calcium out of chicken brains. Two groups of chickens were involved: a control group of 32 chickens and an exposed group of 36 chickens. One measurement was taken on each chicken, and the purpose of the experiment was to measure the average flow ( {c} ) in untreated (control) chickens and the average flow ( {t} ) in treated chickens. The 32 measurements on the control group had a sample mean of 1.013 and a sample standard deviation of ( 0.24 . ) The 36 measurements on the treatment group had a sample mean of 1.173 and a sample standard deviation of 0.20\n\nAssuming the control measurements were taken at random from a normal distribution with mean ( {c} ) and variance ( {c}^{2}, ) what is the posterior distribution of ( {c} ? ) Similarly, use the treatment group measurements to determine the marginal posterior distribution of ( {t} . ) Assume a uniform prior distribution on ( ({c}, {t}, {c}, {t}) )\nWhat is the posterior distribution for the difference, ( {t}-{c} ? ) To get this, you may sample from the independent ( t ) distributions you obtained in part(a) above. Plot a histogram of your samples and give an approximate ( 95 % ) posterior interval for ( {t}-{c} )"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#solution-1",
    "href": "posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html#solution-1",
    "title": "Bayesian Data Analysis: Week 3-> Exercises",
    "section": "Solution",
    "text": "Solution\nLet’s posit two normal probability models for both the control measurements and the treatment measurements, assuming exchangeability among these two groups.\n\nControl group\nTherefore:\n\\[\ny_c | \\mu, \\sigma^2 \\sim N(\\mu_c, \\sigma_c^2) \\\\\np(\\mu_c, \\sigma_c | y) \\propto p (y | \\mu_c, \\sigma_c) p(\\mu_c, \\sigma_c)\n\\] If we posit an uniform prior on \\((\\mu_c, log \\sigma_c)\\)\n\\[\np(\\mu_c, \\sigma_c^2) \\propto (\\sigma_c^2)^{-1}\n\\]\nThen, the marginal posterior distribution for \\(\\mu_c\\) is a t-distribution:\n\\[\n\\dfrac{\\mu_c  - \\bar y_c}{s_c/\\sqrt{n_c}} | y \\sim t_{n_c-1}\n\\]\nFor the control group, we have \\(n_c = 32\\), \\(\\bar y_c = 1.013\\) and \\(s_c = 0.24\\)\n\nmu_c <- rtnew(10000, df = 31, mean = 1.013, scale = 0.24/sqrt(32) )\n\ndata.frame(mu_control = mu_c) %>% \n  ggplot(aes(mu_control)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  labs(title = TeX(\"Posterior distribution for $\\\\mu_c$\"))\n\n\n\n\n\n\nTreatment Group\nThe same likelihood and prior are valid for the treatment measurements. Therefore, the marginal posterior for \\(\\mu_t\\):\n\\[\n\\dfrac{\\mu_t  - \\bar y_t}{s_t/\\sqrt{n_t}} | y \\sim t_{n_t-1}\n\\]\nFor the treatment group, we have \\(n_t = 36\\), \\(\\mu_t = 1.173\\), \\(s_t = 0.2\\):\n\nmu_t <- rtnew(10000, df = 35, mean = 1.173, scale = 0.2/sqrt(36) )\n\ndata.frame(mu_treatment = mu_t) %>% \n  ggplot(aes(mu_treatment)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  labs(title = TeX(\"Posterior distribution for $\\\\mu_t$\"))\n\n\n\n\n\n\nPosterior difference between mu_c and mu_t\nTo get the posterior distribution of the difference, we compare the samples from the marginal posterior of \\(\\mu_c, \\mu_t\\). Therefore, the 95% posterior credibility interval on the different is thus\n\ndifferent_mu <- mu_t - mu_c\ninterval <- rethinking::PI(different_mu, prob = 0.95)\nlower <- interval[[1]]\nupper <- interval[[2]]\nglue::glue(\"The 95% posterior credibility interval for the difference is {round(lower, 2)}, {round(upper, 2)}\")\n\nThe 95% posterior credibility interval for the difference is 0.05, 0.27\n\n\nAnd the full posterior of the difference is thus:\n\ndata.frame(different_mu) %>% \n  ggplot(aes(different_mu)) +\n  geom_histogram(binwidth = 0.01, color = \"black\", fill = \"dodgerblue4\", alpha = 0.7) +\n  labs(title = TeX(\"Posterior distribution for $\\\\mu_t - \\\\mu_c$\"))"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html",
    "href": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html",
    "title": "Bayesian Instrumental Variable Regression",
    "section": "",
    "text": "Statistical Rethinking is a fabulous course on Bayesian Statistics (and much more). In what follows, I’ll give a succinct presentation of Instrumental Variable Regression in a Bayesian setting using simulated data.\nI had already seen the traditional econometrics formulation and yet found Richard’s presentation both illuminating and fun. It’s a testament of his incredible achievement with this book."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#opening-collider-and-estimating-simultaneously",
    "href": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#opening-collider-and-estimating-simultaneously",
    "title": "Bayesian Instrumental Variable Regression",
    "section": "Opening collider and estimating simultaneously",
    "text": "Opening collider and estimating simultaneously\nThe model then must be simultaneous. It must open the collider and regress \\(Y\\) on \\(X\\). The solution is thu:\n\\[ \\begin{bmatrix}\nY_i \\\\\nX_i\n\\end{bmatrix} \\sim MVNormal(\\begin{bmatrix}\n\\mu_{Y,i} \\\\\n\\mu_{X, y}\n\\end{bmatrix}, S) \\]\n\\[ \\mu_{Y,i} = \\alpha_y + \\beta X_i \\]\n\\[ \\mu_{X, i} = \\alpha_x + \\gamma I \\]\nWe are modelling \\(X, Y\\) simultaneously with a joint error structure represented by \\(S\\). Notice, then, that at both linear models we should be adjusting by \\(U\\). Therefore, the errors of our each of our linear regressions, represented by \\(S\\), will be correlated; this is what a fork does and what creates the original bias in our estimates. However, we are opening simultaneously the collider on \\(X\\) by adjusting with \\(I\\). Therefore, statistical information about \\(U\\) is entering into our model in the form of a correlated error structure (represented by \\(S\\)) between the two linear regressions. This statistical information of \\(U\\) will then allow us to causally estimate the effect of \\(X\\) on \\(Y\\).\nNote that, given a DAG, we can algorithmically compute if there is an instrument that we can use.\n\ninstrumentalVariables(dag_instrument, exposure = 'X', outcome = 'Y')\n\n I"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#naive-regression",
    "href": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#naive-regression",
    "title": "Bayesian Instrumental Variable Regression",
    "section": "Naive regression",
    "text": "Naive regression\nA naive regression won’t account by the confounding effect of \\(U\\):\n\nmodel_naive <- ulam(\n  alist(\n    Y ~ normal(mu, sigma),\n    mu <- alpha + beta*X,\n    alpha ~ normal(0, 1),\n    beta ~ normal(0, 1),\n    sigma ~ exponential(1)\n  ),\n  chains = 4, cores = 4,\n  data = data_sim\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.8 seconds.\n\nprecis(model_naive)\n\n               mean         sd        5.5%      94.5%    n_eff     Rhat4\nalpha -0.0007866032 0.02848885 -0.04462621 0.04554595 1496.227 1.0004385\nbeta   0.4439785015 0.02900968  0.39840345 0.49061714 2087.683 1.0016685\nsigma  0.8969762870 0.02008460  0.86616765 0.92862533 1929.604 0.9991075\n\n\nIndeed, we have an estimate with a 87% compatibility interval of (0.40, 0.49) when we know that the true effect is zero. We can plot the expected relationship:\n\ndata.frame(data_sim) %>% \n  data_grid(X = seq_range(X, 50)) %>% \n  add_predicted_draws(model_naive) %>% \n  ggplot(aes(X, Y)) +\n  stat_lineribbon(aes(y = .prediction), alpha = 1/4, fill = \"dodgerblue4\") +\n  geom_point(data = data.frame(data_sim), alpha = 0.4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"Naive model finds a relationship\",\n       subtitle = \"Confounding effect is unaccounted for. True effect of X on Y is null\")"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#instrumental-variable-regression",
    "href": "posts/bayesian-statistics/2020-06-03-bayesian-instrumental-variable-regression.html#instrumental-variable-regression",
    "title": "Bayesian Instrumental Variable Regression",
    "section": "Instrumental Variable Regression",
    "text": "Instrumental Variable Regression\nGiven our DAG and our data, we can do better. We can fit a multivariate model that, by virtue of opening a collider on \\(X\\), will allows us to statistical adjust by the confounding factor \\(U\\).\n\nmodel_instrumental <- ulam(\n  alist(\n    c(Y, X) ~ multi_normal(c(muY, muX), Rho, Sigma),\n    muY <- alphaY + beta*X,\n    muX <- alphaX + gamma*I,\n    c(alphaY, alphaX) ~ normal(0, 0.2),\n    c(beta, gamma) ~ normal(0, 0.5),\n    Rho ~ lkj_corr(2),\n    Sigma ~ exponential(1)\n  ),\n  data = data_sim, chains = 4, cores = 4\n)\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 11.8 seconds.\nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 12.1 seconds.\nChain 4 finished in 12.1 seconds.\nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 12.8 seconds.\n\nprecis(model_instrumental)\n\n                mean         sd        5.5%      94.5%     n_eff    Rhat4\nalphaX -0.0007029589 0.02612907 -0.04213007 0.04238940 1514.2057 1.000685\nalphaY -0.0007681863 0.03114677 -0.04975118 0.04891647 1311.7424 1.000845\ngamma   0.5686533460 0.02628153  0.52624149 0.61137419 1414.5350 1.001943\nbeta    0.0594884285 0.05408430 -0.02861268 0.14559261  970.4573 1.000913\n\n\nWhereas before we posited a positive and relatively large effect of \\(X\\) on \\(Y\\), now we correctly infer that the true effect is null. Because \\(\\beta\\) has lots of its mass around zero.\n\nmodel_instrumental %>% \n  spread_draws(beta) %>% \n  ggplot(aes(beta)) +\n  geom_histogram(color = \"black\", fill = \"dodgerblue4\", alpha = 4/10,\n                 binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0), linetype = 2, color = \"red\") +\n  labs(title = \"Instrumental Variable Regression\",\n       subtitle = \"Accounting for the confounding through IV, finds true null effect\")"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html",
    "title": "Statistical Rethinking Week 8",
    "section": "",
    "text": "This week was our first introduction to Multilevel models. Models where we explicitly model a family of parameters as coming from a common distribution: with each sample, we simultaneously learn each parameter and the parameters of the common distribution. This process of sharing information is called pooling. The end result is shrinkage: each parameter gets pulled towards the estimated mean of the common distribution. I tried my best to understand this process and result by simulating in this post"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-only-predation",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-only-predation",
    "title": "Statistical Rethinking Week 8",
    "section": "Model with only predation",
    "text": "Model with only predation\nLet’s check how the predation variable is encoded:\n\n\n  pred  n\n1   no 24\n2 pred 24\n\n\n\n\n  predation_int pred  n\n1             0   no 24\n2             1 pred 24\n\n\nNow, let’s propose the model with varying intercept for tanks and taking into account whether there were predators or not.\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\n\nLet’s check the posterior and the Rhat values:\n\n\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9994  \n Median :0.9996  \n Mean   :0.9998  \n 3rd Qu.:0.9998  \n Max.   :1.0028  \n\n\nThe \\(\\hat{R}\\) values look good enough, all are close to 0. There appear to not be signs of transient like behavior.\n\n\n            mean        sd      5.5%     94.5%    n_eff    Rhat4\npred  -1.8222334 0.2997461 -2.280908 -1.312238 1410.822 1.001462\na_bar  2.1963295 0.2296880  1.814582  2.555532 1922.014 1.001507\nsigma  0.9106572 0.1645364  0.675735  1.189860 1139.664 1.002823\n\n\nAs expected, tanks with predators have, on average, lower log odds of probability of surviving."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-only-size",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-only-size",
    "title": "Statistical Rethinking Week 8",
    "section": "Model with only size",
    "text": "Model with only size\nLet’s prepare size to the model:\n\n\n   size  n\n1   big 24\n2 small 24\n\n\n\n\n  size_int  size  n\n1        0   big 24\n2        1 small 24\n\n\nNow, let’s add the size to our model with varying intercepts:\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.8 seconds.\nTotal execution time: 1.0 seconds.\n\n\nLet’s check our \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9995  \n Median :0.9999  \n Mean   :1.0001  \n 3rd Qu.:1.0004  \n Max.   :1.0027  \n\n\nLet’s check our precis ouptut:\n\n\n           mean        sd       5.5%     94.5%    n_eff    Rhat4\ns     0.2621837 0.3559195 -0.3168585 0.8315286 1557.368 1.002716\na_bar 1.2242716 0.3072198  0.7418783 1.7224410 1090.394 1.000859\nsigma 1.5785150 0.1997810  1.2827567 1.9168341 1338.498 1.002117\n\n\nIt seems that the size is not that relevant in the log-odds scale. Its 89% PI covers zero and a prety wide interval."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-size-and-predators",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-size-and-predators",
    "title": "Statistical Rethinking Week 8",
    "section": "Model with size and predators",
    "text": "Model with size and predators\nLet’s include both variables:\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.9 seconds.\nChain 2 finished in 0.9 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 1.0 seconds.\n\n\nLet’s check our \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9994  \n Median :0.9996  \n Mean   :0.9998  \n 3rd Qu.:1.0000  \n Max.   :1.0015  \n\n\nThe \\(\\hat{R}\\) values look OK. Let’s check the precis output:\n\n\n            mean        sd        5.5%      94.5%    n_eff     Rhat4\ns      0.3802925 0.2658079 -0.04409109  0.7974136 2220.168 1.0014675\npred  -1.8502836 0.2942538 -2.29862435 -1.3587384 1648.664 1.0011477\na_bar  2.0230849 0.2598151  1.59480645  2.4302732 2075.647 0.9995557\nsigma  0.8718928 0.1674364  0.62479505  1.1597191 1044.173 1.0009064\n\n\nPredators’ effect is still large and negative on the log-odds scale. Also, size’s effect has shifted and, once we have statistically adjusted by the presence of predators, now has most of its posterior mass to the right of zero. Presumably, this arises because size and the presence of predators are related; unless we adjust by the presence of predators, the coefficient for size will pick up some of the predators’ effect."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-an-interaction",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#model-with-an-interaction",
    "title": "Statistical Rethinking Week 8",
    "section": "Model with an interaction",
    "text": "Model with an interaction\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.0 seconds.\nChain 2 finished in 1.0 seconds.\nChain 3 finished in 1.0 seconds.\nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.0 seconds.\nTotal execution time: 1.1 seconds.\n\n\n.\nLet’s check on the \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9995  \n Median :0.9997  \n Mean   :0.9999  \n 3rd Qu.:1.0000  \n Max.   :1.0020  \n\n\nThe \\(\\hat{R}\\) values look OK. Let’s check the precis output:\n\n\n                   mean        sd        5.5%      94.5%    n_eff     Rhat4\ns            0.39027815 0.2917359 -0.06399788  0.8527317 2805.604 0.9998481\npred        -1.86693961 0.3037439 -2.32497935 -1.3481036 1942.572 1.0012481\ninteraction -0.01565226 0.2231252 -0.37841780  0.3360543 4759.155 0.9993405\na_bar        2.02130251 0.2554671  1.61979835  2.4199170 2432.114 1.0007998\nsigma        0.86186357 0.1669389  0.61695795  1.1412422 1423.359 1.0019521\n\n\nPrediction is still large and negative on the log-odds scale. Size also does not appear to change much with the interaction. The interaction has a large standard error and most of its mass lies largely symmetric around zero.\nNow it’s the turn to check how the estimated variation of tanks has changed with the different models:\n\n\n\n\n\nAll the models that include predators have almost identical estimates for the variation across tanks. That is, the presence of predators explain some of the variation across tanks.\nFinally, let’s compare the models according to information criteria:\n\n\n                         WAIC       SE     dWAIC       dSE    pWAIC    weight\nmodel_both           199.0788 7.927714 0.0000000        NA 19.19389 0.3594406\nmodel_only_predators 199.5214 8.146360 0.4425911 1.5345879 19.53953 0.2880844\nmodel_interaction    200.3865 7.898739 1.3076555 0.5588874 19.65161 0.1869276\nmodel_only_size      200.6294 7.227683 1.5505815 3.9940200 21.08975 0.1655475\n\n\nAccording to information criteria, all of the models make essentially have the same expected predictive performance out-of-sample."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#traditional-fixed-effects",
    "href": "posts/bayesian-statistics/2020-05-29-statistical-rethinking-week-8.html#traditional-fixed-effects",
    "title": "Statistical Rethinking Week 8",
    "section": "Traditional fixed-effects",
    "text": "Traditional fixed-effects\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 4.3 seconds.\nChain 2 finished in 4.2 seconds.\nChain 4 finished in 4.2 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 4.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 4.2 seconds.\nTotal execution time: 4.4 seconds.\n\n\nLet’s check the \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9981  \n 1st Qu.:0.9985  \n Median :0.9986  \n Mean   :0.9987  \n 3rd Qu.:0.9988  \n Max.   :1.0003  \n\n\nThe \\(\\hat{R}\\) values look OK. Let’s fit the multilevel model:\n\n\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 5.9 seconds.\nChain 2 finished in 5.9 seconds.\nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 5.9 seconds.\nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 6.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 5.9 seconds.\nTotal execution time: 6.2 seconds.\n\n\nLet’s check on the \\(\\hat{R}\\) values:\n\n\n     Rhat4       \n Min.   :0.9983  \n 1st Qu.:0.9991  \n Median :0.9997  \n Mean   :0.9999  \n 3rd Qu.:1.0002  \n Max.   :1.0049  \n\n\nThe \\(\\hat{R}\\) values look OK.\nNow let’s inspect the values for the distribution of varying intercepts for each district:\n\n\n            mean         sd       5.5%      94.5%    n_eff    Rhat4\nalpha -0.5300451 0.09031838 -0.6757211 -0.3845637 725.4713 1.002841\nsigma  0.5155339 0.08471016  0.3923543  0.6590001 802.9495 1.002967\n\n\nThe overall use of contraceptives seems unlikely across districts, thus the negative alpha.\n\nPlot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model.\n\nNotice that each women, within a same district, has the same prediction.\n\n\n\nLet’s average over the posterior the alpha of the distribution of varying intercepts per district\n\n\n\nLet’s plot the requested graph:\n\n\n\n\n\nWe are seeing the consequences of pooling information from the common distribution of districts: each district’s prediction is overall much closer to the estimated common distribution’s mean than the predictions from the fixed effects model. Therefore, each yellow point is closer to the red line than its corresponding purple point. There are a couple of districts where the difference in predictions between the two models is huge. This are the places where most outside information was used. From what we’ve known about Pooling, these must be the places that were most likely to overfit and had fewer data points. Let’s confirm this intuition:\n\n\n\n\n\nFinally, let’s compare their expected out of sample performance:\n\n\n                        WAIC       SE    dWAIC      dSE    pWAIC     weight\nmodel_multilevel    2515.105 24.92913 0.000000       NA 35.82908 0.98751026\nmodel_fixed_effects 2523.846 28.93510 8.740559 7.733149 53.90337 0.01248974"
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-20-statistical-rethinking-week-6.html",
    "href": "posts/bayesian-statistics/2020-05-20-statistical-rethinking-week-6.html",
    "title": "Statistical Rethinking: Week 6",
    "section": "",
    "text": "The week was a whirlwind tour of:\n\nMaximum entropy and introduction to GLMs.\nThe problems that come when using link functions.\nThe perils of relative effects when studying binomial regression and how complicated it is to directly calculate probabilities with GLMs: all the parameters interact among themselves.\n\nThis week was an introduction to GLMs and the principle of Maximum Entropy. Once we adventure outside the Gaussian, things start to become interesting. However, interesting can quickly devolve into chaotic and arbitrary modelling decisions. Against this, Richard started to introduce the principle of Maximum Entropy: when choosing how to approximate an unknown distribution, pick the most conservative distribution that satisfies your assumptions. This guiding principle works just as well for our likelihood choice, our prior choice and the resulting posterior distribution.\nAlso, once we work with likelihoods other than the normal, we must work with “link” functions: functions that link one of the likelihood’s parameters to a linear combination of our predictors. However, this change is not completely benign: prior setting now has become even more unnatural. Flat priors on the parameter space can now imply very different things in the outcome space. Thus, the heightened importance of prior predictive simulation."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html",
    "href": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html",
    "title": "BDA week 7: LOO and its diagnostics",
    "section": "",
    "text": "Once Stan’s implementation of HMC has run its magic, we finally have samples from the posterior distribution \\(\\pi (\\theta | y))\\). We can then run posterior predictive checks and hopefully our samples looks plausible under our posterior. Nevertheless, this is just an internal validation check: we expect more from our model. We expect it to hold under an external validation check: never seen observations, once predicted, should also look plausible under our posterior.\nLeave-One-Out (LOO) log pointwise predictive density is the preferred Bayesian way to do this. In this blogpost, I’ll explain how we can approximate this metric without the need of refitting the model \\(n\\) times with LOO Pareto Smoothed Importance Sampling (PSIS). Also, I’ll explain how PSIS diagnostics tells us, not unlike HMC, when the algorithm is a poor approximation. As a byproduct, we can also derive a metric to identify if there are influential observations that are driving the inference. Finally, I’ll simulate data to show how we can perform all this with real data.\nAll of this is based on this great paper by Vehtari, Gelman and Gabry."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#what-is-our-metric-log-pointwise-predictive-density",
    "href": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#what-is-our-metric-log-pointwise-predictive-density",
    "title": "BDA week 7: LOO and its diagnostics",
    "section": "What is our metric? Log pointwise predictive density",
    "text": "What is our metric? Log pointwise predictive density\nGiven an observation \\(y_i\\), we define our metric to evaluate how well we have predicted \\(y_i\\) as its log likelihood according to our model. Given our uncertainty over the parameters, we integrate over our posterior distribution for our model. We call this the log pointwise predictive density (lpd):\n\\[\nlpd = \\log \\int \\pi (y_i | \\theta) \\pi (\\theta | y) d\\theta\n\\]\nThe fundamental problem comes when we use \\(y_i\\) to compute the full posterior \\(\\pi (\\theta | y)\\): we are performing an internal check, not an external validation. A solution is to use the resulting posterior without using the observation \\(y_i\\) in the fitting. This is the Leave-One-Out (LOO) posterior: \\(\\pi(\\theta, y_{-i})\\)\nThe problem is that evaluating the LOO posterior is just as computationally expensive as fitting the model all over again. If we then want to use all our observations to perform the external validation check, this amounts to fitting the probability model \\(n\\) times."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#approximating-the-loo-posterior",
    "href": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#approximating-the-loo-posterior",
    "title": "BDA week 7: LOO and its diagnostics",
    "section": "Approximating the LOO posterior",
    "text": "Approximating the LOO posterior\nNot being able to compute from a distribution is an awfully familiar problem in Bayesian Statistics. Which in this case comes in handy. We can use Importance Sampling to use the samples from the full posterior to approximate the LOO posterior. Thus, our Importance Weights for each sample \\(s\\) from the posterior is the ratio of the densities.\n\\[\nr_i^s = \\frac{\\pi (\\theta^s | y_{-i})}{\\pi(\\theta^s|y_i)}\n\\]\nIf we correct, then, our original full posterior samples by these weights, we get equivalent samples from the LOO posterior. Thus, we can compute the log pointwise predictive density (lpd) that can track the out-of-sample performance of our model."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#the-approximation-is-likely-to-fail",
    "href": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#the-approximation-is-likely-to-fail",
    "title": "BDA week 7: LOO and its diagnostics",
    "section": "The approximation is likely to fail",
    "text": "The approximation is likely to fail\nSadly, this approximation to the LOO posterior using the full posterior is likely to fail. Importance Sampling only works when all of the weights are roughly equal. When the weights are very small with a large probability, and very, very large with a small probability, Importance Sampling fails: our computations end up effectively using only the large weights samples, thus drastically reducing our effective number of samples from the LOO posterior. That is, Importance Sampling is likely to fail when the distribution of the weights is fat-tailed.\nSadly, this is very likely to happen with our approximation: the LOO posterior is likely to have a larger variance and fatter tails than the full posterior. Thus, samples from the tails of the full posterior will have large weights to compensate for this fact. Therefore, the distribution of importance weights is likely gonna be fat-tailed."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#correcting-the-approximation-with-psis",
    "href": "posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html#correcting-the-approximation-with-psis",
    "title": "BDA week 7: LOO and its diagnostics",
    "section": "Correcting the approximation with PSIS",
    "text": "Correcting the approximation with PSIS\nVehtari, Gelman and Gabry correct the distribution of Importance Weights and thereby improve the approximation to the LOO posterior. First, they use Extreme Value Theory to fit the tail of the distribution with a Generalized Pareto Distribution with tail shape parameter \\(k\\) (\\(GPD(k)\\)).\nSecondly, they replace the large weights with smoothed over versions of the weights according to expected order statistics of the fitted \\(GPD(k)\\). This in turn gives the name to the method: Pareto Smoothed Importance Sampling (PSIS) Thirdly, they truncate large weights at \\(\\dfrac{3}{4}\\) of the mean of the smoothed weights.\nTherefore, we arrive at a new vector of importance weights \\(w_i^s\\) which, in general, behaves better than the original importance weights \\(r_i^s\\) and thus allow us to perform a better approximation of the LOO posterior.\n\nIt’s about the diagnostics we made along the way\nThe great thing about PSIS, besides creating better importance weights, it’s the diagnostics that it creates along the way. By fitting a \\(GPD(k)\\), the tail shape parameter becomes a diagnostic to assess the reliability of our approximation. When is Pareto Smoothed Importance Sampling (PSIS) a valid approximation to the LOO posterior?\nThe smoothing and the truncating can only do so much. If \\(k > 0.7\\), the importance weights are probably too fat-tailed to begin with and PSIS-LOO will be a poor approximation the LOO posterior. Not only that, it is also a diagnostic that tells us about a fundamental disagreement bewteen the full posterior and the LOO posterior: that is, about observations that are highly influential in determining the posterior.\nTherefore, by performing PSIS-LOO, we also arrive at a diagnostic for highly influential observations that are driving our inference and are thus surprising observations to our model."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-05-12-statistical-rethinking-week-5-interactions.html",
    "href": "posts/bayesian-statistics/2020-05-12-statistical-rethinking-week-5-interactions.html",
    "title": "Statistical Rethinking: Week 5 -> Interactions",
    "section": "",
    "text": "As Richard says in class, interactions are easy to code but incredibly difficult to interpret. By going through the problems in Chapter 8, I hope to gain a bit of practice working with them.\n\n\n\n\nLet’s run the tulips model but this time with the bed variable. Given that this is a categorical variable, it will create a different intercept for each of the beds in the sample.\n\n\n            mean         sd        5.5%       94.5%\na[1]   0.1216469 0.03061961  0.07271085  0.17058296\na[2]   0.1782907 0.02439658  0.13930026  0.21728116\na[3]   0.2406134 0.03059111  0.19172287  0.28950387\nbw     0.2072015 0.02671180  0.16451091  0.24989215\nbs    -0.1725240 0.03737034 -0.23224906 -0.11279901\nbws   -0.1436491 0.03261954 -0.19578146 -0.09151679\nsigma  0.1139477 0.01548856  0.08919395  0.13870138\n\n\nOur model is swayed by the different intercepts given to each bed, as they seem to capture valuable variation. Now, let’s compare it with a model that omits bed.\n\n\n                  WAIC        SE    dWAIC      dSE    pWAIC    weight\nmodel_wo_bed -22.60634 10.046650 0.000000       NA 6.291139 0.7254267\nmodel_bed    -20.66326  9.993276 1.943084 5.996111 9.575747 0.2745733\n\n\nMost of the weight seems to go the model with bed. However, given the lack of data, there’s too much uncertainty around the difference in WAIC between the 2 models.\n\n\n\n\n\n            mean          sd        5.5%       94.5%\na[1]   0.8865695 0.015675627  0.86151687  0.91162223\na[2]   1.0505715 0.009936555  1.03469096  1.06645202\nb[1]   0.1325113 0.074204079  0.01391882  0.25110372\nb[2]  -0.1425710 0.054749136 -0.23007065 -0.05507126\nsigma  0.1094936 0.005935223  0.10000792  0.11897919\n\n\n\n\n             mean          sd        5.5%      94.5%\na[1]   0.87962099 0.015965119  0.85410565  0.9051363\na[2]   1.05057837 0.009858793  1.03482212  1.0663346\nb[1]   0.06787769 0.081004059 -0.06158244  0.1973378\nb[2]  -0.14265391 0.054330616 -0.22948473 -0.0558231\nsigma  0.10862806 0.005905093  0.09919058  0.1180655\n\n\nAs expected, the slope for the african countries decreased. Where its 89% PI went from (0.01, 0.25) to (-0.06, 0.2). Whereas the rest of the parameters are unchanged, as expected. Were we fitting a multi-level model, the slope for non african countries would have also possibly changed.\nLet’s plot what this means:\n\n\n\n\n\nNow let’s add europe into the mix for the model without Seychelles:\n\n\n\n\n\nWhereas before\n\n\n\n\n\nAlthough the model has lost confidence in the relationship between rugedness and log(gdp) for african countries for high levels of ruggedness, the model still allows us to be confident of a positive relationship between ruggedness and log(gdp) at lower levels. We can see this more clearly with the following plots where we analyze the expected difference between countries at different levels of ruggeddness:\n\n\n\n\n\nDropping Seychelle shrinks the expected difference between continents at all levels of ruggedness; however, the effect is stronger at the largest levels of ruggeddness.\n\n\n\n\nIs language diversity a product of food security?\n\n\n              mean         sd       5.5%       94.5%\na       -3.6241843 0.93057731 -5.1114266 -2.13694206\ngrowing  1.3256627 0.53695659  0.4675024  2.18382305\nc       -0.1410468 0.07128679 -0.2549769 -0.02711674\nsigma    1.3846357 0.11311750  1.2038521  1.56541928\n\n\n\n\n\n\n\nIt seems that the effect of growing seasons length, adjusting for area, is positive. The higher the mean growing season, the larger the log of languages per caput.\n\n\n                mean         sd       5.5%       94.5%\na         -3.4989509 0.94653229 -5.0116924 -1.98620953\nsdgrowing -0.8468873 0.69645186 -1.9599519  0.26617726\nc         -0.1493849 0.07265783 -0.2655061 -0.03326362\nsigma      1.4337800 0.11691787  1.2469227  1.62063737\n\n\n\n\n\n\n\nIt also seems that the standard deviation has a negative effect on the logarithm of the languages per caput. However, this effect has a higher level of uncertainty associated to it.\nNow, let’s consider the interaction between the volatility in growing season and the mean duration of the growing season.\n\n\n                    mean         sd       5.5%        94.5%\na           -3.919031711 0.94832455 -5.4346375 -2.403425917\ngrowing      1.380821711 0.53151914  0.5313515  2.230291955\nsdgrowing   -0.980589705 0.68086964 -2.0687509  0.107571483\nc           -0.117471256 0.07280641 -0.2338300 -0.001112548\ninteraction -0.007921323 0.09996142 -0.1676790  0.151836327\nsigma        1.357910739 0.11154742  1.1796364  1.536185063\n\n\n\n\n                WAIC       SE    dWAIC      dSE    pWAIC     weight\nmodel_three 265.2903 16.57336 0.000000       NA 4.477261 0.70292510\nmodel_one   267.2050 16.28920 1.914699 1.984100 4.002462 0.26985934\nmodel_two   271.7932 17.04332 6.502923 3.777558 3.826063 0.02721556\n\n\nThe leading variable seems to be mean growing season. As the model that considers it alone seems to capture all importation variation and its predictions are basically equivalent to those of the model that considers all terms (model3)."
  },
  {
    "objectID": "posts/bayesian-statistics/2020-04-19-statistical-rethinking-week-1.html",
    "href": "posts/bayesian-statistics/2020-04-19-statistical-rethinking-week-1.html",
    "title": "Statistical Rethinking: Week 1",
    "section": "",
    "text": "Week 1 tries to go as deep as possible in the intuition and the mechanics of a very simple model. As always with McElreath, he goes on with both clarity and erudition.\n\nSuppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.\n\n\n\n\n\n\n\n\n[1] 0.5288668\n\n\n\nStart over in 1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. What difference does the better prior make? If it helps, compare posterior distributions (using both priors) to the true value p = 0.7.\n\n\n\n\n\n\nBy rejecting altogether from the beginning the possibility of having less than the half of the world covered with water, the model with the new prior piles on more plausibility on the values closer to the true value. Thus, the more informative prior helps our inference.\n\nThis problem is more open-ended than the others. Feel free to collaborate on the solution. Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this? I won’t require a precise answer. I’m honestly more interested in your approach.\n\n\n\n\n\n\n\n\n\nHow much posterior probability lies below p = 0.2?\n\n\n\nThe probability below p = 0.2 is 0.1%\n\n\n\nHow much posterior probability lies above p = 0.8?\n\n\n\nThe probability above p = 0.8 is 9.1%\n\n\n\nHow much posterior probability lies between p = 0.2 and p = 0.8\n\n\n\nThe probability is 90.8%\n\n\n\n20% of the posterior probability lies below which value of p?\n\n\n\n      20% \n0.5163163 \n\n\n\n20% of the posterior probability lies above which value of p?\n\n\n\n      80% \n0.7427427 \n\n\n\nWhich values of p contain the narrowest interval equal to 66% of the posterior probability?\n\n\n\n    |0.66     0.66| \n0.5135135 0.7697698 \n\n\n\nWhich values of p containt 66% of the posterior probability, assuming equal posterior probability both below and above the interval?\n\n\n\n      17%       83% \n0.4961562 0.7569269 \n\n\n\n\n\n\n\n\n\nHPDI 90% for p\n\n\n\n     |0.9      0.9| \n0.3343343 0.7217217 \n\n\n\nPosterior predictive check. 8 tosses in 15, prediction averaged over our posterior distribution.\n\n\n\n[1] 0.1499\n\n\n\nUsing the posterior distribution contracted from the 8/15 data, now calculate the probability of observing 6 water in 9 tosses.\n\n\n\n[1] 0.1842\n\n\n\n\n\n\n\n\nHPDI 90% for p\n\n\n\n     |0.9      0.9| \n0.5005005 0.7097097 \n\n\n\nPosterior predictive check. 8 tosses in 15, prediction averaged over our posterior distribution.\n\n\n\n[1] 0.163\n\n\n\nUsing the posterior distribution contracted from the 8/15 data, now calculate the probability of observing 6 water in 9 tosses.\n\n\n\n[1] 0.2353\n\n\n\nNumber of tosses have a 99% percentile interval to be only 0.05 wide.\n\nWe know the true value of our problem: \\(p = 0.7\\). We will simulate data for many Ns and find out how precisely we can estimate the interval for each of these values. We will repeat these simulations for each value of N 100 times. Then, we plot the different bounds that we get.\n\n\n\n\n\nIt seems we would have to toss the worldarround 2000 times to get a bound close to 0.05. The marginal benefit that we get, in terms of tighting our estimated bound, decreases as we toss more and more. The greates benefits of increasing the data seem to be at the beginning.\n\n\n\n\n\n\n\n  [1] 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n [38] 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1\n [75] 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n\n\n\n\n  [1] 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0\n [38] 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0\n [75] 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0\n\n\n\nUsing grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform probability…\n\nSo, set up a binomial likelihood with \\(n, k\\):\n\n\n[1] 200 111\n\n\n\n\nMaximum posterior probability is obtained at 0.56\n\n\nA logical answer, considering the slight majority of boys at the sample.\n\nDraw 10000 random samples from the posterior distribution… HPDI for 50%, 89%, and 97%\n\n\n\n    |0.97     |0.89      |0.5      0.5|     0.89|     0.97| \n0.4848485 0.5050505 0.5454545 0.5858586 0.6060606 0.6262626 \n\n\n\nCheck that the model’s implied predictions fit the actual count\n\n\n\n\n\n\n\nNow compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births. How does the model look in this light\n\n\n\n\n\n\nNow the model seems to be underperforming. It’s implied prediction for 100 boys is way larger thatn the actual observed value.\n\nThe model assumes that sex of first and second births are independent. Validate this assumption.\n\nIf the sex of first and second births are independent, after condintioning on the first being a girl, the probability of being a boy should be the same as in the whole sample. Let’s predict with our model conditioning on the boy having an older sister.\n\n\n\n\n\n\n\nOur model only assumes a 0.06% to the observed value\n\n\nThe model under predicts the number of boys that have older sisters. It seems that, in our sample, the sex of the first and second births are not independent. It may be that our sample is biased. Or maybe people keep having babies until they have a boy. Who knows, right?"
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html",
    "title": "Introduction to Survival Analysis",
    "section": "",
    "text": "Survival Analysis is fun, but not often taught in statistics courses. It’s just like regular statistics but with a few twists.\nOne of the most common Survival models is the Cox Proportional Hazard model, which allows for the analysis of censored data and the estimation of the time to the event. In this blog post, we will go over the basic construction of the Cox model and how it deals with censoring and time. We will also provide an example to illustrate the concepts and show how the model can be applied in practice."
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html#survival-math",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html#survival-math",
    "title": "Introduction to Survival Analysis",
    "section": "Survival math",
    "text": "Survival math\nWe are interested in the probability that event will happen as a function of time. There are three equivalent mathematical representations to model this time-to-event modeling:\n\nThe Survival Function: the probability that an event of interest will not occur up to a given time. Mathematically, we can define it thus: \\(S(t) = 1 - F(t)\\), where \\(F(t)\\) is the lifetime distribution function for the time to the event of interest, and \\(f(t)\\) being the event density. Thus, we can expand both mathematical representations thus \\(S(t) = \\Pr(T > t) = \\int_t^{\\infty} f(u)\\,du = 1-F(t)\\) and \\(s(t) = S'(t) = \\frac{d}{dt} S(t) = \\frac{d}{dt} \\int_t^{\\infty} f(u)\\,du = \\frac{d}{dt} [1-F(t)] = -f(t)\\).\nThe Hazard Rate: the instantaneous probability of an event of interest occurring at a given time, given that the event has not occurred up to that time. Mathematically: \\(\\lambda (t) = \\lim_{dt \\rightarrow 0} \\frac{\\Pr(t \\leq T < t+dt)}{dt\\cdot S(t)} = \\frac{f(t)}{S(t)} = - \\frac{S'(t)}{S(t)}\\)\nThe Cumulative Hazard Rate: \\(\\Lambda(t) = \\int_0^{t} \\lambda(u)\\,du = -\\log S(t)\\)\n\nSometimes, a given representation will be more useful than others. Crucially, they are all related thus:\n\\[\nS(t) = \\exp [ -\\Lambda(t) ] = \\frac{f(t)}{\\lambda(t)} = 1-F(t), \\quad t > 0\n\\] We see that with survival math, unlike regular binary classification algorithms, time to event is a first class citizen. Let’s see a particular instance of such model: the Cox Proportional Hazards model."
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html#the-cox-proportional-hazards-model",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html#the-cox-proportional-hazards-model",
    "title": "Introduction to Survival Analysis",
    "section": "The Cox Proportional Hazards Model",
    "text": "The Cox Proportional Hazards Model\nNow that we’ve gotten some notation out of the way, let’s lay out the intuition and the math for the Cox Proportional Hazards Model.\nTo start with, we will model the hazard rate directly by separating the functional form into two parts: a baseline rate function that will vary exclusively with time \\(h_0(t)\\), and a risk function that will NOT vary with time but with whatever predictor variables we have \\(r(X; \\beta)\\):\n\\[\nh(t) =  \\underbrace{h_0(t)}_{\\text{Depends on time}} \\cdot \\underbrace{r(X; \\beta)}_{\\text{Does not depend on time}}\n\\]\nGiven that we want our risk function to always be positive, it’s useful instead to define the hazard rate in terms of the exponential of the risk function (\\(r(X; \\beta)\\) is in the log hazard scale units, whereas \\(\\exp{(r(X; \\beta)})\\) is in the hazard scale units.):\n\\[\nh(t) =  \\underbrace{h_0(t)}_{\\text{Depends on time}} \\cdot \\underbrace{\\exp{(r(X; \\beta)})}_{\\text{Does not depend on time}}\n\\]\nAnd that’s it. That’s the famous functional form for the Cox Proportional Hazards. Before we learn how to actually learn the parameters and the baseline rate function, let’s dive into why it’s called proportional.\n\nWhy the Proportional in the name? Constant hazard Ratios\nThe above functional form for the hazard rate means that the ratio in the hazard rate between two individuals with different covariate values will always be the same multiplicative constant, regardless of the values of the covariates. That is, the respective hazard ratios across time will always be a constant and the hazard curves cannot cross.\n\nThis implies that the hazard of an event in any group is a constant multiple of the hazard in any other group\n\nTherefore, whenever we want to understand the predictions a Cox Proportional Hazard model makes we refer to this relative, constant hazard ratio that we will see when comparing differnt observation groups.\nMathematically, we can see it thus:\nsuppose that we have two individuals with covariates \\(x_1\\) and \\(x_2\\), and regression coefficients \\(\\beta_1\\) and \\(\\beta_2\\). The hazard rate for the first individual is given by:\n\\(h_1(t) = h_0(t) \\cdot \\exp(\\beta_1 x_1 + \\beta_2 x_2)\\)\nand the hazard rate for the second individual is given by:\n\\(h_2(t) = h_0(t) \\cdot \\exp(\\beta_1 x_1' + \\beta_2 x_2')\\)\nThe ratio in the hazard rates for these two individuals is given by:\n\\[\n\\frac{h_1(t)}{h_2(t)} = \\frac{h_0(t) \\cdot \\exp(\\beta_1 x_1 + \\beta_2 x_2)}{h_0(t) \\cdot \\exp(\\beta_1 x_1' + \\beta_2 x_2')} = \\frac{\\cdot \\exp(\\beta_1 x_1 + \\beta_2 x_2)}{\\exp(\\beta_1 x_1' + \\beta_2 x_2')} = \\frac{h_1}{h_2} = \\psi\n\\]\nThis shows that the ratio in the hazard rates for these two individuals is always equal to a constant multiplicative factor, \\(\\psi\\), which is determined by the differences in the covariate values and the regression coefficients, regardless of the survival time in question. This is true for any pair of individuals with different covariate values, so the ratio in the hazard rate between any two groups will always be the same multiplicative constant in the Cox Proportional Hazards model."
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html#constructing-the-likelihood",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html#constructing-the-likelihood",
    "title": "Introduction to Survival Analysis",
    "section": "Constructing the Likelihood",
    "text": "Constructing the Likelihood\nGiven a model, we can construct the likelihood that will describe the probability of observing what actually happend in our sample.\n\nThe key idea behind the construction will be that we can describe non-censored observations with the event density as we would regularly do, but that for Censored observations we will use the Survival function.\n\nWhy? For observations who actually experienced the event during the study, we know their defined state. We can thus describe their end state with the event density, \\(f(t)\\). Whereas for censored observation, their state is undefined at the moment of the study; the only thing we can say is that they have survived to their censored survival time, and that the chance of that happening is described by \\(S(t)\\).\nLet’s \\(c_i\\) be an indicator variable indicating whether the observation is censored or not. For a single observation, we can describe the probability of observing it thus:\n\\[\n[f(t, \\beta, x)]^c \\times[S(t, \\beta, x)]^{1-c},\n\\] Assuming independence, the joint probability of observing the entire sample \\(n\\) is:\n\\[\n\\prod_{i=1}^n\\left\\{\\left[f\\left(t_i, \\beta, x_i\\right)\\right]^{c_i} \\times\\left[S\\left(t_i, \\beta, x_i\\right)\\right]^{1-c_i}\\right\\}\n\\]\nIf we replace the definition of the event density using our previously defined equations in Survival Math:\n\\[\n\\prod_{i=1}^n\\left\\{\\left[\\lambda(t_i, \\beta, x_i) \\cdot S\\left(t_i, \\beta, x_i\\right)\\right]]^{c_i} \\times\\left[S\\left(t_i, \\beta, x_i\\right)\\right]^{1-c_i}\\right\\}\n\\] \\[\n\\prod_{i=1}^n\\left\\{\\left[\\lambda(t_i, \\beta, x_i) \\right]^{c_i} \\times\\left[S\\left(t_i, \\beta, x_i\\right)\\right]\\right\\}\n\\]\nThis means that the contribution of a non-censored observation to the loss function is the product of the hazard and survival functions at the time of the event. Whereas the contribution of a censored observation to the loss function is only the survival function at the time of censoring.\nIntuitively, this means that censored observations have a smaller impact on the loss function than non-censored observations. This is because censored observations provide less information about the event of interest, so they are less useful for estimating the hazard and survival functions. As a result, the Cox model gives less weight to censored observations when estimating the coefficients.\n\nCox’s Partial (Profile) Likelihood\nThe above equation gives us a nice intuitive understanding of how the model works. However, the above requires to integrate over time to be able to derive the Survival Function and that would in turn require to specify a functional form for the Hazard Function for all possible event times, which is to ask a lot. We can simplify the problem if we instead assume that the baseline hazard is unknown.\nHow? The gist is that the baseline rate, \\(h_0(t)\\), is not of our interest; it’s a nuisance function. So we will consider it a fixed value and estimate the regression parameters first. To do so, we will separate the likelihood on that which depends on \\(h_0\\) and that which doesn’t.\n\\[\\begin{aligned}\nL\\left(\\beta, h_0(\\cdot)\\right) &= \\underbrace{L_1(\\beta)}_{\\text{function of } \\beta,  \\text{  whose max converges to }  \\beta} \\\\\n& \\cdot \\underbrace{L_2\\left(\\beta, h_0\\right)}_{\\text{contains little information about} \\beta \\text{only relevant for } h_0}\n\\end{aligned}\\]\nThe process of doing so is tedious and not worth the trouble. If you are curious, for example, one of the most famous formulations is the Efron approximation.\nNevertheless, what’s important is that by performing maximum likelihood on the partial likelihood \\(L_1(\\beta)\\) we can replicate the same regression parameters as if we were maximizing the full likelihood. That is, the maximum we find with the partial likelihood, \\(\\hat{\\beta}\\), converges to the original parameter, \\(\\beta\\)."
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html#kaplan-meier-estimator",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html#kaplan-meier-estimator",
    "title": "Introduction to Survival Analysis",
    "section": "Kaplan Meier Estimator",
    "text": "Kaplan Meier Estimator\n\nThe Kaplan-Meier estimator estimates the survival probability of a population over time by calculating the proportion of individuals that have not experienced the event of interest at each time point.\n\nThe Kaplan Meier’s (consistent, unbiased and efficient) estimate Survival Curve will be a quick check to understand how the survival probabilities change across time in our sample.\n\n# Fit a Kaplan-Meier estimator for each level of the covariate\nfit <-\n  survminer::surv_fit(Surv(y, failed) ~ 1, data = data)\n\n# Plot the Kaplan-Meier estimators\nggsurvplot(fit, ggtheme = theme_pred(), color = clrs[[1]], \n           surv.median.line = 'hv',\n           risk.table = TRUE,\n           conf.int = TRUE,\n           title = \"Kaplan Meier: Non Parametric estimate\"\n           )\n\n\n\n\nWe can see that there’s a huge non-linearity across time that the Kaplan Meier estimator correctly caputres!"
  },
  {
    "objectID": "posts/survival-analysis/2022-11-27-introduction-survival.html#cox-proportional-hazard-model",
    "href": "posts/survival-analysis/2022-11-27-introduction-survival.html#cox-proportional-hazard-model",
    "title": "Introduction to Survival Analysis",
    "section": "Cox Proportional Hazard model",
    "text": "Cox Proportional Hazard model\n\nFitting the model\nLet’s finally fit our model!\n\nNote that as long as the Cox Proportional Hazard assumptions are met (proportional hazards and we’ve correctly specified the linear regression in the risk function), the Cox model is consistent and asymptotically unbiased, meaning that the parameter estimates converge to the true values as the sample size increases.\n\n\ncox_fit <- coxph(Surv(y, failed) ~ X1 + X2 + X3, data = data)\nsummary(cox_fit)\n\nCall:\ncoxph(formula = Surv(y, failed) ~ X1 + X2 + X3, data = data)\n\n  n= 2000, number of events= 1790 \n\n        coef exp(coef)  se(coef)      z Pr(>|z|)    \nX1  0.008528  1.008565  0.047990  0.178    0.859    \nX2  0.422365  1.525565  0.048643  8.683  < 2e-16 ***\nX3 -0.389458  0.677424  0.049633 -7.847 4.27e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n   exp(coef) exp(-coef) lower .95 upper .95\nX1    1.0086     0.9915    0.9180    1.1080\nX2    1.5256     0.6555    1.3868    1.6782\nX3    0.6774     1.4762    0.6146    0.7466\n\nConcordance= 0.578  (se = 0.008 )\nLikelihood ratio test= 129.8  on 3 df,   p=<2e-16\nWald test            = 129.7  on 3 df,   p=<2e-16\nScore (logrank) test = 129.7  on 3 df,   p=<2e-16\n\n\nNote that, by default, we don’t have an intercept, as it would just be a constant shift to the baseline hazard, and not a “real” regression coefficient.\n\n\nInterpreting the Results\nGiven that we simulated the data to make sure that the assumptions are met, our estimates are right on the money! The table results above show the huge uncertainty in the x1 parameter, and a non-statistically significant result.\nIf we remember, we specified the following betas for our simulation:\n\nsim.data$betas\n\n     [,1]\n[1,]  0.0\n[2,]  0.4\n[3,] -0.4\n\n\nAnd here are the model coefficients that pretty nicely line-up with what we know is the truth!\n\nround(cox_fit$coefficients, 4)\n\n     X1      X2      X3 \n 0.0085  0.4224 -0.3895 \n\n\nHowever, the above are coefficients in the log hazard scale, which is very difficult to interpret.\n\nThe results are shown in two scales: the log hazard scale, and the hazard scale, respectively. The Hazard scale can be interpreted as the hazard ratio for a 1-unit increase in the continuous variable, or the hazard ratio for the dummy variable relative to the control variable.\n\nLet’s plot the results in the Hazard Scale:\n\n# Visualize the estimated coefficients\nggforest(cox_fit, data=data, main=\"Forest Plot for coefficients in Hazard Scale\") + \n  theme_pred()\n\n\n\n\nGiven that the results are in the hazard scale and we are assuming proportional hazards, we can interpret the coefficients as hazard ratios: e.g., a coefficient of 1 indicates no difference a hazard ratio of 1 for the change in question.\n\nFor x1, just as expected, the 0 coefficient in the log hazard scale means that an unit increase in X1 is associated with no increase in the proportional hazard.\nWhereas for X2, we find a hazard ratio of 1.53, meaning that an unit increase in X1 is associated with a 53% increase in the hazard, an increase that we assume in constant through time."
  },
  {
    "objectID": "posts/gams/simon-wood-summary.html",
    "href": "posts/gams/simon-wood-summary.html",
    "title": "Notes from Simon Wood’s Generalized Additive Models",
    "section": "",
    "text": "Simon Wood is a world expert on GAMs and creator of the fantastic mgcv package in R. His book on GAMs is a great introduction to the subject. I’ve been re-reading it and taking notes to be able to better teach this to my co-workers. This post is a summary of the notes I’ve taken so far. They are questions that I try to ask myself to make sure that I have a good mental model of the subject. I’ve tried to make them as general as possible, but they are still biased towards my own understanding of the subject."
  },
  {
    "objectID": "posts/gams/simon-wood-summary.html#generalized-additive-model-gam",
    "href": "posts/gams/simon-wood-summary.html#generalized-additive-model-gam",
    "title": "Notes from Simon Wood’s Generalized Additive Models",
    "section": "Generalized Additive Model (GAM)",
    "text": "Generalized Additive Model (GAM)\nQ: What is a Generalized Additive Model?\nA: A linear model with a linear predictor involving a sum of smooth functions of covariates.\nQ: What are the two problems of using GAMs?\nA: 1. How to represent the smooth functions? 2. How smooth these functions should be?\nQ: What’s the traditional solution to the above problems?\nA: 1. Represent them via basis expansions for each smooth with estimation via penalized regression methods. 2. Estimate the function smoothness via cross validation or marginal likelihood maximization.\nQ: What’s the restriction on how we represent a smooth to be estimated as a linear model?\nA: f must be represented as a linear model. Choose a basis expansion of which f is an element of. Estimate parameters for each function in the basis, and the resulting f is the linear combination using these parameters. \\[ f(x) = \\sum_{j=i}^{k} b_j (x) \\beta_j\\].\nQ: Why are polynomial basis expansions frowned upon as it comes to GAMs?\nA: Polynomial basis expansions are one possible choice for these functions. However, they have certain limitations: 1. Runaway Behavior: Polynomials are unbounded and can have extreme values far from the data points, which can cause model instability and poor extrapolation. This is especially problematic at the boundaries of the data range. 2. Global Influence: A change in the data at one point can affect the polynomial fit at all points. In contrast, local regression techniques (like splines or kernel smoothing) used in GAMs are less influenced by distant points. 3. Inflexibility: Polynomials of low degree may not be flexible enough to capture complex non-linear relationships, while high degree polynomials can overfit the data and create oscillations. 4. Curse of Dimensionality: When dealing with multivariate data, the number of polynomial terms grows exponentially with the degree of the polynomial and the number of variables. This can make the model very complex and computationally expensive.\nQ: How can we determine the resulting smoothness of the linear combination of the functions in the basis expansion?\nA: By using penalized regression and adding a penalty on the “wiggliness” (i.e., integrated squared second derivative in the case of cubic splines). Then, the problem of estimating the wiggliness becomes the problem of adding a hyperparameter to control this penalty.\nQ: Given the penalized regression approach to estimate the wiggliness of the resulting linear combination of the basis expansion functions, what’s Simon Woods’ advice to choosing the dimension of the basis expansion?\nA: Choose a large enough k such that the basis is more flexible than we expect to need to represent f(x), then neither the exact choice of k, nor the precise location of the knots, has a great deal of influence on the model fit. Rather, it is the penalty hyperparameter.\nQ: How can we justify using Restricted Maximum Likelihood (REML) to estimate the wiggliness hyperparameter?\nA: By positing a prior on the wiggliness of the linear combination of the basis expansion, we can estimate the model as a mixed model. Then, we can use REML to estimate the hyperparameter as the variance of random effects.\nQ: What alternative to REML can we use to estimate the wiggliness hyperparameters?\nA: We can use Cross Validation. In particular, Generalized Cross Validation to avoid the computational cost of LOO-CV.\nQ: Expand on why adding more than one smooth in a GAM introduces an identifiability problem?\nA: The identifiability problem arises in the context of Generalized Additive Models (GAMs) when we have more than one smooth term in the model. This is because there can be multiple different combinations of smooth functions that give the same fitted values, which means the model is not identifiable. To understand this, consider a simple GAM with two smooth terms: \\[\ny = f_1(x_1) + f_2(x_2) + \\varepsilon\n\\] Now imagine that we add a constant $ c $ to $ f_1(x_1) $ and subtract the same constant from $ f_2(x_2) $. The fitted values remain the same: \\[\ny = [f_1(x_1) + c] + [f_2(x_2) - c] + \\varepsilon = f_1(x_1) + f_2(x_2) + \\varepsilon\n\\] However, the functions \\(f_1(x_1)\\) and \\(f_2(x_2)\\) have changed. This means there are multiple different sets of functions \\(f_1(x_1)\\) and \\(f_2(x_2)\\) that give the same fitted values, so the model is not identifiable.\nQ: What is a common solution to the identifiability problem introduced by having multiple smooths in a GAM?\nA: A common solution to this problem is to impose a constraint that the mean of each smooth function is zero over the range of the data. This effectively removes the freedom to add or subtract constants from the functions. The result is that each function can only change the shape of the fitted values, not the overall level. This makes the model identifiable: there is now a unique set of functions \\(f_1(x_1)\\) and \\(f_2(x_2)\\) that give the fitted values.\nQ: What interpretation change of the smooths must happen once we add the constraint of mean of each smooth be zero over the range of data to deal with identifiability constraints?\nA: This constraint has a nice interpretation: it means that each smooth function represents the deviation from the overall mean response. It does not change the overall level of the response, only its shape. This is consistent with the idea that the smooth functions in a GAM capture the non-linear effects of the predictors.\nQ: How does the identifiability constraint limits the maximum dimension for the basis?\nA: It changes that from any k we choose to k-1, due to the zero centering that we need to impose on it if there are more than one spline.\nQ: What’s a tricky point of setting the dimension k regarding basis with smaller dimensions k?\nA: A k=20 will contain a larger number of functions with effective degrees of freedom=5 than a space with k=10.\nQ: If reducing the k in the smooth won’t necessarily constrain the model to a simpler spline, what can we do to induce larger regularization of the smooth when using the Generalized Cross Validation?\nA: We can use gamma hyperparameter to increase the penalty per degree of freedom increases in the GCV and thus increasingly smooth models are produced.\nQ: In mgcv, when using smooths over several variables, what type of basis expansion are we using?\nA: Tensor products of the marginal basis.\nQ: In mgcv, when specifying the k for a tensor product, what’s the resulting dimension?\nA: The product of the dimension of each of the marginal basis. You can define a marginal basis dimension per variable.\nQ: Name three possible kind of basis expansions (smoothers) to use in GAMs?\nA: - Local Linear basis. - Polynomial - Spline bases\nQ: What’s the least squares method used in GAMs for the penalized likelihood maximization?\nA: Penalized iteratively re weighted least squares (PIRLS)\nQ: What are cubic splines?\nA: A cubic spline is a curve constructed from sections of cubic polynomial joined together so that the curve is continuous up to second derivative.\nQ: What’s the difference between cubic splines interpolators and cubic smoothing splines?\nA: Treat the exact values that the function will take at the control points as n free parameters and estimate them by minimizing the squared error of them plus the integral of the second derivative controlled by a hyper parameter lambda.\nQ: How do B splines improve upon natural polynomial splines?\nA: It modifies the basis functions so their effects are strictly local: each basis function is only nonzero over the intervals between m + 3 adjacent knots.\nQ: What are the improvements of P splines over B Splines?\nA: P-splines extend B-splines by adding a penalty on the differences of the coefficients of the B-splines. This penalty typically targets the second or higher-order differences to ensure the resulting curve is smooth. The idea is to strike a balance between fitting the data closely (like B-splines) and keeping the curve smooth (through the penalty).\nQ: What does it mean for a model smooth to be cyclic?\nA: The resulting smooth has the same value and first few derivatives at its upper and lower boundaries.\nQ: What are adaptive smoothers in the context of GAMs?\nA: A spline where the level of smoothing depends on the values of the covariates.\nQ: Why do most smoothing penalties in GAM have problems outputting the zero function? What can we do about it?\nA: Some non-zero functions are in the null space (because they are considered completely smooth). If we want to encourage a zero function we can add an extra penalty that penalizes functions in the null space. The select argument in mgcv does that.\nQ: What are isotropic smooths in the context of GAMs?\nA: Smooths that in the multiple covariate case produce identical predictions of the response variable under any rotation or reflection of the covariates.\nQ: How do we arrive at the knots selections and basis functions from thin plate splines?\nA: These emerge naturally from the mathematical statement of the smoothing problem.\nQ: What type of basis functions arise out of the thin plate spline formulation?\nA: Radial basis functions.\nQ: What’s the drawback of thin plate splines?\nA: Their computational cost.\nQ: What’s the idea behind thin plate regression splines that reduce their computational cost?\nA: Truncating the space of wiggly components of the thin plate spline while leaving the components of zero wiggliness unchanged.\nQ: When are soap film smoothing useful in gams?\nA: When it’s important to not smooth across boundaries.\nQ: What’s the trick to construct orthogonal non isotropic smooth for interactions of variables?\nA: The marginal bases due to identifiability constraints cannot have a zero. Thus, they don’t have the unit vector and thus the product of the basis cannot contain the main effects."
  },
  {
    "objectID": "bayesian-statistics.html",
    "href": "bayesian-statistics.html",
    "title": "Series: Bayesian Statistics",
    "section": "",
    "text": "BDA Week 9: Large Sample Theory for the Posterior\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nBDA Week 8: Bayesian Decision Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\nBDA week 7: LOO and its diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 5 -> Metropolis\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 4 -> Importance Sampling\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 3-> Exercises\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 3 -> Fitting a Gaussian probability model\n\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 2\n\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 10\n\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 9\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nBayesian Instrumental Variable Regression\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nUnderstanding Pooling across Intercepts and Slopes\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 8\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nSimulating into understanding Multilevel Models\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 7\n\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n16 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 6\n\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2020\n\n\n16 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 5 -> HMC samples\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 5 -> Interactions\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 4\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 3\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 2\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 1\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Posts",
    "section": "",
    "text": "The Bayesian Statistics Series are blogposts that walk you through the awesome homeworks in the Statistical Rethinking book, by Richard McElreath, and the classic Bayesian Data Analysis, by Gelman et. al. The code is all fully reproducible R Code.\n\n\n\nFollowing the work by Nassim Nicholas Taleb, this series of blogposts goes over his work showcasing his main results and intuition through code and visualizations.\n\n\n\nFollowing the work of Judea Pearl, I go over his main contributions to Causal Inference using code, a bit of math, and visualizations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am David Salazar and I am a Machine Learning Engineer dilettantin’ Data Science."
  },
  {
    "objectID": "fat-vs-thin-tails.html",
    "href": "fat-vs-thin-tails.html",
    "title": "Series: Fat Tails vs Thin Tails",
    "section": "",
    "text": "Forecasting elections? Taleb says no\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nTail Risk of diseases in R\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\n14 min\n\n\n\n\n\n\n\n\nGini Index under Fat-Tails\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nProbability Calibration under fat-tails: useless\n\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nExtreme Value Theory for Time Series\n\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nWhen are GARCH (and friends) models warranted?\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nHow to not get fooled by the “Empirical Distribution”\n\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nFisher Tippet Th: a “CLT” for the sample maxima\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2020\n\n\n8 min\n\n\n\n\n\n\n\n\nLLN for higher p Moments\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nCentral Limit Theorem in Action\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2020\n\n\n15 min\n\n\n\n\n\n\n\n\nR-squared and fat tails\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nCorrelation is not Correlation\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2020\n\n\n13 min\n\n\n\n\n\n\n\n\nUnderstanding the tail exponent\n\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nStandard Deviation and Fat Tails\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nWhat does it mean to fatten the tails?\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nSpurious PCA under Thick Tails\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nFat vs Thin: does LLN work?\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2020\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "causality.html",
    "href": "causality.html",
    "title": "Series:CCausal Inference",
    "section": "",
    "text": "Causality: Mediation Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nCausality: Probabilities of Causation\n\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nCausality: Regret? Look at Effect of Treatment on the Treated\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nCausality: Counterfactuals - Clash of Worlds\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nCausality: Testing Identifiability\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2020\n\n\n13 min\n\n\n\n\n\n\n\n\nCausality: The front-door criterion\n\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n8 min\n\n\n\n\n\n\n\n\nCausality: To adjust or not to adjust\n\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nCausality: Invariance under Interventions\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nCausality: Bayesian Networks and Probability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2020\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "David Salazar",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFat vs Thin: does LLN work?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nSpurious PCA under Thick Tails\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nWhat does it mean to fatten the tails?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 5 -> Interactions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nStandard Deviation and Fat Tails\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 5 -> HMC samples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nUnderstanding the tail exponent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2020\n\n\n16 min\n\n\n\n\n\n\n\n\nCorrelation is not Correlation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2020\n\n\n13 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n16 min\n\n\n\n\n\n\n\n\nR-squared and fat tails\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nSimulating into understanding Multilevel Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nCentral Limit Theorem in Action\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2020\n\n\n15 min\n\n\n\n\n\n\n\n\nUnderstanding Pooling across Intercepts and Slopes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nLLN for higher p Moments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBayesian Instrumental Variable Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nStatistical Rethinking: Week 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nStatistical Rethinking Week 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nFisher Tippet Th: a “CLT” for the sample maxima\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2020\n\n\n8 min\n\n\n\n\n\n\n\n\nHow to not get fooled by the “Empirical Distribution”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nWhen are GARCH (and friends) models warranted?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nExtreme Value Theory for Time Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 3 -> Fitting a Gaussian probability model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nProbability Calibration under fat-tails: useless\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 3-> Exercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nGini Index under Fat-Tails\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 4 -> Importance Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBayesian Data Analysis: Week 5 -> Metropolis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nBDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nTail Risk of diseases in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\n14 min\n\n\n\n\n\n\n\n\nBDA week 7: LOO and its diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBDA Week 8: Bayesian Decision Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\nBDA Week 9: Large Sample Theory for the Posterior\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nCausality: Bayesian Networks and Probability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2020\n\n\n18 min\n\n\n\n\n\n\n\n\nCausality: Invariance under Interventions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nCausality: To adjust or not to adjust\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nCausality: The front-door criterion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n8 min\n\n\n\n\n\n\n\n\nCausality: Testing Identifiability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2020\n\n\n13 min\n\n\n\n\n\n\n\n\nCausality: Counterfactuals - Clash of Worlds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nCausality: Regret? Look at Effect of Treatment on the Treated\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nCausality: Probabilities of Causation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nCausality: Mediation Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nForecasting elections? Taleb says no\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nIntroduction to Survival Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\nNotes from Simon Wood’s Generalized Additive Models\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  }
]