{
  "hash": "89d4caa5479ded805654fc4b71258925",
  "result": {
    "markdown": "---\ntitle: 'Causality: The front-door criterion'\nauthor: ''\ndate: '2020-07-30'\nslug: causality-the-front-door-criterion\ncategories: []\ntags: []\naliases: \n  - ../../2020/07/30/causality-the-front-door-criterion/\n---\n\n\n\n\n\n## Motivation\n\nIn a [past blogpost](https://david-salazar.github.io/2020/07/25/causality-to-adjust-or-not-to-adjust/), I've explore the backdoor criterion: a simple *graphical algorithm*, we can define **which variables we must include** in our analysis in order to **cancel out all the information coming from different causal relationships than the one we are interested**. However, these variables are not always measured. What else can we do?\n\nIn this blogpost, I'll explore the front-door criterion: i) an intuitive proof of why it works; (ii) how to estimate it; (iii) what are its fundamental assumptions; finally, (iv) an experiment with monte-carlo samples. Whereas the back-door criterion blocks all the non-causal information that $X$ could possibly pick up, the **front-door exploits the outgoing information from $X$** to derive a causal estimator. \n\n## The limits of the back-door: a quick example\n\nLet's assume the following DAG, which is a darling of Pearl's work. Does smoking cause cancer?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexample <- dagify(x ~ u,\n                  m ~ x,\n                  y ~ u + m,\n                  labels = c(\"x\" = \"Smoking\",\n                             \"y\" = \"Cancer\",\n                             \"m\" = \"Tar\",\n                             \"u\" = \"Genotype\"),\n                  latent = \"u\",\n                  exposure = \"x\",\n                  outcome = \"y\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nGiven that we cannot measure genotype, we cannot use the back-door criterion to stop $Smoking$ from picking up the causal effect of $Genotype$. Therefore, one can not use the back-door criterion to **ascertain which portion of the observed association** between smoking and cancer is *spurious* (because it is attributable to their common cause, Genotype) and what portion is *genuinely causative*. \n\n## Consecutive applications of the back-door criterion\n\nHowever, we notice that we can use the *back-door criterion* to estimate two partial effects: $X \\rightarrow M$ and $M \\rightarrow Y$. **By chaining** these two partial effects, we can obtain *the overall effect* $X \\rightarrow Y$.\n\nThe intuition for the chaining is thus: intervening on the levels of tar in the lungs lead to different probabilities of cancer: $P(Y = y | \\text{do(M = m)})$. However, the levels of tar are themselves determined by how much someone smokes: $P(M= m| \\text{do(X = x)})$. Therefore, by intervening on smoking to determine the levels of tar we can estimate the causal effect of smoking. \n\nWe intervene on smoking and check the respective effect for each value of tar:\n\n$$\nP(Y \\mid d o(X))=\\sum_{M} P(Y \\mid M, d o(X)) \\times P(M \\mid d o(X))\n$$\n\nBecause smoking blocks all the back-door paths from tar into cancer, we can replace the conditioning expression by an intervention expression in the first term.$P(Y \\mid M, d o(X))=P(Y \\mid d o(M), d o(X))$. Given that intervening on smoking, once we have intervened on tar has no effect on cancer, we can also write $P(Y \\mid M, d o(X))=P(Y \\mid d o(M), d o(X))=P(Y \\mid d o(M))$.\n\nGiven that smoking blocks all backdoor paths into tar, we can estimate $P(Y \\mid d o(M))$ using the back-door adjustment:\n\n$$\nP(Y \\mid d o(M))=\\sum_{X} P(Y \\mid X, M) \\times P(X)\n$$\n\nTherefore, we can re-write $P(Y \\mid d o(X))$ thus:\n\n$$\nP(Y \\mid d o(X)) = \\sum_{M} P(M | do(X)) \\sum_{X'} P(Y \\mid X', M) \\times P(X')\n$$\n\nConsidering that there are no backdoor paths from smoking to tar, we can write $P(M | do(X)) = P(M | X)$. Therefore, we can re-write our entire expression for $P(Y \\mid d o(X))$ in terms of pre-intervention probabilities:\n\n$$\nP(Y \\mid d o(X)) = \\sum_{M} P(M | X) \\sum_{X'} P(Y \\mid X', M) \\times P(X')\n$$\n\nThis is the **front-door formula**. \n\n## Empirical estimation\n\n[Conceptually](https://www.canr.msu.edu/afre/events/Bellemare%20and%20Bloem%20(2020).pdf), \n\n> the FDC [Front-door Criterion] approach works by first estimating the effect of X on M, and then estimating the effect of M on Y holding X constant. Both of these effects are unbiased because *nothing confounds* the effect of X on M and X blocks *the only back-door path* between M on Y. **Multiplying these effects by one another yields the FDC estimand**.\n\nTherefore, in a regression setting we can estimate the causal effect using the Average Treatment Effect (ATE) via the FDC thus. Formulate two linear regressions:\n\n\\[\nM_{i}=\\kappa+\\gamma X_{i}+\\omega_{i}\n\\]\nand\n\\[\nY_{i}=\\lambda+\\delta M_{i}+\\phi X_{i}+v_{i}\n\\]\n\nOur estimate of the ATE is given by: \n\n$$\nATE = E[Y|do(X)] = \\delta \\times \\gamma\n$$\n\n## When can we use the Front-Door criterion?\n\nWe've given an intuitive proof of the Front-door criterion and given an empirical estimation technique. But what exactly have we presupposed that allowed us to do all of this? In other words, **what are the fundamental assumptions behind the criterion?**\n\nA set of variables \\( Z \\) is said to satisfy the front-door criterion relative to an ordered pair of variables \\( (X, Y) \\) if:\n\n1. Z intercepts all directed paths from \\( X \\) to \\( Y \\).\n\n1. There is no backdoor path from \\( X \\) to \\( Z \\)\n\n1. All backdoor paths from Z to \\( Y \\) are blocked by X.\n\nWhen these conditions are met, we can use the Front-Door criterion to estimate the causal effect of $X$. \n\n## A Monte-Carlo experiment\n\nLet's work a Monte-Carlo experiment to show the power of the backdoor criterion. Consider the following DAG:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag <- downloadGraph(\"dagitty.net/m331\")\nggdag(dag) +\n  labs(title = \"We only need to measure W to estimate the effect of X on Y\")\n```\n\n::: {.cell-output-display}\n![](2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAssume that only $X, Y$, and one additional variable can be measured. Which variable would would allow the identification of the causal effect $X$ on $Y$? The answer is all in the front-door criterion! We only need to measure $W$ to be able to estimate the effect. Notice that:\n\n1. $W$ intercepts all the direct paths from $X$ into $Y$.\n\n1. There is no backdoor path from $X$ into $W$.\n\n1. All back-door paths from $W$ into $Y$ are blocked \n\nTherefore, we can use the front-door criterion. \n\nLet's use a Monte-Carlo simulation to confirm the answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 500\nb <- rnorm(n)\nc <- rnorm(n)\nz <- -2*b +2*c + rnorm(n)\na <- 3*b + rnorm(n)\nd <- -3*c + rnorm(n)\nx <- 4+a +2*z + rnorm(n)\nw <- -2*x + rnorm(n)\ny <- z + 2*w + d + rnorm(n)\n\ndata <- data.frame(b, c, z, a, d, x, w, y)\n```\n:::\n\n\nIn this simulated dataset, the causal effect of unit of $X$ on $Y$ is -4. Let's recuperate this effect by using the back-door criterion to find out for which variables we must control.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdag_adjustment_set(dag, outcome = \"Y\", exposure = \"X\")\n```\n\n::: {.cell-output-display}\n![](2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\nLet's take the first adjustment set. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_backdoor <- stan_glm(y ~ x + a + z, data = data, refresh = 0)\nmodel_backdoor %>% \n  spread_draws(x) %>% \n  ggplot(aes(x)) +\n  stat_halfeye(alpha = 0.6) + \n  hrbrthemes::theme_ipsum_rc(grid = \"y\") +\n  geom_vline(aes(xintercept = -4), linetype = 2, color = \"red\") +\n  annotate(\"text\", x = -4.08, y = 0.7, label = \"True causal effect\", color = \"red\", \n           family = theme_get()$text[[\"family\"]]) +\n  labs(title = \"Causal inference from Model y ~ x + a + z\",\n       subtitle = \"We deconfound our estimates by conditioning on a and z\")\n```\n\n::: {.cell-output-display}\n![](2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nGiven our DAG, a testable implication is that we must arrive at the same answer by using the front-door criterion. **Remember that it is just a consecutive use of the back-door criterion that translates into two regressions**. Therefore, we can use a multi-variable model thus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_frontdoor <- ulam(\n  alist(\n    c(Y, W) ~ multi_normal(c(muY, muW), Rho, Sigma),\n    muY <- alphaY + delta*W,\n    muW <- alphaW + gamma*X, \n    gq> ate <- gamma * delta, # calculate ate directly in stan\n    c(alphaY, alphaW) ~ normal(0, 0.2),\n    c(gamma, delta) ~ normal(0, 0.5),\n    Rho ~ lkj_corr(2),\n    Sigma ~ exponential(1)\n  ),\n  data = list(Y = data$y, X = data$x, W = data$w), chains = 4, cores = 4, iter = 3000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 3000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 1 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 4 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 2 Iteration:  100 / 3000 [  3%]  (Warmup) \nChain 3 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 1 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 4 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 2 Iteration:  200 / 3000 [  6%]  (Warmup) \nChain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) \nChain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) \nChain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) \nChain 3 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 4 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) \nChain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 3 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 4 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) \nChain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 3 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 4 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) \nChain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 3 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 4 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) \nChain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 3 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 4 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) \nChain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 3 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 4 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) \nChain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 4 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 3 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) \nChain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 4 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 3 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) \nChain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 3 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 4 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) \nChain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 3 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 4 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) \nChain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) \nChain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) \nChain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) \nChain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) \nChain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) \nChain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) \nChain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) \nChain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) \nChain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) \nChain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) \nChain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) \nChain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) \nChain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) \nChain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 1 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 1 finished in 13.5 seconds.\nChain 3 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 3 finished in 13.6 seconds.\nChain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) \nChain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) \nChain 4 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 4 finished in 14.2 seconds.\nChain 2 Iteration: 3000 / 3000 [100%]  (Sampling) \nChain 2 finished in 14.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 13.9 seconds.\nTotal execution time: 14.7 seconds.\n```\n:::\n\n```{.r .cell-code}\nprecis(model_frontdoor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              mean          sd        5.5%      94.5%    n_eff     Rhat4\nalphaW  0.01317289 0.055819588 -0.07701513  0.1010920 4808.342 0.9996915\nalphaY  0.10889162 0.131415392 -0.10501164  0.3210728 4608.519 1.0008755\ndelta   2.00223411 0.011824792  1.98296890  2.0211506 5199.217 1.0003623\ngamma  -1.99645002 0.008484629 -2.01018000 -1.9829200 4639.059 0.9998826\nate    -3.99735509 0.028352648 -4.04329220 -3.9517884 5258.228 0.9998829\n```\n:::\n:::\n\nAnd indeed, we arrive at the same answer! \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## References\n\nBesides Chapter 3 of Pearl's Causality, I found this [terrific paper (PDF)](https://www.canr.msu.edu/afre/events/Bellemare%20and%20Bloem%20(2020).pdf) by Bellemare and Bloem. \n\n\n",
    "supporting": [
      "2020-07-30-causality-the-front-door-criterion_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}