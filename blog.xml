<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>David Salazar</title>
<link>https://david-salazar.github.io/blog.html</link>
<atom:link href="https://david-salazar.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>This is David&#39;s Salazar personal blogpost.</description>
<generator>quarto-1.2.262</generator>
<lastBuildDate>Thu, 03 Sep 2020 05:00:00 GMT</lastBuildDate>
<item>
  <title>Forecasting elections? Taleb says no</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html</link>
  <description><![CDATA[ 




<p>With US elections around the corner, news outlets are constantly deploying new models to try to predict who will win the next presidential elections. Ever since Trump’s ‘surprising’ win in ’16, these (pre-election) polls based models have <a href="http://centerforpolitics.org/crystalball/articles/poll-based-election-forecasts-will-always-struggle-with-uncertainty/">come under scrutiny</a>. Indeed, there is a huge amount of uncertainty that these models do not seem to capture well enough.</p>
<p><a href="https://www.researchgate.net/publication/335003159_All_roads_lead_to_quantitative_finance">Taleb’s and Dhruv Madeka’s</a> point is the following: whilst forecasting an uncertain election, one <strong>cannot invoke probabilistic thinking</strong> unless one imposes severe <strong>constraints</strong> on how the <em>forecast</em> will move up to election day. If one’s forecast is having <em>large swings</em> across time, then, one’s forecast is not coherent with the basic tenet of subjective probability as espoused by De Finetti.</p>
<p>In this blogpost, I’ll explain what is the <strong>basic tenet of subjective probability</strong> according to De Finetti and explain how Taleb expands on this definition to verify what are the consequences that every <em>probabilistic forecast</em> mus satisfy to be considered <strong>coherent forecasts</strong>. At the end, we will arrive at the surprising result that uncertain and highly volatile elections must result in very stable 50/50 forecasts.</p>
<section id="de-finettis-argument" class="level2">
<h2 class="anchored" data-anchor-id="de-finettis-argument">De Finetti’s argument</h2>
<p>De Finetti (an italian writing in French) argued that any subjective probability statement defines a <strong>bet</strong> that one would be willing to accept. That is, subjective probability statements must be backed up by <strong>skin in the game</strong>. In his own words in <a href="http://www.numdam.org/article/AIHP_1937__7_1_1_0.pdf"><em>La prévision: ses lois logiques, ses sources subjectives</em> (PDF)</a></p>
<blockquote class="blockquote">
<p>Il s’agit simplement de préciser mathématiquement l’idée banale et évidente que le degré de probabilité attribué par un individu à un événement donné est révélé par les conditions dans lesquelles il serait disposé de parier sur cet événement</p>
</blockquote>
<blockquote class="blockquote">
<p>Supposons qu’un individu soit obligé d’évaluer le prix p pour lequel il serait disposé d’échanger la possession d’une somme quelconque S (positive ou négative) subordonnée à l’arrivée d’un événement donné, E, avec la possession de la somme p S ; nous dirons par définition que ce nombre p est la mesure du degré de probabilité attribué par l’individu considéré à l’événement E, ou, plus simplement, que p est la probabilité de E.</p>
</blockquote>
<p>Indeed, from this simple statement, all the axioms of probability follow:</p>
<blockquote class="blockquote">
<p>de telle façon que toute la théorie des proba- bilités puisse se déduire immédiatement d’une condition très naturelle ayant une signification évidente</p>
</blockquote>
<p>To be a coherent statement, this <strong>bet must preclude the possibility of arbitrage</strong>. That is, an opposite bettor cannot create a risk-free series of bets such that he always makes a profit.</p>
<blockquote class="blockquote">
<p>Ceci posé, lorsqu’un individu a évalué les probabilités de certains événements, deux cas peuvent se présenter : ou bien il est possible de parier avec lui en s’assurant de gagner à coup sûr, ou bien cette possibilité n’existe pas. Dans le premier cas on doit dire évidemment que l’évaluation de la probabilité donnée par cet individu contient une incohérence, une contradiction intrinsèque; dans l’autre cas, nous dirons que l’individu est cohérent.</p>
</blockquote>
<p>Therefore, the <strong>only coherent probabilistic statements are the ones that imply a bet that precludes arbitrage</strong>.</p>
<blockquote class="blockquote">
<p>C’est précisément cette condition de cohérence qui constitue le seul principe d’où l’on puisse déduire tout le calcul des probabilités : ce calcul apparaît alors comme l’ensemble des règles auxquelles l’évaluation subjective des probabilités de divers événements par un même individu doive être assujettie si l’on ne veut pas qu’il y ait entre elles une contradiction fondamentale.</p>
</blockquote>
</section>
<section id="taleb-expands-de-finettis-argument" class="level2">
<h2 class="anchored" data-anchor-id="taleb-expands-de-finettis-argument">Taleb expands De Finetti’s argument</h2>
<p>Whilst De Finetti worked with <em>one</em> forecast, Taleb expands this <strong>coherence argument</strong> into a series of supposedly probabilistic statements that create a <strong>series of forecast</strong> leading up to the election. Intuitively, highly uncertain elections are uncertain because in whatever time is left, a lot of events can happen that can decide an election.</p>
<p>Given a forecast, how should this <em>forecast evolve through time</em>? Should it be swayed by every piece of evidence into forecasting a <em>clear winner</em>, even months <strong>ahead of the election</strong>? Or should it <em>remain skeptical</em> up to <strong>last minute</strong>? Using <strong>De Finetti’s skin in the game argument</strong>, Taleb and Madeka argue that the problem is equivalent to that of pricing a <a href="https://statmodeling.stat.columbia.edu/2020/06/19/forecast-betting-odds/">binary option on the election date</a>.</p>
<p>Imposing <em>arbitrage boundaries</em> lead us to the conclusion that the more volatile the election, <strong>our forecast must be around 50/50 and very stable (low volatility)</strong>, despite what the current evidence may incline one to believe.</p>
<p><img src="https://david-salazar.github.io/images/update.PNG" class="img-fluid"></p>
<p>Otherwise, the future revised estimates <em>with large swings</em> open up the <strong>possibility of inter-temporal arbitrage</strong> by buying and selling from the “assessor” that is forecasting. From the comic of the <a href="https://twitter.com/alanklement/status/1300515345430609922">Black Swan Man</a>:</p>
<p><img src="https://david-salazar.github.io/images/swanman.jfif" class="img-fluid"></p>
<p>Therefore, a series of forecasts are only coherent if they preclude the possibility of arbitrage. And in the case of highly uncertain elections, the only way to make this happen is to forecast a very stable 50/50 that is only updated at the very last minute.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-09-03-forecasting-elections-taleb-says-no.html</guid>
  <pubDate>Thu, 03 Sep 2020 05:00:00 GMT</pubDate>
  <media:content url="https://david-salazar.github.io/images/update.PNG" medium="image"/>
</item>
<item>
  <title>Causality: Mediation Analysis</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis.html</link>
  <description><![CDATA[ 




<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(ggdag)</span>
<span id="cb1-3">extrafont<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">loadfonts</span>(<span class="at" style="color: #657422;">device=</span><span class="st" style="color: #20794D;">"win"</span>)</span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;">theme_set</span>(<span class="fu" style="color: #4758AB;">theme_dag</span>(<span class="at" style="color: #657422;">base_family =</span> <span class="st" style="color: #20794D;">"Roboto Condensed"</span>))</span></code></pre></div>
</div>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Kids are the prototypical question makers; they never stop asking questions. Just after you have answered a <strong>Why?</strong> question, they ask yet another <em>Why</em>? This is the problem of mediation analysis: if you answer that <em>X causes Y</em>, how does exactly <strong>the causal mechanism work</strong>? Is the causal <strong>effect direct or mediated</strong> through yet another variable M? Mediation analysis aims to disentangle the <strong>direct effect</strong> (which does not pass through the mediator) from the indirect effect (the part that passes through the mediator).</p>
<p>Judah Pearl has formulated an answer to the mediation problem by using counterfactuals. By giving precise counterfactual interpretations to both the <strong>Natural Direct Effects (NDE)</strong> and the <strong>Natural Indirect Effects (NIE)</strong>, we can use the machinery of Causal Inference to solve the mediation problem.</p>
<p>In this post, we’ll study the <strong>counterfactual definition and identification criteria</strong> behind <em>direct and indirect effects</em>. Finally, we’ll solve a numerical example to put what we have learnt into practice.</p>
<p>All quotes come from Chapter 9 of Pearl’s Causality and Chapter 4 of his primer.</p>
</section>
<section id="counterfactual-definitions" class="level2">
<h2 class="anchored" data-anchor-id="counterfactual-definitions">Counterfactual Definitions</h2>
<p>We will use the following canonical Structural Model for a mediation problem to define the following direct and indirect effects.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0At=f_%7BT%7D%5Cleft(u_%7BT%7D%5Cright)%20%5Cquad%20m=f_%7BM%7D%5Cleft(t,%20u_%7BM%7D%5Cright)%20%5Cquad%20y=f_%7BY%7D%5Cleft(t,%20m,%20u_%7BY%7D%5Cright)%0A"> Let <img src="https://latex.codecogs.com/png.latex?T"> be a binary treatment.</p>
<section id="control-freak" class="level3">
<h3 class="anchored" data-anchor-id="control-freak">Control freak</h3>
<p>So far, we have studied the total causal effect of <img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Y">: <img src="https://latex.codecogs.com/png.latex?P(Y%7Cdo(X))">. “The term “direct effect” is meant to quantify an effect that is not mediated by other variables in the model or, more accurately, the sensitivity of <img src="https://latex.codecogs.com/png.latex?Y"> to changes in <img src="https://latex.codecogs.com/png.latex?X"> while all other factors in the analysis are held fixed”. Notice that holding variables fixed implies an <strong>intervention</strong> that <em>cannot always</em> be mimicked by <strong>conditioning</strong>.</p>
<p>We will label this effect the <strong>Controlled Direct Effect (CDE)</strong>. In counterfactual terms, it is defined thus:</p>
[
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%5Coperatorname%7BCDE%7D(m)%20&amp;=E%5Cleft%5BY_%7B1,%20m%7D-Y_%7B0,%20m%7D%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20d%20o(T=1,%20M=m)%5D-E%5BY%20%5Cmid%20d%20o(T=0,%20M=m)%5D%0A%5Cend%7Baligned%7D">
<p>]</p>
<blockquote class="blockquote">
<p>CDE measures the expected increase in ( Y ) as the treatment changes from ( T=0 ) to ( T=1, ) while the mediator is set to a specified level ( M=m ) uniformly over the entire population.</p>
</blockquote>
<p>However, intervening on the mediator is an over-kill. We need to be more intelligent.</p>
</section>
<section id="natural-let-it-flow" class="level3">
<h3 class="anchored" data-anchor-id="natural-let-it-flow">Natural: Let it flow</h3>
<p>A less stringent intervention is defined by studying the expected increase in ( Y ) as the treatment changes from ( T=0 ) to ( T=1, ), “while the mediator is set to whatever value <em>it would have attained (for each individual) prior to the change</em>, that is, under <img src="https://latex.codecogs.com/png.latex?T%20=%200">”. We will label this the <strong>Natural Direct Effect (NDE)</strong>. In counterfactual terms:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AN%20D%20E=E%5Cleft%5BY_%7B1,%20M_%7B0%7D%7D-Y_%7B0,%20M_%7B0%7D%7D%5Cright%5D%0A"> Whereas the CDE is made out of do-expressions, the NDE is defined in terms of <em>nested counterfactuals</em>. Therefore, according to <a href="https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/">Pearl’s Ladder of Causation and Bareinboim’s Causal Hierarchy Theorem</a>, NDE requires a <strong>more elaborate causal knowledge</strong> to be <em>identified</em> than the CDE. That is, whereas the CDE could be estimated using experimental evidence, the NDE, in principle, cannot be estimated using <strong>only</strong> experimental evidence.</p>
<section id="what-about-indirect-effects" class="level4">
<h4 class="anchored" data-anchor-id="what-about-indirect-effects">What about indirect effects?</h4>
<p>Once we have defined a direct effect, the natural thing to do, in order to tackle the mediation problem, is to also define an <em>indirect</em> effect. The comparison of the two terms will allow us to answer the mediation problem.</p>
<p>Notice that we must define the <strong>Natural Indirect Effect (NIE)</strong> such that it measures “the portion of the effect that can be explained by mediation alone. Thus, it must disable the capacity of <img src="https://latex.codecogs.com/png.latex?Y"> to respond to <img src="https://latex.codecogs.com/png.latex?X">”. To do so, we will define the NIE thus:</p>
<blockquote class="blockquote">
<p>NIE measures the expected increase in Y when the treatment is held constant, at <img src="https://latex.codecogs.com/png.latex?T%20=%200">, and <img src="https://latex.codecogs.com/png.latex?M"> changes to whatever value it would have attained (for each individual) under <img src="https://latex.codecogs.com/png.latex?T=1">.</p>
</blockquote>
<p>In counterfactual language:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AN%20I%20E=E%5Cleft%5BY_%7B0,%20M_%7B1%7D%7D-Y_%7B0,%20M_%7B0%7D%7D%5Cright%5D%0A"> Just like with the NDE, we are faced with nested counterfactuals that cannot always be estimated using experimental evidence.</p>
</section>
</section>
<section id="response-fractions" class="level3">
<h3 class="anchored" data-anchor-id="response-fractions">Response fractions</h3>
<p>To answer the mediation question, it is useful to state the direct and indirect effects in terms of the total effect.</p>
<p>What percentage is due to the <strong>direct effect</strong> of <img src="https://latex.codecogs.com/png.latex?X">? The ratio <img src="https://latex.codecogs.com/png.latex?NDE/TE"> “measures the fraction of the response that is transmitted directly, with <img src="https://latex.codecogs.com/png.latex?M"> frozen.”</p>
<p>What percentage is due to the mediator variable, that is, is due to the <strong>indirect effect</strong> of <img src="https://latex.codecogs.com/png.latex?X">?</p>
<blockquote class="blockquote">
<p><img src="https://latex.codecogs.com/png.latex?NIE%E2%88%95TE"> measures the fraction of the response that <strong>may be</strong> transmitted through <img src="https://latex.codecogs.com/png.latex?M">, with <img src="https://latex.codecogs.com/png.latex?Y"> blinded to <img src="https://latex.codecogs.com/png.latex?X">. Consequently, the difference <img src="https://latex.codecogs.com/png.latex?(TE%20%E2%88%92%20NDE)%E2%88%95TE"> measures the fraction of the response that <strong>is necessarily</strong> due to M.</p>
</blockquote>
</section>
</section>
<section id="identification" class="level2">
<h2 class="anchored" data-anchor-id="identification">Identification</h2>
<p>Because both the NDE and the NIE are defined with <em>nested counterfactuals</em>, they imply a contradiction between two <strong>different and clashing causal worlds</strong> that can only be resolved through the invariant information across worlds. However, not all structural causal models (SCM) imply enough restrictions such that this invariant information is enough to estimate the nested counterfactuals with a combination of <strong>observational and experimental evidence</strong>.</p>
<p>What type of causal models yield NDE (and NIE) that are identifiable? In <a href="">this paper</a>, Pearl says that every model where there is a set <img src="https://latex.codecogs.com/png.latex?w"> of measured covariates such that:</p>
<ol type="1">
<li><p>No member of <img src="https://latex.codecogs.com/png.latex?W"> is a descendant of <img src="https://latex.codecogs.com/png.latex?T">.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?W"> blocks all backdoor paths from <img src="https://latex.codecogs.com/png.latex?M"> to <img src="https://latex.codecogs.com/png.latex?Y"> not traversing <img src="https://latex.codecogs.com/png.latex?T">. That is, <img src="https://latex.codecogs.com/png.latex?W"> deconfounds the mediator-outcome relationship (holding <img src="https://latex.codecogs.com/png.latex?T"> constant).</p></li>
</ol>
<p>Then, both effects (NDE and NIE) are <strong>identifiable</strong> with <em>experimental evidence</em>. The formula for the NDE becomes thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AN%20D%20E=%5Csum_%7Bm%7D%20%5Csum_%7Bw%7D%5Cleft%5BE(Y%20%5Cmid%20d%20o(T=1,%20M=m)),%20W=w)-E(Y%20%5Cmid%20d%20o(T=0,%20M=m),%20W=w)%5Cright%5D%20%5C%5C%0AP(M=m%20%5Cmid%20d%20o(T=0),%20W=w)%20P(W=w)%0A%5Cend%7Baligned%7D%0A"></p>
<p>The intuition is the following:</p>
<blockquote class="blockquote">
<p>The natural direct effect is the weighted average of the controlled direct effect ( C D E(m), ) shown in the square brackets, using the no-treatment distribution ( P(M=m T=0) ) as a weighting function.</p>
</blockquote>
<p>Furthermore, if we require <strong>identification</strong> with <em>observational data</em>, we must have a causal model where, <em>besides</em> the former two assumptions, the following two assumptions also hold:</p>
<ul>
<li>The ( W ) -specific effect of the treatment on the mediator is identifiable by some means. [ [P(m d o(t), w) ] ]</li>
<li>The ( W ) -specific joint effect of ( { ) treatment ( + ) mediator ( } ) on the outcome is identifiable by some means. [ [P(y d o(t, m), w) ] ]</li>
</ul>
<p>Then, the equation for the NDE (and the NIE) becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bequation%7D%0A%5Cbegin%7Barray%7D%7Bc%7D%0AN%20D%20E=%5Csum_%7Bm%7D%20%5Csum_%7Bw%7D%5BE(Y%20%5Cmid%20T=1,%20M=m,%20W=w)-E(Y%20%5Cmid%20T=0,%20M=m,%20W=w)%5D%20%5C%5C%0AP(M=m%20%5Cmid%20T=0,%20W=w)%20P(W=w)%20%5C%5C%0AN%20I%20E=%5Csum_%7Bm%7D%20%5Csum_%7Bw%7D%5BP(M=m%20%5Cmid%20T=1,%20W=w)-P(M=m%20%5Cmid%20T=0,%20W=w)%5D%20%5C%5C%0AE(Y%20%5Cmid%20T=0,%20M=m,%20W=w)%20P(W=w)%0A%5Cend%7Barray%7D%0A%5Cend%7Bequation%7D%0A"></p>
<p>Finally, if there is no confounding in our causal model whatsoever, there’s no need fo conditioning on <img src="https://latex.codecogs.com/png.latex?W"> and we arrive at the <strong>mediation formulas</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANDE%20=%20%5Csum_%7Bm%7D%5BE%5BY%20%5Cmid%20T=1,%20M=m%5D-E%5BY%20%5Cmid%20T=0,%20M=m%5D%5D%20P(M=m%20%5Cmid%20T=0)%0A"> Similarly, for the NIE the mediation formula is the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AN%20I%20E=%5Csum_%7Bm%7D%20E%5BY%20%5Cmid%20T=0,%20M=m%5D%5BP(M=m%20%5Cmid%20T=1)-P(M=m%20%5Cmid%20T=0)%5D%0A"> In the following section, I’ll present four examples of causal models where the NDE and NIE may not be identifiable.</p>
<section id="examples-of-identification" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-identification">Examples of Identification</h3>
<section id="first-example" class="level4">
<h4 class="anchored" data-anchor-id="first-example">First example</h4>
<p>Suppose you have the following causal model. Are the NDE and NIE identifiable?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">first_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(y <span class="sc" style="color: #5E5E5E;">~</span> t <span class="sc" style="color: #5E5E5E;">+</span> m,</span>
<span id="cb2-2">                        m <span class="sc" style="color: #5E5E5E;">~</span> t)</span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;">ggdag</span>(first_example) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Are the NDE and NIE identifiable?"</span>,</span>
<span id="cb2-5">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Given that there's no confounding, they are identifiable!"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Yes, the effects are identifiable: there’s no confounding and we can use the <strong>mediator formulas</strong>.</p>
</section>
<section id="second-example" class="level4">
<h4 class="anchored" data-anchor-id="second-example">Second example</h4>
<p>Suppose you have the following causal model. Are the NDE and NIE identifiable?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">second_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(y <span class="sc" style="color: #5E5E5E;">~</span> t <span class="sc" style="color: #5E5E5E;">+</span> m <span class="sc" style="color: #5E5E5E;">+</span> w,</span>
<span id="cb3-2">                        m <span class="sc" style="color: #5E5E5E;">~</span> t <span class="sc" style="color: #5E5E5E;">+</span> w,</span>
<span id="cb3-3">                        t <span class="sc" style="color: #5E5E5E;">~</span> w)</span>
<span id="cb3-4"><span class="fu" style="color: #4758AB;">ggdag</span>(second_example) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-5">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Are the NDE and NIE identifiable?"</span>,</span>
<span id="cb3-6">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"w confounds all three relationships. Adjusting for W, renders NDE and NIE identifiable"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Yes, we can identify the NDE and the NIE. Although <img src="https://latex.codecogs.com/png.latex?W"> confounds all three relationships, by adjusting by <img src="https://latex.codecogs.com/png.latex?W">, we can deconfound them and estimate the NDE and NIE.</p>
</section>
<section id="third-example" class="level4">
<h4 class="anchored" data-anchor-id="third-example">Third example</h4>
<p>Suppose you have the following causal model where the dashed arc represents a common unobserved ancestor. Are the NDE and NIE identifiable?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">third_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(m <span class="sc" style="color: #5E5E5E;">~</span> t,</span>
<span id="cb4-2">                        z <span class="sc" style="color: #5E5E5E;">~</span> m,</span>
<span id="cb4-3">                        y <span class="sc" style="color: #5E5E5E;">~</span> z <span class="sc" style="color: #5E5E5E;">+</span> t,</span>
<span id="cb4-4">                        m <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y)</span>
<span id="cb4-5"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(third_example, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">edge_linetype =</span> linetype)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-8">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-9">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-10">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-11">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Are the NDE and NIE identifiable?"</span>,</span>
<span id="cb4-12">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Although the causal effect M-&gt;Y is identifiable, we cannot deconfound the relationships and</span></span>
<span id="cb4-13"><span class="st" style="color: #20794D;">        thus cannot estimate neither the NDE nor the NIE"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We cannot!</p>
<blockquote class="blockquote">
<p>Unfortunately, although the causal effect of ( {T, M} ) on ( Y, ) as well as the controlled direct effect ( C D E(m) ) are both identifiable (through the front-door estimator), condition (2 ) cannot be satisfied; no covariate can be measured that deconfounds the ( M Y ) relationship. The front-door estimator provides a consistent estimate of the population causal effect, ( P(Y=y d o(M=m)), ) while unconfoundedness, as defined before, requires independence of ( U_{M} ) and ( U_{Y}, ) which measurement of ( Z ) cannot induce.</p>
</blockquote>
<p>This is yet another example of the <strong>Causal Hierarchy Theorem</strong>: experimental evidence is not enough to determine counterfactual information. In this case, causal effects are not enough to determine the nested counterfactuals that define the NDE.</p>
</section>
<section id="four-example" class="level4">
<h4 class="anchored" data-anchor-id="four-example">Four example</h4>
<p>Suppose you have the following causal model. Are the NDE and NIE identifiable?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">fourth_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(y <span class="sc" style="color: #5E5E5E;">~</span> t <span class="sc" style="color: #5E5E5E;">+</span> m <span class="sc" style="color: #5E5E5E;">+</span> w,</span>
<span id="cb5-2">                        m <span class="sc" style="color: #5E5E5E;">~</span> t <span class="sc" style="color: #5E5E5E;">+</span> w,</span>
<span id="cb5-3">                        w <span class="sc" style="color: #5E5E5E;">~</span> t)</span>
<span id="cb5-4"><span class="fu" style="color: #4758AB;">ggdag</span>(fourth_example) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-5">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Are the NDE and NIE identifiable?"</span>,</span>
<span id="cb5-6">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Although W deconfounds M-&gt;Y, it is a descendant of T. Thus, we cannot identify NDE or NIE. "</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>No, neither NDE nor NIE are identifiable. Given our first assumption, there’s the following “general pattern that prevents identification of natural effects in any non-parametric model”:</p>
<blockquote class="blockquote">
<p>Whenever a variable exists, be it measured or unmeasured, that is a descendant of T and an ancestor of both M and Y (W in our examples), NDE is not identifiable.</p>
</blockquote>
<p>However, the restriction does not apply to linear models where every counterfactual is identifiable once the parameters are identified. Sadly, “the increased identification power comes at increasing the danger of mis-specification”.</p>
</section>
</section>
</section>
<section id="numerical-example" class="level2">
<h2 class="anchored" data-anchor-id="numerical-example">Numerical example</h2>
<p>I’ll finish the post by giving the following numerical example to show how we can use what we’ve learnt to estimate natural effects and solve a mediation problem with data. This is the exercise 4.5.4 in Pearl’s primer.</p>
<blockquote class="blockquote">
<p>Suppose that a company is accused of gender discrimination. Let <img src="https://latex.codecogs.com/png.latex?T%20=%201"> standing for male applicants, <img src="https://latex.codecogs.com/png.latex?M%20=%201"> standing for highly qualified applicants, and <img src="https://latex.codecogs.com/png.latex?Y%20=%201"> standing for hiring. (Find the proportion of the hiring disparity that is due to gender, and the proportion that could be explained by disparity in qualification alone.)</p>
</blockquote>
<p>That is, there are two paths whereby there’s discrimination. Male applicants tend to get more easily hired and thus have more qualifications. However, male applicants may also be favored by the company just because they are male. We draw the following DAG:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">gender_discrimination <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(m <span class="sc" style="color: #5E5E5E;">~</span> t,</span>
<span id="cb6-2">                                y <span class="sc" style="color: #5E5E5E;">~</span> m <span class="sc" style="color: #5E5E5E;">+</span> t,</span>
<span id="cb6-3">                                <span class="at" style="color: #657422;">labels =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"m"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Qualifications"</span>,</span>
<span id="cb6-4">                                           <span class="st" style="color: #20794D;">"t"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Gender"</span>,</span>
<span id="cb6-5">                                           <span class="st" style="color: #20794D;">"y"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Hiring"</span>))</span>
<span id="cb6-6">gender_discrimination <span class="sc" style="color: #5E5E5E;">%&gt;%</span>  <span class="fu" style="color: #4758AB;">tidy_dagitty</span>(<span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"tree"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-7">   <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-8">    <span class="fu" style="color: #4758AB;">geom_dag_edges</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-9">    <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-10">    <span class="fu" style="color: #4758AB;">geom_dag_point</span>(<span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-11">    <span class="fu" style="color: #4758AB;">geom_dag_label_repel</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">label =</span> label), <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>,</span>
<span id="cb6-12">      <span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>, <span class="at" style="color: #657422;">show.legend =</span> <span class="cn" style="color: #8f5902;">FALSE</span>, <span class="at" style="color: #657422;">family =</span> <span class="st" style="color: #20794D;">"Roboto Condensed"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s say that we collect the following data:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bccc%7D%0A%5Chline%20%5Ctext%20%7B%20Gender%20%7D%20&amp;%20%5Ctext%20%7B%20Qualification%20%7D%20&amp;%20%5Ctext%20%7B%20Success%20Hiring%20%7D%20%5C%5C%0AT%20&amp;%20M%20&amp;%20E(Y%20%5Cmid%20T=t,%20M=m)%20%5C%5C%0A%5Chline%201%20&amp;%201%20&amp;%200.80%20%5C%5C%0A1%20&amp;%200%20&amp;%200.40%20%5C%5C%0A0%20&amp;%201%20&amp;%200.30%20%5C%5C%0A0%20&amp;%200%20&amp;%200.20%20%5C%5C%0A%5Chline%0A%5Cend%7Barray%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bcc%7D%0A%5Chline%20%5Ctext%20%7B%20Gender%20%7D%20&amp;%20%5Ctext%20%7B%20Qualification%20%7D%20%5C%5C%0AT%20&amp;%20E(M%20%5Cmid%20T=t)%20%5C%5C%0A%5Chline%200%20&amp;%200.40%20%5C%5C%0A1%20&amp;%200.75%20%5C%5C%0A%5Chline%0A%5Cend%7Barray%7D%0A"> Assuming that there’s no confounding, we use the <strong>mediator formulas thus</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AN%20D%20E=&amp;%20%5Csum_%7Bm%7D%5BE%5BY%20%5Cmid%20T=1,%20M=m%5D-E%5BY%20%5Cmid%20T=0,%20M=m%5D%5D%20P(M=m%20%5Cmid%20T=0)%20%5C%5C%0A=&amp;%5BE%5BY%20%5Cmid%20T=1,%20M=0%5D-E%5BY%20%5Cmid%20T=0,%20M=0%5D%5D%20P(M=0%20%5Cmid%20T=0)%20%5C%5C%0A&amp;+%5BE%5BY%20%5Cmid%20T=1,%20M=1%5D-E%5BY%20%5Cmid%20T=0,%20M=1%5D%5D%20P(M=1%20%5Cmid%20T=0)%20%5C%5C%0A=&amp;(0.4-0.2)(1-0.4)+(0.8-0.3)%200.4%20%5C%5C%0A=&amp;%200.32%20%5C%5C%0AN%20I%20E=&amp;%20%5Csum_%7Bm%7D%20E%5BY%20%5Cmid%20T=0,%20M=m%5D%5BP(M=m%20%5Cmid%20T=1)-P(M=m%20%5Cmid%20T=0)%5D%20%5C%5C%0A=&amp;%20E%5BY%20%5Cmid%20T=0,%20M=0%5D%5BP(M=0%20%5Cmid%20T=1)-P(M=0%20%5Cmid%20T=0)%5D%20%5C%5C%0A&amp;+E%5BY%20%5Cmid%20T=0,%20M=1%5D%5BP(M=1%20%5Cmid%20T=1)-P(M=1%20%5Cmid%20T=0)%5D%20%5C%5C%0A=&amp;(0.75-0.4)(0.3-0.2)%20%5C%5C%0A=&amp;%200.035%0A%5Cend%7Baligned%7D%0A"></p>
<p>Therefore, given that the direct effect is substantially larger than the indirect effect, we conclude that it is not the different qualifications in themselves, but the gender that is driving the hiring process in the company.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Mediation analysis aims to disentangle the <strong>NDE</strong> (which does not pass through the mediator) from the <strong>NIE</strong> (the part that passes through the mediator). We’ve seen how the correct definition of these effects requires counterfactual thinking that cannot always be empirIcally identified with either experimental or observational evidence. After having stated the required assumptions for identifiability, we practiced recognizing such assumptions with several causal models. Finally, we practiced with a numerical example out of Pearl’s primer.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-08-26-causality-mediation-analysis.html</guid>
  <pubDate>Wed, 26 Aug 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: Probabilities of Causation</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-08-20-causality-probabilities-of-causation.html</link>
  <description><![CDATA[ 




<section id="why-read-this" class="level2">
<h2 class="anchored" data-anchor-id="why-read-this">Why read this?</h2>
<p>Questions of attribution are everywhere: i.e., did <img src="https://latex.codecogs.com/png.latex?X=x"> cause <img src="https://latex.codecogs.com/png.latex?Y=y">? From legal battles to personal decision making, we are obsessed by them. Can we give a rigorous answer to the <strong>problem of attribution</strong>?</p>
<p>One alternative to solve the problem of attribution is to reason in the following manner: if there is <em>no possible alternative</em> causal process, <strong>which does not involve <img src="https://latex.codecogs.com/png.latex?X"></strong>,that can cause <img src="https://latex.codecogs.com/png.latex?Y=y">, then <img src="https://latex.codecogs.com/png.latex?X=x"> is <strong>necessary</strong> to produce the effect in question. Therefore, the effect <img src="https://latex.codecogs.com/png.latex?Y=y"> <em>could not</em> have happened without <img src="https://latex.codecogs.com/png.latex?X=x">. In causal inference, this type of reasoning is studied by computing the <strong>Probability of Necessity (PN)</strong></p>
<p>In the above alternative, however, the reasoning is tailored to a specific event under consideration. What if we are interested in studying a general tendency of a given effect? In this case, we are asking <strong>how sufficient</strong> is a <em>cause</em>, <img src="https://latex.codecogs.com/png.latex?X=x">, for the production of the effect, <img src="https://latex.codecogs.com/png.latex?Y=y">. We answer this question with the <strong>Probability of Sufficiency (PS)</strong></p>
<p>In this blogpost, we will give <a href="https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/"><strong>counterfactual interpretations</strong></a> to both probabilities: <img src="https://latex.codecogs.com/png.latex?PN"> and <img src="https://latex.codecogs.com/png.latex?PS">. Thereby, we will be able to study them in a systematic fashion using the tools of causal inference. Although we will realize that they are <strong>not generally identifiable</strong> from a causal diagram and data alone, if we are willing to assume <em>monotonicity</em>, we will be able to estimate both <img src="https://latex.codecogs.com/png.latex?PN"> and <img src="https://latex.codecogs.com/png.latex?PS"> with a <em>combination</em> of experimental and observational data.</p>
<p>Finally, we will work through some examples to put what we have learnt into practice. This blogpost follows the notation of <a href="http://bayes.cs.ucla.edu/BOOK-2K/">Pearl’s Causality</a>, Chapter 9 and Pearl’s <a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics: A primer</a>.</p>
</section>
<section id="counterfactual-definitions" class="level2">
<h2 class="anchored" data-anchor-id="counterfactual-definitions">Counterfactual definitions</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> be two binary variables in causal model. In what follows, we will give counterfactual interpretations to both <img src="https://latex.codecogs.com/png.latex?PN"> and <img src="https://latex.codecogs.com/png.latex?PS">.</p>
<section id="probability-of-necessity" class="level3">
<h3 class="anchored" data-anchor-id="probability-of-necessity">Probability of Necessity</h3>
<p>The <strong>Probability of Necessity (PN)</strong> stands for the probability that the <em>event <img src="https://latex.codecogs.com/png.latex?Y=y"> would not have occurred</em> in the <strong>absence of event <img src="https://latex.codecogs.com/png.latex?X=X"></strong>, given that <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> did in fact occur.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APN%20:=%20P(Y_%7Bx'%7D%20=%20false%20%7C%20X%20=%20true,%20Y%20=%20true)%0A"></p>
<p>To gain some intuition, imagine Ms.&nbsp;Jones: a former cancer patient that underwent both a lumpectomy and irradiation. She speculates: <strong>do I owe my life to irradiation?</strong> We can study this question by figuring out <em>how necessary</em> was the lumpectomy for the remission to occur:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APN%20=%20P(Y_%7B%5Ctext%7Bno%20irradiation%7D%7D%20=%20%5Ctext%7Bno%20remission%7D%7C%20X%20=%20irradiation,%20Y%20=%20remission)%0A"></p>
<p>If <img src="https://latex.codecogs.com/png.latex?PN"> is high, then, yes, Ms.&nbsp;Jones owes her life to her decision of having irradiation.</p>
</section>
<section id="probability-of-sufficiency" class="level3">
<h3 class="anchored" data-anchor-id="probability-of-sufficiency">Probability of Sufficiency</h3>
<p>On the other hand:</p>
<blockquote class="blockquote">
<p>The <strong>Probability of Sufficiency (PS)</strong> measures the capacity of <img src="https://latex.codecogs.com/png.latex?x"> to produce <img src="https://latex.codecogs.com/png.latex?y">, and, since “production” implies a transition from absence to presence, we condition <img src="https://latex.codecogs.com/png.latex?Y_x"> on situations where x and y are absent</p>
</blockquote>
<p>Therefore:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APS%20:=%20P(Y_x%20=%20true%20%7C%20X%20=%20false,%20Y%20=%20false)%0A"></p>
<p>The following example may clarify things. Imagine that, contrary to Ms.&nbsp;Jones above, Mrs.&nbsp;Smith had a lumpectomy alone and her tumor recurred. She speculates on her decision and concludes: <em>I should have gone through irradiation</em>. Is this regret warranted? We can quantify this by speaking about <img src="https://latex.codecogs.com/png.latex?PS">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APS%20=%20P(Y_%7Birradiation%7D%20=%20remission%7C%20X%20=%20%5Ctext%7Bno%20irradiation%7D,%20Y%20=%20%5Ctext%7Bno%20remission%7D)%0A"> That is, <img src="https://latex.codecogs.com/png.latex?PS"> computes the probability that remission would have occurred had Mrs.&nbsp;Smith gone through irradiation, given that she did not go and remission did not occur. Thus, it measures the degree to which <em>the action not taken</em>, <img src="https://latex.codecogs.com/png.latex?X=1">, <strong>would have been sufficient for her recovery</strong>.</p>
</section>
<section id="combining-both-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="combining-both-probabilities">Combining both probabilities</h3>
<p>We can compute the probability that the cause is necessary and sufficient thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%20N%20S:=%20P(y_%7Bx%7D,%20y'_%7Bx'%7D)%20=P(x,%20y)%20P%20N+P%5Cleft(x%5E%7B%5Cprime%7D,%20y%5E%7B%5Cprime%7D%5Cright)%20P%20S%0A"></p>
<p>That is, the contribution of <img src="https://latex.codecogs.com/png.latex?PN"> is amplified or reduced by <img src="https://latex.codecogs.com/png.latex?P(x,%20y)"> and likewise for the <img src="https://latex.codecogs.com/png.latex?PS">’s contribution.</p>
</section>
</section>
<section id="idenitfiability" class="level2">
<h2 class="anchored" data-anchor-id="idenitfiability">Idenitfiability</h2>
<p>In the <strong>general case</strong>, when we have a causal diagram and observed (and experimental) data, <strong>neither <img src="https://latex.codecogs.com/png.latex?PN"> nor <img src="https://latex.codecogs.com/png.latex?PS"> are identifiable</strong>. This happens due to the relationship between the counterfactual’s antecedent and the fact that we are conditioning on <img src="https://latex.codecogs.com/png.latex?Y">. If we wanted to model this relationship, we would need to know the functional relationship between <img src="https://latex.codecogs.com/png.latex?X,%20Pa_y"> and <img src="https://latex.codecogs.com/png.latex?Y">.</p>
<p>In practice, if we don’t know the functional relationship, we <em>must</em> at least assume monotonicity of <img src="https://latex.codecogs.com/png.latex?Y"> relative to <img src="https://latex.codecogs.com/png.latex?X"> to be able to identify them. Otherwise, we must content ourselves with <em>theoretically sharp bounds</em> on the probabilities of causation.</p>
<section id="what-is-monotonicity" class="level3">
<h3 class="anchored" data-anchor-id="what-is-monotonicity">What is Monotonicity?</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?u"> be <a href="https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/">the unobserved background variables in a SCM</a>, then, <img src="https://latex.codecogs.com/png.latex?Y"> is monotonic relative to <img src="https://latex.codecogs.com/png.latex?X"> if:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_1(u)%20%5Cgeq%20Y_0%20(u)%0A"></p>
<p>That is, exposure to treatment <img src="https://latex.codecogs.com/png.latex?X=1"> always helps to bring about <img src="https://latex.codecogs.com/png.latex?Y=1">.</p>
</section>
<section id="identifying-the-probabilities-of-causation" class="level3">
<h3 class="anchored" data-anchor-id="identifying-the-probabilities-of-causation">Identifying the probabilities of causation</h3>
<p>If we are willing to assume that Y is monotonic relative to X, then both <img src="https://latex.codecogs.com/png.latex?PN"> and <img src="https://latex.codecogs.com/png.latex?PS"> <strong>are identifiable when the causal effects <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20do(X=1)"> and <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20do(X=0)"> are identifiable</strong>. Whether this causal effect is identifiable because we have <em>experimental evidence</em>, or because we can identify them by using the Back-Door criterion or <em>other graph-assisted identification strategy</em>, it does not matter. Then, we can estimate <img src="https://latex.codecogs.com/png.latex?PN"> with a combination of do-expressions and observational data thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APN%20=%20P(y'_%7Bx'%7D%20%7C%20x,%20y)%20=%20%5Cfrac%7BP(y)%20-%20P(y%7C%20do%20(x'))%7D%7BP(x,%20y)%7D%0A"></p>
<p>Moreover, if monotonicity does not hold, the above expression becomes a lower bound for <img src="https://latex.codecogs.com/png.latex?PN">. The <a href="https://ftp.cs.ucla.edu/pub/stat_ser/R271-U.pdf">complete bound is the following</a>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmax%20%5Cleft%5C%7B0,%20%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7BP(x,%20y)%7D%5Cright%5C%7D%20%5Cleq%20P%20N%20%5Cleq%20%5Cmin%20%5Cleft%5C%7B1,%20%5Cfrac%7BP%5Cleft(y%5E%7B%5Cprime%7D%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)-P%5Cleft(x%5E%7B%5Cprime%7D,%20y%5E%7B%5Cprime%7D%5Cright)%7D%7BP(x,%20y)%7D%5Cright%5C%7D%0A"></p>
<p>Equivalently, <img src="https://latex.codecogs.com/png.latex?PS"> can be estimated thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APS%20=%20P(y_x%20%7C%20x',%20y')%20=%20%5Cfrac%7BP(y%7C%20do(x))%20-%20P(y)%7D%7BP(x',%20y')%7D%0A"></p>
<p>Which becomes the lower bound if we are not willing to assume monotonicity:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmax%20%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bc%7D%0A0,%0A%5Cfrac%7BP%5Cleft(y%20%7C%20do%20(x)%20%5Cright)-P(y)%7D%7BP%5Cleft(x%5E%7B%5Cprime%7D,%20y%5E%7B%5Cprime%7D%5Cright)%7D%0A%5Cend%7Barray%7D%5Cright%5C%7D%20%5Cleq%20P%20S%20%5Cleq%20%5Cmin%20%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bc%7D%0A1,%0A%5Cfrac%7BP%5Cleft(y%20%7C%20do(x)%20%5Cright)-P(x,%20y)%7D%7BP%5Cleft(x%5E%7B%5Cprime%7D,%20y%5E%7B%5Cprime%7D%5Cright)%7D%0A%5Cend%7Barray%7D%5Cright%5C%7D%0A"></p>
<p>Let’s use the estimators and the bounds in the following example.</p>
</section>
</section>
<section id="a-first-example" class="level2">
<h2 class="anchored" data-anchor-id="a-first-example">A first example</h2>
<blockquote class="blockquote">
<p>A lawsuit is filed against the manufacturer of a drug X that was supposed to relieve back-pain. Was the drug a necessary cause for the the death of Mr.&nbsp;A?</p>
</blockquote>
The experimental data provide the estimates [
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AP(y%20%5Cmid%20d%20o(x))%20&amp;=16%20/%201000=0.016%20%5C%5C%0AP%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%20&amp;=14%20/%201000=0.014%0A%5Cend%7Baligned%7D">
] whereas the non-experimental data provide the estimates [ P(y)=30 / 2000=0.015 ] [
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bl%7D%0AP(x,%20y)=2%20/%202000=0.001%20%5C%5C%0AP(y%20%5Cmid%20x)=2%20/%201000=0.002%20%5C%5C%0AP%5Cleft(y%20%5Cmid%20x%5E%7B%5Cprime%7D%5Cright)=28%20/%201000=0.028%0A%5Cend%7Barray%7D">
<p>]</p>
<p>Therefore, assuming that the drug could only cause (but never prevent death), monotonicity holds:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APN%20=%20%5Cfrac%7B0.015-0.014%7D%7B0.001%7D%20=%201%0A"></p>
<blockquote class="blockquote">
<p>The plaintiff was correct; barring sampling errors, the data provide us with 100% assurance that drug x was in fact responsible for the death of Mr A.</p>
</blockquote>
</section>
<section id="a-second-example" class="level2">
<h2 class="anchored" data-anchor-id="a-second-example">A second example</h2>
<p>Remember Ms.&nbsp;Jones? Is she right in attributing her recovery to the irradiation therapy. Suppose she gets her hands on the following data:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%20&amp;=0.3%20%5C%5C%0AP%5Cleft(x%5E%7B%5Cprime%7D%20%5Cmid%20y%5E%7B%5Cprime%7D%5Cright)%20&amp;=0.7%20%5C%5C%0AP(y%20%5Cmid%20d%20o(x))%20&amp;=0.39%20%5C%5C%0AP%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%20&amp;=0.14%0A%5Cend%7Baligned%7D%0A"></p>
<p>We can therefore start to bound <img src="https://latex.codecogs.com/png.latex?PN"> to figure out whether irradiation was <strong>necessary</strong> for remission:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP%20N%20&amp;%20%5Cgeq%20%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7BP(x,%20y)%7D%20%5C%5C%0A&amp;=%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7BP(y%20%5Cmid%20x)%20P(x)%7D%20%5C%5C%0A&amp;=%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7B%5Cleft(1-%5Cfrac%7BP%5Cleft(x%20%5Cmid%20y%5E%7B%5Cprime%7D%5Cright)%20P%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%7D%7BP(x)%7D%5Cright)%20P(x)%7D%20%5C%5C%0A&amp;=%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7BP(x)-P%5Cleft(x%20%5Cmid%20y%5E%7B%5Cprime%7D%5Cright)%20P%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>We don’t have data for <img src="https://latex.codecogs.com/png.latex?P(x)">. However, given that we are interested in a lower bound, we can choose the parametrization that yields the smallest bound: <img src="https://latex.codecogs.com/png.latex?P(x)%20=%201">. Therefore, the bound becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP%20N%20&amp;%20%5Cgeq%20%5Cfrac%7BP(y)-P%5Cleft(y%20%5Cmid%20d%20o%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%5Cright)%7D%7BP(x)-P%5Cleft(x%20%5Cmid%20y%5E%7B%5Cprime%7D%5Cright)%20P%5Cleft(y%5E%7B%5Cprime%7D%5Cright)%7D%20%5C%5C%0A&amp;%20%5Cgeq%20%5Cfrac%7B0.7-0.14%7D%7B1-(1-0.7)%20*%200.3%7D%20=%20%5C%5C%0A&amp;%200.62%20%3E%200.5%0A%5Cend%7Baligned%7D%0A"></p>
<p>Therefore, irradiation was more likely than not <strong>necessary</strong> for her remission.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this blogpost, we saw how we can analyze <strong>attribution problems</strong> by <em>giving counterfactual interpretations</em> to the probability that a cause is <strong>necessary and/or sufficient</strong>. These quantities turn out be generally not identifiable because they are sensitive to the specific functional relationships that connect <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> in a SCM.</p>
<p>However, we can give theoretically sharp bounds for them by <strong>combining experimental and observational data</strong>. If we are willing to <em>assume monotonicity</em>, the bounds collapse to give a point estimate for both probabilities of causation, <img src="https://latex.codecogs.com/png.latex?PN"> and <img src="https://latex.codecogs.com/png.latex?PS">.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-08-20-causality-probabilities-of-causation.html</guid>
  <pubDate>Thu, 20 Aug 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: Regret? Look at Effect of Treatment on the Treated</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html</link>
  <description><![CDATA[ 




<section id="why-read-this" class="level2">
<h2 class="anchored" data-anchor-id="why-read-this">Why read this?</h2>
<p>Regret about our actions stems from a <a href="https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/"><strong>counterfactual question</strong></a>: <em>What if I had acted differently?</em>. Therefore, to answer such question, we need a more elaborate language than the one we need to answer prediction or intervention questions. Why? Because we need to compare <strong>what happened</strong> with <em>what would had happened</em> if we <strong>had acted differently</strong>. We need to compute the <strong>Effect of Treatment on the Treated (ETT)</strong>.</p>
<p>To compute the ETT, we need to formulate a Structural Causal Model and leverage the <strong>invariant qualities</strong> across the <em>observed world and the hypothetical</em> world: the unobserved background variables. Indeed, the definition of the Effect of Treatment on the Treated (ETT) is defined for a binary treatment thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5BY_1%20-%20Y_0%20%7C%20X%20=%201%5D%20%5C%5C%0A=%20%5Csum_u%20%5BP(y_1%20%7C%20x_1,%20u)%20-%20P(y_1%20%7C%20x_0,%20u)%5D%20P(u%20%7Cx_1)%0A"></p>
<p>Of course, we don’t have access to the background variables. In this post, we will learn to answer two questions: <strong>when is the ETT identifiable?</strong> And if so, can we give an <strong>estimator</strong> for such counterfactual <em>in terms of non-experimental data</em>?</p>
<p>We will first study a binary treatment and answer both questions. Then, we will tackle the more general case of any treatment.</p>
</section>
<section id="a-motivating-binary-example" class="level2">
<h2 class="anchored" data-anchor-id="a-motivating-binary-example">A Motivating binary example</h2>
<p>The following example is taken from Pearl’s (et alter) book <a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics: A primer</a>.</p>
<blockquote class="blockquote">
<p>Imagine an average adolescent: Joe. He has started smoking ever since he began High School. Should he regret his decision? That is, given that he has started smoking, has he significantly increased his chances of suffering from lung cancer compared to his chances had he never begun in the first place?</p>
</blockquote>
<p>Therefore, what Joe cares about is the <strong>Effect of Treatment on the Treated</strong>: <img src="https://latex.codecogs.com/png.latex?E%5BCancer_1%20-%20Cancer_0%20%7C%20Smoking%20=%201%5D">. If ETT &gt; 0, having smoked has caused a higher chance of lung cancer for Joe compared to the hypothetical world where he had never smoked in the first place. How can we calculate the ETT?</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AE%20T%20T%20&amp;=E%5Cleft%5BY_%7Bx%7D-Y_%7Bx%5E%7B%5Cprime%7D%7D%20%5Cmid%20X=x%5Cright%5D%20%5C%5C%0A&amp;=E%5BY_x%20%5Cmid%20X=x%5D-E%5Cleft%5BY_%7Bx%5E%7B%5Cprime%7D%7D%20%5Cmid%20X=x%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></p>
<p>The challenge, thus, is to estimate the counterfactual expression <img src="https://latex.codecogs.com/png.latex?E%5Cleft%5BY_%7Bx%5E%7B%5Cprime%7D%7D%20%5Cmid%20X=x%5Cright%5D"></p>
<section id="expressing-ett-in-terms-of-observational-data-and-experimental-data" class="level3">
<h3 class="anchored" data-anchor-id="expressing-ett-in-terms-of-observational-data-and-experimental-data">Expressing ETT in terms of observational data and experimental data</h3>
<p>Our treatment is binary. Therefore, let’s begin by using the law of total probability thus to write <img src="https://latex.codecogs.com/png.latex?E%5BY_x%5D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5Cleft%5BY_%7Bx%7D%5Cright%5D=E%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5Cright%5D%20P(X=x)+E%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D%20P%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%0A"></p>
<p>We will use the consistency axiom: <img src="https://latex.codecogs.com/png.latex?E%5BY_x%20%7C%20X%20=%20x%5D%20=%20E%5BY%20%7C%20X%20=%20x%5D">; that is, a counterfactual predicated on an actual observation is no counterfactual.</p>
<p>Therefore, we can re-write the above expression thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5Cleft%5BY_%7Bx%7D%5Cright%5D=E%5BY%20%5Cmid%20X=x%5D%20P(X=x)+E%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D%20P%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%0A"></p>
<p>In the above expression there’s only one term that cannot be computed using observational data, <img src="https://latex.codecogs.com/png.latex?E%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D">: the same term that causes trouble in our ETT estimation. Let’s solve for it:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D=%5Cfrac%7BE%5Cleft%5BY_%7Bx'%7D%5Cright%5D-E%5BY%20%5Cmid%20X=x%5D%20P(X=x)%7D%7BP%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%7D%20%5C%5C%0AE%5Cleft%5BY_%7Bx%7D%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D=%5Cfrac%7BE%5Cleft%5BY%20%7C%20do(X%20=%20X')%5Cright%5D-E%5BY%20%5Cmid%20X=x%5D%20P(X=x)%7D%7BP%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%7D%0A"></p>
<p>By plugging-in this term, we can express our ETT with terms that can be computed with a mix of observational and experimental data.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AE%20T%20T%20&amp;=E%5Cleft%5BY_%7Bx%7D-Y_%7Bx%5E%7B%5Cprime%7D%7D%20%5Cmid%20X=x%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=x%5D-E%5Cleft%5BY_%7Bx%5E%7B%5Cprime%7D%7D%20%5Cmid%20X=x%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=x%5D-%5Cfrac%7BE%5Cleft%5BY%20%5Cmid%20d%20o%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%5Cright%5D-E%5Cleft%5BY%20%5Cmid%20X=x%5E%7B%5Cprime%7D%5Cright%5D%20P%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%7D%7BP(X=x)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Therefore, if the treatment is binary, whenever the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> can be identified, the ETT can also be identified.</p>
</section>
<section id="going-back-to-joe" class="level3">
<h3 class="anchored" data-anchor-id="going-back-to-joe">Going back to Joe</h3>
<p>Let’s go back to our motivating example. We can express the ETT using the above derivation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AE%20T%20T%20&amp;=E%5Cleft%5BY_%7B1%7D-Y_%7B0%7D%20%5Cmid%20X=1%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=1%5D-E%5Cleft%5BY_%7B0%7D%20%5Cmid%20X=1%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=1%5D-%5Cfrac%7BE%5BY%20%5Cmid%20d%20o(X=0)%5D-E%5BY%20%5Cmid%20X=0%5D%20P(X=0)%7D%7BP(X=1)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Thus, we can estimate the ETT with only observational data if we can estimate <img src="https://latex.codecogs.com/png.latex?E%5BY%20%5Cmid%20d%20o(X=0)%5D"> with observational data. Given that we know that we cannot estimate causal effects without making causal assumptions, let’s formulate ours.</p>
<p>Let’s say that our causal DAG for the effects of Smoking on Cancer is the following:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span> u,</span>
<span id="cb1-2">                  m <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb1-3">                  y <span class="sc" style="color: #5E5E5E;">~</span> u <span class="sc" style="color: #5E5E5E;">+</span> m,</span>
<span id="cb1-4">                  <span class="at" style="color: #657422;">labels =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"x"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Smoking"</span>,</span>
<span id="cb1-5">                             <span class="st" style="color: #20794D;">"y"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Cancer"</span>,</span>
<span id="cb1-6">                             <span class="st" style="color: #20794D;">"m"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Tar"</span>,</span>
<span id="cb1-7">                             <span class="st" style="color: #20794D;">"u"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Genotype"</span>),</span>
<span id="cb1-8">                  <span class="at" style="color: #657422;">latent =</span> <span class="st" style="color: #20794D;">"u"</span>,</span>
<span id="cb1-9">                  <span class="at" style="color: #657422;">exposure =</span> <span class="st" style="color: #20794D;">"x"</span>,</span>
<span id="cb1-10">                  <span class="at" style="color: #657422;">outcome =</span> <span class="st" style="color: #20794D;">"y"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Therefore, we can use the <a href="https://david-salazar.github.io/2020/07/30/causality-the-front-door-criterion/">front-door formula</a> to estimate the causal effect of smoking: <img src="https://latex.codecogs.com/png.latex?E%5Cleft%5BY%20%5Cmid%20d%20o%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%5Cright%5D">.</p>
<p>Suppose, then, that we collect the following data:</p>
<p><img src="https://david-salazar.github.io/posts/causality/https:/cdn.mathpix.com/snip/images/6n3SO6qiX30qj9G6jSntJnaSnlZfjaUZv2rdgmlqxbk.rendered.fullsize.png" class="img-fluid"></p>
<p>Then, using the front-door criterion, the causal effect <img src="https://latex.codecogs.com/png.latex?E%5BY%20%5Cmid%20d%20o(X=0)%5D"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AE%5BY%20%5Cmid%20d%20o(X=0)%5D=&amp;%20%5Csum_%7Bz%7D%20P(Z=z%20%5Cmid%20X=0)%20%5Csum_%7Bx%5E%7B%5Cprime%7D%7D%20P%5Cleft(Y=1%20%5Cmid%20X=x%5E%7B%5Cprime%7D,%20Z=z%5Cright)%20P%5Cleft(X=x%5E%7B%5Cprime%7D%5Cright)%20%5C%5C%0A=&amp;%20P(Z=1%20%5Cmid%20X=0)%5BP(Y=1%20%5Cmid%20X=1,%20Z=1)%20P(X=1)%5C%5C%0A&amp;+P(Y=1%20%5Cmid%20X=0,%20Z=1)%20P(X=0)%5D%20%5C%5C%0A&amp;+P(Z=0%20%5Cmid%20X=0)%5BP(Y=1%20%5Cmid%20X=1,%20Z=0)%20P(X=1)%5C%5C%0A&amp;+P(Y=1%20%5Cmid%20X=0,%20Z=0)%20P(X=0)%5D%20%5C%5C%0A=&amp;%2020%20/%20400%20*%5B0.15%20*%200.5+0.95%20*%200.5%5D+380%20/%20400%20*%5B0.1%20*%200.5+0.9%20*%200.5%5D%20%5C%5C%0A=&amp;%200.5025%0A%5Cend%7Baligned%7D%0A"></p>
<p>Finally, we can calculate the ETT for Joe:</p>
[
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AE%20T%20T%20&amp;=E%5Cleft%5BY_%7B1%7D-Y_%7B0%7D%20%5Cmid%20X=1%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=1%5D-E%5Cleft%5BY_%7B0%7D%20%5Cmid%20X=1%5Cright%5D%20%5C%5C%0A&amp;=E%5BY%20%5Cmid%20X=1%5D-%5Cfrac%7BE%5BY%20%5Cmid%20d%20o(X=0)%5D-E%5BY%20%5Cmid%20X=0%5D%20P(X=0)%7D%7BP(X=1)%7D%20%5C%5C%0A&amp;=0.15-%5Cfrac%7B0.5025-0.9025%20*%200.5%7D%7B0.5%7D%20%5C%5C%0A&amp;=0.0475%3E0%0A%5Cend%7Baligned%7D">
<p>]</p>
<p>Therefore, given that <img src="https://latex.codecogs.com/png.latex?ETT%3E0">, <strong>by smoking Joe has increased his chances of suffering from Cancer</strong>. Thus, he <em>should</em> feel regret: the <em>causal effect</em> smoking has had <strong>in his life</strong> has been to increase his chances of suffering from cancer, relative to those chances in the hypothetical world where he never smoked in the first place.</p>
</section>
</section>
<section id="the-more-general-case" class="level2">
<h2 class="anchored" data-anchor-id="the-more-general-case">The more general case</h2>
<p>Let’s say that our treatment is discrete, but not binary. Is the Effect of Treatment on the Treated (ETT) identifiable? <a href="https://arxiv.org/abs/1205.2615">Pearl and Shipster</a> have given an answer to this question using <a href="https://david-salazar.github.io/2020/07/31/causality-testing-identifiability/"><strong>C-components</strong></a></p>
<section id="identifiability-using-c-components" class="level3">
<h3 class="anchored" data-anchor-id="identifiability-using-c-components">Identifiability using C-components</h3>
<p>Remember, two variables are assigned to the same c-component iff <em>they are connected by a bi-directed path</em>. The c-components themselves induce a factorization of the joint probability distribution in terms of <strong>c-factors</strong>: post-intervention distribution of the variables in the respective c-component under an intervention on all the other variables.</p>
<p>Just as before the causal effect was identified when <img src="https://latex.codecogs.com/png.latex?X"> and its children are in different C-components (i.e., there’s no bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its children that are also ancestors of <img src="https://latex.codecogs.com/png.latex?Y">), the <em>necessary counterfactual expression</em> to <strong>compute the ETT</strong>, <img src="https://latex.codecogs.com/png.latex?P(Y_x%20=%20y%7Cx')">, <strong>is identifiable if and only if</strong> <img src="https://latex.codecogs.com/png.latex?X"> and its children are in different C-components.</p>
<p>Indeed, whereas before (when we were trying to estimate the causal effect) we summed out <img src="https://latex.codecogs.com/png.latex?x"> from the <em>c-factor</em>, we now replace <img src="https://latex.codecogs.com/png.latex?x"> by <img src="https://latex.codecogs.com/png.latex?x'"> <a href="(https://david-salazar.github.io/2020/07/31/causality-testing-identifiability/)">from the c-factor and divide by <img src="https://latex.codecogs.com/png.latex?P(x')"></a>. Then, we take the decomposition induced by the c-factors and marginalize and condition on the appropriate variables to get the variable of interest.</p>
<p>That is, the same test is a sufficient test for causal effects identifiability and both a necessary and sufficient test for ETT identifiability.</p>
</section>
<section id="confirming-our-former-result" class="level3">
<h3 class="anchored" data-anchor-id="confirming-our-former-result">Confirming our former result</h3>
<p>Let’s take our former example of the causal model of Smoking on Cancer. This time, we will use bi-directed paths to show that there’s an unobserved confounder:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y,</span>
<span id="cb2-2">                  m <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb2-3">                  y <span class="sc" style="color: #5E5E5E;">~</span> m)</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(example, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">node_descendants</span>(<span class="st" style="color: #20794D;">"x"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">edge_linetype =</span> linetype, <span class="at" style="color: #657422;">color =</span> descendant)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-5">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-6">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-7">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"The ETT is identifiable!"</span>,</span>
<span id="cb3-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Because there's no bi-directed path between x and m"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Since <img src="https://latex.codecogs.com/png.latex?X"> has no bi-directed path to its child <img src="https://latex.codecogs.com/png.latex?m">, the counterfactual query <img src="https://latex.codecogs.com/png.latex?P(Y_x%20=%20y%20%7C%20x')"> is identifiable. Thus, the ETT is identifiable; confirming what we have done until now.</p>
<p>However, if it were not binary, we could derive an estimator for the ETT using the induced factorization by the c-components. First, we replace with <img src="https://latex.codecogs.com/png.latex?x'"> in the c-component where <img src="https://latex.codecogs.com/png.latex?x"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(x,%20y%20%7C%20do(m))%20=%20P(y%7Cm,%20x')%20P(x')%0A"></p>
<p>Whereas the other c-component is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(m%7C%20do(y,%20x))%20=%20P(m%7C%20do(x))%20=%20P(m%7Cx)%0A"></p>
<p>Therefore, the conditional distribution on <img src="https://latex.codecogs.com/png.latex?x'"></p>
<p>Conditioning on <img src="https://latex.codecogs.com/png.latex?x'"> and marginalizing <img src="https://latex.codecogs.com/png.latex?m">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y_x%20%7C%20x')%20=%20(%5Csum_z%20P(y%7C%20z,%20x')%20P(x')%20P(z%20%7C%20x))/(P(x'))%0A"></p>
<p>That is, we replace <img src="https://latex.codecogs.com/png.latex?x"> with <img src="https://latex.codecogs.com/png.latex?x'"> in the c-component where <img src="https://latex.codecogs.com/png.latex?x"> is, we condition on <img src="https://latex.codecogs.com/png.latex?x'"> by dividing the joint density and marginalize <img src="https://latex.codecogs.com/png.latex?m">. Thus, we derive the estimator for the ETT in this model by using the c-factors.</p>
</section>
<section id="not-identifiable" class="level3">
<h3 class="anchored" data-anchor-id="not-identifiable">Not identifiable</h3>
<p>Now let’s work with an example where the causal effect is identifiable, yet the <strong>counterfactual query <img src="https://latex.codecogs.com/png.latex?P(Y_x%20=%20y%20%7C%20x')"> is not</strong>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">example_not <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(s <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y,</span>
<span id="cb4-2">                      x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> s, </span>
<span id="cb4-3">                      x <span class="sc" style="color: #5E5E5E;">~</span> z,</span>
<span id="cb4-4">                      z <span class="sc" style="color: #5E5E5E;">~</span> s,</span>
<span id="cb4-5">                      y <span class="sc" style="color: #5E5E5E;">~</span> x</span>
<span id="cb4-6">                      )</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(example_not, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-2">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-3">    <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">edge_linetype =</span> linetype)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-4">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-5">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-6">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-7">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"The ETT is not identifiable"</span>,</span>
<span id="cb5-8">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"X is connected by a bi-directed path with S"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>We’ve seen how regret is logically defined in terms of the Effect of Treatment on the Treated (ETT). We’ve also realized what are the conditions for the ETT to be identifiable and how to derive an estimator for it in terms of observational data.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-08-16-causality-effect-of-treatment-on-the-treated.html</guid>
  <pubDate>Sun, 16 Aug 2020 05:00:00 GMT</pubDate>
  <media:content url="https://cdn.mathpix.com/snip/images/6n3SO6qiX30qj9G6jSntJnaSnlZfjaUZv2rdgmlqxbk.rendered.fullsize.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Causality: Counterfactuals - Clash of Worlds</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html</link>
  <description><![CDATA[ 




<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>We’ve seen how the language of causality require an exogenous intervention on the values of <img src="https://latex.codecogs.com/png.latex?X">; so far we’ve studied interventions on all the population, represented by the expression <img src="https://latex.codecogs.com/png.latex?do(X)">. Nevertheless, with this language, there are plenty of interventions that remain outside our realm: most notably, <strong>counterfactual expressions where the antecedent is in contradiction with the observed behavior</strong>: there’s a <em>clash</em> between the observed world and the <strong>hypothetical world of interest</strong>.</p>
<p>To solve this conundrum we will need to set up a more elaborate language whereby we leverage the <strong>invariant information</strong> from the observed world into the <em>hypothetical world</em>.</p>
<p>These type of counterfactual queries are fundamental in our study of causality; as they enable us to answer such questions as: interventions on sub-populations; additive interventions; mediation analysis through ascertaining direct and indirect effects; and to study probability of causation (sufficient and necessary causes).</p>
<p>In this post, all citations come from Pearl’s book: Causality.</p>
</section>
<section id="game-plan" class="level2">
<h2 class="anchored" data-anchor-id="game-plan">Game Plan</h2>
<p>In this blogpost, we will define <strong>Structural Causal Models</strong> (SCM) and explore how they encapsulate all the information we need in order to <em>study counterfactuals</em>. First, we’ll analyze why we cannot use the do-calculus to study counterfactuals where the antecedent contradicts the observed world. Secondly, we will define counterfactuals as <strong>derived properties</strong> of SCM and realize how interventional data undetermines counterfactual information. Thirdly, we will formulate a SCM to put what we have learned into action.</p>
</section>
<section id="we-change-by-taking-the-road-less-traveled-by" class="level2">
<h2 class="anchored" data-anchor-id="we-change-by-taking-the-road-less-traveled-by">We change by taking the road less traveled by</h2>
<p>Let’s play with Frost’s famous poem:</p>
<blockquote class="blockquote">
<p>Two roads diverged in a wood, and I— I took the one less traveled by, And that has made all the difference.</p>
</blockquote>
<p>What if Frost <strong>hadn’t taken</strong> the road less traveled by? Let’s say that by taking the road less traveled by, it took Frost <img src="https://latex.codecogs.com/png.latex?Y=1"> hour of driving time. Given how long it took him on the road less traveled by, how long would it had taken him on the other road?</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5B%20Y%7C%20%5C%20%20%5Ctext%7Bdo(Other%20road)%7D,%20Y%20=%201%5D%0A"> There’s a <strong>clash</strong> between the <img src="https://latex.codecogs.com/png.latex?Y"> we are trying to estimate and the observed <img src="https://latex.codecogs.com/png.latex?Y%20=%201">. Unfortunately, the do-operator does not offer us the possibility of distinguishing between the two variables themselves: one standing for the <img src="https://latex.codecogs.com/png.latex?Y"> if we take the road less traveled by, the other <img src="https://latex.codecogs.com/png.latex?Y"> for the <strong>hypothetical</strong> <img src="https://latex.codecogs.com/png.latex?Y"> if Frost had taken the other road. That is, the <strong>different <img src="https://latex.codecogs.com/png.latex?y">’s are events occurring in different worlds</strong>.</p>
<p>Because the do-calculus offers <em>no way of connecting the information across the different worlds</em>, it means that <strong>we cannot use interventional experiments to estimate the counterfactuals</strong>. Indeed, the Frost <em>after</em> taking the road less traveled by is a very different Frost <em>than he was before</em> taking any of the roads.</p>
</section>
<section id="the-ladder-of-causation" class="level2">
<h2 class="anchored" data-anchor-id="the-ladder-of-causation">The Ladder of Causation</h2>
<p>Before, we had seen that observational information <a href="https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/">is not enough to distinguish between different causal diagrams</a>. We’ll show that the same thing happens with counterfactuals: information from interventions is not enough to distinguish between different Structural Causal Diagrams. Indeed, prediction, intervention and counterfactuals represent a <strong>natural hierarchy</strong> of <em>reasoning tasks</em>, with increasing levels of refinement and increasing demands on the knowledge required to accomplish them. Pearl calls this hierarchy the <strong>Ladder of Causation</strong>:</p>
<p><img src="https://david-salazar.github.io/images/ladder.png" class="img-fluid"></p>
<p>Whereas for prediction one only needs a joint distribution function, the analysis of intervention requires a causal structure; finally, <strong>processing counterfactuals</strong> requires information about the <strong>functional relationships</strong> that determine that determine the variables and/or the distribution of the <strong>omitted factors</strong>. We will encode all this necessary information with <strong>Structural Causal Models (SCM)</strong>.</p>
</section>
<section id="defining-counterfactuals" class="level2">
<h2 class="anchored" data-anchor-id="defining-counterfactuals">Defining Counterfactuals</h2>
<p>A Structural Causal Model is a triplet of Unobserved Exogenous Variables (<img src="https://latex.codecogs.com/png.latex?U">) called <strong>background variables</strong>, Observed Endogenous Variables (<img src="https://latex.codecogs.com/png.latex?V">) and Functional relationships (<img src="https://latex.codecogs.com/png.latex?F">) that map for each <img src="https://latex.codecogs.com/png.latex?V_i"> from their respective domain <img src="https://latex.codecogs.com/png.latex?U_i%20%5Ccup%20Pa_i"> (<img src="https://latex.codecogs.com/png.latex?Pa_i"> are the parents of <img src="https://latex.codecogs.com/png.latex?i">) into <img src="https://latex.codecogs.com/png.latex?V_i"> thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Av_i%20=%20f_i(Pa_i,%20u_i)%0A"></p>
<p>Every SCM can be associated with a causal DAG. However, the graph merely identifies the endogenous and background variables; it does not specify the functional form of <img src="https://latex.codecogs.com/png.latex?f_i"> nor the distribution of the background variables.</p>
<p><strong>A counterfactual</strong> is defined by a submodel, <img src="https://latex.codecogs.com/png.latex?M_x">, where the the functional relationship for <img src="https://latex.codecogs.com/png.latex?X"> is replaced to make <img src="https://latex.codecogs.com/png.latex?X=x"> hold true under any <img src="https://latex.codecogs.com/png.latex?u">. Thus, the potential response of <img src="https://latex.codecogs.com/png.latex?Y"> to action <img src="https://latex.codecogs.com/png.latex?do(X=x)"> denoted by <img src="https://latex.codecogs.com/png.latex?Y_x(u)"> is the solution for <img src="https://latex.codecogs.com/png.latex?Y"> on the set of equations <img src="https://latex.codecogs.com/png.latex?F_x"> in <img src="https://latex.codecogs.com/png.latex?M_x">.</p>
<p>If we define a probability function over the background variables, <img src="https://latex.codecogs.com/png.latex?P(u)">, we can define the probability over the endogenous variables thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20=%20y)%20:=%20%5Csum_%7Bu%20%7C%20Y(u)%20=%20y%7D%20P(u)%0A"></p>
<p>Therefore, the probability of counterfactual statements is thus derived:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y_x%20=%20y)%20:=%20%5Csum_%7Bu%20%7C%20Y_x(u)%20=%20y%7D%20P(u)%0A"></p>
<p>Note that we can define <img src="https://latex.codecogs.com/png.latex?P(Y%20=%20y%20%7C%20do(X=x))%20=%20P(Y_x%20=%20y)">. This solution to the SCM coincides with the truncated factorization obtained by pruning arrows from a causal DAG.</p>
<section id="connecting-different-worlds-through-background-variables" class="level3">
<h3 class="anchored" data-anchor-id="connecting-different-worlds-through-background-variables">Connecting different worlds through background variables</h3>
<p>The determining feature of most counterfactuals is that we are interested in a <strong>conditional probability</strong> such that the information we are updating on is in contradiction with the counterfactual antecedent. In math terms:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y_%7Bx'%7D%20=%20y'%20%7C%20X%20=%20x,%20Y%20=%20y)%20=%20%5Csum_u%20P(Y_%7Bx'%7D(u)%20=%20y')%20P(u%20%7C%20x,%20y)%0A"> First, notice that we are using the information from one causal world (<img src="https://latex.codecogs.com/png.latex?%3CM,%20u%3E">) where we observe <img src="https://latex.codecogs.com/png.latex?(X=x,%20Y%20=%20y)"> to find out the probability of a statement <img src="https://latex.codecogs.com/png.latex?Y_x"> in a different causal world (<img src="https://latex.codecogs.com/png.latex?%3CM_x,%20u%3E">). That is, the <em>counterfactual antecedent</em> “<strong>must be evaluated under the same background conditions</strong> as those prevailing in the observed world”.</p>
<p>“The background variables are thus the main carriers of information from the actual world to the hypothetical world; they serve as the guardians of invariance.” To do so, we must first update our knowledge of <img src="https://latex.codecogs.com/png.latex?P(u)"> to obtain <img src="https://latex.codecogs.com/png.latex?P(u%7Cx,%20y)">. Therefore, to be able to answer counterfactual queries we must have a <strong>distribution for the background variables</strong>. Indeed, this key step is known as <strong>abduction</strong>: reasoning from evidence (<img src="https://latex.codecogs.com/png.latex?(x,%20y)"> observed) to explanation (the background variables).</p>
<p>This is the fundamental characteristic counterfactual statements: we need to <em>route the impact of known facts</em> through U”.</p>
</section>
<section id="after-abduction-comes-action-and-prediction" class="level3">
<h3 class="anchored" data-anchor-id="after-abduction-comes-action-and-prediction">After abduction comes action and prediction</h3>
<p>Once we have evaluated the prevailing background conditions, we use these in the sub-model <img src="https://latex.codecogs.com/png.latex?M_%7Bx'%7D">, where <img src="https://latex.codecogs.com/png.latex?x'"> is the antecedent of the counterfactual. Finally, we use the equations in this modified SCM to predict the probability of <img src="https://latex.codecogs.com/png.latex?Y_%7Bx'%7D">, the consequence of the counterfactual.</p>
<p>Pearl has a great temporal metaphor for this whole process:</p>
<blockquote class="blockquote">
<p>Abduction explains the past (U) in light of the current evidence. The action bends the course of history to comply with the hypothetical condition <img src="https://latex.codecogs.com/png.latex?X=%20x'">. Finally, we predict the future based on our new understanding of the past and our newly established condition, <img src="https://latex.codecogs.com/png.latex?X=x'">.</p>
</blockquote>
</section>
</section>
<section id="probabilities-for-the-dead" class="level2">
<h2 class="anchored" data-anchor-id="probabilities-for-the-dead">Probabilities for the dead</h2>
<p>Let’s formulate the following example that will show why information from interventions undetermines counterfactuals and that will serve as practice in computing counterfactuals. The example is taken from the excellent paper from <a href="https://causalai.net/r60.pdf">Bareinboim (et alter) (PDF)</a>:</p>
Let ( ^{<em>}=={U_{1}, U_{2}}, ={X, Y}, ^{</em>}, P(U), ) where [ ^{*}={
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bll%7D%0AX%20&amp;%20%5Cleftarrow%20U_%7B1%7D%20%5C%5C%0AY%20&amp;%20%5Cleftarrow%20U_%7B2%7D%0A%5Cend%7Barray%7D">
<p>. ]</p>
<p>and <img src="https://latex.codecogs.com/png.latex?U_1,%20U_2"> are binary.</p>
<p>Notice that we expect that any intervention will lead us to conclude that the treatment <img src="https://latex.codecogs.com/png.latex?X"> is not effective: <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20do(X))%20=%20P(Y)">. Suppose that we conclude exactly this with a RCT.</p>
<p><strong>Is this intervention evidence</strong> enough to argue for <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5E%7B*%7D">? No! Interventional information undetermines counterfactual information. Notice that other SCM, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5E%7B'%7D">, is also consistent with such causal effects and yet leads to a different counterfactual answer:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BF%7D%5E%7B%5Cprime%7D=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0AX%20&amp;%20%5Cleftarrow%20U_%7B1%7D%20%5C%5C%0AY%20&amp;%20%5Cleftarrow%20X%20U_%7B2%7D+(1-X)%5Cleft(1-U_%7B2%7D%5Cright)%0A%5Cend%7Barray%7D%5Cright.%0A"></p>
<p>In both <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5E%7B'%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%5E%7B*%7D">, we expect an intervention on <img src="https://latex.codecogs.com/png.latex?X"> to lead to no causal effect: <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20do(X))%20=%20P(Y)">.</p>
<p><img src="https://david-salazar.github.io/posts/causality/https:/cdn.mathpix.com/snip/images/iQHSNdrTUmxs6kJrz577MAtWDUpuP1GqchRppNY_lXo.original.fullsize.png" class="img-fluid"></p>
<p>However, notice that they lead to very different answers for counterfactual queries. Suppose, then, that you have a patient <img src="https://latex.codecogs.com/png.latex?S"> that took the treatment and died: what is the probability that ( S ) would have survived had they not been treated? We write this as ( P(Y_{X=0}=1 X=1, Y=0), )</p>
<blockquote class="blockquote">
<p>In ( ^{<em>}, ) we have ( P<sup>{</sup>{</em>}}(Y_{X=0}=1 X=1, Y=0)=0, ) whereas in ( ^{} ) we have the exact opposite pattern, ( P<sup>{</sup>{}}(Y_{X=0}=1 X=1, Y=0)=1 ). These two models thus make diametrically opposed predictions about whether ( S ) would have survived had they not taken the treatment.</p>
</blockquote>
<p>In other words, the best explanation for ( S ) ’s death may be completely different depending on whether the world is like ( ^{<em>} ) or ( ^{} ). In ( ^{</em>}, S ) would hav died anyway, while in ( ^{}, S ) would actually have survived, if only they had not been give the treatment.</p>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>Counterfactual queries are a crucial part of our reasoning tools. Yet they pose a fundamental challenge: most of the time, the counterfactual antecedent contradicts the observed evidence. Thus, creating a clashing of worlds between the observed world and the hypothetical world that is our object of study.</p>
<p>To reconcile these two worlds, we must posit a SCM that <strong>leverages the invariant information across causal worlds</strong>: the background variables. Once we have this information, we can answer counterfactual queries. Finally, we saw how interventional information is far from being sufficient to deliver answer to these queries.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-08-10-causality-counterfactuals-clash-of-worlds.html</guid>
  <pubDate>Mon, 10 Aug 2020 05:00:00 GMT</pubDate>
  <media:content url="https://david-salazar.github.io/images/ladder.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Causality: Testing Identifiability</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability.html</link>
  <description><![CDATA[ 




<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>We’ve defined causal effects as an <a href="https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/">interventional</a> distribution and posit two identification strategies to estimate them: the <a href="https://david-salazar.github.io/2020/07/25/causality-to-adjust-or-not-to-adjust/">back-door</a> and the <a href="https://david-salazar.github.io/2020/07/30/causality-the-front-door-criterion/">front-door</a> criteria. However, we cannot always use these criteria; sometimes, we <em>cannot measure the necessary variables</em> to use either of them.</p>
<p>More generally, given a causal model and some incomplete set of measurements, when is <em>the causal effect</em> of interest <strong>identifiable?</strong> In this blog post, we will develop a <strong>graphical criterion</strong> to answer this question by exploiting the concept of c-components. Finally, we will put the criterion in practice with multiple examples.</p>
</section>
<section id="all-you-can-estimate-markov-models" class="level2">
<h2 class="anchored" data-anchor-id="all-you-can-estimate-markov-models">All you can estimate: Markov Models</h2>
<p>When we can obtain measurements of all the variables in the causal model, we say that our causal model is Markovian. In this case, the <strong>adjustment formula</strong> is our identification strategy: any causal effect <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Y"> is identifiable if we have measurements of the parents of <img src="https://latex.codecogs.com/png.latex?X">, <img src="https://latex.codecogs.com/png.latex?Pa(X)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y=y%7C%5Ctext%7Bdo%7D(X=x))%20=%20%5Csum_%7Bz%7D%20P(Y=y%20%7C%20X=x,%20Pa(X)=z)%20P(Pa(X)=z)%0A"> What happens when you cannot observe the parents of <img src="https://latex.codecogs.com/png.latex?x">?</p>
</section>
<section id="semi-markovian-models" class="level2">
<h2 class="anchored" data-anchor-id="semi-markovian-models">Semi-Markovian Models</h2>
<p>If a variable that is unobserved has two descendants in the graph, the Markovian property is violated. We may or may not be able to use the adjustment formula. For example, if one of the parents of <img src="https://latex.codecogs.com/png.latex?X"> is unobserved, we cannot use it as our identification strategy. Even then, we <strong>may be able</strong> to use either the back-door or the front-door criteria.</p>
<p>Let’s start studying the problem with the following example. In this case, a <em>bi-directed dashed edge</em> represents a <strong>hidden common cause</strong> between the variables. We refer to all unmeasured variables by <img src="https://latex.codecogs.com/png.latex?U">, all of the observed variables by <img src="https://latex.codecogs.com/png.latex?V"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> z2,</span>
<span id="cb1-2">                  z1 <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb1-3">                  z1 <span class="sc" style="color: #5E5E5E;">~</span> z2,</span>
<span id="cb1-4">                  z1 <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y,</span>
<span id="cb1-5">                  y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span>z1 <span class="sc" style="color: #5E5E5E;">+</span>z2)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To identify the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> on all of the other observed variables <img src="https://latex.codecogs.com/png.latex?v">, we must be able to estimate the post intervention probabilities, <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))">, from the pre-intervention probabilities that we can observe.</p>
<p>To begin to study this question, we must remember that our causal model is simultaneously a probabilistic model. In particular, they induce a <strong>decomposition of the joint probability distribution</strong> because each variable is independent of all its non-descendants given its direct parents in the graph. However, when our model contains <em>unobserved confounders</em>, we must <em>marginalize them</em> in order to obtain the joint probability distribution of the observed variables:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v)%20=%20%5Csum_u%20%5Cprod_i%20P(v_i%7C%20pa_i,%20u%5Ei)%20P(u)%0A"></p>
<p>In this case, the decomposition of the observables is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP(v)=&amp;%20%5Csum_%7Bu_%7B1%7D%7D%20P%5Cleft(x%20%5Cmid%20u_%7B1%7D%5Cright)%20P%5Cleft(z_%7B2%7D%20%5Cmid%20z_%7B1%7D,%20u_%7B1%7D%5Cright)%20P%5Cleft(u_%7B1%7D%5Cright)%20%5C%5C%0A&amp;%20%5Cquad%20%5Csum_%7Bu_2%7D%20P%5Cleft(z_%7B1%7D%20%5Cmid%20x,%20u_%7B2%7D%5Cright)%20P%5Cleft(y%20%5Cmid%20x,%20z_%7B1%7D,%20z_%7B2%7D,%20u_%7B2%7D%5Cright)%20P%5Cleft(u_%7B2%7D%5Cright)%0A%5Cend%7Baligned%7D%0A"></p>
<p>Given that <img src="https://latex.codecogs.com/png.latex?P(v%7Cdo(X=x))"> represents an intervention, it can be represented by truncating the above expression such that we do not calculate the probability of <img src="https://latex.codecogs.com/png.latex?X">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP(v%20%7C%20do(X))=&amp;%20%5Csum_%7Bu_%7B1%7D%7D%20P%5Cleft(z_%7B2%7D%20%5Cmid%20z_%7B1%7D,%20u_%7B1%7D%5Cright)%20P%5Cleft(u_%7B1%7D%5Cright)%20%5C%5C%0A&amp;%20%5Ccdot%20%5Csum_%7Bu_2%7D%20P%5Cleft(z_%7B1%7D%20%5Cmid%20x,%20u_%7B2%7D%5Cright)%20P%5Cleft(y%20%5Cmid%20x,%20z_%7B1%7D,%20z_%7B2%7D,%20u_%7B2%7D%5Cright)%20P%5Cleft(u_%7B2%7D%5Cright)%0A%5Cend%7Baligned%7D%0A"></p>
<p>Can we express <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))"> in terms of observed variables? First, we must take a brief de-tour by confounded components.</p>
</section>
<section id="confounded-components" class="level2">
<h2 class="anchored" data-anchor-id="confounded-components">Confounded components</h2>
<p>Notice that in both expressions the unobserved confounders partition into disjoint groups the observed variables: <strong>two variables are assigned to the same group if and only if they are connected by a bi-directed path</strong>. Each group, <img src="https://latex.codecogs.com/png.latex?S_k">, is called a <strong>confounded</strong> component (c-component). In this case, we have two c-components that induce two factorizations (c-factors):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ_%7B1%7D=%5Csum_%7Bu_%7B1%7D%7D%20P%5Cleft(x%20%5Cmid%20u_%7B1%7D%5Cright)%20P%5Cleft(z_%7B2%7D%20%5Cmid%20z_%7B1%7D,%20u_%7B1%7D%5Cright)%20P%5Cleft(u_%7B1%7D%5Cright)%20%5C%5C%0AQ_%7B2%7D=%5Csum_%7Bu_2%7D%20P%5Cleft(z_%7B1%7D%20%5Cmid%20x,%20u_%7B2%7D%5Cright)%20P%5Cleft(y%20%5Cmid%20x,%20z_%7B1%7D,%20z_%7B2%7D,%20u_%7B2%7D%5Cright)%20P%5Cleft(u_%7B2%7D%5Cright)%0A"></p>
<p>Notice that <strong>each (c-factor) <img src="https://latex.codecogs.com/png.latex?Q_k"> can be interpreted as the post-intervention distribution</strong> of the variables in <img src="https://latex.codecogs.com/png.latex?S_k"> under an intervention on all the other variables. Observe that we can express the joint observed distribution as a product of the c-factors:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v)%20=%20Q_1%20%5Ccdot%20Q_2%0A"> We can in turn define <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))"> in terms of <img src="https://latex.codecogs.com/png.latex?Q_1,%20Q_2"> <strong>if we marginalize <img src="https://latex.codecogs.com/png.latex?P(x%7C%20u_1)"> out of <img src="https://latex.codecogs.com/png.latex?Q_1"></strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v%20%7C%20do(X))%20=%20Q_2%20%5Csum_x%20Q_1%20=%20Q_2%20%5Ccdot%20Q_1%5Ex%0A"></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))"> <em>will be identifiable</em> if: (a) we can compute the post-intervention probabilities <img src="https://latex.codecogs.com/png.latex?Q_1,%20Q_2"> in terms of pre-intervention probabilities; and (b) we can marginalize <img src="https://latex.codecogs.com/png.latex?x"> out of the estimated <img src="https://latex.codecogs.com/png.latex?Q_1"> with pre-intervention probabilities to compute <img src="https://latex.codecogs.com/png.latex?Q_1%5Ex">.</p>
<p>In fact, <a href="https://ftp.cs.ucla.edu/pub/stat_ser/R290-A.pdf">Tian and Pearl (PDF)</a> show that <strong>each c-factor is always identifiable</strong>. Therefore, the only condition to compute <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))"> if and only if <img src="https://latex.codecogs.com/png.latex?Q_1%5Ex"> is identifiable, too. In this case:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ_1%20=%20P(z_2%20%7C%20x,%20z_1)%20P(x)%0A"> Thus, we can marginalize <img src="https://latex.codecogs.com/png.latex?x"> out of <img src="https://latex.codecogs.com/png.latex?Q_1"> by summing over the values of <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AQ_1%5Ex%20=%20%5Csum_%7Bx'%7D%20%20P(z_2%20%7C%20x',%20z_1)%20P(x')%0A"></p>
<p>Finally, our estimate for <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X))"> is the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v%20%7C%20do(X))%20=%20%5Cfrac%7BP(v)%7D%7BQ_1%7D%20%5Csum_%7Bx%5E%7B%5Cprime%7D%7D%20P%5Cleft(z_%7B2%7D%20%5Cmid%20x%5E%7B%5Cprime%7D,%20z_%7B1%7D%5Cright)%20P%5Cleft(x%5E%7B%5Cprime%7D%5Cright)%0A"> Let’s generalize from this example.</p>
</section>
<section id="a-general-criteria-for-identification" class="level2">
<h2 class="anchored" data-anchor-id="a-general-criteria-for-identification">A general criteria for identification</h2>
<p>First, notice that for any graph with bi-directed paths, we can decompose the joint probability distribution by using the partition into c-components and their respective c-factors:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v)=%5Cprod_%7Bj=1%7D%5E%7Bk%7D%20Q_%7Bj%7D%20%20%0A"></p>
<p>Also notice that the truncated distribution generated by intervening on <img src="https://latex.codecogs.com/png.latex?x"> can be represented with c-factors thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(v%20%7C%20do(X=x))=Q_%7Bx%7D%5E%7Bx%7D%20%5Cprod_%7Bi%7D%20Q_%7Bi%7D%0A"> Where <img src="https://latex.codecogs.com/png.latex?Q_%7Bx%7D%5E%7Bx%7D"> is the c-factor where <img src="https://latex.codecogs.com/png.latex?x"> is located once we remove <img src="https://latex.codecogs.com/png.latex?x"> from the factorization. Therefore, <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X=x)"> is identifiable if <img src="https://latex.codecogs.com/png.latex?Q_%7Bx%7D%5E%7Bx%7D"> is identifiable, too.</p>
<p>In fact, <a href="https://ftp.cs.ucla.edu/pub/stat_ser/R290-A.pdf">Tian and Pearl (PDF)</a> show that <img src="https://latex.codecogs.com/png.latex?Q_%7Bx%7D%5E%7Bx%7D"> is identifiable if and only if there is no bi-directed path (<strong>a path with only bi-directed edges</strong>) connecting <img src="https://latex.codecogs.com/png.latex?X"> to any of its descendants. Therefore, we arrive at the following test to decide whether <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X=x)"> is identifiable:</p>
<blockquote class="blockquote">
<p><img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X=x)"> is identifiable if and only if <strong>there is no bi-directed path connecting <img src="https://latex.codecogs.com/png.latex?X"> to any of its descendants</strong>.</p>
</blockquote>
<p>Notice that if <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X=x)"> is identifiable so it is <img src="https://latex.codecogs.com/png.latex?P(Y%20%7C%20do(X=x))">. Therefore, <strong>our criterion is sufficient to determine whether <img src="https://latex.codecogs.com/png.latex?P(v%20%7C%20do(X=x))"> is non-identifiable</strong>. Given that we are only interested in the causal effect on a single variable <img src="https://latex.codecogs.com/png.latex?Y">, we can simplify the problem by only considering the subgraph of all the variables that are ancestors of <img src="https://latex.codecogs.com/png.latex?Y"></p>
<section id="intuition" class="level3">
<h3 class="anchored" data-anchor-id="intuition">Intuition</h3>
<p>What is the intuition of our identifiability test? The key to identifiability <em>lies not</em> in blocking back-door paths between X and Y but, rather, in <strong>blocking back-door paths between <img src="https://latex.codecogs.com/png.latex?X"> and any of its descendants that is an ancestor of <img src="https://latex.codecogs.com/png.latex?Y"></strong>. Thus, by blocking these paths, we can ascertain <em>which part of the association</em> we observe is <strong>spurious</strong> and which <em>genuinely causative</em>.</p>
<p>Let’s put this intuition into practice with the following examples.</p>
</section>
</section>
<section id="examples" class="level2">
<h2 class="anchored" data-anchor-id="examples">Examples</h2>
<section id="first-example" class="level3">
<h3 class="anchored" data-anchor-id="first-example">First Example</h3>
<p>Let’s start with our former example. Why was it identifiable? All the other variables are ancestors of <img src="https://latex.codecogs.com/png.latex?Y">. Therefore, we cannot simplify the problem. We must look, then, if there is a bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its children:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(example, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-2">  <span class="fu" style="color: #4758AB;">node_descendants</span>(<span class="st" style="color: #20794D;">"x"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">color =</span> descendant)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-5">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>), <span class="at" style="color: #657422;">edge_linetype =</span> linetype)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-6">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-7">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"The causal effect of X is identifiable"</span>,</span>
<span id="cb2-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"There's no bi-directed path between X and its descendats"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Given that there is no bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its descendants, the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> is identifiable.</p>
</section>
<section id="second-example" class="level3">
<h3 class="anchored" data-anchor-id="second-example">Second Example</h3>
<p>Let’s take another example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">non_identifiable_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span> z,</span>
<span id="cb3-2">                                   x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~~</span> z,</span>
<span id="cb3-3">                                   x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y,</span>
<span id="cb3-4">                                   w <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb3-5">                                   w <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> z,</span>
<span id="cb3-6">                                   y <span class="sc" style="color: #5E5E5E;">~</span> w,</span>
<span id="cb3-7">                                   y <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> z)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>To find out whether the effect is identifiable, we look for a bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its descendants. If there is none, the effect is identifiable.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(non_identifiable_example, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;">node_descendants</span>(<span class="st" style="color: #20794D;">"x"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">color =</span> descendant)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>), <span class="at" style="color: #657422;">edge_linetype =</span> linetype)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-7">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Notice that there is a bi-directed path from <img src="https://latex.codecogs.com/png.latex?X"> to <img src="https://latex.codecogs.com/png.latex?W"> (which is one of its descendants) through <img src="https://latex.codecogs.com/png.latex?Z">. Then, according to our criterion, the effect is non-identifiable.</p>
</section>
<section id="third-example" class="level3">
<h3 class="anchored" data-anchor-id="third-example">Third example</h3>
<p>Finally, let’s end with the following example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">third_example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(z1 <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> z2,</span>
<span id="cb5-2">                        x <span class="sc" style="color: #5E5E5E;">~</span> z2,</span>
<span id="cb5-3">                        x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> z2,</span>
<span id="cb5-4">                        x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y, </span>
<span id="cb5-5">                        z2 <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> y,</span>
<span id="cb5-6">                        z3 <span class="sc" style="color: #5E5E5E;">~</span> z2,</span>
<span id="cb5-7">                        x <span class="sc" style="color: #5E5E5E;">~</span><span class="er" style="color: #AD0000;">~</span> z3,</span>
<span id="cb5-8">                        y <span class="sc" style="color: #5E5E5E;">~</span> z1 <span class="sc" style="color: #5E5E5E;">+</span> z3)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As in the previous examples, we look for a bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its descendants.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">tidy_dagitty</span>(third_example, <span class="at" style="color: #657422;">layout =</span> <span class="st" style="color: #20794D;">"nicely"</span>, <span class="at" style="color: #657422;">seed =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">node_descendants</span>(<span class="st" style="color: #20794D;">"x"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-3">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">if_else</span>(direction <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"-&gt;"</span>, <span class="st" style="color: #20794D;">"solid"</span>, <span class="st" style="color: #20794D;">"dashed"</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">x =</span> x, <span class="at" style="color: #657422;">y =</span> y, <span class="at" style="color: #657422;">xend =</span> xend, <span class="at" style="color: #657422;">yend =</span> yend, <span class="at" style="color: #657422;">color =</span> descendant)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">geom_dag_edges</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">end_cap =</span> ggraph<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">circle</span>(<span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">"mm"</span>), <span class="at" style="color: #657422;">edge_linetype =</span> linetype)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-6">  <span class="fu" style="color: #4758AB;">geom_dag_point</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-7">  <span class="fu" style="color: #4758AB;">geom_dag_text</span>(<span class="at" style="color: #657422;">col =</span> <span class="st" style="color: #20794D;">"white"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Notice that <img src="https://latex.codecogs.com/png.latex?X"> has no bi-directed path with its only descendant that is not <img src="https://latex.codecogs.com/png.latex?Y">. Therefore, the causal effect is identifiable.</p>
</section>
</section>
<section id="what-about-a-necessary-condition-for-identifiability" class="level2">
<h2 class="anchored" data-anchor-id="what-about-a-necessary-condition-for-identifiability">What about a necessary condition for identifiability?</h2>
<p>Our stated <strong>test is sufficient but not necessary</strong> for identifiability. Is there a necessary and sufficient condition? Yes, <a href="https://ftp.cs.ucla.edu/pub/stat_ser/r327.pdf">there is such an algorithm by Pearl and Shipster (PDF)</a>. It extends the ideas we’ve seen in this post and returns an estimator of the causal effect in question in terms of pre-intervention probabilities. It is complete and equivalent to Pearl’s do-calculus.</p>
<p>In R, the <code>causaleffect</code> package has an implementation of this algorithm. It can be used thus for our first example:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">first_example_igraph <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">graph.formula</span>(x <span class="sc" style="color: #5E5E5E;">-+</span> z_2,</span>
<span id="cb7-2">                                      z_2 <span class="sc" style="color: #5E5E5E;">-+</span> x, </span>
<span id="cb7-3">                                      x <span class="sc" style="color: #5E5E5E;">-+</span> z_1,</span>
<span id="cb7-4">                                      z_2 <span class="sc" style="color: #5E5E5E;">-+</span> z_1,</span>
<span id="cb7-5">                                      z_1 <span class="sc" style="color: #5E5E5E;">-+</span> y,</span>
<span id="cb7-6">                                      y <span class="sc" style="color: #5E5E5E;">-+</span> z_1,</span>
<span id="cb7-7">                                      x <span class="sc" style="color: #5E5E5E;">-+</span> y,</span>
<span id="cb7-8">                                      z_1 <span class="sc" style="color: #5E5E5E;">-+</span> y,</span>
<span id="cb7-9">                                      z_2 <span class="sc" style="color: #5E5E5E;">-+</span> y, <span class="at" style="color: #657422;">simplify =</span> <span class="cn" style="color: #8f5902;">FALSE</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb7-10">                      <span class="fu" style="color: #4758AB;">set.edge.attribute</span>(<span class="st" style="color: #20794D;">"description"</span>, <span class="at" style="color: #657422;">index =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">6</span>), <span class="st" style="color: #20794D;">"U"</span>)</span>
<span id="cb7-11">ce <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">causal.effect</span>(<span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">"y"</span>, <span class="at" style="color: #657422;">x =</span> <span class="st" style="color: #20794D;">"x"</span>, <span class="at" style="color: #657422;">z =</span> <span class="cn" style="color: #8f5902;">NULL</span>, <span class="at" style="color: #657422;">G =</span> first_example_igraph, <span class="at" style="color: #657422;">expr =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb7-12"><span class="fu" style="color: #4758AB;">plot</span>(<span class="fu" style="color: #4758AB;">TeX</span>(ce), <span class="at" style="color: #657422;">cex =</span> <span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In Semi-Markovian models, we have hidden common causes between our variables that can derail all of our identification strategies. We’ve seen that a sufficient test for identifiability is given by the nature of the hidden common causes that we represent with bi-directed edges. If there’s a bi-directed path between <img src="https://latex.codecogs.com/png.latex?X"> and its descendants that are also ancestors of <img src="https://latex.codecogs.com/png.latex?Y">, the causal effect is non-identifiable.</p>
<p>We’ve also presented a sufficient and necessary condition and showed how to use it in R. The condition is complete and, when the effect is identifiable, returns an estimator that we can use to estimate the causal effect using observational data.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-07-31-causality-testing-identifiability.html</guid>
  <pubDate>Fri, 31 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: The front-door criterion</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion.html</link>
  <description><![CDATA[ 




<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In a <a href="https://david-salazar.github.io/2020/07/25/causality-to-adjust-or-not-to-adjust/">past blogpost</a>, I’ve explore the backdoor criterion: a simple <em>graphical algorithm</em>, we can define <strong>which variables we must include</strong> in our analysis in order to <strong>cancel out all the information coming from different causal relationships than the one we are interested</strong>. However, these variables are not always measured. What else can we do?</p>
<p>In this blogpost, I’ll explore the front-door criterion: i) an intuitive proof of why it works; (ii) how to estimate it; (iii) what are its fundamental assumptions; finally, (iv) an experiment with monte-carlo samples. Whereas the back-door criterion blocks all the non-causal information that <img src="https://latex.codecogs.com/png.latex?X"> could possibly pick up, the <strong>front-door exploits the outgoing information from <img src="https://latex.codecogs.com/png.latex?X"></strong> to derive a causal estimator.</p>
</section>
<section id="the-limits-of-the-back-door-a-quick-example" class="level2">
<h2 class="anchored" data-anchor-id="the-limits-of-the-back-door-a-quick-example">The limits of the back-door: a quick example</h2>
<p>Let’s assume the following DAG, which is a darling of Pearl’s work. Does smoking cause cancer?</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span> u,</span>
<span id="cb1-2">                  m <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb1-3">                  y <span class="sc" style="color: #5E5E5E;">~</span> u <span class="sc" style="color: #5E5E5E;">+</span> m,</span>
<span id="cb1-4">                  <span class="at" style="color: #657422;">labels =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"x"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Smoking"</span>,</span>
<span id="cb1-5">                             <span class="st" style="color: #20794D;">"y"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Cancer"</span>,</span>
<span id="cb1-6">                             <span class="st" style="color: #20794D;">"m"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Tar"</span>,</span>
<span id="cb1-7">                             <span class="st" style="color: #20794D;">"u"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Genotype"</span>),</span>
<span id="cb1-8">                  <span class="at" style="color: #657422;">latent =</span> <span class="st" style="color: #20794D;">"u"</span>,</span>
<span id="cb1-9">                  <span class="at" style="color: #657422;">exposure =</span> <span class="st" style="color: #20794D;">"x"</span>,</span>
<span id="cb1-10">                  <span class="at" style="color: #657422;">outcome =</span> <span class="st" style="color: #20794D;">"y"</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Given that we cannot measure genotype, we cannot use the back-door criterion to stop <img src="https://latex.codecogs.com/png.latex?Smoking"> from picking up the causal effect of <img src="https://latex.codecogs.com/png.latex?Genotype">. Therefore, one can not use the back-door criterion to <strong>ascertain which portion of the observed association</strong> between smoking and cancer is <em>spurious</em> (because it is attributable to their common cause, Genotype) and what portion is <em>genuinely causative</em>.</p>
</section>
<section id="consecutive-applications-of-the-back-door-criterion" class="level2">
<h2 class="anchored" data-anchor-id="consecutive-applications-of-the-back-door-criterion">Consecutive applications of the back-door criterion</h2>
<p>However, we notice that we can use the <em>back-door criterion</em> to estimate two partial effects: <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20M"> and <img src="https://latex.codecogs.com/png.latex?M%20%5Crightarrow%20Y">. <strong>By chaining</strong> these two partial effects, we can obtain <em>the overall effect</em> <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Y">.</p>
<p>The intuition for the chaining is thus: intervening on the levels of tar in the lungs lead to different probabilities of cancer: <img src="https://latex.codecogs.com/png.latex?P(Y%20=%20y%20%7C%20%5Ctext%7Bdo(M%20=%20m)%7D)">. However, the levels of tar are themselves determined by how much someone smokes: <img src="https://latex.codecogs.com/png.latex?P(M=%20m%7C%20%5Ctext%7Bdo(X%20=%20x)%7D)">. Therefore, by intervening on smoking to determine the levels of tar we can estimate the causal effect of smoking.</p>
<p>We intervene on smoking and check the respective effect for each value of tar:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20%5Cmid%20d%20o(X))=%5Csum_%7BM%7D%20P(Y%20%5Cmid%20M,%20d%20o(X))%20%5Ctimes%20P(M%20%5Cmid%20d%20o(X))%0A"></p>
<p>Because smoking blocks all the back-door paths from tar into cancer, we can replace the conditioning expression by an intervention expression in the first term.<img src="https://latex.codecogs.com/png.latex?P(Y%20%5Cmid%20M,%20d%20o(X))=P(Y%20%5Cmid%20d%20o(M),%20d%20o(X))">. Given that intervening on smoking, once we have intervened on tar has no effect on cancer, we can also write <img src="https://latex.codecogs.com/png.latex?P(Y%20%5Cmid%20M,%20d%20o(X))=P(Y%20%5Cmid%20d%20o(M),%20d%20o(X))=P(Y%20%5Cmid%20d%20o(M))">.</p>
<p>Given that smoking blocks all backdoor paths into tar, we can estimate <img src="https://latex.codecogs.com/png.latex?P(Y%20%5Cmid%20d%20o(M))"> using the back-door adjustment:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20%5Cmid%20d%20o(M))=%5Csum_%7BX%7D%20P(Y%20%5Cmid%20X,%20M)%20%5Ctimes%20P(X)%0A"></p>
<p>Therefore, we can re-write <img src="https://latex.codecogs.com/png.latex?P(Y%20%5Cmid%20d%20o(X))"> thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20%5Cmid%20d%20o(X))%20=%20%5Csum_%7BM%7D%20P(M%20%7C%20do(X))%20%5Csum_%7BX'%7D%20P(Y%20%5Cmid%20X',%20M)%20%5Ctimes%20P(X')%0A"></p>
<p>Considering that there are no backdoor paths from smoking to tar, we can write <img src="https://latex.codecogs.com/png.latex?P(M%20%7C%20do(X))%20=%20P(M%20%7C%20X)">. Therefore, we can re-write our entire expression for <img src="https://latex.codecogs.com/png.latex?P(Y%20%5Cmid%20d%20o(X))"> in terms of pre-intervention probabilities:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20%5Cmid%20d%20o(X))%20=%20%5Csum_%7BM%7D%20P(M%20%7C%20X)%20%5Csum_%7BX'%7D%20P(Y%20%5Cmid%20X',%20M)%20%5Ctimes%20P(X')%0A"></p>
<p>This is the <strong>front-door formula</strong>.</p>
</section>
<section id="empirical-estimation" class="level2">
<h2 class="anchored" data-anchor-id="empirical-estimation">Empirical estimation</h2>
<p><a href="https://www.canr.msu.edu/afre/events/Bellemare%20and%20Bloem%20(2020).pdf">Conceptually</a>,</p>
<blockquote class="blockquote">
<p>the FDC [Front-door Criterion] approach works by first estimating the effect of X on M, and then estimating the effect of M on Y holding X constant. Both of these effects are unbiased because <em>nothing confounds</em> the effect of X on M and X blocks <em>the only back-door path</em> between M on Y. <strong>Multiplying these effects by one another yields the FDC estimand</strong>.</p>
</blockquote>
<p>Therefore, in a regression setting we can estimate the causal effect using the Average Treatment Effect (ATE) via the FDC thus. Formulate two linear regressions:</p>
<p>[ M_{i}=+X_{i}+<em>{i} ] and [ Y</em>{i}=+M_{i}+X_{i}+v_{i} ]</p>
<p>Our estimate of the ATE is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AATE%20=%20E%5BY%7Cdo(X)%5D%20=%20%5Cdelta%20%5Ctimes%20%5Cgamma%0A"></p>
</section>
<section id="when-can-we-use-the-front-door-criterion" class="level2">
<h2 class="anchored" data-anchor-id="when-can-we-use-the-front-door-criterion">When can we use the Front-Door criterion?</h2>
<p>We’ve given an intuitive proof of the Front-door criterion and given an empirical estimation technique. But what exactly have we presupposed that allowed us to do all of this? In other words, <strong>what are the fundamental assumptions behind the criterion?</strong></p>
<p>A set of variables ( Z ) is said to satisfy the front-door criterion relative to an ordered pair of variables ( (X, Y) ) if:</p>
<ol type="1">
<li><p>Z intercepts all directed paths from ( X ) to ( Y ).</p></li>
<li><p>There is no backdoor path from ( X ) to ( Z )</p></li>
<li><p>All backdoor paths from Z to ( Y ) are blocked by X.</p></li>
</ol>
<p>When these conditions are met, we can use the Front-Door criterion to estimate the causal effect of <img src="https://latex.codecogs.com/png.latex?X">.</p>
</section>
<section id="a-monte-carlo-experiment" class="level2">
<h2 class="anchored" data-anchor-id="a-monte-carlo-experiment">A Monte-Carlo experiment</h2>
<p>Let’s work a Monte-Carlo experiment to show the power of the backdoor criterion. Consider the following DAG:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">dag <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">downloadGraph</span>(<span class="st" style="color: #20794D;">"dagitty.net/m331"</span>)</span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;">ggdag</span>(dag) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-3">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"We only need to measure W to estimate the effect of X on Y"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Assume that only <img src="https://latex.codecogs.com/png.latex?X,%20Y">, and one additional variable can be measured. Which variable would would allow the identification of the causal effect <img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Y">? The answer is all in the front-door criterion! We only need to measure <img src="https://latex.codecogs.com/png.latex?W"> to be able to estimate the effect. Notice that:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?W"> intercepts all the direct paths from <img src="https://latex.codecogs.com/png.latex?X"> into <img src="https://latex.codecogs.com/png.latex?Y">.</p></li>
<li><p>There is no backdoor path from <img src="https://latex.codecogs.com/png.latex?X"> into <img src="https://latex.codecogs.com/png.latex?W">.</p></li>
<li><p>All back-door paths from <img src="https://latex.codecogs.com/png.latex?W"> into <img src="https://latex.codecogs.com/png.latex?Y"> are blocked</p></li>
</ol>
<p>Therefore, we can use the front-door criterion.</p>
<p>Let’s use a Monte-Carlo simulation to confirm the answer.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">n <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">500</span></span>
<span id="cb3-2">b <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-3">c <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-4">z <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>b <span class="sc" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>c <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-5">a <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">3</span><span class="sc" style="color: #5E5E5E;">*</span>b <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-6">d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span><span class="sc" style="color: #5E5E5E;">*</span>c <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-7">x <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">4</span><span class="sc" style="color: #5E5E5E;">+</span>a <span class="sc" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>z <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-8">w <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>x <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-9">y <span class="ot" style="color: #003B4F;">&lt;-</span> z <span class="sc" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>w <span class="sc" style="color: #5E5E5E;">+</span> d <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb3-10"></span>
<span id="cb3-11">data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(b, c, z, a, d, x, w, y)</span></code></pre></div>
</div>
<p>In this simulated dataset, the causal effect of unit of <img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Y"> is -4. Let’s recuperate this effect by using the back-door criterion to find out for which variables we must control.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;">ggdag_adjustment_set</span>(dag, <span class="at" style="color: #657422;">outcome =</span> <span class="st" style="color: #20794D;">"Y"</span>, <span class="at" style="color: #657422;">exposure =</span> <span class="st" style="color: #20794D;">"X"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Let’s take the first adjustment set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">model_backdoor <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> a <span class="sc" style="color: #5E5E5E;">+</span> z, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb5-2">model_backdoor <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-3">  <span class="fu" style="color: #4758AB;">spread_draws</span>(x) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(x)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-5">  <span class="fu" style="color: #4758AB;">stat_halfeye</span>(<span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.6</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-6">  hrbrthemes<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">theme_ipsum_rc</span>(<span class="at" style="color: #657422;">grid =</span> <span class="st" style="color: #20794D;">"y"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-7">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">4</span>), <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-8">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"text"</span>, <span class="at" style="color: #657422;">x =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">4.08</span>, <span class="at" style="color: #657422;">y =</span> <span class="fl" style="color: #AD0000;">0.7</span>, <span class="at" style="color: #657422;">label =</span> <span class="st" style="color: #20794D;">"True causal effect"</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, </span>
<span id="cb5-9">           <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">theme_get</span>()<span class="sc" style="color: #5E5E5E;">$</span>text[[<span class="st" style="color: #20794D;">"family"</span>]]) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb5-10">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Causal inference from Model y ~ x + a + z"</span>,</span>
<span id="cb5-11">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"We deconfound our estimates by conditioning on a and z"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Given our DAG, a testable implication is that we must arrive at the same answer by using the front-door criterion. <strong>Remember that it is just a consecutive use of the back-door criterion that translates into two regressions</strong>. Therefore, we can use a multi-variable model thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">model_frontdoor <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">ulam</span>(</span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">alist</span>(</span>
<span id="cb6-3">    <span class="fu" style="color: #4758AB;">c</span>(Y, W) <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">multi_normal</span>(<span class="fu" style="color: #4758AB;">c</span>(muY, muW), Rho, Sigma),</span>
<span id="cb6-4">    muY <span class="ot" style="color: #003B4F;">&lt;-</span> alphaY <span class="sc" style="color: #5E5E5E;">+</span> delta<span class="sc" style="color: #5E5E5E;">*</span>W,</span>
<span id="cb6-5">    muW <span class="ot" style="color: #003B4F;">&lt;-</span> alphaW <span class="sc" style="color: #5E5E5E;">+</span> gamma<span class="sc" style="color: #5E5E5E;">*</span>X, </span>
<span id="cb6-6">    gq<span class="sc" style="color: #5E5E5E;">&gt;</span> ate <span class="ot" style="color: #003B4F;">&lt;-</span> gamma <span class="sc" style="color: #5E5E5E;">*</span> delta, <span class="co" style="color: #5E5E5E;"># calculate ate directly in stan</span></span>
<span id="cb6-7">    <span class="fu" style="color: #4758AB;">c</span>(alphaY, alphaW) <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="fl" style="color: #AD0000;">0.2</span>),</span>
<span id="cb6-8">    <span class="fu" style="color: #4758AB;">c</span>(gamma, delta) <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">normal</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="fl" style="color: #AD0000;">0.5</span>),</span>
<span id="cb6-9">    Rho <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">lkj_corr</span>(<span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb6-10">    Sigma <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">exponential</span>(<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb6-11">  ),</span>
<span id="cb6-12">  <span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">Y =</span> data<span class="sc" style="color: #5E5E5E;">$</span>y, <span class="at" style="color: #657422;">X =</span> data<span class="sc" style="color: #5E5E5E;">$</span>x, <span class="at" style="color: #657422;">W =</span> data<span class="sc" style="color: #5E5E5E;">$</span>w), <span class="at" style="color: #657422;">chains =</span> <span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">cores =</span> <span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">3000</span></span>
<span id="cb6-13">)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC with 4 parallel chains, with 1 thread(s) per chain...

Chain 1 Iteration:    1 / 3000 [  0%]  (Warmup) 
Chain 2 Iteration:    1 / 3000 [  0%]  (Warmup) 
Chain 3 Iteration:    1 / 3000 [  0%]  (Warmup) 
Chain 4 Iteration:    1 / 3000 [  0%]  (Warmup) 
Chain 3 Iteration:  100 / 3000 [  3%]  (Warmup) 
Chain 1 Iteration:  100 / 3000 [  3%]  (Warmup) 
Chain 4 Iteration:  100 / 3000 [  3%]  (Warmup) 
Chain 2 Iteration:  100 / 3000 [  3%]  (Warmup) 
Chain 3 Iteration:  200 / 3000 [  6%]  (Warmup) 
Chain 1 Iteration:  200 / 3000 [  6%]  (Warmup) 
Chain 4 Iteration:  200 / 3000 [  6%]  (Warmup) 
Chain 2 Iteration:  200 / 3000 [  6%]  (Warmup) 
Chain 3 Iteration:  300 / 3000 [ 10%]  (Warmup) 
Chain 1 Iteration:  300 / 3000 [ 10%]  (Warmup) 
Chain 4 Iteration:  300 / 3000 [ 10%]  (Warmup) 
Chain 2 Iteration:  300 / 3000 [ 10%]  (Warmup) 
Chain 3 Iteration:  400 / 3000 [ 13%]  (Warmup) 
Chain 4 Iteration:  400 / 3000 [ 13%]  (Warmup) 
Chain 1 Iteration:  400 / 3000 [ 13%]  (Warmup) 
Chain 2 Iteration:  400 / 3000 [ 13%]  (Warmup) 
Chain 1 Iteration:  500 / 3000 [ 16%]  (Warmup) 
Chain 3 Iteration:  500 / 3000 [ 16%]  (Warmup) 
Chain 4 Iteration:  500 / 3000 [ 16%]  (Warmup) 
Chain 2 Iteration:  500 / 3000 [ 16%]  (Warmup) 
Chain 3 Iteration:  600 / 3000 [ 20%]  (Warmup) 
Chain 1 Iteration:  600 / 3000 [ 20%]  (Warmup) 
Chain 4 Iteration:  600 / 3000 [ 20%]  (Warmup) 
Chain 2 Iteration:  600 / 3000 [ 20%]  (Warmup) 
Chain 1 Iteration:  700 / 3000 [ 23%]  (Warmup) 
Chain 3 Iteration:  700 / 3000 [ 23%]  (Warmup) 
Chain 4 Iteration:  700 / 3000 [ 23%]  (Warmup) 
Chain 2 Iteration:  700 / 3000 [ 23%]  (Warmup) 
Chain 1 Iteration:  800 / 3000 [ 26%]  (Warmup) 
Chain 3 Iteration:  800 / 3000 [ 26%]  (Warmup) 
Chain 4 Iteration:  800 / 3000 [ 26%]  (Warmup) 
Chain 2 Iteration:  800 / 3000 [ 26%]  (Warmup) 
Chain 1 Iteration:  900 / 3000 [ 30%]  (Warmup) 
Chain 3 Iteration:  900 / 3000 [ 30%]  (Warmup) 
Chain 4 Iteration:  900 / 3000 [ 30%]  (Warmup) 
Chain 2 Iteration:  900 / 3000 [ 30%]  (Warmup) 
Chain 1 Iteration: 1000 / 3000 [ 33%]  (Warmup) 
Chain 3 Iteration: 1000 / 3000 [ 33%]  (Warmup) 
Chain 4 Iteration: 1000 / 3000 [ 33%]  (Warmup) 
Chain 2 Iteration: 1000 / 3000 [ 33%]  (Warmup) 
Chain 1 Iteration: 1100 / 3000 [ 36%]  (Warmup) 
Chain 3 Iteration: 1100 / 3000 [ 36%]  (Warmup) 
Chain 4 Iteration: 1100 / 3000 [ 36%]  (Warmup) 
Chain 2 Iteration: 1100 / 3000 [ 36%]  (Warmup) 
Chain 1 Iteration: 1200 / 3000 [ 40%]  (Warmup) 
Chain 4 Iteration: 1200 / 3000 [ 40%]  (Warmup) 
Chain 3 Iteration: 1200 / 3000 [ 40%]  (Warmup) 
Chain 2 Iteration: 1200 / 3000 [ 40%]  (Warmup) 
Chain 1 Iteration: 1300 / 3000 [ 43%]  (Warmup) 
Chain 4 Iteration: 1300 / 3000 [ 43%]  (Warmup) 
Chain 3 Iteration: 1300 / 3000 [ 43%]  (Warmup) 
Chain 2 Iteration: 1300 / 3000 [ 43%]  (Warmup) 
Chain 1 Iteration: 1400 / 3000 [ 46%]  (Warmup) 
Chain 3 Iteration: 1400 / 3000 [ 46%]  (Warmup) 
Chain 4 Iteration: 1400 / 3000 [ 46%]  (Warmup) 
Chain 2 Iteration: 1400 / 3000 [ 46%]  (Warmup) 
Chain 1 Iteration: 1500 / 3000 [ 50%]  (Warmup) 
Chain 1 Iteration: 1501 / 3000 [ 50%]  (Sampling) 
Chain 3 Iteration: 1500 / 3000 [ 50%]  (Warmup) 
Chain 3 Iteration: 1501 / 3000 [ 50%]  (Sampling) 
Chain 4 Iteration: 1500 / 3000 [ 50%]  (Warmup) 
Chain 4 Iteration: 1501 / 3000 [ 50%]  (Sampling) 
Chain 2 Iteration: 1500 / 3000 [ 50%]  (Warmup) 
Chain 2 Iteration: 1501 / 3000 [ 50%]  (Sampling) 
Chain 1 Iteration: 1600 / 3000 [ 53%]  (Sampling) 
Chain 3 Iteration: 1600 / 3000 [ 53%]  (Sampling) 
Chain 4 Iteration: 1600 / 3000 [ 53%]  (Sampling) 
Chain 2 Iteration: 1600 / 3000 [ 53%]  (Sampling) 
Chain 1 Iteration: 1700 / 3000 [ 56%]  (Sampling) 
Chain 3 Iteration: 1700 / 3000 [ 56%]  (Sampling) 
Chain 4 Iteration: 1700 / 3000 [ 56%]  (Sampling) 
Chain 1 Iteration: 1800 / 3000 [ 60%]  (Sampling) 
Chain 2 Iteration: 1700 / 3000 [ 56%]  (Sampling) 
Chain 3 Iteration: 1800 / 3000 [ 60%]  (Sampling) 
Chain 4 Iteration: 1800 / 3000 [ 60%]  (Sampling) 
Chain 1 Iteration: 1900 / 3000 [ 63%]  (Sampling) 
Chain 3 Iteration: 1900 / 3000 [ 63%]  (Sampling) 
Chain 2 Iteration: 1800 / 3000 [ 60%]  (Sampling) 
Chain 4 Iteration: 1900 / 3000 [ 63%]  (Sampling) 
Chain 1 Iteration: 2000 / 3000 [ 66%]  (Sampling) 
Chain 3 Iteration: 2000 / 3000 [ 66%]  (Sampling) 
Chain 2 Iteration: 1900 / 3000 [ 63%]  (Sampling) 
Chain 4 Iteration: 2000 / 3000 [ 66%]  (Sampling) 
Chain 1 Iteration: 2100 / 3000 [ 70%]  (Sampling) 
Chain 3 Iteration: 2100 / 3000 [ 70%]  (Sampling) 
Chain 2 Iteration: 2000 / 3000 [ 66%]  (Sampling) 
Chain 4 Iteration: 2100 / 3000 [ 70%]  (Sampling) 
Chain 1 Iteration: 2200 / 3000 [ 73%]  (Sampling) 
Chain 3 Iteration: 2200 / 3000 [ 73%]  (Sampling) 
Chain 2 Iteration: 2100 / 3000 [ 70%]  (Sampling) 
Chain 4 Iteration: 2200 / 3000 [ 73%]  (Sampling) 
Chain 1 Iteration: 2300 / 3000 [ 76%]  (Sampling) 
Chain 3 Iteration: 2300 / 3000 [ 76%]  (Sampling) 
Chain 2 Iteration: 2200 / 3000 [ 73%]  (Sampling) 
Chain 4 Iteration: 2300 / 3000 [ 76%]  (Sampling) 
Chain 1 Iteration: 2400 / 3000 [ 80%]  (Sampling) 
Chain 3 Iteration: 2400 / 3000 [ 80%]  (Sampling) 
Chain 1 Iteration: 2500 / 3000 [ 83%]  (Sampling) 
Chain 2 Iteration: 2300 / 3000 [ 76%]  (Sampling) 
Chain 3 Iteration: 2500 / 3000 [ 83%]  (Sampling) 
Chain 4 Iteration: 2400 / 3000 [ 80%]  (Sampling) 
Chain 4 Iteration: 2500 / 3000 [ 83%]  (Sampling) 
Chain 1 Iteration: 2600 / 3000 [ 86%]  (Sampling) 
Chain 2 Iteration: 2400 / 3000 [ 80%]  (Sampling) 
Chain 3 Iteration: 2600 / 3000 [ 86%]  (Sampling) 
Chain 1 Iteration: 2700 / 3000 [ 90%]  (Sampling) 
Chain 2 Iteration: 2500 / 3000 [ 83%]  (Sampling) 
Chain 3 Iteration: 2700 / 3000 [ 90%]  (Sampling) 
Chain 4 Iteration: 2600 / 3000 [ 86%]  (Sampling) 
Chain 1 Iteration: 2800 / 3000 [ 93%]  (Sampling) 
Chain 2 Iteration: 2600 / 3000 [ 86%]  (Sampling) 
Chain 3 Iteration: 2800 / 3000 [ 93%]  (Sampling) 
Chain 4 Iteration: 2700 / 3000 [ 90%]  (Sampling) 
Chain 1 Iteration: 2900 / 3000 [ 96%]  (Sampling) 
Chain 3 Iteration: 2900 / 3000 [ 96%]  (Sampling) 
Chain 2 Iteration: 2700 / 3000 [ 90%]  (Sampling) 
Chain 4 Iteration: 2800 / 3000 [ 93%]  (Sampling) 
Chain 1 Iteration: 3000 / 3000 [100%]  (Sampling) 
Chain 1 finished in 13.5 seconds.
Chain 3 Iteration: 3000 / 3000 [100%]  (Sampling) 
Chain 3 finished in 13.6 seconds.
Chain 2 Iteration: 2800 / 3000 [ 93%]  (Sampling) 
Chain 4 Iteration: 2900 / 3000 [ 96%]  (Sampling) 
Chain 2 Iteration: 2900 / 3000 [ 96%]  (Sampling) 
Chain 4 Iteration: 3000 / 3000 [100%]  (Sampling) 
Chain 4 finished in 14.2 seconds.
Chain 2 Iteration: 3000 / 3000 [100%]  (Sampling) 
Chain 2 finished in 14.5 seconds.

All 4 chains finished successfully.
Mean chain execution time: 13.9 seconds.
Total execution time: 14.7 seconds.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><span class="fu" style="color: #4758AB;">precis</span>(model_frontdoor)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              mean          sd        5.5%      94.5%    n_eff     Rhat4
alphaW  0.01317289 0.055819588 -0.07701513  0.1010920 4808.342 0.9996915
alphaY  0.10889162 0.131415392 -0.10501164  0.3210728 4608.519 1.0008755
delta   2.00223411 0.011824792  1.98296890  2.0211506 5199.217 1.0003623
gamma  -1.99645002 0.008484629 -2.01018000 -1.9829200 4639.059 0.9998826
ate    -3.99735509 0.028352648 -4.04329220 -3.9517884 5258.228 0.9998829</code></pre>
</div>
</div>
<p>And indeed, we arrive at the same answer!</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Besides Chapter 3 of Pearl’s Causality, I found this <a href="https://www.canr.msu.edu/afre/events/Bellemare%20and%20Bloem%20(2020).pdf">terrific paper (PDF)</a> by Bellemare and Bloem.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-07-30-causality-the-front-door-criterion.html</guid>
  <pubDate>Thu, 30 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: To adjust or not to adjust</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html</link>
  <description><![CDATA[ 




<section id="what-is-this-blogpost-about" class="level2">
<h2 class="anchored" data-anchor-id="what-is-this-blogpost-about">What is this blogpost about?</h2>
<p>In this blogpost, I’ll simulate data to show how <strong>conditioning on as many variables as possible</strong> <em>is not a good idea</em>. Sometimes, conditioning can lead to de-confound an effect; other times, however, conditioning on a variable can create unnecessary confounding and bias the effect that we are trying to understand.</p>
<p><strong>It all depends on our causal story</strong>: by applying the backdoor-criterion to our Causal Graph, we can derive an unambiguous answer to decide which variables should we use as controls in our statistical analysis.</p>
</section>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In the last <a href="https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/">couple of posts</a>, we’ve seen how to define causal effects in terms of interventions and how to represent these <em>interventions</em> in terms of <em>graph surgeries</em>. We’ve also seen that <strong>observational data undetermines interventional data</strong>. Therefore, we cannot gain causal understanding unless we accompany our data-analysis with a causal story.</p>
<p>In particular, we’ve seen that by leveraging the invariant qualities under intervention, we are able to estimate causal effects with the adjustment formula by conditioning on the parents of the exposure <img src="https://latex.codecogs.com/png.latex?X">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y=y%7C%5Ctext%7Bdo%7D(X=x))%20=%20%5Csum_%7Bz%7D%20P(Y=y%20%7C%20X=x,%20pa=z)%20P(pa=z)%0A"> However, what happens when we do not measure the parents of <img src="https://latex.codecogs.com/png.latex?X"> and thus cannot adjust for them? <strong>Can we adjust for some other observed variable(s)</strong>?</p>
</section>
<section id="of-confounding" class="level2">
<h2 class="anchored" data-anchor-id="of-confounding">Of confounding</h2>
<p>Let’s say that we are interested in estimating the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Y"> and we assume the following Causal Graph:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">example <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(z <span class="sc" style="color: #5E5E5E;">~</span> b <span class="sc" style="color: #5E5E5E;">+</span> c,</span>
<span id="cb1-2">                  a <span class="sc" style="color: #5E5E5E;">~</span> b,</span>
<span id="cb1-3">                  d <span class="sc" style="color: #5E5E5E;">~</span> c,</span>
<span id="cb1-4">                  x <span class="sc" style="color: #5E5E5E;">~</span> a <span class="sc" style="color: #5E5E5E;">+</span> z,</span>
<span id="cb1-5">                  w <span class="sc" style="color: #5E5E5E;">~</span> x,</span>
<span id="cb1-6">                  y <span class="sc" style="color: #5E5E5E;">~</span> w <span class="sc" style="color: #5E5E5E;">+</span> z <span class="sc" style="color: #5E5E5E;">+</span> d)</span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;">ggdag</span>(example) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"We want to estimate x -&gt; y"</span>,</span>
<span id="cb1-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"But we don't observe $A$ and thus we cannot use the adjustment formula"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The adjustment formula above states that we should adjust for the parents of <img src="https://latex.codecogs.com/png.latex?x">. However, assume that we <strong>cannot measure</strong> <img src="https://latex.codecogs.com/png.latex?a">. Can we still estimate the causal effect?</p>
<p>To answer this question we must understand why is that it’s useful to adjust for the parents. First, remember that in a Causal Graph, <strong>the statistical information flows freely</strong> in the Graph, <em>regardless</em> of the direction of the arrows. Therefore, in the above graph, the causal information from <img src="https://latex.codecogs.com/png.latex?c"> to <img src="https://latex.codecogs.com/png.latex?y"> may end up getting picked up by <img src="https://latex.codecogs.com/png.latex?x"> or vice versa.</p>
<p><strong>We condition on the parents of <img src="https://latex.codecogs.com/png.latex?x"> such that we block all the information coming from other causal relationships</strong>. Thus, we don’t end up adding up the causal effect of some other variable in the process. However, if we cannot control by its parents, it’s possible that some of this causal effect coming from other variables will be picked up by <img src="https://latex.codecogs.com/png.latex?X"> through the arrows that go into it. Therefore, we will have <strong>confounding bias</strong>. Which we defined thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y%20%7C%20%5Ctext%7Bdo(X%20=%20x)%7D)%20%5Cneq%20P(Y%7C%20X=x)%0A"></p>
<section id="blocking-non-causal-paths" class="level3">
<h3 class="anchored" data-anchor-id="blocking-non-causal-paths">Blocking non-causal paths</h3>
<p>For example, in the above graph there are the following 4 paths from <img src="https://latex.codecogs.com/png.latex?X"> into <img src="https://latex.codecogs.com/png.latex?Y">:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">ggdag_paths</span>(example, <span class="at" style="color: #657422;">from =</span> <span class="st" style="color: #20794D;">'x'</span>, <span class="at" style="color: #657422;">to =</span> <span class="st" style="color: #20794D;">'y'</span>, <span class="at" style="color: #657422;">shadow =</span> T) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-2">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Paths from x into y"</span>,</span>
<span id="cb2-3">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Of 4 possible paths, only one of them is a causal path"</span>) </span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/all-paths-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>We have 4 paths but <em>only</em> 1 of them is causal: only one is a direct descendant of <img src="https://latex.codecogs.com/png.latex?X">. Therefore, if we want to estimate the causal effect, we must make sure that <strong>all the other non-causal paths (the backdoor paths that have arrows into X) are blocked</strong>. Luckily, we already defined a graphical algorithm to find out which variables we must adjust for in order to block some path: the <strong>d-separation criterion</strong>.</p>
<p>Therefore, we arrive at the following four possible adjustment sets that guarantee that all non-causal paths will be blocked. By conditioning on them, we will correctly estimate the causal effect:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">ggdag_adjustment_set</span>(example, <span class="at" style="color: #657422;">exposure =</span> <span class="st" style="color: #20794D;">'x'</span>, <span class="at" style="color: #657422;">outcome =</span> <span class="st" style="color: #20794D;">'y'</span>, <span class="at" style="color: #657422;">shadow =</span> T)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/adjustment-paths-1.png" class="img-fluid" width="1152"></p>
</div>
</div>
<p>Let’s see why each of the four available adjustment set works:</p>
<ol type="1">
<li><p>By conditioning on <img src="https://latex.codecogs.com/png.latex?z">, we open a collider between <img src="https://latex.codecogs.com/png.latex?b"> and <img src="https://latex.codecogs.com/png.latex?c">. However, the causal effect of <img src="https://latex.codecogs.com/png.latex?c"> on <img src="https://latex.codecogs.com/png.latex?y"> does not get picked up by <img src="https://latex.codecogs.com/png.latex?x"> because we are blocking the path by conditioning on <img src="https://latex.codecogs.com/png.latex?a">.</p></li>
<li><p>By conditioning on <img src="https://latex.codecogs.com/png.latex?z">, we open a collider between <img src="https://latex.codecogs.com/png.latex?b"> and <img src="https://latex.codecogs.com/png.latex?c">. However, given that we adjust for <img src="https://latex.codecogs.com/png.latex?b">, the effect of <img src="https://latex.codecogs.com/png.latex?c"> on <img src="https://latex.codecogs.com/png.latex?y"> won’t be picked up by <img src="https://latex.codecogs.com/png.latex?x">.</p></li>
<li><p>By conditioning on <img src="https://latex.codecogs.com/png.latex?z">, we open a collider between <img src="https://latex.codecogs.com/png.latex?b"> and <img src="https://latex.codecogs.com/png.latex?c">. However, given that we adjust for <img src="https://latex.codecogs.com/png.latex?c">, the effect of <img src="https://latex.codecogs.com/png.latex?c"> on <img src="https://latex.codecogs.com/png.latex?y"> won’t be picked up by <img src="https://latex.codecogs.com/png.latex?x">.</p></li>
<li><p>By conditioning on <img src="https://latex.codecogs.com/png.latex?z">, we open a collider between <img src="https://latex.codecogs.com/png.latex?b"> and <img src="https://latex.codecogs.com/png.latex?c">. However, by adjusting for <img src="https://latex.codecogs.com/png.latex?d">, we block the effect of <img src="https://latex.codecogs.com/png.latex?c"> on <img src="https://latex.codecogs.com/png.latex?y"> and thus this effect won’t get picked up by <img src="https://latex.codecogs.com/png.latex?x">.</p></li>
</ol>
</section>
</section>
<section id="the-backdoor-criterion" class="level2">
<h2 class="anchored" data-anchor-id="the-backdoor-criterion">The Backdoor Criterion</h2>
<p>Given that we cannot directly adjust by the parents of <img src="https://latex.codecogs.com/png.latex?x">, <strong>what variables should we condition on to obtain the correct effect?</strong> The question boils down to <strong>finding a set of variables that satisfy the backdoor criterion</strong>:</p>
<p>Given an ordered pair of variables ( (X, Y) ) in a directed acyclic graph ( G, ) a set of variables ( Z ) satisfies the backdoor criterion relative to ( (X, Y) ) if no node in ( ) is a descendant of ( X, ) and ( ) blocks every path between ( X ) and ( Y ) that contains an arrow into X. If a set of variables ( Z ) satisfies the backdoor criterion for ( X ) and ( Y, ) then the causal effect of ( X ) on ( Y ) is given by the formula [ P(Y=y d o(X=x))=_{z} P(Y=y X=x, Z=z) P(Z=z) ]</p>
<p>Note that the parents of <img src="https://latex.codecogs.com/png.latex?X"> always satisfy the backdoor criterion. Notice also, quite importantly, that the criterion simultaneously says <strong>which variables we should use as control variables and which ones we shouldn’t</strong>. Indeed, by adjusting for the wrong variable we may end up opening a non-causal path into <img src="https://latex.codecogs.com/png.latex?x"> and thus introducing confounding bias into our estimates.</p>
</section>
<section id="correclty-adjusting" class="level2">
<h2 class="anchored" data-anchor-id="correclty-adjusting">Correclty adjusting</h2>
<p>Let’s keep working with our current Graphical Model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="fu" style="color: #4758AB;">ggdag_exogenous</span>(example)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can simulate data coming from such a model thus :</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">n <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">500</span></span>
<span id="cb5-2">b <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-3">c <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-4">z <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>b <span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">3</span><span class="sc" style="color: #5E5E5E;">*</span>c <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-5">a <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="sc" style="color: #5E5E5E;">-</span>b <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-6">x <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>a <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>z <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-7">d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">1.5</span><span class="sc" style="color: #5E5E5E;">*</span>c <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-8">w <span class="ot" style="color: #003B4F;">&lt;-</span> x<span class="sc" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">2</span> <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb5-9">y <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">5</span><span class="sc" style="color: #5E5E5E;">*</span>w <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>d<span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">5</span><span class="sc" style="color: #5E5E5E;">*</span>z <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span></code></pre></div>
</div>
<p>Thus, the causal effect of one unit of <img src="https://latex.codecogs.com/png.latex?x"> on <img src="https://latex.codecogs.com/png.latex?y"> is always <img src="https://latex.codecogs.com/png.latex?10%20=%205%20%5Ctimes%202">. Notice that we have four possible adjustment sets to estimate these causal query and all of these possibilities should yield the same answer. Therefore, <strong>this constraint becomes yet another testable implication of our model.</strong></p>
<p>Let’s fit a Gaussian Linear regression with each of the possible adjustment sets and compare the results. But first let’s fit a naive model with no adjustment whatsoever.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(b, c, z, a, x, d, w, y)</span>
<span id="cb6-2">model_one <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>) </span>
<span id="cb6-3"></span>
<span id="cb6-4">model_one <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">spread_draws</span>(x) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-6">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(x)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-7">  <span class="fu" style="color: #4758AB;">stat_halfeye</span>(<span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.6</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-8">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"text"</span>, <span class="at" style="color: #657422;">x =</span> <span class="fl" style="color: #AD0000;">10.2</span>, <span class="at" style="color: #657422;">y =</span> <span class="fl" style="color: #AD0000;">0.7</span>, <span class="at" style="color: #657422;">label =</span> <span class="st" style="color: #20794D;">"True causal effect"</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, </span>
<span id="cb6-9">           <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">theme_get</span>()<span class="sc" style="color: #5E5E5E;">$</span>text[[<span class="st" style="color: #20794D;">"family"</span>]]) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-10">  hrbrthemes<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">theme_ipsum_rc</span>(<span class="at" style="color: #657422;">grid =</span> <span class="st" style="color: #20794D;">"y"</span>)  <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-11">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="dv" style="color: #AD0000;">10</span>), <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-12">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Causal Inference from Model y ~ x"</span>,</span>
<span id="cb6-13">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"This estimation overestimates the causal effect due to confounding bias"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As expected, our estimations are off. Following the backdoor criterion, the following estimations should be unbiased:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">model_two <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> a <span class="sc" style="color: #5E5E5E;">+</span>z, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Much better! Let’s check the other models:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">model_three <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> b <span class="sc" style="color: #5E5E5E;">+</span>z, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Also a correct estimation! Let’s fit the model where we adjust by <img src="https://latex.codecogs.com/png.latex?c">:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">model_four <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> c <span class="sc" style="color: #5E5E5E;">+</span>z, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Finally, the model where we condition on <img src="https://latex.codecogs.com/png.latex?z,%20d">:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">model_five <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> d <span class="sc" style="color: #5E5E5E;">+</span>z, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="incorreclty-adjusting" class="level2">
<h2 class="anchored" data-anchor-id="incorreclty-adjusting">Incorreclty adjusting</h2>
<p>Adjusting on a variable can sometimes open backdoor paths that would had otherwise remained closed. Take the following example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">hurting <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dagify</span>(Y <span class="sc" style="color: #5E5E5E;">~</span> X <span class="sc" style="color: #5E5E5E;">+</span> u1,</span>
<span id="cb11-2">                  X <span class="sc" style="color: #5E5E5E;">~</span> u2,</span>
<span id="cb11-3">                  L <span class="sc" style="color: #5E5E5E;">~</span> u1 <span class="sc" style="color: #5E5E5E;">+</span> u2)</span>
<span id="cb11-4"><span class="fu" style="color: #4758AB;">ggdag</span>(hurting) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb11-5">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"If we don't condition on L, the only backdoor path between X and Y is closed"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Notice that there is only one backdoor path for <img src="https://latex.codecogs.com/png.latex?X">. However, it contains a collider: <img src="https://latex.codecogs.com/png.latex?L">. Therefore, unless we condition on <img src="https://latex.codecogs.com/png.latex?L">, the backdoor path will remain closed. In this case, we must not adjust for any of the variables, as there is no confounding bias: <img src="https://latex.codecogs.com/png.latex?P(Y%20%7C%20do%20(X%20=%20x))%20=%20P(Y%7C%20X%20=%20x)">.</p>
<p>If we are naive, and we consider that controlling for observables is always a step in the right direction toward causal inference, we will end up with the wrong inference. By condition on <img src="https://latex.codecogs.com/png.latex?L">, we open up a collider that will create a relationship between <img src="https://latex.codecogs.com/png.latex?u_1"> and <img src="https://latex.codecogs.com/png.latex?u_2">; therefore, <img src="https://latex.codecogs.com/png.latex?X"> will pick up the causal effect of <img src="https://latex.codecogs.com/png.latex?u_1"> on <img src="https://latex.codecogs.com/png.latex?y">.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="fu" style="color: #4758AB;">ggdag_adjust</span>(hurting, <span class="at" style="color: #657422;">var =</span> <span class="st" style="color: #20794D;">"L"</span>) <span class="sc" style="color: #5E5E5E;">+</span>  </span>
<span id="cb12-2">  <span class="fu" style="color: #4758AB;">scale_color_brewer</span>(<span class="at" style="color: #657422;">type =</span> <span class="st" style="color: #20794D;">"qual"</span>, <span class="at" style="color: #657422;">palette =</span> <span class="st" style="color: #20794D;">"Set2"</span>)  <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-3">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Conditioning on a collider opens a backdoor path between X and Y"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can easily confirm this insight by simulating some data and fitting models.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">u1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb13-2">u2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb13-3">x <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>u2 <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb13-4">y <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>x <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span><span class="sc" style="color: #5E5E5E;">*</span>u1 <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb13-5">l <span class="ot" style="color: #003B4F;">&lt;-</span> u1 <span class="sc" style="color: #5E5E5E;">+</span> u2 <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rnorm</span>(n)</span>
<span id="cb13-6">data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">data.frame</span>(u1, u2, x, y, l)</span></code></pre></div>
</div>
<p>Thus, the correct causal effect of one unit of <img src="https://latex.codecogs.com/png.latex?x"> on <img src="https://latex.codecogs.com/png.latex?y"> is always 2.</p>
<p>First, let’s fit the correct model where we don’t condition on any of the other variables:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1">model_correct <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span> <span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>However, if we adjust for <img src="https://latex.codecogs.com/png.latex?L">, our estimations will be biased:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1">model_incorrect <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan_glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> x <span class="sc" style="color: #5E5E5E;">+</span> l, <span class="at" style="color: #657422;">data =</span> data, <span class="at" style="color: #657422;">refresh =</span><span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>Stats people know that <strong>correlation coefficients do not imply causal effects</strong>. Yet, very often, <em>partial correlation</em> coefficients from <strong>regressions with an ever growing set of ‘control variables’</strong> are unequivocally interpreted as a step in the <em>right direction</em> toward estimating a <strong>causal effect</strong>. This mistaken intuition was aptly named by Richard McElreath, in his fantastic <a href="https://xcelab.net/rm/statistical-rethinking/">Stats course</a>, as <em>Causal Salad</em>: people toss a bunch of control variables and hope to get a casual effect out of it.</p>
<p>With the introduction of Causal Graphs, we gain a invaluable tool with the <strong>backdoor criterion</strong>. With a simple <em>graphical algorithm</em>, we can define <strong>which variables we must include</strong> in our analysis in order to <strong>cancel out all the information coming from different causal relationships than the one we are interested</strong>.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-07-25-causality-to-adjust-or-not-to-adjust.html</guid>
  <pubDate>Sat, 25 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: Invariance under Interventions</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions.html</link>
  <description><![CDATA[ 




<p>In the <a href="https://david-salazar.github.io/2020/07/18/causality-bayesian-networks/">last post</a> we saw how two causal models can yield the same testable implications and thus cannot be distinguished from data alone. That is, we cannot gain causal understanding from data alone. Does that mean that we cannot ever gain causal understanding? Far from it; it just means that we must have a causal model.</p>
<p><strong>Thus, causal effects cannot be estimated from the data itself without a causal story.</strong> In this blogpost, I’ll show how exactly the combination between causal models and observational data can lead us into estimating causal effects. In short, causal effects can be estimated by leveraging the invariant information that the pre-intervention distribution can provide. Doing so, we connect pre-intervention probabilities with the post-intervention probabilities that define the causal effect.</p>
<section id="defining-the-causal-effect-with-the-do-operator" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-causal-effect-with-the-do-operator">Defining the causal effect with the do-operator</h2>
<p>Fundamentally, we cannot gain causal understanding with data because <strong>the data we see could have been generated by many a causal models</strong>. That is, the associations we see, <img src="https://latex.codecogs.com/png.latex?P(Y%20%7C%20X)">, can be the result of many interactions; <strong>some of them causal and some purely observational</strong>. We can say that any statistically meaningful association is <em>the result of a causal relationship</em> <strong>somewhere in the system</strong>, but <em>not necessarily</em> of the causal effect of interest, <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Y">.</p>
<p>To disentangle this confusion, then, let’s define a causal effect. <a href="https://fabiandablander.com/r/Causal-Inference.html">Following Pearl</a>, we will take an <strong>interventionist position</strong> and say that a variable <img src="https://latex.codecogs.com/png.latex?X"> has a causal influence on <img src="https://latex.codecogs.com/png.latex?Y"> if intervening to change <img src="https://latex.codecogs.com/png.latex?X"> leads to changes in <img src="https://latex.codecogs.com/png.latex?Y">. Intervening on <img src="https://latex.codecogs.com/png.latex?X"> means lifting <img src="https://latex.codecogs.com/png.latex?X"> from whatever mechanism previously defined its value and now set it to a particular value <img src="https://latex.codecogs.com/png.latex?X=x"> in an exogenous way.</p>
<p>Thus, the <strong>causal effect</strong> is defined as a <em>function</em> from the values <img src="https://latex.codecogs.com/png.latex?X"> can take to the space of <em>probability distributions</em> on <img src="https://latex.codecogs.com/png.latex?Y">. For example, if <img src="https://latex.codecogs.com/png.latex?X%20:=%20x">, then we arrive at the <strong>interventional distribution</strong> <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20%5Ctext%7Bdo%7D(x))">: the population distribution of <img src="https://latex.codecogs.com/png.latex?Y"> if <em>everyone</em> in the population had their <img src="https://latex.codecogs.com/png.latex?X"> value fixed at <img src="https://latex.codecogs.com/png.latex?x">.</p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdo%7D"> operator defines the exogenous process through which we have intervened to set the value of <img src="https://latex.codecogs.com/png.latex?X%20:=%20x">. Finally, we derive <img src="https://latex.codecogs.com/png.latex?P(Y%7C%20%5Ctext%7Bdo%7D(x))"> for every possible <img src="https://latex.codecogs.com/png.latex?x"> and test whether the distribution changes as we change the value <img src="https://latex.codecogs.com/png.latex?X"> takes.</p>
<p>Therefore, to study the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> is to change the system by determining the value of <img src="https://latex.codecogs.com/png.latex?X"> outside of it and seeing how the effects cascade thorough the system. However, before we change a system we must define it. How to represent the system? With a Causal Graph!</p>
</section>
<section id="causal-graphs" class="level2">
<h2 class="anchored" data-anchor-id="causal-graphs">Causal Graphs</h2>
<p>The question, then, becomes: <strong>how can we simulate the effects of intervening in the causal system?</strong>. First, however, we must define the system in question.</p>
<p>Let each node represent one of the variables of interest. We will draw an arrow from <img src="https://latex.codecogs.com/png.latex?X"> to <img src="https://latex.codecogs.com/png.latex?Y"> if there is a <strong>direct causal effect</strong> from <img src="https://latex.codecogs.com/png.latex?X"> to <img src="https://latex.codecogs.com/png.latex?Y"> for at least one individual. Alternatively, <strong>the lack of an arrow</strong> means that <em>there’s no</em> causal effect for any individual in the population. We will assume that the system is adequately written if <strong>all common causes</strong> of <em>any pair</em> of variables on the graph are <strong>themselves on the graph</strong>. Finally, we’ll say that a variable is always a cause of its descendants.</p>
<p>We will link Causal Graphs to Bayesian graphs by assuming that each variable, conditional on its parents, is independent of any variable for which it is not a cause (i.e., all its predecessors). In turn, this will imply that the <strong>Graph defines the same recursive decomposition of the joint distribution</strong> as a <em>Bayesian Graph</em>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%5Cleft(x_%7B1%7D,%20%5Cldots,%20x_%7Bn%7D%5Cright)=%5Cprod_%7Bj%7D%20P%5Cleft(x_%7Bj%7D%20%5Cmid%20pa_j%5Cright)%0A"></p>
<p>Thereby, we can derive, using the d-separation criterion, <strong>testable implications</strong> of our causal models.</p>
<p>To make things more concrete, let’s work with the following fork: let’s say that a new treatment is developed to reduce cholesterol. However, women take the treatment more/less than men and have higher/lower levels of cholesterol. How to compute the causal effect of the treatment on cholesterol?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions_files/figure-html/drug, coffee-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="interventions-eliminating-incoming-arrows" class="level2">
<h2 class="anchored" data-anchor-id="interventions-eliminating-incoming-arrows">Interventions: Eliminating incoming arrows</h2>
<p>Intervening on <img src="https://latex.codecogs.com/png.latex?X"> such that <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bdo(X%20=%201)%7D"> amounts to curtailing the previous mechanism that defined <img src="https://latex.codecogs.com/png.latex?X">. In Graph lingo: <strong>eliminate the incoming arrows into <img src="https://latex.codecogs.com/png.latex?X"></strong>: <em>gender no longer cause <img src="https://latex.codecogs.com/png.latex?X"></em>. Therefore, we eliminate the arrow from Gender into treatment. Thus, an intervention is equivalent to eliminating arrows in a Causal Graph. Let’s label this new graph <img src="https://latex.codecogs.com/png.latex?G_m"></p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions_files/figure-html/drug-eliminated-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="invariant-probabilities-under-intervention" class="level2">
<h2 class="anchored" data-anchor-id="invariant-probabilities-under-intervention">Invariant probabilities under intervention</h2>
<p>The mutilated graph is still a Causal Graph. Thus, it implies a <em>particular decomposition of the joint probability</em> (<img src="https://latex.codecogs.com/png.latex?P_m">) of it’s own. With respect to this post-intervention distribution, we can define the causal effect: <img src="https://latex.codecogs.com/png.latex?P(Y=y%7C%5Ctext%7Bdo%7D(X=x))%20:=%20P_m%20(Y=y%7CX=x)">. However, this new post-intervention distribution <img src="https://latex.codecogs.com/png.latex?P_m"> is <strong>not totally disconnected</strong> from the pre-intervention distribution (<img src="https://latex.codecogs.com/png.latex?P">) that we can study with observational data.</p>
<p>There are two <strong>invariant qualities</strong> that are the same in the <em>pre-intervention and post-intervention</em> distribution:</p>
<ul>
<li><p>Our intervention is <strong>atomic</strong>: there are no side effects that alter the way non-descendants of <img src="https://latex.codecogs.com/png.latex?X"> are determined. Thus, <img src="https://latex.codecogs.com/png.latex?P_m(Z=z%7C%20X=x)%20=%20P(Z=z)">.</p></li>
<li><p>The conditional probability <img src="https://latex.codecogs.com/png.latex?Y"> is invariant, because the mechanism by which Y responds to <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Z"> remains the same, <em>regardless</em> of whether <img src="https://latex.codecogs.com/png.latex?X"> <strong>changes spontaneously or by deliberate manipulation</strong>. Thus; <img src="https://latex.codecogs.com/png.latex?P_m(Y%7C%20X=x,%20Z%20=%20z)%20=%20P(Y%7CX=x,%20Z=z)">.</p></li>
</ul>
<section id="connecting-pre-intervention-probabilities-with-post-treament" class="level3">
<h3 class="anchored" data-anchor-id="connecting-pre-intervention-probabilities-with-post-treament">Connecting pre-intervention probabilities with post-treament</h3>
<p>Therefore, using probability laws and our independence assumption between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Z"> in the mutilated graph, we define the causal effect in terms of post-intervention distribution thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bl%7D%0AP(Y%20=%20y%20%7C%20do(X=x))%20:=%20P_m%20(Y=y%7CX=x)%20%5C%5C%0A=%5Csum_%7Bz%7D%20P_%7Bm%7D(Y=y%20%5Cmid%20X=x,%20Z=z)%20P_%7Bm%7D(Z=z%20%5Cmid%20X=x)%20%5C%5C%0A=%5Csum_%7Bz%7D%20P_%7Bm%7D(Y=y%20%5Cmid%20X=x,%20Z=z)%20P_%7Bm%7D(Z=z)%0A%5Cend%7Barray%7D%0A"> Luckily, all the terms invariant: both terms can be <strong>connected to the original pre-intervention</strong> probability distribution:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y=y%20%5Cmid%20d%20o(X=x))=%5Csum_%7Bz%7D%20P(Y=y%20%5Cmid%20X=x,%20Z=z)%20P(Z=z)%0A"> Therefore, we arrive at a definition of the <strong>causal effect in terms of the pre-treatment distribution</strong>. Thus, we can <strong>estimate the causal effect from observational studies</strong> without the need of <em>actually carrying out</em> the intervention.</p>
</section>
</section>
<section id="the-adjustment-formula" class="level2">
<h2 class="anchored" data-anchor-id="the-adjustment-formula">The Adjustment Formula</h2>
<p>More generally, we define the causal effect in terms of pre-intervention probability thus. Given a graph <img src="https://latex.codecogs.com/png.latex?G"> in which a set of variables <img src="https://latex.codecogs.com/png.latex?pa"> are designated as the parents of <img src="https://latex.codecogs.com/png.latex?X">, the causal effect of <img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Y"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y=y%7C%5Ctext%7Bdo%7D(X=x))%20=%20%5Csum_%7Bz%7D%20P(Y=y%20%7C%20X=x,%20P%20A=z)%20P(pa=z)%0A"> Therefore, we can conclude why it is necessary to have a causal story to be able to estimate the causal effect: <strong>to identify the parents of <img src="https://latex.codecogs.com/png.latex?X"> and adjust for them</strong>: first condition <img src="https://latex.codecogs.com/png.latex?P(Y=y%7C%20X%20=x)"> on <img src="https://latex.codecogs.com/png.latex?PA"> and then average the result, weighted the prior probability of <img src="https://latex.codecogs.com/png.latex?pa%20=%20z">.</p>
</section>
<section id="an-example" class="level2">
<h2 class="anchored" data-anchor-id="an-example">An example</h2>
<p>Let’s follow our thought experiment with our previous graph. In the <a href="https://fabiandablander.com/r/Causal-Inference.html">experiment</a>, we observe both men and women who decide whether they take the drug or not. The results are the following:</p>
<div class="cell">
<div class="cell-output-display">

<div id="apqbwdcovh" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

:where(#apqbwdcovh) .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

:where(#apqbwdcovh) .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

:where(#apqbwdcovh) .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

:where(#apqbwdcovh) .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

:where(#apqbwdcovh) .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

:where(#apqbwdcovh) .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

:where(#apqbwdcovh) .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

:where(#apqbwdcovh) .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

:where(#apqbwdcovh) .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

:where(#apqbwdcovh) .gt_from_md > :first-child {
  margin-top: 0;
}

:where(#apqbwdcovh) .gt_from_md > :last-child {
  margin-bottom: 0;
}

:where(#apqbwdcovh) .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

:where(#apqbwdcovh) .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#apqbwdcovh) .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

:where(#apqbwdcovh) .gt_row_group_first td {
  border-top-width: 2px;
}

:where(#apqbwdcovh) .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#apqbwdcovh) .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_first_summary_row.thick {
  border-top-width: 2px;
}

:where(#apqbwdcovh) .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#apqbwdcovh) .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

:where(#apqbwdcovh) .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#apqbwdcovh) .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#apqbwdcovh) .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#apqbwdcovh) .gt_left {
  text-align: left;
}

:where(#apqbwdcovh) .gt_center {
  text-align: center;
}

:where(#apqbwdcovh) .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

:where(#apqbwdcovh) .gt_font_normal {
  font-weight: normal;
}

:where(#apqbwdcovh) .gt_font_bold {
  font-weight: bold;
}

:where(#apqbwdcovh) .gt_font_italic {
  font-style: italic;
}

:where(#apqbwdcovh) .gt_super {
  font-size: 65%;
}

:where(#apqbwdcovh) .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

:where(#apqbwdcovh) .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

:where(#apqbwdcovh) .gt_indent_1 {
  text-indent: 5px;
}

:where(#apqbwdcovh) .gt_indent_2 {
  text-indent: 10px;
}

:where(#apqbwdcovh) .gt_indent_3 {
  text-indent: 15px;
}

:where(#apqbwdcovh) .gt_indent_4 {
  text-indent: 20px;
}

:where(#apqbwdcovh) .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">Recovered</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">N</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">Treatment</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1" scope="col">Gender</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">81</td>
<td class="gt_row gt_right">87</td>
<td class="gt_row gt_right">1</td>
<td class="gt_row gt_left">Male</td></tr>
    <tr><td class="gt_row gt_right">234</td>
<td class="gt_row gt_right">270</td>
<td class="gt_row gt_right">0</td>
<td class="gt_row gt_left">Male</td></tr>
    <tr><td class="gt_row gt_right">192</td>
<td class="gt_row gt_right">263</td>
<td class="gt_row gt_right">1</td>
<td class="gt_row gt_left">Female</td></tr>
    <tr><td class="gt_row gt_right">55</td>
<td class="gt_row gt_right">80</td>
<td class="gt_row gt_right">0</td>
<td class="gt_row gt_left">Female</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
<p>When we study the data across genders, we find out that the patients who didn’t take the drug had a higher rate of recovery:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>However, once we separate the data by gender, the opposite picture arises:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>We have a case of Simpson’s Paradox! Let’s use the causal knowledge embedded in our graph to estimate the true causal effect of the treatment. Given that Gender is the only parent of Treatment, we will adjust for it:</p>
<p>[ P(Y=1 d o(X=1))=+=0.832 ] while, similarly, [ P(Y=1 d o(X=0))=+=0.7818 ] Thus, comparing the effect of drug-taking ( (X=1) ) to the effect of nontaking ( (X=0), ) we obtain [ A C E=P(Y=1 d o(X=1))-P(Y=1 d o(X=0))=0.832-0.7818=0.0502 ]</p>
<p>However, if Gender had not been a parent of Treatment (i.e., if both Genders decide to take the treatment equally), our Causal effect would be different because we would adjust for Gender in the first place.</p>
</section>
<section id="identifiable" class="level2">
<h2 class="anchored" data-anchor-id="identifiable">Identifiable</h2>
<p>We’ve estimated causal effects with a pretty simple strategy: adjust for the parents of the exposure and average those effects weighted by the probability of the parents.</p>
<p>Therefore, according to our strategy, the causal effect will be <strong>identifiable</strong> whenever both <img src="https://latex.codecogs.com/png.latex?X,%20Y"> and the parents of <img src="https://latex.codecogs.com/png.latex?X">, <img src="https://latex.codecogs.com/png.latex?pa"> are measured. Whenever measurements for some of them are missing, we must use other techniques to estimate the causal effect.</p>
</section>
<section id="addendum-rct" class="level2">
<h2 class="anchored" data-anchor-id="addendum-rct">Addendum: RCT</h2>
<p>Randomized Control Trials are sometimes referred to as the gold standard in causal inference. However, in our framework, they are nothing more than a <strong>different graph surgery</strong>. Whereas before we cut all the incoming arrows into treatment, now we replace all the incoming arrows with only with one arrow that signifies the randomization of the treatment:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions_files/figure-html/rct-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Therefore, now we must simply adjust by randomization to estimate the causal effect of treatment. Does that mean that they are not useful? No, they will always have the upper hand when we are uncertain about our causal model. If there is another parent of treatment that we are not accounting for, Randomization will offer a clean solution.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-07-22-causality-invariance-under-interventions.html</guid>
  <pubDate>Wed, 22 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Causality: Bayesian Networks and Probability Distributions</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks.html</link>
  <description><![CDATA[ 




<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Stats people know that <strong>correlation coefficients do not imply causal effects</strong>. Yet, very often, <em>partial correlation</em> coefficients from <strong>regressions with an ever growing set of ‘control variables’</strong> are unequivocally interpreted as a step in the <em>right direction</em> toward estimating a <strong>causal effect</strong>. This mistaken intuition was aptly named by Richard McElreath, in his fantastic <a href="https://xcelab.net/rm/statistical-rethinking/">Stats course</a>, as <em>Causal Salad</em>: people toss a bunch of control variables and hope to get a casual effect out of it.</p>
<p>In his fantastic course, Richard offers a clear and intuitive <em>antidote</em> for the Causal Salad: <strong>Graph Models for Causality</strong>. In these series of blogposts, I’ll explore the work of <a href="http://bayes.cs.ucla.edu/jp_home.html">Judea Pearl</a>, the father of the Graphical approach toward Causality. In particular, I’ll share what I learn from his book <a href="http://bayes.cs.ucla.edu/jp_home.html">Causality: Models, Reasoning and Inference</a></p>
<p>In this blogpost, I’ll explore Bayesian Networks: the simplest of probability networks to represent a joint distribution and how we can derive testable implications from them using the <strong>d-separation</strong> criterion. Thorough the blopost, I’ll be using the excellent packages <code>dagitty</code> and <code>ggdag</code>.</p>
</section>
<section id="bayesian-networks" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-networks">Bayesian Networks</h2>
<p><em>Joint probability distributions</em> are tricky objects to represent: both in our heads and in our computers. They can imply an unworldly number of relationships. Probability theory gives us in the chain rule of probability a tool to <strong>decompose</strong> a joint probability distribution.</p>
<p>Suppose we have a distribution ( P ) defined on ( n ) discrete variables, which we may <strong>order</strong> arbitrarily as ( X_{1}, X_{2}, , X_{n} . ) The chain rule of probability calculus always permits us to decompose ( P ) as a product of ( n ) <strong>conditional distributions</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%5Cleft(x_%7B1%7D,%20%5Cldots,%20x_%7Bn%7D%5Cright)=%5Cprod_%7Bj%7D%20P%5Cleft(x_%7Bj%7D%20%5Cmid%20x_%7B1%7D,%20%5Cldots,%20x_%7Bj-1%7D%5Cright)%0A"></p>
<p>Given <em>structural knowledge</em> about the problem at hand, we can <em>simplify this decomposition</em> given the <strong>conditional independence</strong> we posit in our model of the joint distribution. That is, it is not always necessary to condition on all the other variables: it <em>suffices</em> to control in a minimal set of variables to render <strong>other predecessor variables independent</strong>. For a given variable <img src="https://latex.codecogs.com/png.latex?x_j"> let’s name this set of variables <img src="https://latex.codecogs.com/png.latex?pa_j">: the parents of the variable <img src="https://latex.codecogs.com/png.latex?x_j">.</p>
<p>We can then start constructing a graph (a Bayesian Network): each node will be a random variable; for any node, the arrows that enter into it represent the fact that <em>conditional on its parents</em>, <strong>the variable is conditionally independent of all other preceding variables</strong>. That is, at the <img src="https://latex.codecogs.com/png.latex?j">th stage of construction, we only draw an incoming arrow into <img src="https://latex.codecogs.com/png.latex?x_j"> from its parents; conditional on its parents, all other predecessors are independent. Therefore, a <strong>missing arrow</strong> between any two nodes means that they are independent, once we know the values of their parents.</p>
<section id="an-example" class="level3">
<h3 class="anchored" data-anchor-id="an-example">An example</h3>
<p>Let’s say we’re looking at the relationship between smoking and cardiac arrest. We might assume that smoking causes changes in cholesterol, which causes cardiac arrest. We start assuming that unhealthy lifestyle is the only <strong>‘Parent’</strong> for both smoking and weight:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/first-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Then, we assume that the <strong>‘Parents’</strong> of cholesterol are Smoking and Weight:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/second-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Finally, we assume that Cholesterol is the only <em>‘Parent’</em> of Cardiac Arrest</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/third-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Therefore, the arrows, by specifying what are the conditional independencies that hold in our model, tell us that what is the <em>recursive decomposition</em> of the joint probability distribution implied by the model. In this case, thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%5Cleft(x_%7B1%7D,%20x_2,%20x_3,%20x_4,%20x_%7B5%7D%5Cright)%20=%20p(x_1)%20p(x_2%20%7C%20x_1)%20p%20(x_3%20%7C%20x_1)%20p(x_4%7C%20x_2,%20x_3)%20p%20(x_5%20%7C%20x_4)%0A"></p>
</section>
<section id="probability-connections-markov-compatibility" class="level3">
<h3 class="anchored" data-anchor-id="probability-connections-markov-compatibility">Probability connections: Markov Compatibility</h3>
<p>Whenever a joint probability distribution <img src="https://latex.codecogs.com/png.latex?P"> admits the decomposition posited by our graphical model <img src="https://latex.codecogs.com/png.latex?G">, we say that <img src="https://latex.codecogs.com/png.latex?P"> is Markov Relative to <img src="https://latex.codecogs.com/png.latex?G">. That is, it means that <img src="https://latex.codecogs.com/png.latex?G"> can explain the generation of the data represented by <img src="https://latex.codecogs.com/png.latex?P">.</p>
<p>Once we know the probabilistic connection between our Graph and a probability distribution, we may be interested in deriving the <strong>testable implications</strong> of our graphical model <img src="https://latex.codecogs.com/png.latex?G">. What other conditions does it imply about the probability distribution P?</p>
<p>In particular, in deriving which variables are independent and dependent, both marginally and conditional on other variables. Thanks to our graphical representation, we can derive an algorithm that returns all <em>implied independencies</em> that the model expects that will hold in the <em>data</em>: <strong>the d-separation criterion</strong>.</p>
</section>
</section>
<section id="d-separation-blocking-the-information-flows" class="level2">
<h2 class="anchored" data-anchor-id="d-separation-blocking-the-information-flows">d-separation: blocking the information flows</h2>
<p>Deriving testable implications from our assumptions is not as easy as it looks. Take the following example:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/example-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Can we make <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> conditionally dependent? The answer is not evident. To answer this question, we must understand how the <strong>statistical information moves</strong> through our graph.</p>
</section>
<section id="open-paths" class="level2">
<h2 class="anchored" data-anchor-id="open-paths">Open paths</h2>
<p>In our Graph <img src="https://latex.codecogs.com/png.latex?G">, the information moves across variables <em>regardless</em> of the direction of the arrows. If there is an open path between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, then, the information will flow and thus make the two variables dependent. The question, then, is what is an open path? The d-criterion answers precisely this question:</p>
<p>( A ) path ( p ) is said to be blocked (or ( d ) -separated) by a set of nodes ( Z ) if and only if any of the following conditions hold:</p>
<ol type="1">
<li><p>if ( p ) contains a chain ( i m j ) or a fork ( i m j ) such that the middle node ( m ) is in ( Z ).</p></li>
<li><p>if ( p ) contains a collider ( i m j ) such that the middle node ( m ) is not in ( Z ) and such that no descendant of ( m ) is in ( Z ).</p></li>
</ol>
<p>Let’s examine why is that both conditions guarantee that the path is closed.</p>
<section id="of-chains-and-forks" class="level3">
<h3 class="anchored" data-anchor-id="of-chains-and-forks">Of chains and forks</h3>
<p>In both chains and forks, <em>the extreme variables</em> are <strong>marginally dependent</strong>. However, once we <em>adjust by the middle variable</em> they become <strong>marginally independent</strong>. That is, the information between the extreme variables that was previously flowing stops flowing altogether. Why?</p>
</section>
<section id="chains" class="level3">
<h3 class="anchored" data-anchor-id="chains">Chains</h3>
<p>Let’s look at an example of the chain where addictive behavior causes the person to smoke which causes Cancer. If we don’t adjust by any variable, the information flows freely: knowing whether some has an addictive behavior can tell us whether they smoke, and thus tells us something about the probability that they have cancer.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/mediation-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>However, if we adjust by a particular level of smoking (as in the above figure), knowing the addictive behavior of someone tells us nothing about the probability that they have cancer: the level of smoking says it all already. Thus, addictive behavior and cancer are d-separated by smoking.</p>
</section>
<section id="forks" class="level3">
<h3 class="anchored" data-anchor-id="forks">Forks</h3>
<p>With forks, the story is extremely similar: the middle variable is a common cause for both of the extreme variables. Thus, they are marginally dependent. For example, let’s say that addictive behavior causes both smoking and drinking coffee.</p>
<p>If we don’t know whether someone has an addictive behavior or not, the fact that they drink lots of coffee tells us that they are likely to have an addicting behavior and thus are more likely to smoke.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/fork, coffee-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>However, once we adjust by a specific value of addictive behavior (as in the above figure), drinking coffee does not say anything about smoking, the value of addictive behavior has said it all already: <strong>they are independent conditional on addictive behavior</strong>.</p>
<p>It is only in the absence of knowing addictive behavior that they capture information from one another. That is, they are only dependent when we don’t adjust by addictive behavior.</p>
<p>Therefore, whenever a <strong>path has either a fork or a chain, we can block that path by conditioning on the middle variable</strong>.</p>
</section>
<section id="of-colliders" class="level3">
<h3 class="anchored" data-anchor-id="of-colliders">Of Colliders</h3>
<p>Whereas before we spoke of open paths that were closed once we adjust, a collider starts being a closed path an only is opened once we adjust by the middle variable. The middle variable is a common consequence of two marginally independent causes: if we adjust by the consequence, we render the common causes dependent.</p>
<p>Take the following example: being a hollywood actor results from either being attractive or being talented. Being attractive and being talented are independent; however, once we know whether someone is a hollywood actor, knowing one of the characteristics tells us about the other. e.g., if the actor is not talented, it tells us that the actor, to be in hollywood, then must be attractive.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/collider-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Therefore, if a path has a collider it will remained closed unless we adjust by the middle variable or one of its descendants.</p>
</section>
<section id="the-probabilistic-implications-of-the-d-separation-criterion" class="level3">
<h3 class="anchored" data-anchor-id="the-probabilistic-implications-of-the-d-separation-criterion">The probabilistic implications of the d-separation criterion</h3>
<p>Once we have understood our d-separation criterion, we can connect this graphical analysis with a counterpart probability implication:</p>
<p>If ( X ) and ( Y ) are ( d ) -separated by ( Z ) in a DAG ( G, ) then ( X ) is independent of ( Y ) conditional on ( Z ) in every distribution compatible with ( G ). Conversely, if ( X ) and ( Y ) are not ( d ) -separated by ( Z ) in a DAG ( G ), then ( X ) and ( Y ) are dependent conditional on ( Z )</p>
</section>
<section id="putting-the-d-separation-criterion-to-the-test" class="level3">
<h3 class="anchored" data-anchor-id="putting-the-d-separation-criterion-to-the-test">Putting the d-separation criterion to the test</h3>
<p>Armed with the d-separation criterion, we can ask ourselves, given our earlier that I reproduce below, how can we make <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> conditionally dependent?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/example-repe-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The answer, of course, is the d-separation criterion: can we open the path from <img src="https://latex.codecogs.com/png.latex?X"> to <img src="https://latex.codecogs.com/png.latex?Y"> by conditioning on some variable? Armed with what we just learned, the answer is now clear: <img src="https://latex.codecogs.com/png.latex?z_1"> is a collider that lies in the path. To open it, we must condition on it. Therefore, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> are dependent conditional on <img src="https://latex.codecogs.com/png.latex?z_1">. We can easily check our logic with <code>dagitty::dconnected</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">dconnected</span>(example, <span class="st" style="color: #20794D;">"X"</span>, <span class="st" style="color: #20794D;">"Y"</span>, <span class="st" style="color: #20794D;">"Z1"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>We can repeat this exercise for <strong>every pair of variables</strong> and check whether they are <em>conditionally (or marginally) independent</em> given some set of controlling variables. The set of variables that are conditionally independent, then, constitute the <strong>set of testable implications</strong> of our model. Therefore, we can check whether our Graphical model is consistent with a given dataset by <em>comparing</em> the <strong>implied conditional independence</strong> with the <strong>observed conditional independence</strong> in our data.</p>
<p>For the above model, then, the implied conditional independencies are:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">impliedConditionalIndependencies</span>(example)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X _||_ Y
X _||_ Z2
X _||_ Z3
Y _||_ Z1 | Z2
Y _||_ Z1 | Z3
Y _||_ Z2 | Z3
Z1 _||_ Z3 | Z2</code></pre>
</div>
</div>
</section>
</section>
<section id="can-we-distinguish-models-from-data-alone" class="level2">
<h2 class="anchored" data-anchor-id="can-we-distinguish-models-from-data-alone">Can we distinguish models from data alone?</h2>
<p>We finally have an strategy to test whether our model implications are in accordance with our observed values. However, it stands to reason that our model is not unique in this regard: there are other models that imply the same conditional independencies. We say, thus, that the two graphical models <img src="https://latex.codecogs.com/png.latex?G_1"> and <img src="https://latex.codecogs.com/png.latex?G_2"> are <strong>observationally equivalent</strong> when they imply the same conditional independencies. The set of all the models with indistinguishable implications is called an equivalence class.</p>
<p>That is, if two models are observationally equivalent, we cannot use data alone to distinguish from them. We must use our structural knowledge about the problem at hand to decide which model is the right one. Therefore, <strong>Observational equivalence</strong> places a <em>limit</em> on our ability to <strong>infer directionality from probabilities alone</strong>.</p>
<p>For <em>simple models</em>, the <strong>limitations are draconian</strong>. Let’s take as an example the fork we just analyzed where addictive behavior is a fork between drinking coffee and smoking:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/fork-repeated-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s check what are this model’s testable implications:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="fu" style="color: #4758AB;">impliedConditionalIndependencies</span>(<span class="fu" style="color: #4758AB;">dagify</span>(x <span class="sc" style="color: #5E5E5E;">~</span> f,</span>
<span id="cb5-2">       y <span class="sc" style="color: #5E5E5E;">~</span> f,</span>
<span id="cb5-3">       <span class="at" style="color: #657422;">labels =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"x"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Coffee"</span>, </span>
<span id="cb5-4">                  <span class="st" style="color: #20794D;">"y"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Smoking"</span>, <span class="st" style="color: #20794D;">"f"</span> <span class="ot" style="color: #003B4F;">=</span> <span class="st" style="color: #20794D;">"Addictive </span><span class="sc" style="color: #5E5E5E;">\n</span><span class="st" style="color: #20794D;">Behavior"</span>)))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x _||_ y | f</code></pre>
</div>
</div>
<p>Not surprisingly, the only conditional implication implied is that drinking coffee is independent on smoking once we condition on the addictive behavior. That is, <strong>that coffee is d-separated from smoking by addictive behavior</strong>.</p>
</section>
<section id="identifying-models" class="level2">
<h2 class="anchored" data-anchor-id="identifying-models">Identifying models</h2>
<p>Suppose for a moment that the arrows in our Graph are now endowed with causal meaning: there’s an arrow from <img src="https://latex.codecogs.com/png.latex?X"> to <img src="https://latex.codecogs.com/png.latex?Y"> if <img src="https://latex.codecogs.com/png.latex?X"> causes <img src="https://latex.codecogs.com/png.latex?Y">. For the moment, it will suffice your intuitive understanding of what this means.</p>
<p>The Causal Graph will still have all the characteristics of a simple Bayesian Graph. Then, we can ask: What <em>other causal models</em> have only this <strong>testable implication</strong>? That is, with only observational data, can we distinguish between causal (interventional) models?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks_files/figure-html/equivalent-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Thus, <strong>from data-alone we cannot infer the directionality of any of three posited causal relationships</strong>. That is, data alone cannot settle the issue of whether the appropriate model is a fork or a chain that begins at either coffee or smoking. As Pearl says, <strong>data are fundamentally dumb</strong>: if we rely only in data to inform our models, we are extremely limited on what we can learn from them. Therefore, we must extend our theory beyond <strong>conditional probabilities</strong>.</p>
<p>That is, we cannot predict the consequences of intervening in one of the variables with only observational data. That is, <strong>we cannot gain causal understanding with only observational data</strong>: we must assume a causal model to predict the the consequences of any intervention (i.e., the causal effects).</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/causality/2020-07-18-causality-bayesian-networks.html</guid>
  <pubDate>Sat, 18 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>BDA Week 9: Large Sample Theory for the Posterior</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html</link>
  <description><![CDATA[ 




<p>As Richard McElreath says in his fantastic <a href="https://xcelab.net/rm/statistical-rethinking/">Statistics course</a>, Frequentist statistics is more a framework to evaluate estimators than a framework for deriving them. Therefore, we can use frequentist tools to evaluate the posterior. In particular, what happens to the posterior as more and more data arrive from the same sampling distribution?</p>
<p>In this blogpost, I’ll follow chapter 4 of <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a> and the material in week 9 of <a href="https://github.com/avehtari/BDA_course_Aalto">Aki Vehtari’s course</a> to study the <strong>Posterior Distribution</strong> under the framework of Large Sample Theory.</p>
<section id="asymptotic-normality-and-consistency" class="level2">
<h2 class="anchored" data-anchor-id="asymptotic-normality-and-consistency">Asymptotic Normality and Consistency</h2>
<p>Suppose then that the true data distribution is <img src="https://latex.codecogs.com/png.latex?f(y)">. If the true data distribution is included in the parametric family, then, for some <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0"> we have <img src="https://latex.codecogs.com/png.latex?f(y)%20=%20%5Cpi(y%20%7C%5Ctheta_0)">. <em>Asymptotic consistency</em> then, guarantees that as the sample size increases, our posterior distribution will converge to a point mass at the true parameter value <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0">. Both the <em>median, the mean and the mode</em> of the posterior are consistent estimators for <img src="https://latex.codecogs.com/png.latex?%5Ctheta_0">.</p>
<p>Indeed, asymptotic normality, in its turn, guarantees that the limiting distribution of the posterior can be approximated with a Gaussian centered at the mode <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20%5Ctheta"> of the posterior:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(%5Ctheta%20%7C%20y)%20%5Capprox%20N(%5Cwidehat%20%5Ctheta,%20%5B%5Cfrac%7Bd%5E2%7D%7Bd%5Ctheta%5E2%7D%20%5Clog%20%5Cpi(%5Ctheta%20%7C%20y)%5D_%7B%5Ctheta%20=%20%5Cwidehat%20%5Ctheta%7D%5E%7B-1%7D%20)%0A"></p>
<p>Naturally, we can then approximate the posterior by finding the mode by finding the maximum of the posterior and then estimating the curvature at that point. As more and more data arrives, the approximation will be more and more precise.</p>
</section>
<section id="big-data-and-the-normal-approximation" class="level2">
<h2 class="anchored" data-anchor-id="big-data-and-the-normal-approximation">Big Data and the Normal Approximation</h2>
<p>If more and more data makes the normal approximation better and better, does that mean that the era of <strong>Big Data will usher a new era</strong> where the posterior will be easily and reliably <em>approximated with a Gaussian</em>?</p>
<p>Not quite: as we have more and more data, <strong>so we have more and more questions that we can possibly ask</strong>. We can then create <em>more complex models with more parameters</em> to try to answer these more complicated questions. In this scenario, as more and more data arrives, the posterior distribution will not converge to the Gaussian approximation in the expanding parameter space that reflects the increasing complexity of our model. The reason? <strong>The curse of dimensionality</strong></p>
<p>The more dimensions we have, ceteris paribus, due to <strong>the concentration of measure</strong>, the mode and the area around the mode will be farther and farther way from the typical set. Thus, the Gaussian approximation that concentrates most of the mass around the mode will be a poor substitute of the typical set and thus a poor approximation for the posterior distribution.</p>
</section>
<section id="normal-approximation-for-the-marginals" class="level2">
<h2 class="anchored" data-anchor-id="normal-approximation-for-the-marginals">Normal Approximation for the Marginals</h2>
<p>Nevertheless, even if we cannot approximate the joint posterior distribution with the Gaussian approximation, the normal approximation is not that faulty if we instead focus on the marginal posterior. The reason? Determining the marginal distribution of a component of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is equivalent to averaging over all the other components of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, and averaging a family of distributions generally brings them closer to normality, by the same logic that underlies the central limit theorem.</p>
<p>This fact explains why we see so many approximately Gaussian marginals in practice and why can sometimes summarize them with a point estimate and a standard error.</p>
</section>
<section id="unbiasedness-and-hierarchical-models" class="level2">
<h2 class="anchored" data-anchor-id="unbiasedness-and-hierarchical-models">Unbiasedness and Hierarchical Models</h2>
<p>Frequentist methods place great emphasis on unbiasedness: <img src="https://latex.codecogs.com/png.latex?E%5B%5Cwidehat%20%5Ctheta%5D%20=%20%5Ctheta_0">. However, when parameters are related and partial knowledge of some of the parameter is clearly relevant to the estimation of others, the emphasis on unbiasedness is clearly misplaced: it leads to a naive point in the bias-variance trade-off.</p>
<p>Indeed, if we have a familiy of parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j"> that are related, a Bayesian would fit a hierarchical model by positing a common distribution from which these parameters are sampled. This procedure leads to pooling of the information and thus to shrink the individual parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j"> toward each other; thereby biasing the individual estimates <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j"> but reducing their variance.</p>
<p>Thus, this highlights the problem that it is often not possible to estimate several parameters at once in an even approximately unbiased manner: unbiased <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j"> leads to ignoring relevant information about their common distribution, which in turn leads to a biased estimate of the variance of the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-13-bda-week-9-large-sample-theory-for-the-posterior.html</guid>
  <pubDate>Mon, 13 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>BDA Week 8: Bayesian Decision Analysis</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-10-bda-week-8-bayesian-decision-analysis.html</link>
  <description><![CDATA[ 




<p>Many if not most statistical analyses are performed for the ultimate goal of decision making. Bayesian Statistics has the advantage of direct use of probability to quantify the uncertainty around unobserved quantities of interest: whether those are parameters or predictions, we end up with a posterior distribution.</p>
<p>Indeed, our posterior predictions interact uniquely for each possible action <img src="https://latex.codecogs.com/png.latex?d"> we can take to create a unique distribution of our utility <img src="https://latex.codecogs.com/png.latex?U(x)%7Cd">. Then, it amounts to compare which of these distributions benefits of the most. For example, we can choose the action <img src="https://latex.codecogs.com/png.latex?d"> with the highest expected utility. With the help of Mathematica, I go over the following toy problem:</p>
<blockquote class="blockquote">
<p>Widgets cost $2 each to manufacture and you can sell them for $3. Your forecast for the market for widgets is (approximately) normally distributed with mean 10,000 and standard deviation 5,000. How many widgets should you manufacture in order to maximize your expected net profit?</p>
</blockquote>
<p>Which action, production number, leads to maximize our expected profit? Read <a href="https://www.wolframcloud.com/obj/c888fd64-0372-4531-af70-94a53cafe364">this post</a> to find out.</p>



<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-10-bda-week-8-bayesian-decision-analysis.html</guid>
  <pubDate>Fri, 10 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>BDA week 7: LOO and its diagnostics</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html</link>
  <description><![CDATA[ 




<p>Once Stan’s implementation of HMC has run its magic, we finally have samples from the posterior distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20(%5Ctheta%20%7C%20y))">. We can then run posterior predictive checks and hopefully our samples looks plausible under our posterior. Nevertheless, this is just an internal validation check: <strong>we expect more from our model</strong>. We expect it to hold under an external validation check: never seen observations, once predicted, should also look plausible under our posterior.</p>
<p><strong>Leave-One-Out (LOO) log pointwise predictive density</strong> is the preferred Bayesian way to do this. In this blogpost, I’ll explain how we can <strong>approximate</strong> this metric <em>without the need of refitting the model <img src="https://latex.codecogs.com/png.latex?n"> times</em> with <strong>LOO Pareto Smoothed Importance Sampling (PSIS)</strong>. Also, I’ll explain how PSIS diagnostics tells us, not unlike HMC, when the algorithm is a poor approximation. As a byproduct, we can also derive a metric to identify if there are influential observations that are driving the inference. Finally, I’ll simulate data to show how we can perform all this with real data.</p>
<p>All of this is based on this <a href="https://arxiv.org/abs/1507.04544">great paper</a> by Vehtari, Gelman and Gabry.</p>
<section id="what-is-our-metric-log-pointwise-predictive-density" class="level2">
<h2 class="anchored" data-anchor-id="what-is-our-metric-log-pointwise-predictive-density">What is our metric? Log pointwise predictive density</h2>
<p>Given an observation <img src="https://latex.codecogs.com/png.latex?y_i">, we define our metric to evaluate how well we have predicted <img src="https://latex.codecogs.com/png.latex?y_i"> as its log likelihood according to our model. Given our uncertainty over the parameters, we integrate over our posterior distribution for our model. We call this the <strong>log pointwise predictive density (lpd)</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Alpd%20=%20%5Clog%20%5Cint%20%5Cpi%20(y_i%20%7C%20%5Ctheta)%20%5Cpi%20(%5Ctheta%20%7C%20y)%20d%5Ctheta%0A"></p>
<p>The fundamental problem comes when we use <img src="https://latex.codecogs.com/png.latex?y_i"> to compute the full posterior <img src="https://latex.codecogs.com/png.latex?%5Cpi%20(%5Ctheta%20%7C%20y)">: we are performing an internal check, not an external validation. A solution is to use the resulting posterior without using the observation <img src="https://latex.codecogs.com/png.latex?y_i"> in the fitting. <strong>This is the Leave-One-Out (LOO) posterior</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Ctheta,%20y_%7B-i%7D)"></p>
<p>The problem is that evaluating the LOO posterior is just as computationally expensive as fitting the model all over again. If we then want to use all our observations to perform the external validation check, this amounts to fitting the probability model <img src="https://latex.codecogs.com/png.latex?n"> times.</p>
</section>
<section id="approximating-the-loo-posterior" class="level2">
<h2 class="anchored" data-anchor-id="approximating-the-loo-posterior">Approximating the LOO posterior</h2>
<p>Not being able to compute from a distribution is an awfully familiar problem in Bayesian Statistics. Which in this case comes in handy. We can use <a href="https://david-salazar.github.io/2020/06/27/bayesian-data-analysis-week-4-importance-sampling/">Importance Sampling</a> to use <strong>the samples from the full posterior to approximate the LOO posterior</strong>. Thus, our Importance Weights for each sample <img src="https://latex.codecogs.com/png.latex?s"> from the posterior is the ratio of the densities.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar_i%5Es%20=%20%5Cfrac%7B%5Cpi%20(%5Ctheta%5Es%20%7C%20y_%7B-i%7D)%7D%7B%5Cpi(%5Ctheta%5Es%7Cy_i)%7D%0A"></p>
<p>If we correct, then, our original full posterior samples by these weights, we get equivalent samples from the LOO posterior. Thus, we can compute the <strong>log pointwise predictive density (lpd)</strong> that can track the out-of-sample performance of our model.</p>
</section>
<section id="the-approximation-is-likely-to-fail" class="level2">
<h2 class="anchored" data-anchor-id="the-approximation-is-likely-to-fail">The approximation is likely to fail</h2>
<p>Sadly, this approximation to the LOO posterior using the full posterior is likely to fail. Importance Sampling only works when all of the weights are roughly equal. When the weights are very small with a large probability, and very, very large with a small probability, Importance Sampling fails: our computations end up <em>effectively</em> using <strong>only the large weights samples</strong>, thus drastically reducing our <strong>effective number of samples from the LOO posterior</strong>. That is, Importance Sampling is likely to <strong>fail when the distribution of the weights is fat-tailed</strong>.</p>
<p>Sadly, this is very likely to happen with our approximation: the LOO posterior is likely to have a <em>larger variance and fatter tails</em> than the full posterior. Thus, samples from the tails of the full posterior will have large weights to compensate for this fact. Therefore, the distribution of importance weights is likely gonna be fat-tailed.</p>
</section>
<section id="correcting-the-approximation-with-psis" class="level2">
<h2 class="anchored" data-anchor-id="correcting-the-approximation-with-psis">Correcting the approximation with PSIS</h2>
<p>Vehtari, Gelman and Gabry correct the distribution of Importance Weights and thereby improve the approximation to the LOO posterior. First, they use Extreme Value Theory to fit the tail of the distribution with a Generalized Pareto Distribution with tail shape parameter <img src="https://latex.codecogs.com/png.latex?k"> (<img src="https://latex.codecogs.com/png.latex?GPD(k)">).</p>
<p>Secondly, they replace the large weights with smoothed over versions of the weights according to expected order statistics of the fitted <img src="https://latex.codecogs.com/png.latex?GPD(k)">. This in turn gives the name to the method: <strong>Pareto Smoothed Importance Sampling (PSIS)</strong> Thirdly, they truncate large weights at <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7B3%7D%7B4%7D"> of the mean of the smoothed weights.</p>
<p>Therefore, we arrive at a new vector of importance weights <img src="https://latex.codecogs.com/png.latex?w_i%5Es"> which, in general, behaves better than the original importance weights <img src="https://latex.codecogs.com/png.latex?r_i%5Es"> and thus allow us to perform a better approximation of the LOO posterior.</p>
<section id="its-about-the-diagnostics-we-made-along-the-way" class="level3">
<h3 class="anchored" data-anchor-id="its-about-the-diagnostics-we-made-along-the-way">It’s about the diagnostics we made along the way</h3>
<p>The great thing about PSIS, besides creating better importance weights, it’s the diagnostics that it creates along the way. By fitting a <img src="https://latex.codecogs.com/png.latex?GPD(k)">, the tail shape parameter becomes a diagnostic to assess the reliability of our approximation. When is Pareto Smoothed Importance Sampling (PSIS) a valid approximation to the LOO posterior?</p>
<p>The smoothing and the truncating can only do so much. If <img src="https://latex.codecogs.com/png.latex?k%20%3E%200.7">, the importance weights are probably too fat-tailed to begin with and PSIS-LOO will be a poor approximation the LOO posterior. Not only that, it is also a diagnostic that tells us about a fundamental disagreement bewteen the full posterior and the LOO posterior: that is, <strong>about observations that are highly influential in determining the posterior.</strong></p>
<p>Therefore, by performing PSIS-LOO, we also arrive at a diagnostic for highly influential observations that are driving our inference and are thus surprising observations to our model.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-08-bda-week-7-loo-and-its-diagnostics.html</guid>
  <pubDate>Wed, 08 Jul 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Tail Risk of diseases in R</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html</link>
  <description><![CDATA[ 




<p>Pasquale Cirillo and Nassim Taleb published a short, interesting and important paper on the <a href="https://arxiv.org/abs/2004.08658">Tail Risk of contagious diseases</a>. In short, the distribution of fatalities is strongly fat-tailed: thus rendering any forecast, whether is pointwise or a distributional forecast, useless and dangerous. The <strong>distributional evidence is there</strong>: the lack of a characteristic scale makes our uncertainty at any point in the pandemic maximal: we can only say that it can always get worse.</p>
<p>Read the paper! It’s short and packed of ideas. In this blogpost, I’ll reproduce the main plots, the main model that uses Extreme Value Theory to model the tail of the distribution of casualties and, finally, I will re-implement the model in a Bayesian framework using Stan.</p>
<section id="the-data" class="level2">
<h2 class="anchored" data-anchor-id="the-data">The Data</h2>
<p>Taleb and Cirillo collected data for 72 events with more than 1,000 estimated victims.</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">72</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">8</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">character</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">7</td>
</tr>
<tr class="even">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 19%">
<col style="width: 13%">
<col style="width: 19%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">min</th>
<th style="text-align: right;">max</th>
<th style="text-align: right;">empty</th>
<th style="text-align: right;">n_unique</th>
<th style="text-align: right;">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">name</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">34</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">69</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 16%">
<col style="width: 9%">
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 4%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">start_year</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1619.08</td>
<td style="text-align: right;">517.26</td>
<td style="text-align: right;">-429</td>
<td style="text-align: right;">1595.00</td>
<td style="text-align: right;">1813.0</td>
<td style="text-align: right;">1916.50</td>
<td style="text-align: right;">2019</td>
<td style="text-align: left;">▁▁▁▁▇</td>
</tr>
<tr class="even">
<td style="text-align: left;">end_year</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1613.35</td>
<td style="text-align: right;">520.65</td>
<td style="text-align: right;">-426</td>
<td style="text-align: right;">1593.00</td>
<td style="text-align: right;">1813.5</td>
<td style="text-align: right;">1923.75</td>
<td style="text-align: right;">2020</td>
<td style="text-align: left;">▁▁▁▁▇</td>
</tr>
<tr class="odd">
<td style="text-align: left;">lower</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2660.74</td>
<td style="text-align: right;">9915.03</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">10.00</td>
<td style="text-align: right;">75.5</td>
<td style="text-align: right;">850.00</td>
<td style="text-align: right;">75000</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">avg_est</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4877.66</td>
<td style="text-align: right;">19132.36</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">10.00</td>
<td style="text-align: right;">82.0</td>
<td style="text-align: right;">850.00</td>
<td style="text-align: right;">137500</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td style="text-align: left;">upper_est</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">7094.56</td>
<td style="text-align: right;">28705.00</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">10.00</td>
<td style="text-align: right;">88.0</td>
<td style="text-align: right;">850.00</td>
<td style="text-align: right;">200000</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">rescaled_avg_est</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">84874.62</td>
<td style="text-align: right;">409099.85</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">41.75</td>
<td style="text-align: right;">738.0</td>
<td style="text-align: right;">6112.25</td>
<td style="text-align: right;">2678283</td>
<td style="text-align: left;">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td style="text-align: left;">population</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2037.94</td>
<td style="text-align: right;">2434.28</td>
<td style="text-align: right;">50</td>
<td style="text-align: right;">554.00</td>
<td style="text-align: right;">990.0</td>
<td style="text-align: right;">1817.25</td>
<td style="text-align: right;">7643</td>
<td style="text-align: left;">▇▂▁▁▂</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The estimates for the number of casualties are in the thousands. The <code>avg_est</code> is the estimate for the number of casualities that we are going to be working with.</p>
</section>
<section id="the-plots" class="level2">
<h2 class="anchored" data-anchor-id="the-plots">The plots</h2>
<p>Taleb and Cirillo begin the paper with a graphical analysis of the properties of the data. In it, they show that the data present all of the traits of fat-tailed random variables. To reproduce most of the figures, I use an <a href="https://github.com/David-Salazar/ggtails/">R package</a> that I wrote: <code>ggtails</code>.</p>
<section id="max-to-sum-ratio" class="level3">
<h3 class="anchored" data-anchor-id="max-to-sum-ratio">Max-to-Sum ratio</h3>
<p>Taleb and Cirillo begin by examining the <a href="https://david-salazar.github.io/2020/06/02/lln-for-higher-p-moments/">Max-to-Sum ratio plots</a>. A consequence of the Law of Large Numbers (LLN) is the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5BX%5Ep%5D%20%3C%20%5Cinfty%20%20%5Ciff%20R_n%5Ep%20=%20%5Cdfrac%7Bmax(X_1%5Ep,%20%5Cdots,%20X_n%5Ep)%7D%7B%5Csum_%7Bi=1%7D%5En%20X_i%5Ep%7D%20%5Cto%200,%20%5C%20%5Ctext%7Bas%7D%20%5C%20n%20%5Cto%20%5Cinfty%0A"> That is, the theoretical moment <img src="https://latex.codecogs.com/png.latex?p"> exists if and only if the ratio of the partial max to the partial sum converges to <img src="https://latex.codecogs.com/png.latex?0">. Neither of the fourth moments converges for neither of the fatalities’ estimates</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">data <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">sample =</span> avg_est)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-3">  ggtails<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">stat_max_sum_ratio_plot</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">limits =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>), <span class="at" style="color: #657422;">breaks =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="fl" style="color: #AD0000;">0.2</span>, <span class="fl" style="color: #AD0000;">0.4</span>, <span class="fl" style="color: #AD0000;">0.6</span>, <span class="fl" style="color: #AD0000;">0.8</span>, <span class="dv" style="color: #AD0000;">1</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-5">  <span class="fu" style="color: #4758AB;">scale_color_brewer</span>(<span class="at" style="color: #657422;">palette =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">type =</span> <span class="st" style="color: #20794D;">"qual"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-6">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Max-to-Sum ratio plot"</span>,</span>
<span id="cb1-7">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"There's no convergence. No finite moment is likely to exist"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/maxtosum-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Given that none of the moments converges, it is likely that we are dealing with such a fat-tailed random variable that all of the theoretical moments are undefined. Or if the theoretical moments exist, that the Law of Large Numbers works way too slowly for us to use it. In which case any method that relies on any sample moment estimator of the empirical distribution is useless.</p>
</section>
<section id="histogram" class="level3">
<h3 class="anchored" data-anchor-id="histogram">Histogram</h3>
<p>Let’s check what exactly is the range of values that Cirillo and Taleb have collected:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">data <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(avg_est<span class="sc" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">1000</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-3">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="dv" style="color: #AD0000;">10</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">8</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">80</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-4">  <span class="fu" style="color: #4758AB;">scale_x_continuous</span>(<span class="at" style="color: #657422;">labels =</span> <span class="cf" style="color: #003B4F;">function</span>(x) scales<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">number</span>(x, <span class="at" style="color: #657422;">scale =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">10</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">8</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-5">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Casualties (x $10^8$)"</span>),</span>
<span id="cb2-6">       <span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Histogram of casualties"</span>,</span>
<span id="cb2-7">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"The data encodes a huge array of variation. No characteristic scale, typical of fat-tails"</span>) </span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/histogram-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The data contains an incredible arrange of variation. This is typical for fat-tailed random variables.</p>
</section>
<section id="the-zipf-plot" class="level3">
<h3 class="anchored" data-anchor-id="the-zipf-plot">The Zipf plot</h3>
<p>For a Pareto random variable, the slope of the log of the Survival function in log space decays linearly. With the Zipf plot, we compare the decay of the log of the empirical survival function with the linear decay that we expect with a Pareto.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">data <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">sample =</span> <span class="fu" style="color: #4758AB;">log10</span>(avg_est<span class="sc" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">1000</span>))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-3">  ggtails<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">stat_zipf</span>(<span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>)  <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-4">  <span class="fu" style="color: #4758AB;">scale_x_log10</span>(<span class="at" style="color: #657422;">label=</span>scientific_10) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-5">  <span class="fu" style="color: #4758AB;">scale_y_log10</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-6">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Zipf plot of casualties"</span>,</span>
<span id="cb3-7">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"There's the linear decay we expect with fat-tailed variables"</span>,</span>
<span id="cb3-8">       <span class="at" style="color: #657422;">x =</span> <span class="st" style="color: #20794D;">"log(x)"</span>,</span>
<span id="cb3-9">       <span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">"log(Survival)"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/loglogplot-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The empirical survival function indeed decays slowly: it’s almost linear. Thus, giving us a hint that we are dealing with a fat-tailed random variable.</p>
</section>
<section id="mean-excess-plot" class="level3">
<h3 class="anchored" data-anchor-id="mean-excess-plot">Mean Excess Plot</h3>
<p>For a given threshold <img src="https://latex.codecogs.com/png.latex?v">, the Mean Excess for a random variable <img src="https://latex.codecogs.com/png.latex?X"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5BX%20-%20v%20%7C%20X%20%3E%20v%5D%0A"> For a Pareto, we expect this mean excess to scale linearly with the threshold.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">data <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">sample =</span> avg_est<span class="sc" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">1000</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-3">  ggtails<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">stat_mean_excess</span>()<span class="ot" style="color: #003B4F;">-&gt;</span> p </span>
<span id="cb4-4">dat <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">layer_data</span>(p)</span>
<span id="cb4-5"></span>
<span id="cb4-6">dat <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb4-7">  <span class="fu" style="color: #4758AB;">filter</span>(y <span class="sc" style="color: #5E5E5E;">&lt;</span> <span class="fl" style="color: #AD0000;">3.95e07</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-8">  <span class="fu" style="color: #4758AB;">ggplot</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-9">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(x, y), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-10">  <span class="fu" style="color: #4758AB;">scale_x_continuous</span>(<span class="at" style="color: #657422;">labels =</span> <span class="cf" style="color: #003B4F;">function</span>(x) scales<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">number</span>(x, <span class="at" style="color: #657422;">scale =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">10</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">6</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-11">  <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> <span class="cf" style="color: #003B4F;">function</span>(x) scales<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">number</span>(x, <span class="at" style="color: #657422;">scale =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">10</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">7</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-12">  <span class="fu" style="color: #4758AB;">expand_limits</span>(<span class="at" style="color: #657422;">y =</span> <span class="fl" style="color: #AD0000;">5e07</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-13">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Mean Excess Plot for casualties"</span>,</span>
<span id="cb4-14">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"There's the linear slope that we expect with fat-tailed variables"</span>,</span>
<span id="cb4-15">       <span class="at" style="color: #657422;">caption =</span> <span class="st" style="color: #20794D;">"More volatile observations were excluded."</span>,</span>
<span id="cb4-16">       <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Threshold (x $10^6$)"</span>),</span>
<span id="cb4-17">       <span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">" Mean Excess Funtion (x $10^7$)"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/meplot-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Given that the mean excess plot increases linearly, we are even more convinced that the number of casualties is indeed fat-tailed.</p>
</section>
</section>
<section id="fitting-the-tail" class="level2">
<h2 class="anchored" data-anchor-id="fitting-the-tail">Fitting the tail</h2>
<p>The graphical analysis tells us that we are likely dealing with a fat-tailed random variable: the survival function decays very slowly. Thus, the fat-tails make a) an extremely large array of possibilities relevant; b) thus, eliminating the possibility of a characteristic scale or “typical” catastrophe; c) and possibly making the theoretical moments undefined.</p>
<section id="wait-a-moment-infinite-casualties" class="level3">
<h3 class="anchored" data-anchor-id="wait-a-moment-infinite-casualties">Wait a moment: infinite casualties?</h3>
<p>We know that the number of casualties is bounded by the total population. Thus, the variable only has the appearance of an infinite mean given its upper bound. Graphically, by ignoring the upper bound, we are positing a continuous tail thus:</p>
<p><img src="https://david-salazar.github.io/images/apparenttail.PNG" class="img-fluid"></p>
<p>The difference, thus, is only relevant in the vicinity of the upper bound <img src="https://latex.codecogs.com/png.latex?H">. One could thus keep modeling ignoring the upper bound without too many practical consequences. Nevertheless, it would be epistemologically wrong. To solve this problem, Taleb and Cirillo introduce a log transformation that eliminates the upper bound:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ%20=%20%5Cvarphi(Y)=L-H%20%5Clog%20%5Cleft(%5Cfrac%7BH-Y%7D%7BH-L%7D%5Cright)%0A"></p>
<p>Taleb and Cirillo call <img src="https://latex.codecogs.com/png.latex?Z"> the dual observations. <img src="https://latex.codecogs.com/png.latex?Z"> is the variable that we will model with Extreme Value theory.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">L <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb5-2">H <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">7700000</span></span>
<span id="cb5-3">data_to_model <span class="ot" style="color: #003B4F;">&lt;-</span> data <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-4">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">dual =</span> L <span class="sc" style="color: #5E5E5E;">-</span>  H<span class="sc" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">log</span>( (H<span class="sc" style="color: #5E5E5E;">-</span>avg_est) <span class="sc" style="color: #5E5E5E;">/</span> (H<span class="sc" style="color: #5E5E5E;">-</span>L) ) )</span></code></pre></div>
</div>
</section>
<section id="extreme-value-theory-on-the-dual-observations" class="level3">
<h3 class="anchored" data-anchor-id="extreme-value-theory-on-the-dual-observations">Extreme Value theory on the dual observations</h3>
<p>A logical question, then, is how fat-tailed is exactly the tail of casualties from contagious diseases? Extreme Value Theory offers an answer. Indeed, the Pickands–Balkema–de Haan theorem states that tail events (events larger than a large threshold) have as a limiting distribution a Generalized Pareto Distribution (GPD). In math, for large u, the conditional excess function is thus defined and approximated by a GPD:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AG_%7Bu%7D(z)=P(Z%20%5Cleq%20z%20%5Cmid%20Z%3Eu)=%5Cfrac%7BG(z)-G(u)%7D%7B1-G(u)%7D%20%5Capprox%20GPD(z;%20%5Cxi,%20%5Cbeta,%20u)%0A"> Where <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is the crucial shape parameter that determines how slowly the tail decays. The larger <img src="https://latex.codecogs.com/png.latex?%5Cxi">, the more slowly it decays. For example, the variance is only defined for <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%3C%200.5">. For <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%3E%201">, the theoretical mean is not defined.</p>
<p>Crucially, we can approximate the tail of the original distribution <img src="https://latex.codecogs.com/png.latex?G(z)"> with <em>a <img src="https://latex.codecogs.com/png.latex?GPD"> with the same shape parameter</em> <img src="https://latex.codecogs.com/png.latex?%5Cxi">. Finally, Taleb and Cirillo use <img src="https://latex.codecogs.com/png.latex?200,000"> as a threshold <img src="https://latex.codecogs.com/png.latex?u">. We can check both in the Mean Excess Plot and the Zipf plot that around this value we observe a power-law like behavior.</p>
</section>
<section id="maximum-likelihood-estimate" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimate">Maximum Likelihood estimate</h3>
<p>We can fit the GPD via maximum likelihood.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">fit <span class="ot" style="color: #003B4F;">&lt;-</span> evir<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">gpd</span>(data_to_model<span class="sc" style="color: #5E5E5E;">$</span>dual, <span class="at" style="color: #657422;">threshold =</span> <span class="dv" style="color: #AD0000;">200</span>)</span>
<span id="cb6-2"><span class="fu" style="color: #4758AB;">round</span>(fit<span class="sc" style="color: #5E5E5E;">$</span>par.ests, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     xi    beta 
   1.62 1174.74 </code></pre>
</div>
</div>
<p>Which are the same estimates as Taleb and Cirillo. The standard errors are thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><span class="fu" style="color: #4758AB;">round</span>(fit<span class="sc" style="color: #5E5E5E;">$</span>par.ses, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    xi   beta 
  0.52 534.44 </code></pre>
</div>
</div>
<p>Just as we saw with our graphical analysis, the variable is definitely fat-tailed: <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%3E%200">, which thanks to the relationship of the the Pickands–Balkema–de Haan with the Fisher-Tippet theorem, tells us that the Maximum Domain of Attraction is thus a Fréchet. From Taleb and Cirillo’s paper:</p>
<blockquote class="blockquote">
<p>As expected <img src="https://latex.codecogs.com/png.latex?%5Cxi"> &gt; 1 once again supporting the idea of an infinite first moment… Looking at the standard error of <img src="https://latex.codecogs.com/png.latex?%5Cxi">, one could also argue that, with more data from the upper tail, the first moment could possibly become finite. Yet there is no doubt about the non-existence of the second moment, and thus about the unreliability of the sample mean, which remains too volatile to be safely used.</p>
</blockquote>
<p>Let’s see if we can reproduce this conclusions in a bayesian framework.</p>
</section>
</section>
<section id="bayesian-model" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-model">Bayesian model</h2>
<p>I’ll sample from the posterior using <a href="https://mc-stan.org/users/interfaces/rstan">Stan’s</a> incredible implementation of Hamiltonian Monte Carlo. Most of the heavy lifting in Stan has already been done by Aki Vehtari in a <a href="https://mc-stan.org/users/documentation/case-studies/gpareto_functions.html">case study</a> using the GPD.</p>
<p>Our bayesian model will be thus defined:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20%5Csim%20GPD(%5Cxi,%20%5Cbeta,%20u%20=%20200)%20%5C%5C%0A%5Cxi%20%5Csim%20Normal(1,%201)%20%5C%5C%0A%5Cbeta%20%5Csim%20Normal(1000,%20300)%0A"> The prior for <img src="https://latex.codecogs.com/png.latex?%5Cxi"> is weakly informative and yet still opens the opportunity for the data to move our posterior towards a finite mean and finite variance.</p>
<section id="simulating-fake-data" class="level3">
<h3 class="anchored" data-anchor-id="simulating-fake-data">Simulating fake data</h3>
<p>To verify that our code is correctly working, we’ll simulate data and assess parameter recovery.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">expose_stan_functions</span>(<span class="st" style="color: #20794D;">"gpd.stan"</span>)</span>
<span id="cb10-2"></span>
<span id="cb10-3">fake_data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">replicate</span>(<span class="fl" style="color: #AD0000;">1e3</span>, <span class="fu" style="color: #4758AB;">gpareto_rng</span>(<span class="at" style="color: #657422;">ymin =</span> <span class="dv" style="color: #AD0000;">200</span>, <span class="fl" style="color: #AD0000;">0.8</span>, <span class="dv" style="color: #AD0000;">1000</span>))</span>
<span id="cb10-4">ds<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">ymin=</span><span class="dv" style="color: #AD0000;">200</span>, <span class="at" style="color: #657422;">N=</span><span class="fl" style="color: #AD0000;">1e3</span>, <span class="at" style="color: #657422;">y=</span>fake_data)</span>
<span id="cb10-5">fit_gpd <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan</span>(<span class="at" style="color: #657422;">file=</span><span class="st" style="color: #20794D;">'gpd.stan'</span>, <span class="at" style="color: #657422;">data=</span>ds, <span class="at" style="color: #657422;">refresh=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb10-6">                     <span class="at" style="color: #657422;">chains=</span><span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">seed=</span><span class="dv" style="color: #AD0000;">100</span>, <span class="at" style="color: #657422;">cores =</span> <span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC with 4 parallel chains...

Chain 1 finished in 1.3 seconds.
Chain 2 finished in 1.3 seconds.
Chain 3 finished in 1.3 seconds.
Chain 4 finished in 1.3 seconds.

All 4 chains finished successfully.
Mean chain execution time: 1.3 seconds.
Total execution time: 1.8 seconds.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="fu" style="color: #4758AB;">print</span>(fit_gpd, <span class="at" style="color: #657422;">pars =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"xi"</span>, <span class="st" style="color: #20794D;">"beta"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference for Stan model: gpd-202211241219-1-288048.
4 chains, each with iter=1000; warmup=500; thin=1; 
post-warmup draws per chain=500, total post-warmup draws=2000.

        mean se_mean    sd   2.5%     25%     50%     75%   97.5% n_eff Rhat
xi      0.78    0.00  0.06   0.67    0.74    0.78    0.82    0.90   942    1
beta 1111.34    2.24 65.43 983.84 1068.06 1112.49 1153.74 1241.58   856    1

Samples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:38 2022.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
</div>
<p>The credible intervals are in line with the true parameters. Graphically:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1">posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as.matrix</span>(fit_gpd, <span class="at" style="color: #657422;">pars =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"xi"</span>, <span class="st" style="color: #20794D;">"beta"</span>))</span>
<span id="cb14-2">true <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.8</span>, <span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb14-3"><span class="fu" style="color: #4758AB;">mcmc_recover_hist</span>(posterior, true) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb14-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Parameter recovery"</span>,</span>
<span id="cb14-5">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"We successfully recover the parameters"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/par-recover-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>We can thus reasonably recover our parameter of interest <img src="https://latex.codecogs.com/png.latex?%5Cxi"> with our current model. Therefore, we can follow along and fit our model to the real data.</p>
</section>
<section id="fitting-the-model" class="level3">
<h3 class="anchored" data-anchor-id="fitting-the-model">Fitting the model</h3>
<p>Using the dual observations, we end up with <img src="https://latex.codecogs.com/png.latex?25/72"> observations, around <img src="https://latex.codecogs.com/png.latex?34.7">% of the total number of observations</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1">data_to_model <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb15-2">  <span class="fu" style="color: #4758AB;">filter</span>(dual <span class="sc" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">200</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb15-3">  <span class="fu" style="color: #4758AB;">pull</span>(dual) <span class="ot" style="color: #003B4F;">-&gt;</span> dual_observations</span>
<span id="cb15-4"><span class="fu" style="color: #4758AB;">summary</span>(dual_observations)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    200     800    1500   14040    7504  138742 </code></pre>
</div>
</div>
<p>Fitting the model is just as simple as fitting it to fake data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1">ds<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">ymin=</span><span class="dv" style="color: #AD0000;">200</span>, <span class="at" style="color: #657422;">N=</span><span class="dv" style="color: #AD0000;">25</span>, <span class="at" style="color: #657422;">y=</span>dual_observations)</span>
<span id="cb17-2">fit_gpd <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan</span>(<span class="at" style="color: #657422;">file=</span><span class="st" style="color: #20794D;">'gpd.stan'</span>, <span class="at" style="color: #657422;">data=</span>ds,</span>
<span id="cb17-3">                     <span class="at" style="color: #657422;">chains=</span><span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">seed=</span><span class="dv" style="color: #AD0000;">100</span>, <span class="at" style="color: #657422;">cores =</span> <span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">5000</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC with 4 parallel chains...

Chain 1 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 1 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 1 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 1 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 1 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 2 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 2 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 2 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 2 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 2 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 3 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 3 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 3 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 3 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 3 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 4 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 4 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 4 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 4 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 4 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 1 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 2 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 3 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 4 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 1 finished in 0.3 seconds.
Chain 2 finished in 0.3 seconds.
Chain 3 finished in 0.3 seconds.
Chain 4 finished in 0.3 seconds.

All 4 chains finished successfully.
Mean chain execution time: 0.3 seconds.
Total execution time: 0.4 seconds.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><span class="fu" style="color: #4758AB;">print</span>(fit_gpd, <span class="at" style="color: #657422;">pars =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"xi"</span>, <span class="st" style="color: #20794D;">"beta"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference for Stan model: gpd-202211241219-1-6efc8a.
4 chains, each with iter=5000; warmup=2500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=10000.

        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat
xi      1.69    0.01   0.43   0.99   1.39    1.65    1.95    2.66  5847    1
beta 1077.46    3.32 246.93 624.37 902.09 1069.18 1240.92 1579.44  5519    1

Samples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:51 2022.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
</div>
<p>For know, the <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R"> values look OK, which indicates that there’s no much disagreement between the Markov Chains. The credible intervals rule out the possibility that <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%3C%200">.</p>
<p>We can intuitively interrogate the posterior for more precise questions in a Bayesian settings. For example, what is the probability that the theoretical mean is undefined, i.e., that <img src="https://latex.codecogs.com/png.latex?%5Cxi%20%3E%201">:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/theoretical_mean-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Therefore, we conclude that it is very likely that <strong>the theoretical mean is undefined and we rule out definitely the possibility of a finite second moment</strong>. Thus, we reproduce the paper’s conclusions.</p>
<p>Given the fat-tailedness of the data and the resulting lack of characteristic scale from the huge array of variability that it’s possible, there’s no <em>possibility</em> of forecasting what we may face with either a pointwise prediction or a distributional forecast.</p>
</section>
<section id="posterior-predictive-checks" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-checks">Posterior predictive checks</h3>
<p>If the model fits well to the data, the replicated data under the model should look similar to the observed data. We can easily generate data from our model because our model is generative: we draw simulated values from the posterior predictive distribution of replicated data.</p>
<p>However, with <em>fat-tailed</em> data: what exactly does it mean for our replicated data to track our observed data. For example, what about the <strong>replicated maxima</strong>? These definitely should include the observed maxima: <strong>but also much larger values. That is the whole point of being in the MDA of a Fréchet.</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1">yrep <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">extract</span>(fit_gpd)<span class="sc" style="color: #5E5E5E;">$</span>yrep </span>
<span id="cb21-2"><span class="fu" style="color: #4758AB;">ppc_stat</span>(<span class="fu" style="color: #4758AB;">log</span>(dual_observations), <span class="fu" style="color: #4758AB;">log</span>(yrep), <span class="at" style="color: #657422;">stat =</span> <span class="st" style="color: #20794D;">"max"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb21-3">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Posterior predictive check (log scale)"</span>,</span>
<span id="cb21-4">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Maxima across replicated datasets track observed maximum and beyond, just like it should"</span>,</span>
<span id="cb21-5">       <span class="at" style="color: #657422;">x =</span> <span class="st" style="color: #20794D;">"log(maxima)"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/ppc-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>And this is exactly what our model does: a difference of 30 in a log-scale is huge. We can just as well expect maxima as large as observed, and even much larger.</p>
</section>
<section id="convergence-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="convergence-diagnostics">Convergence diagnostics</h3>
<p>One of the main benefits of Stan’s implementation of HMC is that it yells at you when things have gone wrong. We can thus check a variety of diagnostics to check for convergence. Above, we examined that the <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R"> values look OK. We can also check trace plots:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1">posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as.array</span>(fit_gpd)</span>
<span id="cb22-2"><span class="fu" style="color: #4758AB;">color_scheme_set</span>(<span class="st" style="color: #20794D;">"viridis"</span>)</span>
<span id="cb22-3"><span class="fu" style="color: #4758AB;">mcmc_trace</span>(posterior, <span class="at" style="color: #657422;">pars =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"xi"</span>, <span class="st" style="color: #20794D;">"beta"</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb22-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Traceplots"</span>,</span>
<span id="cb22-5">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Traceplots are stationary, well mixed and the chains converge"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/traceplot-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>We can check HMC specific diagnostics:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><span class="fu" style="color: #4758AB;">check_divergences</span>(fit_gpd)</span>
<span id="cb23-2"><span class="fu" style="color: #4758AB;">check_treedepth</span>(fit_gpd)</span></code></pre></div>
</div>
</section>
</section>
<section id="stressing-the-data" class="level2">
<h2 class="anchored" data-anchor-id="stressing-the-data">Stressing the data</h2>
<p>Taleb and Cirillo are well aware that they are not working with the more precise of data. Thus, they ‘stress’ the data and check whether the results still hold. Given that we are working with fat-tailed data, the tail wags the dog: data problems can only modify the results if they are in the tail.</p>
<section id="measurement-error" class="level3">
<h3 class="anchored" data-anchor-id="measurement-error">Measurement error</h3>
<p>To account for measurement error, Taleb and Cirillo recreate 10,000 of their datasets, but this time where each observation is allowed to vary between 80% and 120% of its recorded values. They find that their results are robust to this modification.</p>
<p>In a bayesian setting, we can very easily extend our model to account for uncertainty around the true data. Indeed, in a bayesian model there’s no fundamental difference between a <em>parameter and an observation</em>: one is observed and the other is not. Thus, we formulate the true casualties being measured as <a href="https://mc-stan.org/docs/2_18/stan-users-guide/bayesian-measurement-error-model.html">missing data</a>.</p>
<p>Therefore, we specify that the measurement comes a normal with unknown mean, the true number of casualties, and that their standard deviation is 20% of the observed value.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20%5Csim%20Normal(y_%7Btrue%7D,%20y*0.2)%0A"> We fit the model and let Bayes do the rest:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1">ds<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">ymin=</span><span class="dv" style="color: #AD0000;">200</span>, <span class="at" style="color: #657422;">N=</span><span class="dv" style="color: #AD0000;">25</span>, <span class="at" style="color: #657422;">y=</span>dual_observations, <span class="at" style="color: #657422;">noise =</span> <span class="fl" style="color: #AD0000;">0.2</span><span class="sc" style="color: #5E5E5E;">*</span>dual_observations)</span>
<span id="cb24-2">fit_gpd_m <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">stan</span>(<span class="at" style="color: #657422;">file=</span><span class="st" style="color: #20794D;">'gpd_measurementerror.stan'</span>, <span class="at" style="color: #657422;">data=</span>ds,</span>
<span id="cb24-3">                     <span class="at" style="color: #657422;">chains=</span><span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">seed=</span><span class="dv" style="color: #AD0000;">100</span>, <span class="at" style="color: #657422;">cores =</span> <span class="dv" style="color: #AD0000;">4</span>, <span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">5000</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running MCMC with 4 parallel chains...

Chain 1 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 1 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 1 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 1 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 1 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 2 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 2 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 2 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 2 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 2 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 3 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 3 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 3 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 3 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 3 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 4 Iteration:    1 / 5000 [  0%]  (Warmup) 
Chain 4 Iteration:  100 / 5000 [  2%]  (Warmup) 
Chain 4 Iteration:  200 / 5000 [  4%]  (Warmup) 
Chain 4 Iteration:  300 / 5000 [  6%]  (Warmup) 
Chain 4 Iteration:  400 / 5000 [  8%]  (Warmup) 
Chain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) 
Chain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) 
Chain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) 
Chain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) 
Chain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) 
Chain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) 
Chain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) 
Chain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) 
Chain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) 
Chain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) 
Chain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) 
Chain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) 
Chain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) 
Chain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) 
Chain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) 
Chain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) 
Chain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) 
Chain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) 
Chain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) 
Chain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) 
Chain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) 
Chain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) 
Chain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) 
Chain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) 
Chain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) 
Chain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) 
Chain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) 
Chain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) 
Chain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) 
Chain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) 
Chain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) 
Chain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) 
Chain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) 
Chain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) 
Chain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) 
Chain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) 
Chain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 1 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 2 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) 
Chain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) 
Chain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) 
Chain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) 
Chain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) 
Chain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) 
Chain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) 
Chain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) 
Chain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) 
Chain 1 finished in 0.6 seconds.
Chain 2 finished in 0.6 seconds.
Chain 3 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) 
Chain 4 Iteration: 5000 / 5000 [100%]  (Sampling) 
Chain 3 finished in 0.6 seconds.
Chain 4 finished in 0.6 seconds.

All 4 chains finished successfully.
Mean chain execution time: 0.6 seconds.
Total execution time: 0.8 seconds.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><span class="fu" style="color: #4758AB;">print</span>(fit_gpd_m, <span class="at" style="color: #657422;">pars =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"xi"</span>, <span class="st" style="color: #20794D;">"beta"</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference for Stan model: gpd_measurementerror-202211241219-1-98c1f4.
4 chains, each with iter=5000; warmup=2500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=10000.

        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat
xi      1.66    0.00   0.43   0.95   1.35    1.62    1.92    2.61 13151    1
beta 1056.99    1.99 248.82 583.47 887.32 1049.45 1219.83 1562.54 15706    1

Samples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:57 2022.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
</div>
<p>The model sampled remarkably well. We can thus check whether our inferences still hold:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/infer-measurement-1.png" class="img-fluid" width="768"></p>
</div>
</div>
</section>
</section>
<section id="influential-observations" class="level2">
<h2 class="anchored" data-anchor-id="influential-observations">Influential observations</h2>
<p>Taleb and Cirillo also stress their data by recreating the dataset 10,000 times and then eliminating from 1 to 7 of the observations with a jacknife resampling procedure. Thus, they confirm that no single observation is driving the inference.</p>
<p>In a bayesian setting, we can check for influential observations by <em>comparing</em> the <strong>full-data predictive posterior distribution to the Leave-one-out predictive posterior for each left out point</strong>. That is: we compare <img src="https://latex.codecogs.com/png.latex?p(y_i%20%7C%20y)"> with <img src="https://latex.codecogs.com/png.latex?p(y_i,%20%7C%20y_%7B-i%7D)">.</p>
<p>We can quickly estimate <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta_i,%20%7C%20y_%7B-i%7D)"> for each <img src="https://latex.codecogs.com/png.latex?i"> by just sampling from <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%20%7C%20y)"> using <a href="https://mc-stan.org/loo/reference/psis.html">Pareto Smoothed Importance Sampling LOO</a>. By examining the distribution of the importance weights, we can compare how different the two distributions are. If the weights are too fat-tailed, and the variance is infinite for the <img src="https://latex.codecogs.com/png.latex?j">th observation, then the <img src="https://latex.codecogs.com/png.latex?j">th observation is highly <a href="https://arxiv.org/pdf/1709.01449.pdf">influential observation determining our posterior distribution</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1">loglik <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">extract</span>(fit_gpd)<span class="sc" style="color: #5E5E5E;">$</span>log_lixi</span>
<span id="cb28-2">loopsis <span class="ot" style="color: #003B4F;">&lt;-</span> loo<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">loo</span>(loglik, loo<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">relative_eff</span>(<span class="fu" style="color: #4758AB;">exp</span>(loglik)))</span>
<span id="cb28-3"><span class="fu" style="color: #4758AB;">plot</span>(loopsis)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Indeed, no single observation is driving our inference.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Taleb and Cirillo show that the number of casualties are patently fat-tailed. Thus, there’s no typical catastrophe nor characteristic scale: a huge array of possibilities are likely and relevant for any analysis. Thus, there’s no <em>possibility</em> of forecasting what we may face with either a pointwise prediction or a distributional forecast.</p>
<p>Indeed, we cannot even use our sample mean to do anything useful. Given the asymmetric risks involved and the huge uncertainty, the whole of Taleb’s work is very clear: we must kill the pandemic in the egg.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-07-05-tail-risk-of-diseases-in-r.html</guid>
  <pubDate>Sun, 05 Jul 2020 05:00:00 GMT</pubDate>
  <media:content url="https://david-salazar.github.io/images/apparenttail.PNG" medium="image"/>
</item>
<item>
  <title>BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html</link>
  <description><![CDATA[ 




<p>In the last couple of weeks, We’ve seen how the most difficult part of Bayesian Statistics is computing the posterior distribution. In particular, in the last week, we’ve studied the <a href="https://david-salazar.github.io/2020/06/29/bayesian-data-analysis-week-5-metropolis/">Metropolis Algorithm</a>. In this blogpost, I’ll study why Metropolis <strong>does not scale well enough to high dimensions</strong> and give an <em>intuitive explanation</em> of our best alternative: <strong>Hamiltonian Monte Carlo</strong> (HMC).</p>
<p>This blogpost is my personal digestion of the excellent content that Michael Betancourt has put out there to explain HMC. I particularly enjoyed his <a href="https://www.youtube.com/watch?v=jUSZboSq1zg&amp;t=1638s">YouTube lecture</a> and his blogpost about <a href="https://betanalpha.github.io/assets/case_studies/probabilistic_computation.html">Probabilistic computation</a>.</p>
<section id="metropolis-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="metropolis-algorithm">Metropolis Algorithm</h2>
<p>As we’ve seen, the Metropolis algorithm is just a random walk through parameter space where the proposed jumps are corrected by a comparison of posterior densities to determine whether to move or not. That is, we can see the Metropolis algorithm as:</p>
<blockquote class="blockquote">
<p>A stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density until it finds the mode and then only sometimes accepting steps that decrease the posterior density.</p>
</blockquote>
<p>Thus, as long as the algorithm has run long enough to find the posterior mode, <strong>and the area around the mode is a good representation of the overall posterior</strong>, the Metropolis Algorithm will work. Sadly, this assumption only holds for lower dimensions</p>
</section>
<section id="an-expectation-in-low-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="an-expectation-in-low-dimensions">An Expectation in low dimensions</h2>
<p>Fundamentally, we sample from the posterior in order to compute Monte-Carlo estimators. That is, we want to approximate expectations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Bf%5D=%5Cint_%7BQ%7D%20%5Cmathrm%7Bd%7D%20q%20%5Cpi(q)%20f(q)%0A"></p>
<p>With the Metropolis algorithm, we are saying that we can approximate these expectation by computing at values near the mode of <img src="https://latex.codecogs.com/png.latex?%5Cpi(q)"> and occasionally exploring lower density parts. Which is a pretty good approximation of most unimodal low dimensional probability distributions. However, this entirely ignores the contribution of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bd%7D%20q">, the differential volume, to the integral.</p>
</section>
<section id="an-expectation-in-high-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="an-expectation-in-high-dimensions">An Expectation in high dimensions</h2>
<p>However, as the number of dimensions grows, so does the importance of <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bd%7D%20q">, the differential volume, to the integral. Therefore, in higher dimensions, ignoring the differential volume when computing the expectation guarantees biased samples. Why does this happen? This is due to a phenomenon called <strong>concentration of measure</strong>.</p>
<p>Intuitively, an integral is nothing more than a weighted sum of parts of the distribution with height equal to function heights. However, as dimensions grows, <strong>the contribution of any single part of the distribution to the integral decreases very rapidly</strong>. Why? Because, the relative differential volume of this single part of the distribution decreases as we increase the number of dimensions. Michael’s Betancourt box-experiment is very illuminating.</p>
<section id="corner-madness" class="level3">
<h3 class="anchored" data-anchor-id="corner-madness">Corner Madness</h3>
<blockquote class="blockquote">
<p>Let’s begin with a simple example - a square box representing the ambient space. In each dimension we will partition the box into three smaller boxes, with the central box corresponding to the neighborhood around the mode and the outer two side boxes corresponding to the rest of the space.</p>
</blockquote>
<p>In one dimension, the central box encapsulates one third of the total volume. In two dimensions, there are nine boxes, thus the central box represents only 1/9 of the volume. In three dimensions, we must fill the corners of the corners with more boxes, thus creating 27 boxes. The central box now only represents 1/27 of the total volume. We can keep increasing dimensions with the aid of Monte-Carlo samples.</p>
<blockquote class="blockquote">
<p>The relative importance of the central box is its probability with respect to a uniform distribution over the enclosing box, which we can estimate by sampling from that uniform distribution and computing the Monte Carlo estimator of the expectation value of the corresponding inclusion function</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(CentralBox)%20%5Capprox%20%5Cdfrac%7BN_%7BcentralBox%7D%7D%7BN%7D%0A"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">N <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">10000</span></span>
<span id="cb1-2">Ds <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb1-3">prob_means <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">0</span> <span class="sc" style="color: #5E5E5E;">*</span> Ds</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="cf" style="color: #003B4F;">for</span> (D <span class="cf" style="color: #003B4F;">in</span> Ds) {</span>
<span id="cb1-6">  is_central_samples <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">1</span>, N)</span>
<span id="cb1-7">  <span class="cf" style="color: #003B4F;">for</span> (n <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>N) {</span>
<span id="cb1-8">    <span class="co" style="color: #5E5E5E;"># Sample a new point one dimension at a time</span></span>
<span id="cb1-9">    <span class="cf" style="color: #003B4F;">for</span> (d <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>D) {</span>
<span id="cb1-10">      q_d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">runif</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb1-11">      </span>
<span id="cb1-12">    <span class="co" style="color: #5E5E5E;"># If the sampled component is not contained </span></span>
<span id="cb1-13">    <span class="co" style="color: #5E5E5E;"># within the central interval then set the </span></span>
<span id="cb1-14">    <span class="co" style="color: #5E5E5E;"># inclusion function to zero</span></span>
<span id="cb1-15">      <span class="cf" style="color: #003B4F;">if</span> (<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span> <span class="sc" style="color: #5E5E5E;">&gt;</span> q_d <span class="sc" style="color: #5E5E5E;">||</span> q_d <span class="sc" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-16">        is_central_samples[n] <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb1-17">    }</span>
<span id="cb1-18">  }</span>
<span id="cb1-19">  <span class="co" style="color: #5E5E5E;"># Estimate the relative volume as a probability</span></span>
<span id="cb1-20">  prob_means[D] <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(is_central_samples)</span>
<span id="cb1-21">}</span>
<span id="cb1-22"></span>
<span id="cb1-23"><span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">dimensions =</span> Ds, <span class="at" style="color: #657422;">prob_central_box =</span> prob_means) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-24">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(dimensions, prob_central_box)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-25">  <span class="fu" style="color: #4758AB;">geom_step</span>(<span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-26">  <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> scales<span class="sc" style="color: #5E5E5E;">::</span>percent) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-27">  <span class="fu" style="color: #4758AB;">scale_x_continuous</span>(<span class="at" style="color: #657422;">breaks =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-28">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">"Probability of central box"</span>,</span>
<span id="cb1-29">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"As dimensions grow, volume of a single box decreases rapidly"</span>,</span>
<span id="cb1-30">       <span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Central Box Volume in high dimensions"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo_files/figure-html/box-1.png" class="img-fluid" width="768"></p>
</div>
</div>
</section>
<section id="all-that-matters-the-typical-set" class="level3">
<h3 class="anchored" data-anchor-id="all-that-matters-the-typical-set">All that matters: the typical set</h3>
<p>Thus, the volume of a single neighborhood of the distribution decreases very rapidly as the number of dimensions grow. In our expectation, thus, the mode of the density <img src="https://latex.codecogs.com/png.latex?%5Cpi(q)"> represents a neighborhood of the distribution with less and less volume and thus less and less probability mass. Indeed, in higher dimensions, our earlier <strong>intuition of a a probability distribution as concentrating around the mode and quickly decreasing is terribly wrong.</strong></p>
<p>Indeed, the probability of mass of any neighborhood of the distribution really depends on the interplay between its volume and the density. Without both of them, the probability mass at that neighborhood the distribution vanishes. Indeed, Michael Betancourt’s has a great plot showing where the volume and the target density interact to create large probability mass:</p>
<p><img src="https://david-salazar.github.io/images/typicalset.PNG" class="img-fluid"></p>
<blockquote class="blockquote">
<p>The neighborhood immediately around the mode features large densities, but in more than a few dimensions the small volume of that neighborhood prevents it from having much contribution to any expectation. On the other hand, the complimentary neighborhood far away from the mode features a much larger volume, but the vanishing densities lead to similarly negligible contributions expectations. The only significant contributions come from the neighborhood between these two extremes known as the typical set</p>
</blockquote>
<p>Therefore, to calculate an expectation <strong>it suffices to sample from the typical set</strong> instead of the <em>entire</em> parameter space. The question, then, is what is exactly the shape of this typical set?</p>
</section>
<section id="a-surface-concentrating-away-from-the-mode" class="level3">
<h3 class="anchored" data-anchor-id="a-surface-concentrating-away-from-the-mode">A surface concentrating away from the mode</h3>
<p>As the number of dimensions grows and grows, the increasing tension between probability density and volume yields a worse compromise, and those compromises are found at points further and further from the mode. <strong>This is known as concentration of measure</strong>. Thus:</p>
<blockquote class="blockquote">
<p>The repulsion of the typical set away from the mode implies that the relative breadth of the typical set, how far its fuzziness diffuses, shrinks with increasing dimension. As we consider higher-dimensional spaces the typical set becomes ever more thin and gossamer, and all the harder to find and explore.</p>
</blockquote>
<p>Thus, the typical set, as the number of dimensions grow, becomes a narrow ridge of probability that becomes very difficult to explore with guess and check algorithms like the Metropolis algorithm.</p>
</section>
</section>
<section id="metropolis-and-the-typical-set" class="level2">
<h2 class="anchored" data-anchor-id="metropolis-and-the-typical-set">Metropolis and the typical set</h2>
<p>The Metropolis Algorithm is not equipped to sample from the typical set. As we’ve seen, it’s a mode finding algorithm that explores the neighborhood around it. However, this neighborhood, as we’ve just seen, has barely any volume in high dimensions and thus have barely any probability mass. Indeed, given the narrowness of the typical set, almost all proposed jumps of the random walk will be outside the typical set and thus will be rejected.</p>
<p>Therefore, <strong>the Metropolis Algorithm in high dimensions results in inaction</strong>: the Metropolis algorithm will end up without moving at all for a long time. Thus, being a waste of our computational resources.</p>
<p>In two dimensions, we can visualize a typical set as a thin donut: at any point within it, there’s only a narrow ridge of points next to it that also belong to the typical set. <strong>Any ad-hoc proposal distribution that guesses in which direction we should move will be biased to propose points outside the typical set that will be rejected once we compare the Metropolis acceptance probability</strong>.</p>
<p>Chi Feng <a href="https://chi-feng.github.io/mcmc-demo/">has awesome interactive visualizations</a> that represent exactly this conundrum:</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD <img src="https://david-salazar.github.io/images/metropolis.gif" class="img-fluid"> ======= <img src="https://david-salazar.github.io/images/metropolis.GIF" class="img-fluid"> &gt;&gt;&gt;&gt;&gt;&gt;&gt; abe09e3cd313dfc12630c3063c969959af729a6a</p>
</section>
<section id="hamiltonian-monte-carlo" class="level1">
<h1>Hamiltonian Monte-Carlo</h1>
<p>So far, we’ve identified the fundamental problem with the random walk in the Metropolis algorithm: <em>in higher dimensions</em>, its <strong>adhoc proposal distribution guesses too many dumb jumps that take us out of the narrow ridge of high probability mass that is the typical set</strong>. How to fix this? Create a proposal distribution that takes into account the geometry of the typical set and makes intelligent proposal?</p>
<p>An intelligent proposal should make proposals within the typical set AND make proposals that are far away from where we currently are. Thus, we will explore the typical set as efficiently as possible. Hamiltonian Monte-Carlo algorithms, when well tuned, satisfy both conditions.</p>
<section id="crafting-a-proposal-distribution-for-the-typical-set" class="level2">
<h2 class="anchored" data-anchor-id="crafting-a-proposal-distribution-for-the-typical-set">Crafting a proposal distribution for the typical set</h2>
<p>We’ve identified that an intelligent proposal distribution will encode the geometry of the typical set. A natural way of encoding this geometry is with a <em>vector field</em> <strong>aligned with the typical set</strong>. Thus, instead of taking a random-walk through parameter space, we will simply follow the vector field for some time. Therefore:</p>
<blockquote class="blockquote">
<p>Continuing this process traces out a coherent trajectory through the typical set that efficiently moves us far away from the initial point to new, unexplored regions of the typical set as quickly as possible.</p>
</blockquote>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD The question, then, is how to create these vector field? A first approximation is to exploit the differential structure of the posterior through the gradient. However, following the gradient pulls us away from the typical set and into the mode of the posterior. Here comes Physics to the rescue. This is an equivalent problem to the problem of placing a satellite in a stable orbit around a hypothetical planet. ======= The question, then, is how to create these vector field? A first approximation is to exploit the differential structure of the posterior thoruhg the gradient. However, following the gradient pulls us away from the typical set and into the mode of the posterior. Here comes Physics to the rescue. This is an equivalent problem to the problem of placing a satellite in a stable orbit around a hypothetical planet. &gt;&gt;&gt;&gt;&gt;&gt;&gt; abe09e3cd313dfc12630c3063c969959af729a6a</p>
<p>The key to do it is to endow our walk through the typical set by with enough momentum (<img src="https://latex.codecogs.com/png.latex?p">) to counteract the gravitation attraction of the mode. Therefore:</p>
<blockquote class="blockquote">
<p>We can <em>twist</em> the gradient vector field into a vector field aligned with the typical set if we expand our original probabilistic system with the introduction of auxiliary momentum parameters.</p>
</blockquote>
<section id="augmenting-with-the-momenta" class="level3">
<h3 class="anchored" data-anchor-id="augmenting-with-the-momenta">Augmenting with the momenta</h3>
<p>We augment our probabilistic system with the momenta <img src="https://latex.codecogs.com/png.latex?p"> thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(q,%20p)=%5Cpi(p%20%5Cmid%20q)%20%5Cpi(q)%0A"></p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Which allows us to ignore the momenta when necessary by marginalizing it. The joint density <img src="https://latex.codecogs.com/png.latex?%5Cpi(q,%20p)"> defines a Hamiltonian</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AH(p,%20q)%20=%20-%20log%20%5Cpi(p,%5Ctheta)%0A"></p>
<p>Thus, given some point in the phase space <img src="https://latex.codecogs.com/png.latex?(q_0,%20p_0)"> that is in the typical set, the Hamiltonian defines how to generate a new sample such that we stay in the typical set.</p>
</section>
</section>
</section>
<section id="therefore" class="level1">
<h1>Therefore:</h1>
<p>Which allows us to ignore the momenta when necessary by marginalizing it. The joint density <img src="https://latex.codecogs.com/png.latex?%5Cpi(q,%20p)"> is constructed following Hamiltonian dynamics. Therefore: &gt;&gt;&gt;&gt;&gt;&gt;&gt; abe09e3cd313dfc12630c3063c969959af729a6a</p>
<blockquote class="blockquote">
<p>Following the Hamiltonian vector field for some time, ( t ), generates trajectories, ( _{t}(q, p), ) that rapidly move through phase <img src="https://latex.codecogs.com/png.latex?(q_n,%20p_n)"> space while being constrained to the typical set. Projecting these trajectories back down onto the target parameter space finally yields the efficient exploration of the target typical set for which we are searching.</p>
</blockquote>
<section id="stages-to-generate-a-proposal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="stages-to-generate-a-proposal-distribution">3 stages to generate a proposal distribution</h3>
<p>Suppose you are at <img src="https://latex.codecogs.com/png.latex?q_0">. How to generate a new proposal?</p>
<ol type="1">
<li>We start by sampling a momentum from <img src="https://latex.codecogs.com/png.latex?p_0"> from:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_0%20%5Csim%20%5Cpi(p%20%7C%20q)%0A"></p>
<p>We know are in the phase space <img src="https://latex.codecogs.com/png.latex?(q_0,%20p_0)">.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 2. If we follow the vector field by following Hamilton’s equations that we have created ( <em>{t}(q_0, p_0) = q^<em>, p^</em>, ) by integrating for some time <img src="https://latex.codecogs.com/png.latex?t">, we coherently push the Markov transition away from the initial point while staying confined to the joint typical set. This movement through the field space is done via discrete <img src="https://latex.codecogs.com/png.latex?L"> <em>‘leapfrog steps’</em> with discretization time <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">. Stan’s implementation of HMC automatically chooses both of these values. ======= 2. If we follow the vector field that we have created ( </em>{t}(q_0, p_0) = q^<em>, p^</em>, ) by integrating for some time <img src="https://latex.codecogs.com/png.latex?t">, we coherently push the Markov transition away from the initial point while staying confined to the joint typical set. This movement through the field space is done via discrete <img src="https://latex.codecogs.com/png.latex?L"> <em>‘leapfrog steps’</em>. Stan’s implementation of HMC automatically chooses the leapfrog steps and their respective length. &gt;&gt;&gt;&gt;&gt;&gt;&gt; abe09e3cd313dfc12630c3063c969959af729a6a</p>
<ol start="3" type="1">
<li>If we marginalize the momenta, we project it away and go back to the parameter space with a proposal <img src="https://latex.codecogs.com/png.latex?q%5E*">.</li>
</ol>
<p>Because we have followed the vector field in phase space and projected the momenta away, if we started in the typical set, our proposal is guaranteed to be on the typical set, too. <strong>Thus, allowing extremely efficient exploration of the typical set.</strong></p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Finally, we will accept or reject our proposed jump with a Metropolis ratio.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar%20=%20%5Cmin%20%5Cleft(1,%20%5Cexp%20%5Cleft(H(p_0,%20%5Ctheta_0)-H%5Cleft(p%5E%7B*%7D,%20%5Ctheta%5E%7B*%7D%5Cright)%5Cright)%5Cright)%0A"></p>
<p>Thus, with probability <img src="https://latex.codecogs.com/png.latex?r">, we set <img src="https://latex.codecogs.com/png.latex?%5Ctheta_1%20=%20%5Ctheta%5E*">. Otherwise, <img src="https://latex.codecogs.com/png.latex?%5Ctheta_1%20=%20%5Ctheta_0">.</p>
<p>We can visualize the incredible efficiency that Hamiltonian Monte Carlo can deliver. In the exact same example as with the donut we just saw with Metropolis, HMC samples much more efficiently.</p>
</section>
</section>
<section id="section" class="level1">
<h1><img src="https://david-salazar.github.io/images/hmc.gif" class="img-fluid"></h1>
<p>Finally, we will accept or reject our proposed jump with a Metropolis-Hastings ratio.</p>
<p>We can visualize the incredible efficiency that Hamiltonian Monte Carlo can deliver. In the exact same example as with the donut we just saw with Metropolis, HMC samples much more efficiently.</p>
<p><img src="https://david-salazar.github.io/images/hmc.GIF" class="img-fluid"></p>
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<blockquote class="blockquote">
<p>abe09e3cd313dfc12630c3063c969959af729a6a</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>High dimensional probability distributions are difficult to understand. Instead of piling much of the probability mass around the mode, the probability mass lies at the intersection of the neighborhoods where there is volume and there is target density. <strong>This area of probability mass is a narrow surface that lies far from the mode and is called the typical set.</strong> Random walk algorithms, like Metropolis, are fundamentally ill-suited to explore such narrow surfaces: their ad-hoc proposal distributions are biased to propose jumps that lie outside the typical set and thus the jumps are rejected. The end result, the chains explore the typical set very inefficiently.</p>
<p>On the other hand, <strong>Hamiltonian Monte Carlo (HMC) algorithms are precisely constructed to exploit the geometry of the typical set</strong> and make intelligent proposals. By borrowing Hamiltonian dynamics from physics, we create proposals that follow a vector-field that is aligned with the typical set. Thus, our proposals will lie on the typical set and will be much more likely to be accepted than the proposals of a random walk like those of Metropolis. Therefore, HMC allows us to efficiently explore the typical set.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-07-02-bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo.html</guid>
  <pubDate>Thu, 02 Jul 2020 05:00:00 GMT</pubDate>
  <media:content url="https://david-salazar.github.io/images/typicalset.PNG" medium="image"/>
</item>
<item>
  <title>Bayesian Data Analysis: Week 5 -&gt; Metropolis</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html</link>
  <description><![CDATA[ 




<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>So far in the course, we have seen how the main obstacle in the way of performing Bayesian Statistics is the computation of the posterior. Thus, we must ask ourselves: if we <em>cannot fully</em> compute the <strong>posterior</strong>, but we <em>can</em> evaluate an unnormalized version, how can we <strong>approximate</strong> the posterior distribution?</p>
<p>In this week, we started analyzing a promising alternative: <strong>Monte-Carlo Markov Chains (MCMC)</strong>. In this blogpost, I’ll give a succinct overview of the most basic MCMC algorithm: the <em>Metropolis Algorithm</em> and quick example of it with some real data.</p>
<section id="a-monte-carlo-markov-chain" class="level2">
<h2 class="anchored" data-anchor-id="a-monte-carlo-markov-chain">A Monte-Carlo Markov Chain</h2>
<p>The <strong>Metropolis</strong> Algorithm is just a random walk through parameter space. At each iteration of the algorithm, say <img src="https://latex.codecogs.com/png.latex?t">, where we are depends only on where we were at <img src="https://latex.codecogs.com/png.latex?t-1">. That is <img src="https://latex.codecogs.com/png.latex?P(%5Ctheta_t%20%7C%20%5Ctheta_%7Bt-1%7D,%20%5Ctheta_%7Bt-2%7D,%20%5Ccdots,%20%5Ctheta_%7B0%7D)%20=%20P(%5Ctheta_t%20%7C%20%5Ctheta_%7Bt-1%7D">. This is the <em>Markov</em> part. At each time step, then, we must define transition distribution: the probability of <img src="https://latex.codecogs.com/png.latex?P(%5Ctheta_t%20%7C%20%5Ctheta_%7Bt-1%7D)">.</p>
<p>The Monte-Carlo part comes because we use these different samples <img src="https://latex.codecogs.com/png.latex?(%5Ctheta_%7Bt-1%7D,%20%5Ctheta_%7Bt-2%7D,%20%5Ccdots,%20%5Ctheta_%7B0%7D)"> to estimate the posterior distribution. We can only do this if at time step <img src="https://latex.codecogs.com/png.latex?T%20%5Cto%20%5Cinfty">, <img src="https://latex.codecogs.com/png.latex?P(%5Ctheta_T)%20=%20P(%5Ctheta_T%20%7C%20y)">. That is, if the stationary distribution (the probability that we are at any given point in time <img src="https://latex.codecogs.com/png.latex?T">) is the target posterior distribution. The <em>challenge</em> then, is <strong>how to engineer each transition distribution such that the stationary distribution is the posterior.</strong> We will check how the Metropolis algorithm solves this problem with a numerical example.</p>
<p>If we can construct such a Markov Chain, then our <em>Monte-Carlo estimate</em>s using these samples will be <strong>asymptotically consistent</strong>. However, two problems arise: first, there’s an <em>auto-correlation</em> in our samples from the Markov chains. Although the Central Limit Theorem still holds, our <strong>effective sample size for our Monte-Carlo estimates will be lower than our number of Markov Chain iterations</strong>. Secondly, we cannot know if we have run the Markov Chain long enough such that our samples are in proportion according to their stationary distribution: that is, <strong>we cannot know if the chains have converged toward the posterior distribution</strong>. We will check both problems with convergence diagnostics once we have worked out a numerical example of the Metropolis algorithm.</p>
</section>
<section id="the-metropolis-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-metropolis-algorithm">The Metropolis Algorithm</h2>
<p>The Metropolis algorithm is thus defined. A random walk through parameter space such that at each iteration of the Markov Chain, our samples are corrected such that they approximate our posterior distribution. In particular, begin at some point <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B0%7D">. Then, we generate a proposed move by direct sampling from a proposal distribution: say a normal centered around <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B0%7D">. The suggestion then is <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*">. We will then decide if we move to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*"> by comparing the ratio of unnormalized posterior distribution densities at <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*"> and <img src="https://latex.codecogs.com/png.latex?%5Ctheta_%7B0%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar%20=%20%5Cdfrac%7Bq(%5Ctheta%5E*%7Cy)%7D%7Bq(%5Ctheta_0%7C%20y)%7D%0A"> Which, given that both are normalized by the same constant in the posterior distribution, is equivalent to comparing the posterior densities at both points:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar%20=%20%5Cdfrac%7Bq(%5Ctheta%5E*%7Cy)%7D%7Bq(%5Ctheta_0%7C%20y)%7D%20=%20%5Cdfrac%7Bq(%5Ctheta%5E*%7Cy)%20/%20%5Cint%20q(%5Ctheta%7Cy)%20d%5Ctheta%7D%7Bq(%5Ctheta_0%7C%20y)/%20%5Cint%20q(%5Ctheta%7Cy)%20d%5Ctheta%7D%20=%20%20%5Cdfrac%7Bp(%5Ctheta%5E*%7Cy)%7D%7Bp(%5Ctheta_0%20%7C%20y)%7D%0A"> Finally, we decide whether to move to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*"> by a Bernoulli trial with probability <img src="https://latex.codecogs.com/png.latex?min(r,%201)">. That is:</p>
<ul>
<li>if the proposed jump increases the posterior (<img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%5E*%7Cy)%20%3E%20p(%5Ctheta_0%7Cy)">), then our Markov Chain moves to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*"> and we set <img src="https://latex.codecogs.com/png.latex?%5Ctheta_t%20=%20%5Ctheta%5E*">.</li>
<li>if the proposed jump decreases the posterior (<img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%5E*%7Cy)%20%3C%20p(%5Ctheta_0%7Cy)">), then our Markov Chain then we may or may not move to <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E*">. The probability that we do move decreases as the decreased density resulting from the jump increases.</li>
</ul>
<p>Therefore:</p>
<blockquote class="blockquote">
<p>The Metropolis algorithm can thus be viewed as a stochastic version of a stepwise mode-finding algorithm, always accepting steps that increase the density until it finds the mode and then only sometimes accepting steps that decrease the posterior density.</p>
</blockquote>
<p>Thus, as long as the algorithm has run long enough to find the posterior mode, <strong>and the area around the mode is a good representation of the overall posterior</strong>, the Metropolis Algorithm will work.</p>
</section>
<section id="an-example-of-the-metropolis-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="an-example-of-the-metropolis-algorithm">An example of the Metropolis Algorithm</h2>
<p>The data come from the excellent Bayesian course <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>. Which is the best statistics course that I’ve ever taken.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">data</span>(<span class="st" style="color: #20794D;">"Howell1"</span>)</span>
<span id="cb1-2">heights <span class="ot" style="color: #003B4F;">&lt;-</span> Howell1</span>
<span id="cb1-3">skimr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">skim</span>(heights)</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">heights</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">544</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">4</td>
</tr>
<tr class="odd">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 15%">
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">height</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">138.26</td>
<td style="text-align: right;">27.60</td>
<td style="text-align: right;">53.98</td>
<td style="text-align: right;">125.10</td>
<td style="text-align: right;">148.59</td>
<td style="text-align: right;">157.48</td>
<td style="text-align: right;">179.07</td>
<td style="text-align: left;">▁▂▂▇▇</td>
</tr>
<tr class="even">
<td style="text-align: left;">weight</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">35.61</td>
<td style="text-align: right;">14.72</td>
<td style="text-align: right;">4.25</td>
<td style="text-align: right;">22.01</td>
<td style="text-align: right;">40.06</td>
<td style="text-align: right;">47.21</td>
<td style="text-align: right;">62.99</td>
<td style="text-align: left;">▃▂▃▇▂</td>
</tr>
<tr class="odd">
<td style="text-align: left;">age</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">29.34</td>
<td style="text-align: right;">20.75</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">12.00</td>
<td style="text-align: right;">27.00</td>
<td style="text-align: right;">43.00</td>
<td style="text-align: right;">88.00</td>
<td style="text-align: left;">▇▆▅▂▁</td>
</tr>
<tr class="even">
<td style="text-align: left;">male</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.47</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: left;">▇▁▁▁▇</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We will therefore model a very simple Gaussian probability model for the height:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aheight_i%20%5Csim%20Normal(%5Cmu,%20%5Csigma)%20%5C%5C%0A%5Cmu%20%5Csim%20Normal(150,%2020)%20%5C%5C%0A%5Csigma%20%5Csim%20Normal(5,%2010)%0A"></p>
<p>Thus, for a given <img src="https://latex.codecogs.com/png.latex?%5Cmu,%20%5Csigma">, the model’s log-likelihood is thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">model_log_likelihood <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(mu, sigma) {</span>
<span id="cb2-2">  <span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">dnorm</span>(heights<span class="sc" style="color: #5E5E5E;">$</span>height, mu, sigma, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>))</span>
<span id="cb2-3">}</span></code></pre></div>
</div>
<p>The unnormalized posterior is thus the log-likelihood plus the prior density log:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">log_unnormalized_posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(mu, sigma) {</span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">dnorm</span>(mu, <span class="dv" style="color: #AD0000;">150</span>, <span class="dv" style="color: #AD0000;">20</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">dnorm</span>(sigma, <span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">10</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">model_log_likelihood</span>(mu, sigma)</span>
<span id="cb3-3">}</span></code></pre></div>
</div>
<p>Therefore, if our proposal algorithm is a normal centered around the past iteration with scale of 5 for <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and scale of 2 for <img src="https://latex.codecogs.com/png.latex?%5Csigma">, the metropolis algorithm can be written thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">density_ratio <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(alpha_propose, alpha_previous, sigma_propose, sigma_previous) {</span>
<span id="cb4-2">  <span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">log_unnormalized_posterior</span>(alpha_propose, sigma_propose) <span class="sc" style="color: #5E5E5E;">-</span>  <span class="fu" style="color: #4758AB;">log_unnormalized_posterior</span>(alpha_previous, sigma_previous))</span>
<span id="cb4-3">}</span>
<span id="cb4-4"></span>
<span id="cb4-5">single_metropolis <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(<span class="at" style="color: #657422;">total_iter =</span> <span class="dv" style="color: #AD0000;">10000</span>) {</span>
<span id="cb4-6">    </span>
<span id="cb4-7">  alpha <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">vector</span>(<span class="at" style="color: #657422;">length =</span> total_iter)</span>
<span id="cb4-8">  sigma <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">vector</span>(<span class="at" style="color: #657422;">length =</span> total_iter)</span>
<span id="cb4-9">  alpha[<span class="dv" style="color: #AD0000;">1</span>] <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">runif</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">min =</span> <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">150</span>) <span class="co" style="color: #5E5E5E;"># initialize the chains at random points</span></span>
<span id="cb4-10">  sigma[<span class="dv" style="color: #AD0000;">1</span>] <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">runif</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">min =</span> <span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">40</span>) <span class="co" style="color: #5E5E5E;"># initialize the chains at random points</span></span>
<span id="cb4-11">  <span class="cf" style="color: #003B4F;">for</span> (i <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">:</span>total_iter) {</span>
<span id="cb4-12">    </span>
<span id="cb4-13">    <span class="co" style="color: #5E5E5E;"># sample proposal</span></span>
<span id="cb4-14">    alpha_propose <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">mean =</span> alpha[i<span class="dv" style="color: #AD0000;">-1</span>], <span class="at" style="color: #657422;">sd =</span> <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb4-15">    sigma_propose <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">mean =</span> sigma[i<span class="dv" style="color: #AD0000;">-1</span>], <span class="at" style="color: #657422;">sd =</span> <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb4-16">    <span class="co" style="color: #5E5E5E;"># compare posterior at past and proposal</span></span>
<span id="cb4-17">    ratio <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">density_ratio</span>(alpha_propose, alpha[i<span class="dv" style="color: #AD0000;">-1</span>], sigma_propose, sigma[i<span class="dv" style="color: #AD0000;">-1</span>])</span>
<span id="cb4-18">    ratio <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">min</span>(<span class="dv" style="color: #AD0000;">1</span>, ratio) </span>
<span id="cb4-19">    <span class="co" style="color: #5E5E5E;"># check whether you move</span></span>
<span id="cb4-20">    bool_move <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbernoulli</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">p =</span> ratio)</span>
<span id="cb4-21">    </span>
<span id="cb4-22">    <span class="cf" style="color: #003B4F;">if</span> (bool_move <span class="sc" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>) {</span>
<span id="cb4-23">      alpha[i] <span class="ot" style="color: #003B4F;">&lt;-</span> alpha_propose</span>
<span id="cb4-24">      sigma[i] <span class="ot" style="color: #003B4F;">&lt;-</span> sigma_propose</span>
<span id="cb4-25">    }</span>
<span id="cb4-26">    </span>
<span id="cb4-27">    <span class="cf" style="color: #003B4F;">else</span>{</span>
<span id="cb4-28">      alpha[i] <span class="ot" style="color: #003B4F;">&lt;-</span> alpha[i<span class="dv" style="color: #AD0000;">-1</span>]</span>
<span id="cb4-29">      sigma[i] <span class="ot" style="color: #003B4F;">&lt;-</span> sigma[i<span class="dv" style="color: #AD0000;">-1</span>]</span>
<span id="cb4-30">    }</span>
<span id="cb4-31">    </span>
<span id="cb4-32">  }</span>
<span id="cb4-33">  <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">alpha =</span> alpha[<span class="dv" style="color: #AD0000;">5001</span><span class="sc" style="color: #5E5E5E;">:</span>total_iter], <span class="at" style="color: #657422;">sigma =</span> sigma[<span class="dv" style="color: #AD0000;">5001</span><span class="sc" style="color: #5E5E5E;">:</span>total_iter])</span>
<span id="cb4-34">}</span></code></pre></div>
</div>
<p>Notice that we do not use all of our iterations. In fact, we discard half of them. The reason? At the beginning of the chain, the probabilities have not converged to that of the stationary distribution. Thus, they are not correct samples from the posterior distribution. This beginning period serves to <em>warm-up</em> the Chains long enough until they find the stationary distribution and start yielding usable samples from the posterior.</p>
<p>To run multiple chains of the Metropolis algorithm:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">multiple_metropolis <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(<span class="at" style="color: #657422;">chains =</span> <span class="dv" style="color: #AD0000;">4</span>) {</span>
<span id="cb5-2">  alpha <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">list</span>()</span>
<span id="cb5-3">  sigma <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">list</span>()</span>
<span id="cb5-4">  <span class="cf" style="color: #003B4F;">for</span> (chain <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>chains) {</span>
<span id="cb5-5">    </span>
<span id="cb5-6">    result_chain <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">single_metropolis</span>()</span>
<span id="cb5-7">    alpha[[chain]] <span class="ot" style="color: #003B4F;">&lt;-</span> result_chain[[<span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb5-8">    sigma[[chain]] <span class="ot" style="color: #003B4F;">&lt;-</span> result_chain[[<span class="dv" style="color: #AD0000;">2</span>]]</span>
<span id="cb5-9">  }</span>
<span id="cb5-10">  <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">alpha =</span> alpha, <span class="at" style="color: #657422;">sigma =</span> sigma)</span>
<span id="cb5-11">}</span>
<span id="cb5-12"></span>
<span id="cb5-13">results <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">multiple_metropolis</span>()</span></code></pre></div>
</div>
<p>Our results, then, can be summarised just as we work with Monte-Carlo samples from other methods. For example, the posterior mean can is thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>alpha) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>, .) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-3">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="sc" style="color: #5E5E5E;">-</span>iter, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"alpha"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>name) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">left_join</span>(<span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>sigma) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-6">    <span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>, .) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-7">    <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="sc" style="color: #5E5E5E;">-</span>iter, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"sigma"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-8">    <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>name)) <span class="ot" style="color: #003B4F;">-&gt;</span> results_plot</span>
<span id="cb6-9"></span>
<span id="cb6-10">results_plot <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-11">  <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>iter) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-12">  <span class="fu" style="color: #4758AB;">summarise_all</span>(mean)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  alpha sigma
  &lt;dbl&gt; &lt;dbl&gt;
1  138.  27.5</code></pre>
</div>
</div>
<p>We can visualize the samples from the posterior:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">results_plot <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb8-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(alpha, sigma)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb8-3">  <span class="fu" style="color: #4758AB;">geom_jitter</span>(<span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.1</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb8-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Samples from the posterior"</span>,</span>
<span id="cb8-5">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Posterior obtained with Metropolis algorithm"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="convergence-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="convergence-diagnostics">Convergence Diagnostics</h2>
<p>If we run multiple chains, we can check whether each chain converged to explore the same areas of the parameter space in the same proportions. If the chains are in not in agreement between each other, then, it’s a sure sign that the chains have yet to converge. We can visualize the path that each chain took through the parameter space with trace plots:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>alpha) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb9-2">  <span class="fu" style="color: #4758AB;">data.frame</span>(., <span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb9-3">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="sc" style="color: #5E5E5E;">-</span>iter, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"chain"</span>, <span class="at" style="color: #657422;">names_prefix =</span> <span class="st" style="color: #20794D;">"X"</span>,</span>
<span id="cb9-4">               <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"alpha"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb9-5">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(iter, alpha, <span class="at" style="color: #657422;">color =</span> chain)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb9-6">  <span class="fu" style="color: #4758AB;">geom_line</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb9-7">  <span class="fu" style="color: #4758AB;">scale_color_brewer</span>(<span class="at" style="color: #657422;">palette =</span> <span class="st" style="color: #20794D;">"Set2"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb9-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Trace plot for alpha"</span>,</span>
<span id="cb9-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"The chains have converged to explore the same areas"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis_files/figure-html/alpha-trace-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>sigma) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-2">  <span class="fu" style="color: #4758AB;">data.frame</span>(., <span class="at" style="color: #657422;">iter =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-3">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="sc" style="color: #5E5E5E;">-</span>iter, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"chain"</span>, <span class="at" style="color: #657422;">names_prefix =</span> <span class="st" style="color: #20794D;">"X"</span>,</span>
<span id="cb10-4">               <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"sigma"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-5">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(iter, sigma, <span class="at" style="color: #657422;">color =</span> chain)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb10-6">  <span class="fu" style="color: #4758AB;">geom_line</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb10-7">  <span class="fu" style="color: #4758AB;">scale_color_brewer</span>(<span class="at" style="color: #657422;">palette =</span> <span class="st" style="color: #20794D;">"Set2"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb10-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Trace plot for sigma"</span>,</span>
<span id="cb10-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"The chains have converged to explore the same areas"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>A numerical convergence-diagnostic is <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R">. It measures agreement between the chains by comparing the within chain variance <img src="https://latex.codecogs.com/png.latex?W"> with the estimated variance using all of the available data <img src="https://latex.codecogs.com/png.latex?var(%5Ctheta%20%7C%20y)">. If all of the chains have converged, <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?var(%5Ctheta%20%7C%20y)"> should be equal. Thus, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R">, which is the squared root of their ratio should be 1:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%20R%20=%20%5Csqrt%7B%5Cdfrac%7BVar(%5Ctheta%20%7C%20y)%7D%7BW%7D%7D%0A"> However, if the chains are in disagreement between each other because they have not converged, they will underestimate the total variance <img src="https://latex.codecogs.com/png.latex?Var(%5Ctheta%20%7C%20y)">. Why? Because they have yet to explore the full posterior scale. Thus, <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R"> will be larger than 1. As the chains converge (as the number of iterations grows), we expect <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%20R"> to converge to 1 from above.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">iterations <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">20</span>, <span class="dv" style="color: #AD0000;">50</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">5000</span>)</span>
<span id="cb11-2"><span class="fu" style="color: #4758AB;">names</span>(iterations) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">20</span>, <span class="dv" style="color: #AD0000;">50</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">5000</span>)</span>
<span id="cb11-3"><span class="fu" style="color: #4758AB;">map_df</span>(iterations, <span class="sc" style="color: #5E5E5E;">~</span> rstan<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">Rhat</span>(<span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>alpha)[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>.x])) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb11-4">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="fu" style="color: #4758AB;">everything</span>(), <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"iterations"</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"Rhat"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb11-5">  gt<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">gt</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb11-6">  gt<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">fmt_number</span>(<span class="fu" style="color: #4758AB;">vars</span>(Rhat))</span></code></pre></div>
<div class="cell-output-display">

<div id="qrzyiplwex" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

:where(#qrzyiplwex) .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

:where(#qrzyiplwex) .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

:where(#qrzyiplwex) .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

:where(#qrzyiplwex) .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

:where(#qrzyiplwex) .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

:where(#qrzyiplwex) .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

:where(#qrzyiplwex) .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

:where(#qrzyiplwex) .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

:where(#qrzyiplwex) .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

:where(#qrzyiplwex) .gt_from_md > :first-child {
  margin-top: 0;
}

:where(#qrzyiplwex) .gt_from_md > :last-child {
  margin-bottom: 0;
}

:where(#qrzyiplwex) .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

:where(#qrzyiplwex) .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#qrzyiplwex) .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

:where(#qrzyiplwex) .gt_row_group_first td {
  border-top-width: 2px;
}

:where(#qrzyiplwex) .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#qrzyiplwex) .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_first_summary_row.thick {
  border-top-width: 2px;
}

:where(#qrzyiplwex) .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#qrzyiplwex) .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

:where(#qrzyiplwex) .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#qrzyiplwex) .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#qrzyiplwex) .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#qrzyiplwex) .gt_left {
  text-align: left;
}

:where(#qrzyiplwex) .gt_center {
  text-align: center;
}

:where(#qrzyiplwex) .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

:where(#qrzyiplwex) .gt_font_normal {
  font-weight: normal;
}

:where(#qrzyiplwex) .gt_font_bold {
  font-weight: bold;
}

:where(#qrzyiplwex) .gt_font_italic {
  font-style: italic;
}

:where(#qrzyiplwex) .gt_super {
  font-size: 65%;
}

:where(#qrzyiplwex) .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

:where(#qrzyiplwex) .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

:where(#qrzyiplwex) .gt_indent_1 {
  text-indent: 5px;
}

:where(#qrzyiplwex) .gt_indent_2 {
  text-indent: 10px;
}

:where(#qrzyiplwex) .gt_indent_3 {
  text-indent: 15px;
}

:where(#qrzyiplwex) .gt_indent_4 {
  text-indent: 20px;
}

:where(#qrzyiplwex) .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">iterations</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">Rhat</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">20</td>
<td class="gt_row gt_right">1.82</td></tr>
    <tr><td class="gt_row gt_right">50</td>
<td class="gt_row gt_right">1.16</td></tr>
    <tr><td class="gt_row gt_right">100</td>
<td class="gt_row gt_right">1.07</td></tr>
    <tr><td class="gt_row gt_right">5000</td>
<td class="gt_row gt_right">1.01</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
<p>Now, for sigma:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">iterations <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">20</span>, <span class="dv" style="color: #AD0000;">50</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">5000</span>)</span>
<span id="cb12-2"><span class="fu" style="color: #4758AB;">names</span>(iterations) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">20</span>, <span class="dv" style="color: #AD0000;">50</span>, <span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">5000</span>)</span>
<span id="cb12-3"><span class="fu" style="color: #4758AB;">map_df</span>(iterations, <span class="sc" style="color: #5E5E5E;">~</span> rstan<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">Rhat</span>(<span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>sigma)[<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>.x])) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-4">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="fu" style="color: #4758AB;">everything</span>(), <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"iterations"</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"Rhat"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-5">  gt<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">gt</span>() <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-6">  gt<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">fmt_number</span>(<span class="fu" style="color: #4758AB;">vars</span>(Rhat))</span></code></pre></div>
<div class="cell-output-display">

<div id="xcfbqrllmp" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

:where(#xcfbqrllmp) .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

:where(#xcfbqrllmp) .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

:where(#xcfbqrllmp) .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

:where(#xcfbqrllmp) .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

:where(#xcfbqrllmp) .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

:where(#xcfbqrllmp) .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

:where(#xcfbqrllmp) .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

:where(#xcfbqrllmp) .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

:where(#xcfbqrllmp) .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

:where(#xcfbqrllmp) .gt_from_md > :first-child {
  margin-top: 0;
}

:where(#xcfbqrllmp) .gt_from_md > :last-child {
  margin-bottom: 0;
}

:where(#xcfbqrllmp) .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

:where(#xcfbqrllmp) .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#xcfbqrllmp) .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

:where(#xcfbqrllmp) .gt_row_group_first td {
  border-top-width: 2px;
}

:where(#xcfbqrllmp) .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#xcfbqrllmp) .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_first_summary_row.thick {
  border-top-width: 2px;
}

:where(#xcfbqrllmp) .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#xcfbqrllmp) .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

:where(#xcfbqrllmp) .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#xcfbqrllmp) .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#xcfbqrllmp) .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#xcfbqrllmp) .gt_left {
  text-align: left;
}

:where(#xcfbqrllmp) .gt_center {
  text-align: center;
}

:where(#xcfbqrllmp) .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

:where(#xcfbqrllmp) .gt_font_normal {
  font-weight: normal;
}

:where(#xcfbqrllmp) .gt_font_bold {
  font-weight: bold;
}

:where(#xcfbqrllmp) .gt_font_italic {
  font-style: italic;
}

:where(#xcfbqrllmp) .gt_super {
  font-size: 65%;
}

:where(#xcfbqrllmp) .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

:where(#xcfbqrllmp) .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

:where(#xcfbqrllmp) .gt_indent_1 {
  text-indent: 5px;
}

:where(#xcfbqrllmp) .gt_indent_2 {
  text-indent: 10px;
}

:where(#xcfbqrllmp) .gt_indent_3 {
  text-indent: 15px;
}

:where(#xcfbqrllmp) .gt_indent_4 {
  text-indent: 20px;
}

:where(#xcfbqrllmp) .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">iterations</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col">Rhat</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_right">20</td>
<td class="gt_row gt_right">1.77</td></tr>
    <tr><td class="gt_row gt_right">50</td>
<td class="gt_row gt_right">1.04</td></tr>
    <tr><td class="gt_row gt_right">100</td>
<td class="gt_row gt_right">1.22</td></tr>
    <tr><td class="gt_row gt_right">5000</td>
<td class="gt_row gt_right">1.01</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
</section>
<section id="estimating-the-effective-samples-sizes" class="level2">
<h2 class="anchored" data-anchor-id="estimating-the-effective-samples-sizes">Estimating the effective samples sizes</h2>
<p>As we said before, the sample size is not equal to the number of iterations times the number of chains. There’s an autocorrelation between the samples that we must take into account to find out how many equivalent independent samples from the posterior our iterations represent. To do so, we correct the number of total iterations by the sum of all autocorrelation lags <img src="https://latex.codecogs.com/png.latex?%5Crho_t">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As_%7Beff%7D%20=%20%5Cdfrac%7Biterations%20*%20chains%7D%7B1%20+%202%5Csum%5E%7B%5Cinfty%7D%20%5Crho_t%7D%0A"></p>
<p>Which can be estimated for alpha thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">rstan<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">ess_bulk</span>(<span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>alpha))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1469.648</code></pre>
</div>
</div>
<p>We ran 5,000 iterations. Yet, we only have an effective sample size much smaller. Finally, for sigma:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1">rstan<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">ess_bulk</span>(<span class="fu" style="color: #4758AB;">do.call</span>(cbind, results<span class="sc" style="color: #5E5E5E;">$</span>sigma))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1223.447</code></pre>
</div>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-29-bayesian-data-analysis-week-5-metropolis.html</guid>
  <pubDate>Mon, 29 Jun 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Data Analysis: Week 4 -&gt; Importance Sampling</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html</link>
  <description><![CDATA[ 




<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>In this blogpost, I’ll go over one of the main topics of Week 4: Importance Sampling; I’ll also solve a couple of the exercises for Chapter 10 of the book. Week 4 was all about preparing the way for MCMC: if we <em>cannot fully</em> compute the <strong>posterior</strong>, but we <em>can</em> evaluate an unnormalized version, how can we <strong>approximate</strong> the posterior distribution?</p>
<section id="importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="importance-sampling">Importance Sampling</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?q(%5Ctheta%7Cy)"> be the unnormalized posterior density that we can compute at some values for theta. Then, the posterior expectation is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE(%5Ctheta%7Cy)%20=%20%5Cdfrac%7B%5Cint%20%5Ctheta%20q(%5Ctheta%7Cy)%20d%5Ctheta%7D%7B%5Cint%20q(%5Ctheta%7Cy)%20d%5Ctheta%7D%0A"> However, the denominator is often intractable. Importance sampling, then, reduces to consider another density <img src="https://latex.codecogs.com/png.latex?g(%5Ctheta)"> from which we can draw direct samples. Then, if we multiply and divide by <img src="https://latex.codecogs.com/png.latex?g(%5Ctheta)"> in both numerator and denominator:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cint%5B%5Ctheta%20q(%5Ctheta%20%5Cmid%20y)%20/%20g(%5Ctheta)%5D%20g(%5Ctheta)%20d%20%5Ctheta%7D%7B%5Cint%5Bq(%5Ctheta%20%5Cmid%20y)%20/%20g(%5Ctheta)%5D%20g(%5Ctheta)%20d%20%5Ctheta%7D%0A"> Which can be considered expectations with respect to <img src="https://latex.codecogs.com/png.latex?g(%5Ctheta)">. Therefore, we can estimate both numerator and denominator by direct sampling from <img src="https://latex.codecogs.com/png.latex?g(%5Ctheta)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cfrac%7B1%7D%7BS%7D%20%5Csum_%7Bs=1%7D%5E%7BS%7D%20%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)%20w%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)%7D%7B%5Cfrac%7B1%7D%7BS%7D%20%5Csum_%7Bs=1%7D%5E%7BS%7D%20w%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?w%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)"> are called the <strong>importance weights</strong> and are defined thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aw%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)=%5Cfrac%7Bq%5Cleft(%5Ctheta%5E%7Bs%7D%20%5Cmid%20y%5Cright)%7D%7Bg%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)%7D%0A"></p>
<p>Therefore, importance sampling is <em>sampling</em> from an <strong>approximation to the posterior</strong> and then <em>correcting</em> the importance that each sample has in the computation of a specific expectation. We correct by the ratio of the unnormalized posterior density to the density of the approximation.</p>
<p>Importance sampling will give precise estimates of the expectation if the weights are roughly uniform. However, if the importance weights vary substantially, the method will yield unsatisfactory estimates. Indeed:</p>
<blockquote class="blockquote">
<p>The worst possible scenario occurs when the importance ratios are small with high probability but with a low probability are huge, which happens, for example, if ( q ) has wide tails compared to ( g, ) as a function of ( )</p>
</blockquote>
</section>
<section id="putting-it-into-practice" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-into-practice">Putting it into practice</h2>
<p>Here I’ll solve exercise 6 and exercise 7</p>
<section id="exercise-6" class="level3">
<h3 class="anchored" data-anchor-id="exercise-6">Exercise 6</h3>
<p>Importance sampling when the importance weights are well behaved: consider a univariate posterior distribution, ( p(y), ) which we wish to approximate and then calculate moments of, using importance sampling from an unnormalized density, ( g() ). Suppose the posterior distribution is normal, and the approximation is ( t_{3} ) with mode and curvature matched to the posterior density.</p>
<p>The ( t_{3} ) has a variance of 3. Therefore, we sample from a normal with standard deviation of 3.</p>
<blockquote class="blockquote">
<ol type="a">
<li>Draw a sample of size ( S=100 ) from the approximate density and compute the importance ratios. Plot a histogram of the log importance ratios.</li>
</ol>
</blockquote>
<p>First, we draw samples from the approximate density ( t_{3} )</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">approximate_samples <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rt</span>(<span class="dv" style="color: #AD0000;">100</span>, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>)</span></code></pre></div>
</div>
<p>Then, we compute the density <img src="https://latex.codecogs.com/png.latex?g(%5Ctheta)"> at these points:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">approximate_density <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dt</span>(approximate_samples, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span></code></pre></div>
</div>
<p>Finally, we compute the density at the sampled points:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">unnormalized_posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dnorm</span>(approximate_samples, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span></code></pre></div>
</div>
<p>The log importance weights are then:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">log_imp_weights <span class="ot" style="color: #003B4F;">=</span> unnormalized_posterior <span class="sc" style="color: #5E5E5E;">-</span> approximate_density</span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;">data.frame</span>(log_imp_weights) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-3">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Importance weights"</span>,</span>
<span id="cb4-6">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Approximating a normal with a t distribution"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>Estimate ( (y) ) and ( (y) ) using importance sampling. Compare to the true values.</li>
</ol>
</blockquote>
<p>First, we exponentiate and normalize the weights:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">norm_weights <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))</span></code></pre></div>
</div>
<p>Finally, we compute ( (y) ) simply as the sum of the product of the samples and these weights:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">mean_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">*</span>norm_weights)</span>
<span id="cb6-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Mean estimate: {round(mean_estimate, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean estimate: -0.12</code></pre>
</div>
</div>
<p>And ( (y) ):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">variance_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>norm_weights) <span class="sc" style="color: #5E5E5E;">+</span> mean_estimate<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb8-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Variance estimate: {round(variance_estimate, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance estimate: 3.41</code></pre>
</div>
</div>
<p>Which makes for a Monte-Carlo standard error for the mean of:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">variance_estimate<span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">100</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.03412438</code></pre>
</div>
</div>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>Repeat (a) and (b) for ( S=10,000 )</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">approximate_samples <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rt</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>)</span>
<span id="cb12-2">approximate_density <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dt</span>(approximate_samples, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>)</span>
<span id="cb12-3">unnormalized_posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dnorm</span>(approximate_samples, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb12-4"></span>
<span id="cb12-5">log_imp_weights <span class="ot" style="color: #003B4F;">=</span> unnormalized_posterior <span class="sc" style="color: #5E5E5E;">-</span> approximate_density</span>
<span id="cb12-6"><span class="fu" style="color: #4758AB;">data.frame</span>(log_imp_weights) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb12-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-8">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb12-9">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Importance weights"</span>,</span>
<span id="cb12-10">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Approximating a normal with a t distribution"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Finally, the computations:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1">norm_weights <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))</span>
<span id="cb13-2">mean_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">*</span>norm_weights)</span>
<span id="cb13-3">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Mean estimate: {round(mean_estimate, 3)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean estimate: 0.031</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1">variance_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>norm_weights) <span class="sc" style="color: #5E5E5E;">+</span> mean_estimate<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb15-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Variance estimate: {round(variance_estimate, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance estimate: 2.88</code></pre>
</div>
</div>
<p>Which makes for a Monte-Carlo standard error for the mean of:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1">variance_estimate<span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">10000</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0002883421</code></pre>
</div>
</div>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>Using the sample obtained in (c), compute an estimate of effective sample size using (10.4) on page 266</li>
</ol>
</blockquote>
<p>The effective sample size is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS_%7B%5Cmathrm%7Beff%7D%7D=%5Cfrac%7B1%7D%7B%5Csum_%7Bs=1%7D%5E%7BS%7D%5Cleft(%5Ctilde%7Bw%7D%5Cleft(%5Ctheta%5E%7Bs%7D%5Cright)%5Cright)%5E%7B2%7D%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctilde%20w"> are the normalized weights.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1">s_eff <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(norm_weights<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb19-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Effective sample size: {round(s_eff,0)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Effective sample size: 8246</code></pre>
</div>
</div>
<p>Given that most of the weights are very small, we can have a reasonably efficient approximation of the posterior using importance sampling</p>
</section>
<section id="exercise-7" class="level3">
<h3 class="anchored" data-anchor-id="exercise-7">Exercise 7</h3>
<p>Importance sampling when the importance weights are too variable: repeat the previous exercise, but with a ( t_{3} ) posterior distribution and a normal approximation. Explain why the estimates of ( (y) ) are systematically too low.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1">approximate_samples <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">100</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb21-2">approximate_density <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dnorm</span>(approximate_samples,  <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb21-3">unnormalized_posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dt</span>(approximate_samples, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) </span>
<span id="cb21-4"></span>
<span id="cb21-5">log_imp_weights <span class="ot" style="color: #003B4F;">=</span> unnormalized_posterior <span class="sc" style="color: #5E5E5E;">-</span> approximate_density</span>
<span id="cb21-6"><span class="fu" style="color: #4758AB;">data.frame</span>(log_imp_weights) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb21-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb21-8">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb21-9">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Importance weights"</span>,</span>
<span id="cb21-10">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Approximating a t-distribution with a normal"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1">norm_weights <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))</span>
<span id="cb22-2">mean_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">*</span>norm_weights)</span>
<span id="cb22-3">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Mean estimate: {round(mean_estimate, 3)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean estimate: -0.115</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1">variance_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>norm_weights) <span class="sc" style="color: #5E5E5E;">+</span> mean_estimate<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb24-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Variance estimate: {round(variance_estimate, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance estimate: 2.64</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1">s_eff <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(norm_weights<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb26-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Effective sample size: {round(s_eff,0)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Effective sample size: 86</code></pre>
</div>
</div>
<p>Even if we increase the sample size, the problems still remain:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1">approximate_samples <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rnorm</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb28-2">approximate_density <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dnorm</span>(approximate_samples,  <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>, <span class="at" style="color: #657422;">sd =</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb28-3">unnormalized_posterior <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">dtnew</span>(approximate_samples, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">3</span>, <span class="at" style="color: #657422;">log =</span> <span class="cn" style="color: #8f5902;">TRUE</span>) </span>
<span id="cb28-4"></span>
<span id="cb28-5">log_imp_weights <span class="ot" style="color: #003B4F;">=</span> unnormalized_posterior <span class="sc" style="color: #5E5E5E;">-</span> approximate_density</span>
<span id="cb28-6"><span class="fu" style="color: #4758AB;">data.frame</span>(log_imp_weights) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb28-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb28-8">  <span class="fu" style="color: #4758AB;">geom_density</span>(<span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb28-9">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Importance weights"</span>,</span>
<span id="cb28-10">      <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Approximating a t-distribution with a normal"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1">norm_weights <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">exp</span>(log_imp_weights))</span>
<span id="cb29-2">mean_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">*</span>norm_weights)</span>
<span id="cb29-3">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Mean estimate: {round(mean_estimate, 3)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean estimate: 0.03</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1">variance_estimate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(approximate_samples<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>norm_weights) <span class="sc" style="color: #5E5E5E;">+</span> mean_estimate<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb31-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"Variance estimate: {round(variance_estimate, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance estimate: 2.11</code></pre>
</div>
</div>
<p>Now, the posterior will have fatter tails than the normal approximation. Therefore, we will have to correct our normal samples with large importance weights, highlighting the inadequacy of our samples generated from the normal to approximate the tails of the posterior. Thus, we cannot adequately account for the tail effect in neither the mean nor the variance. However, the effect is worsened for the variance, as the L2 norm amplifies the consequences of inadequately sampling from the tail.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-27-bayesian-data-analysis-week-4-importance-sampling.html</guid>
  <pubDate>Sat, 27 Jun 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Gini Index under Fat-Tails</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html</link>
  <description><![CDATA[ 




<p>I have recently been exploring Nassim Taleb’s latest <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">technical book: Statistical Consequences of Fat Tails</a>. In this blogpost, I’ll follow Taleb’s exposition of the Gini Index under fat-tails in Chapter 13 of his book.</p>
<p>Intuitively, if we use the “empirical distribution” to estimate the Gini Index, under fat-tails, we underestimate the tail of the distribution and thus underestimate the Gini index. This is <a href="https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/">yet another example</a> of how we <em>fool</em> ourselves when we are using the “empirical” distribution. Instead, Taleb recommends first understanding the tail behavior of the “Gini Index” <a href="https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/">by estimating the tail index of the distribution with Maxmimum Likelihood</a> and <em>then</em> using the <strong>functional form</strong> of the maximum likelihood estimator for the Gini Index.</p>
<section id="what-is-the-gini-index" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-gini-index">What is the Gini Index?</h2>
<p>The Gini index is a measure of concentration commonly used in the income and wealth inequalities discussions. The stochastic representation of the Gini <img src="https://latex.codecogs.com/png.latex?g"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ag=%5Cfrac%7B1%7D%7B2%7D%20%5Cfrac%7B%5Cmathbb%7BE%7D%5Cleft(%5Cleft%7CX%5E%7B%5Cprime%7D-X%5E%7B%5Cprime%20%5Cprime%7D%5Cright%7C%5Cright)%7D%7B%5Cmu%7D%20%5Cin%5B0,1%5D%0A"></p>
<p>where ( X^{} ) and ( X^{} ) are i.i.d. copies of a random variable ( X ) with c.d.f. ( F(x) (2020-06-26-gini-index-under-fat-tails_files/figure-html/lognormal-1.png){width=768} ::: :::</p>
<section id="fat-tails-non-parametric-estimator" class="level3">
<h3 class="anchored" data-anchor-id="fat-tails-non-parametric-estimator">Fat-tails: Non-parametric estimator</h3>
<p>However, when the data-generating process of <img src="https://latex.codecogs.com/png.latex?X"> is in <a href="https://david-salazar.github.io/2020/06/10/fisher-tippet-th-a-clt-for-the-sample-maxima/">the MDA of the Fréchet</a> (i.e., it’s a fat-tailed variable), the non-parametric estimator of the Gini Index loses its properties of normality. Indeed, the limiting distribution of the non-parametric index becomes a skewed-to-the-right <img src="https://latex.codecogs.com/png.latex?%5Calpha">-stable law. Thus, the non-parametric estimate underestimates the true Gini Index.</p>
<p>This can be checked with Monte-Carlo simulations. I’ll perform <img src="https://latex.codecogs.com/png.latex?10%5E4"> Monte-Carlo experiments: in each of them, I’ll generate a 1000 samples from a Pareto with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201.16">. Then, I’ll calculate the Gini estimate using the non-parametric estimator.</p>
<p>Given this Pareto, the “true” Gini is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ag%20=%20%5Cdfrac%7B1%7D%7B2%5Calpha%20-1%7D%20=%200.7575758%0A"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">rpareto <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(n) {</span>
<span id="cb1-2">    alpha <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">1.16</span></span>
<span id="cb1-3">   (<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">runif</span>(n)<span class="sc" style="color: #5E5E5E;">^</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>alpha)) <span class="co" style="color: #5E5E5E;"># inverse transform sampling</span></span>
<span id="cb1-4">}</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;">crossing</span>(<span class="at" style="color: #657422;">experiment =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">10</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">4</span>,</span>
<span id="cb1-7">         <span class="at" style="color: #657422;">sample_size =</span> <span class="dv" style="color: #AD0000;">1000</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">map</span>(sample_size, <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">rpareto</span>(.)),</span>
<span id="cb1-9">         <span class="at" style="color: #657422;">gini =</span> <span class="fu" style="color: #4758AB;">map_dbl</span>(data, <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">gini</span>(.))) <span class="ot" style="color: #003B4F;">-&gt;</span> gini_pareto</span>
<span id="cb1-10"></span>
<span id="cb1-11">gini_pareto <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-12">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(gini)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-13">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-14">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="fl" style="color: #AD0000;">0.7575758</span>), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-15">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"text"</span>, <span class="at" style="color: #657422;">x =</span> <span class="fl" style="color: #AD0000;">0.78</span>, <span class="at" style="color: #657422;">y =</span> <span class="dv" style="color: #AD0000;">400</span>, <span class="at" style="color: #657422;">label =</span> <span class="st" style="color: #20794D;">"True gini"</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, </span>
<span id="cb1-16">           <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">theme_get</span>()<span class="sc" style="color: #5E5E5E;">$</span>text[[<span class="st" style="color: #20794D;">"family"</span>]]) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-17">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Non parametric Gini estimator"</span>,</span>
<span id="cb1-18">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Under fat-tails, non parametric estimator is skewed to the right. Thus, downward bias"</span>,</span>
<span id="cb1-19">       <span class="at" style="color: #657422;">caption =</span> <span class="st" style="color: #20794D;">"Data generating process is Pareto with alpha = 1.16"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails_files/figure-html/pareto-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>Therefore, under fat-tails, <strong>the non-parametric Gini estimator will approach its true value more slowly, and from below.</strong></p>
</section>
</section>
<section id="the-maximum-likelihood-alternative" class="level2">
<h2 class="anchored" data-anchor-id="the-maximum-likelihood-alternative">The Maximum Likelihood alternative</h2>
<p>A better alternative when working with fat-tails, it’s to first estimate the tail and then derive your quantity of interest. Indeed, one does not need too much data to derive the properties of the tail. With a Pareto, for example, the Maximum Likelihood estimator for the tail exponent follows an inverse Gamma distribution that rapidly converges to a Gaussian tightly <em>around</em> the true <img src="https://latex.codecogs.com/png.latex?%5Calpha">. Therefore, one can reliably estimate the tail exponent of the Pareto and thus understand the properties of the distribution with relatively few data.</p>
<p>The ML estimator for the tail exponent of a Pareto is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%20%5Calpha%20=%20%5Cfrac%7Bn%7D%7B%5Csum%20_i%20%20%5Cln%20(x_i)%20%7D%0A"> Then, we can derive our Maximum Likelihood estimate for the Gini Index:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ag%20=%20%5Cdfrac%7B1%7D%7B2%5Cwidehat%20%5Calpha%20-1%7D%0A"></p>
<p>Indeed, Taleb shows that this estimator for the Gini Index is <strong>not just asymptotically normal, but also asymptotically efficient</strong>. We can test for these using our Monte-Carlo simulations. For each of our simulated datasets, we can derive our Maximum Likelihood estimate and then derive our Maximum Likelihood estimate for the Gini Index.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">estimate_alpha_ml <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(observations) {</span>
<span id="cb2-2">  alpha <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">length</span>(observations)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(<span class="fu" style="color: #4758AB;">log</span>(observations))</span>
<span id="cb2-3">  <span class="cf" style="color: #003B4F;">if</span> (alpha <span class="sc" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">1</span>) {</span>
<span id="cb2-4">    alpha <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">1.0005</span> </span>
<span id="cb2-5">  }</span>
<span id="cb2-6">  alpha</span>
<span id="cb2-7">}</span>
<span id="cb2-8"></span>
<span id="cb2-9">gini_pareto <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-10">  <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">alpha_ml =</span> <span class="fu" style="color: #4758AB;">map_dbl</span>(data, <span class="sc" style="color: #5E5E5E;">~</span> <span class="fu" style="color: #4758AB;">estimate_alpha_ml</span>(.)),</span>
<span id="cb2-11">         <span class="at" style="color: #657422;">gini_ml =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>alpha_ml <span class="sc" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>)) <span class="ot" style="color: #003B4F;">-&gt;</span> gini_pareto</span>
<span id="cb2-12"></span>
<span id="cb2-13">gini_pareto <span class="sc" style="color: #5E5E5E;">%&gt;%</span></span>
<span id="cb2-14">  <span class="fu" style="color: #4758AB;">rename</span>(<span class="at" style="color: #657422;">nonparametric =</span> gini,</span>
<span id="cb2-15">         <span class="at" style="color: #657422;">maximum_likelihood =</span> gini_ml) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-16">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="fu" style="color: #4758AB;">c</span>(nonparametric, maximum_likelihood), <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">"estimator"</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">"gini"</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-17">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(gini, <span class="at" style="color: #657422;">fill =</span> estimator)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-18">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>,</span>
<span id="cb2-19">                 <span class="at" style="color: #657422;">position =</span> <span class="st" style="color: #20794D;">"identity"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-20">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="fl" style="color: #AD0000;">0.7575758</span>), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-21">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"text"</span>, <span class="at" style="color: #657422;">x =</span> <span class="fl" style="color: #AD0000;">0.78</span>, <span class="at" style="color: #657422;">y =</span> <span class="dv" style="color: #AD0000;">1000</span>, <span class="at" style="color: #657422;">label =</span> <span class="st" style="color: #20794D;">"True gini"</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, </span>
<span id="cb2-22">           <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">theme_get</span>()<span class="sc" style="color: #5E5E5E;">$</span>text[[<span class="st" style="color: #20794D;">"family"</span>]]) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-23">  <span class="fu" style="color: #4758AB;">scale_fill_viridis_d</span>() <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-24">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position =</span> <span class="st" style="color: #20794D;">"bottom"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb2-25">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Comparison of Gini estimators: Non-parametric vs Maximum Likelihood"</span>,</span>
<span id="cb2-26">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Under fat-tails, unlike the non parametric estimator, Max Likelihood estimate is still asymptotically normal"</span>,</span>
<span id="cb2-27">       <span class="at" style="color: #657422;">caption =</span> <span class="st" style="color: #20794D;">"Data generating process is a Pareto with alpha = 1.16"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails_files/figure-html/comparison-1.png" class="img-fluid" width="960"></p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>When the underlying distribution is fat-tailed, which is always in the case of income or wealth, the non-parametric estimator for the Gini index is skewed to the right and thus underestimates the true Gini index. In this case, it is a much statistically sound strategy to first estimate the tail behavior of the distribution with Maximum Likelihood and then estimate the Gini Index with its plug-in estimator.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/fat-vs-thin-tails/2020-06-26-gini-index-under-fat-tails.html</guid>
  <pubDate>Fri, 26 Jun 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Data Analysis: Week 3-&gt; Exercises</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html</link>
  <description><![CDATA[ 




<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>In this blogpost, I’ll go over a couple of the selected exercises for week 3: exercise number 2 and exercise number 3.</p>
<section id="exercise-2" class="level2">
<h2 class="anchored" data-anchor-id="exercise-2">Exercise 2</h2>
<p>Comparison of two multinomial observations: on September ( 25,1988, ) the evening of a presidential campaign debate, ABC News conducted a survey of registered voters in the United States; 639 persons were polled before the debate, and 639 different persons were polled after. Assume the surveys are independent simple random samples from the population of registered voters. Model the data with two different multinomial distributions. For ( t=1,2, ) let ( _{t} ) be the proportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey ( t . ) <strong>Plot a histogram of the posterior density for ( <em>{2}-</em>{1} . ) What is the posterior probability that there was a shift toward Bush?</strong></p>
<p>The results of the surveys are thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Bc%7Cccc%7Cc%7D%0A%5Ctext%20%7B%20Survey%20%7D%20&amp;%20%5Ctext%20%7B%20Bush%20%7D%20&amp;%20%5Ctext%20%7B%20Dukakis%20%7D%20&amp;%20%5Ctext%20%7B%20No%20opinion/other%20%7D%20&amp;%20%5Ctext%20%7B%20Total%20%7D%20%5C%5C%0A%5Chline%20%5Ctext%20%7B%20pre-debate%20%7D%20&amp;%20294%20&amp;%20307%20&amp;%2038%20&amp;%20639%20%5C%5C%0A%5Ctext%20%7B%20post-debate%20%7D%20&amp;%20288%20&amp;%20332%20&amp;%2019%20&amp;%20639%0A%5Cend%7Barray%7D%0A"></p>
<section id="solution" class="level3">
<h3 class="anchored" data-anchor-id="solution">Solution</h3>
<p>Therefore, for the pre-debate we posit a multinomial model. A multinomial model is nothing more than the extension of the binomial model to more than 2 categories. Here we have 3: Bush, Dukakis and other. For both models, <strong>we assume that the 639 observations are independent and exchangeable</strong>. The likelihood for each survey is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(y%20%5Cmid%20%5Ctheta)%20%5Cpropto%20%5Cprod_%7Bj=1%7D%5E%7Bk%7D%20%5Ctheta_%7Bj%7D%5E%7By_%7Bj%7D%7D%0A"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j"> is the probability of choosing the <img src="https://latex.codecogs.com/png.latex?j"> option. The conjugate prior for the distribution is a multivariate generalization of the beta distribution known as Dirichlet:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(%5Ctheta%20%5Cmid%20%5Cbeta)%20%5Cpropto%20%5Cprod_%7Bj=1%7D%5E%7Bk%7D%20%5Ctheta_%7Bj%7D%5E%7B%5Cbeta_%7Bj%7D-1%7D%0A"></p>
<p>If we set all <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%201">, we get an uniform distribution on the possible distributions for the <img src="https://latex.codecogs.com/png.latex?%5Ctheta">’s. That is, <strong>just as the beta distribution, the Dirichlet distribution is a distribution of distributions.</strong></p>
<p>The resulting posterior distribution for the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">’s is a Dirichlet with parameters <img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20+%20y_j">. The question, then, is how to go from the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">, the proportion that favors the option <img src="https://latex.codecogs.com/png.latex?j">, to the requested <img src="https://latex.codecogs.com/png.latex?%5Calpha_t">:</p>
<blockquote class="blockquote">
<p>Proportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey t.</p>
</blockquote>
<p>Note that given the inherent restriction on the Dirichlet, we can rewrite the distribution of the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">’s as <img src="https://latex.codecogs.com/png.latex?(%5Ctheta_1,%20%5Ctheta_2,%201%20-%20%5Ctheta_1%20-%20%5Ctheta_2)">. We can then perform a change of variables: <img src="https://latex.codecogs.com/png.latex?(%5Calpha,%20%5Cgamma)%20=%20(%5Cdfrac%7B%5Ctheta_1%7D%7B%5Ctheta_1%20+%20%5Ctheta_2%7D,%20%5Ctheta_1%20+%20%5Ctheta_2)">. Which it can be shown that <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is then distributed thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha%20%7C%20y%20%5Csim%20Beta(y_1%20+%20%5Cbeta_1,%20y_2%20+%20%5Cbeta_2)%0A"> ### Pre-Debate</p>
<p>Therefore, setting an uniform prior (<img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%201%20%5C%20%5Cforall%20j">) on the possible distribution of the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">’s, the posterior distribution is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Ctheta_%7Bbush%7D,%20%5Ctheta_%7Bdukakis%7D,%20%5Ctheta_%7Bneither%7D)%20%7C%20y%20%5Csim%20Dirichlet(295,%20308,%2039)%0A"> Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the pre-debate, that is, <img src="https://latex.codecogs.com/png.latex?%5Calpha_1"> is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_1%20%7C%20y%20%5Csim%20Beta(295,%20308)%0A"> Which we can visualize thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">alpha1 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">rbeta</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="dv" style="color: #AD0000;">295</span>, <span class="dv" style="color: #AD0000;">308</span>)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">data.frame</span>(alpha1) <span class="ot" style="color: #003B4F;">-&gt;</span> simulations_alpha1</span>
<span id="cb1-3"></span>
<span id="cb1-4">simulations_alpha1 <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-5">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(alpha1)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-6">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="fl" style="color: #AD0000;">0.5</span>), <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_1$"</span>),</span>
<span id="cb1-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">" Proportion of voters who preferred Bush, out of those who had a preference </span></span>
<span id="cb1-10"><span class="st" style="color: #20794D;">       for either Bush or Dukakis at pre-debate"</span>,</span>
<span id="cb1-11">       <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"$</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_1$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/alpha1-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>That is, our posterior distribution points that at the pre-debate, there was already a majority of people (among the already decided) who favored Dukakis. Indeed:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">predebate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">pbeta</span>(<span class="fl" style="color: #AD0000;">0.5</span>, <span class="dv" style="color: #AD0000;">295</span>, <span class="dv" style="color: #AD0000;">308</span>)</span>
<span id="cb2-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"There's a {round(predebate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in</span></span>
<span id="cb2-3"><span class="st" style="color: #20794D;">           the pre-debate."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>There's a 70% posterior probability that among decided voters Dukakis had a majority in
the pre-debate.</code></pre>
</div>
</div>
</section>
<section id="post-debate" class="level3">
<h3 class="anchored" data-anchor-id="post-debate">Post-Debate</h3>
<p>Therefore, setting an uniform prior (<img src="https://latex.codecogs.com/png.latex?%5Cbeta_j%20=%201%20%5C%20%5Cforall%20j">) on the possible distribution of the <img src="https://latex.codecogs.com/png.latex?%5Ctheta_j">’s, the posterior distribution is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Ctheta_%7Bbush%7D,%20%5Ctheta_%7Bdukakis%7D,%20%5Ctheta_%7Bneither%7D)%20%7C%20y%20%5Csim%20Dirichlet(289,%20333,%2039)%0A"> Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the post-debate, that is, <img src="https://latex.codecogs.com/png.latex?%5Calpha_2"> is thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_1%20%7C%20y%20%5Csim%20Beta(289,%20333)%0A"></p>
<p>Which we can visualize thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">alpha2 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">rbeta</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="dv" style="color: #AD0000;">289</span>, <span class="dv" style="color: #AD0000;">333</span>)</span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;">data.frame</span>(alpha2) <span class="ot" style="color: #003B4F;">-&gt;</span> simulations_alpha2</span>
<span id="cb4-3"></span>
<span id="cb4-4">simulations_alpha2 <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(alpha2)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-6">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-7">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="fl" style="color: #AD0000;">0.5</span>), <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-8">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_2$"</span>),</span>
<span id="cb4-9">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">" Proportion of voters who preferred Bush, out of those who had a preference </span></span>
<span id="cb4-10"><span class="st" style="color: #20794D;">       for either Bush or Dukakis at post-debate"</span>,</span>
<span id="cb4-11">       <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"$</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_2$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/alpha2-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>After the debate, Dukakis won an even larger majority among the decided voters:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">postdebeate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">pbeta</span>(<span class="fl" style="color: #AD0000;">0.5</span>, <span class="dv" style="color: #AD0000;">289</span>, <span class="dv" style="color: #AD0000;">333</span>)</span>
<span id="cb5-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"There's a {round(postdebeate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in</span></span>
<span id="cb5-3"><span class="st" style="color: #20794D;">           the pre-debate."</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>There's a 96% posterior probability that among decided voters Dukakis had a majority in
the pre-debate.</code></pre>
</div>
</div>
</section>
<section id="a-shift-toward-bush" class="level3">
<h3 class="anchored" data-anchor-id="a-shift-toward-bush">A shift toward Bush?</h3>
<p>We have the posterior probability for both <img src="https://latex.codecogs.com/png.latex?%5Calpha_1"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha_2">. Sampling form these posteriors, we can then arrive at a posterior distribution for <img src="https://latex.codecogs.com/png.latex?%5Calpha_2%20-%20%5Calpha_1"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">difference <span class="ot" style="color: #003B4F;">&lt;-</span> alpha2 <span class="sc" style="color: #5E5E5E;">-</span> alpha1</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="fu" style="color: #4758AB;">data.frame</span>(difference) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb7-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(difference)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb7-5">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="dv" style="color: #AD0000;">0</span>), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb7-6">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>)  <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb7-7">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_2 - </span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">alpha_1$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/difference-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The posterior probability that there was a shift toward Bush is the probability that <img src="https://latex.codecogs.com/png.latex?%5Calpha_2%20-%20%5Calpha_1%20%3E%200"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">shift <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sum</span>(difference <span class="sc" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">length</span>(difference)</span>
<span id="cb8-2">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"The posterior probability that there was a shift toward Bush is thus {round(shift, 2)*100}%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The posterior probability that there was a shift toward Bush is thus 19%</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-3" class="level2">
<h2 class="anchored" data-anchor-id="exercise-3">Exercise 3</h2>
<p>Estimation from two independent experiments: an experiment was performed on the effects of magnetic fields on the flow of calcium out of chicken brains. Two groups of chickens were involved: a control group of 32 chickens and an exposed group of 36 chickens. One measurement was taken on each chicken, and the purpose of the experiment was to measure the average flow ( <em>{c} ) in untreated (control) chickens and the average flow ( </em>{t} ) in treated chickens. The 32 measurements on the control group had a sample mean of 1.013 and a sample standard deviation of ( 0.24 . ) The 36 measurements on the treatment group had a sample mean of 1.173 and a sample standard deviation of 0.20</p>
<ol type="a">
<li><p>Assuming the control measurements were taken at random from a normal distribution with mean ( <em>{c} ) and variance ( </em>{c}^{2}, ) what is the posterior distribution of ( <em>{c} ? ) Similarly, use the treatment group measurements to determine the marginal posterior distribution of ( </em>{t} . ) Assume a uniform prior distribution on ( (<em>{c}, </em>{t}, <em>{c}, </em>{t}) )</p></li>
<li><p>What is the posterior distribution for the difference, ( <em>{t}-</em>{c} ? ) To get this, you may sample from the independent ( t ) distributions you obtained in part(a) above. Plot a histogram of your samples and give an approximate ( 95 % ) posterior interval for ( <em>{t}-</em>{c} )</p></li>
</ol>
</section>
<section id="solution-1" class="level2">
<h2 class="anchored" data-anchor-id="solution-1">Solution</h2>
<p>Let’s posit two normal probability models for both the control measurements and the treatment measurements, assuming exchangeability among these two groups.</p>
<section id="control-group" class="level3">
<h3 class="anchored" data-anchor-id="control-group">Control group</h3>
<p>Therefore:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay_c%20%7C%20%5Cmu,%20%5Csigma%5E2%20%5Csim%20N(%5Cmu_c,%20%5Csigma_c%5E2)%20%5C%5C%0Ap(%5Cmu_c,%20%5Csigma_c%20%7C%20y)%20%5Cpropto%20p%20(y%20%7C%20%5Cmu_c,%20%5Csigma_c)%20p(%5Cmu_c,%20%5Csigma_c)%0A"> If we posit an uniform prior on <img src="https://latex.codecogs.com/png.latex?(%5Cmu_c,%20log%20%5Csigma_c)"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(%5Cmu_c,%20%5Csigma_c%5E2)%20%5Cpropto%20(%5Csigma_c%5E2)%5E%7B-1%7D%0A"></p>
<p>Then, the marginal posterior distribution for <img src="https://latex.codecogs.com/png.latex?%5Cmu_c"> is a t-distribution:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdfrac%7B%5Cmu_c%20%20-%20%5Cbar%20y_c%7D%7Bs_c/%5Csqrt%7Bn_c%7D%7D%20%7C%20y%20%5Csim%20t_%7Bn_c-1%7D%0A"></p>
<p>For the control group, we have <img src="https://latex.codecogs.com/png.latex?n_c%20=%2032">, <img src="https://latex.codecogs.com/png.latex?%5Cbar%20y_c%20=%201.013"> and <img src="https://latex.codecogs.com/png.latex?s_c%20=%200.24"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">mu_c <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rtnew</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">31</span>, <span class="at" style="color: #657422;">mean =</span> <span class="fl" style="color: #AD0000;">1.013</span>, <span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.24</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">32</span>) )</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">mu_control =</span> mu_c) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb10-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(mu_control)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb10-5">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb10-6">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">mu_c$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/muc-1.png" class="img-fluid" width="768"></p>
</div>
</div>
</section>
<section id="treatment-group" class="level3">
<h3 class="anchored" data-anchor-id="treatment-group">Treatment Group</h3>
<p>The same likelihood and prior are valid for the treatment measurements. Therefore, the marginal posterior for <img src="https://latex.codecogs.com/png.latex?%5Cmu_t">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdfrac%7B%5Cmu_t%20%20-%20%5Cbar%20y_t%7D%7Bs_t/%5Csqrt%7Bn_t%7D%7D%20%7C%20y%20%5Csim%20t_%7Bn_t-1%7D%0A"></p>
<p>For the treatment group, we have <img src="https://latex.codecogs.com/png.latex?n_t%20=%2036">, <img src="https://latex.codecogs.com/png.latex?%5Cmu_t%20=%201.173">, <img src="https://latex.codecogs.com/png.latex?s_t%20=%200.2">:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">mu_t <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rtnew</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">35</span>, <span class="at" style="color: #657422;">mean =</span> <span class="fl" style="color: #AD0000;">1.173</span>, <span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">0.2</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">36</span>) )</span>
<span id="cb11-2"></span>
<span id="cb11-3"><span class="fu" style="color: #4758AB;">data.frame</span>(<span class="at" style="color: #657422;">mu_treatment =</span> mu_t) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb11-4">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(mu_treatment)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb11-5">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb11-6">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">mu_t$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/mut-1.png" class="img-fluid" width="768"></p>
</div>
</div>
</section>
<section id="posterior-difference-between-mu_c-and-mu_t" class="level3">
<h3 class="anchored" data-anchor-id="posterior-difference-between-mu_c-and-mu_t">Posterior difference between mu_c and mu_t</h3>
<p>To get the posterior distribution of the difference, we compare the samples from the marginal posterior of <img src="https://latex.codecogs.com/png.latex?%5Cmu_c,%20%5Cmu_t">. Therefore, the 95% posterior credibility interval on the different is thus</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">different_mu <span class="ot" style="color: #003B4F;">&lt;-</span> mu_t <span class="sc" style="color: #5E5E5E;">-</span> mu_c</span>
<span id="cb12-2">interval <span class="ot" style="color: #003B4F;">&lt;-</span> rethinking<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">PI</span>(different_mu, <span class="at" style="color: #657422;">prob =</span> <span class="fl" style="color: #AD0000;">0.95</span>)</span>
<span id="cb12-3">lower <span class="ot" style="color: #003B4F;">&lt;-</span> interval[[<span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb12-4">upper <span class="ot" style="color: #003B4F;">&lt;-</span> interval[[<span class="dv" style="color: #AD0000;">2</span>]]</span>
<span id="cb12-5">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"The 95% posterior credibility interval for the difference is {round(lower, 2)}, {round(upper, 2)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The 95% posterior credibility interval for the difference is 0.05, 0.27</code></pre>
</div>
</div>
<p>And the full posterior of the difference is thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="fu" style="color: #4758AB;">data.frame</span>(different_mu) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb14-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(different_mu)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb14-3">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.01</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.7</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb14-4">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="fu" style="color: #4758AB;">TeX</span>(<span class="st" style="color: #20794D;">"Posterior distribution for $</span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">mu_t - </span><span class="sc" style="color: #5E5E5E;">\\</span><span class="st" style="color: #20794D;">mu_c$"</span>))</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/differentmu-1.png" class="img-fluid" width="768"></p>
</div>
</div>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-25-bayesian-data-analysis-week-3-exercises.html</guid>
  <pubDate>Thu, 25 Jun 2020 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Data Analysis: Week 3 -&gt; Fitting a Gaussian probability model</title>
  <dc:creator>David Salazar</dc:creator>
  <link>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html</link>
  <description><![CDATA[ 




<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>Instead of going through the homeworks (due to the fear of ruining the fun for future students of Aki’s), I’ll go through some of the examples of the book as case studies. In this blogpost, I’ll (wrongly) estimate the speed of light from the measurements of Simon Newcomb’s 1882 experiments.</p>
<section id="the-gaussian-probability-model" class="level2">
<h2 class="anchored" data-anchor-id="the-gaussian-probability-model">The Gaussian Probability model</h2>
<p>When fitting a Gaussian probability model, there are to parameters to estimate: <img src="https://latex.codecogs.com/png.latex?%5Cmu,%20%5Csigma">. Therefore, we arrive at a joint posterior distribution:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20%7C%20%5Cmu,%20%5Csigma%5E2%20%5Csim%20N(%5Cmu,%20%5Csigma%5E2)%20%5C%5C%0Ap(%5Cmu,%20%5Csigma%20%7C%20y)%20%5Cpropto%20p%20(y%20%7C%20%5Cmu,%20%5Csigma)%20p(%5Cmu,%20%5Csigma)%0A"> In this case, <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is a nuisance parameter: we are only really interested in knowing <img src="https://latex.codecogs.com/png.latex?%5Cmu">. The following we assume the non-informative prior:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(%5Cmu,%20%5Csigma%5E2)%20%5Cpropto%20(%5Csigma%5E2)%5E%7B-1%7D%0A"></p>
<section id="posterior-marginal-of-sigma" class="level3">
<h3 class="anchored" data-anchor-id="posterior-marginal-of-sigma">Posterior marginal of sigma</h3>
<p>We can show that the marginal posterior distribution for <img src="https://latex.codecogs.com/png.latex?%5Csigma"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%20%7C%20y%20%5Csim%20Inv%20-%5Cchi%5E2%20(n%20-%201,%20s%5E2)%0A"> Where <img src="https://latex.codecogs.com/png.latex?s%5E2"> is the sample variance of the <img src="https://latex.codecogs.com/png.latex?y_i">’s.</p>
</section>
<section id="marginal-conditional-posterior-for-mu" class="level3">
<h3 class="anchored" data-anchor-id="marginal-conditional-posterior-for-mu">Marginal Conditional posterior for mu</h3>
<p>Then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmu%20%7C%20%5Csigma%5E2,%20y%20%5Csim%20N(%5Cbar%20y,%20%5Csigma%5E2%20/%20n)%0A"> &gt; The posterior distribution of <img src="https://latex.codecogs.com/png.latex?%5Cmu"> can be regarded as a mixture of normal distributions, mixed over the scaled inverse <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> distribution for the variance, <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">.</p>
</section>
<section id="marginal-posterior-for-mu" class="level3">
<h3 class="anchored" data-anchor-id="marginal-posterior-for-mu">Marginal posterior for mu</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdfrac%7B%5Cmu%20%20-%20%5Cbar%20y%7D%7Bs/%5Csqrt%7Bn%7D%7D%20%7C%20y%20%5Csim%20t_%7Bn-1%7D%0A"></p>
<p>Which has a nice correspondence with the distribution used for the mean estimator in frequentist statistics in the case of small samples.<br>
## Fitting it with data</p>
<p>Then, let’s read the measurements from Newcomb’s experiment:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">read_delim</span>(<span class="st" style="color: #20794D;">"http://www.stat.columbia.edu/~gelman/book/data/light.asc"</span>, </span>
<span id="cb1-2">                     <span class="at" style="color: #657422;">delim =</span> <span class="st" style="color: #20794D;">" "</span>, <span class="at" style="color: #657422;">skip =</span> <span class="dv" style="color: #AD0000;">3</span>,<span class="at" style="color: #657422;">col_names =</span> F) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-3">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="fu" style="color: #4758AB;">everything</span>()) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-4">  <span class="fu" style="color: #4758AB;">select</span>(value) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-5">  <span class="fu" style="color: #4758AB;">drop_na</span>()<span class="ot" style="color: #003B4F;">-&gt;</span> measurements</span>
<span id="cb1-6"></span>
<span id="cb1-7">measurements <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(value)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-9">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-10">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Newcomb's measurements for the speed of light"</span>,</span>
<span id="cb1-11">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"There are clearly problems with the data"</span>,</span>
<span id="cb1-12">       <span class="at" style="color: #657422;">x =</span> <span class="st" style="color: #20794D;">"measurement"</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There are clearly some problems with the data. Garbage in, garbage out.</p>
</section>
<section id="marginal-for-sigma" class="level3">
<h3 class="anchored" data-anchor-id="marginal-for-sigma">Marginal for sigma</h3>
<p>We therefore can derive the marginal distribution for <img src="https://latex.codecogs.com/png.latex?%5Csigma">:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">measurements<span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-2">  skimr<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">skim</span>()</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<caption>Data summary</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">Name</td>
<td style="text-align: left;">Piped data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number of rows</td>
<td style="text-align: left;">66</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Number of columns</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">_______________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Column type frequency:</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">________________________</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Group variables</td>
<td style="text-align: left;">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table class="table table-sm table-striped">
<colgroup>
<col style="width: 17%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 7%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">skim_variable</th>
<th style="text-align: right;">n_missing</th>
<th style="text-align: right;">complete_rate</th>
<th style="text-align: right;">mean</th>
<th style="text-align: right;">sd</th>
<th style="text-align: right;">p0</th>
<th style="text-align: right;">p25</th>
<th style="text-align: right;">p50</th>
<th style="text-align: right;">p75</th>
<th style="text-align: right;">p100</th>
<th style="text-align: left;">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">value</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">26.21</td>
<td style="text-align: right;">10.75</td>
<td style="text-align: right;">-44</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">27</td>
<td style="text-align: right;">30.75</td>
<td style="text-align: right;">40</td>
<td style="text-align: left;">▁▁▁▂▇</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Thus:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma%5E2%20%7C%20y%20%5Csim%20Inv%20-%5Cchi%5E2%20(65,%2010.7%5E2)%0A"></p>
<p>Whereas for mu we have :</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdfrac%7B%5Cmu%20%20-%2026.2%7D%7B10.7/%5Csqrt%7B66%7D%7D%20%7C%20y%20%5Csim%20t_%7Bn-1%7D%0A"> In code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">rsinvchisq <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(n, nu, s2, ...) nu<span class="sc" style="color: #5E5E5E;">*</span>s2 <span class="sc" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">rchisq</span>(n , nu, ...)</span>
<span id="cb3-2"></span>
<span id="cb3-3">sigma_draw <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">rinvchisq</span>(<span class="dv" style="color: #AD0000;">1000</span>, <span class="dv" style="color: #AD0000;">65</span>, <span class="fl" style="color: #AD0000;">10.7</span><span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb3-4">mu_draw <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">rtnew</span>(<span class="dv" style="color: #AD0000;">10000</span>, <span class="at" style="color: #657422;">df =</span> <span class="dv" style="color: #AD0000;">65</span>, <span class="at" style="color: #657422;">mean =</span> <span class="fl" style="color: #AD0000;">26.2</span>, <span class="at" style="color: #657422;">scale =</span> <span class="fl" style="color: #AD0000;">10.7</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(<span class="dv" style="color: #AD0000;">66</span>) )</span></code></pre></div>
</div>
<p>The 95% credible interval for our <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is thus:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">interval <span class="ot" style="color: #003B4F;">&lt;-</span> rethinking<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">PI</span>(mu_draw, <span class="at" style="color: #657422;">prob =</span> <span class="fl" style="color: #AD0000;">0.95</span>)</span>
<span id="cb4-2">lower <span class="ot" style="color: #003B4F;">&lt;-</span> interval[[<span class="dv" style="color: #AD0000;">1</span>]]</span>
<span id="cb4-3">upper <span class="ot" style="color: #003B4F;">&lt;-</span> interval[[<span class="dv" style="color: #AD0000;">2</span>]]</span>
<span id="cb4-4">glue<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">glue</span>(<span class="st" style="color: #20794D;">"The 95% credible interval is thus: {round(lower, 1)}, {round(upper, 1)}"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The 95% credible interval is thus: 23.6, 28.8</code></pre>
</div>
</div>
<p>A visualization may help:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">data.frame</span>(mu_draw) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(mu_draw)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-3">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="at" style="color: #657422;">binwidth =</span> <span class="fl" style="color: #AD0000;">0.1</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">"dodgerblue4"</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> lower), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> upper), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"red"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-6">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept =</span> <span class="dv" style="color: #AD0000;">33</span>), <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">"black"</span>, <span class="at" style="color: #657422;">linetype =</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-7">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">"Speed of light: Posterior draws for mu"</span>,</span>
<span id="cb6-8">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">"Contemporary estimates for the speed of light in the experiment is 33. Garbage in, garbage..."</span>)</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://david-salazar.github.io/posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model_files/figure-html/mu-1.png" class="img-fluid" width="768"></p>
</div>
</div>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section></div> ]]></description>
  <guid>https://david-salazar.github.io/posts/bayesian-statistics/2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model.html</guid>
  <pubDate>Wed, 24 Jun 2020 05:00:00 GMT</pubDate>
</item>
</channel>
</rss>
