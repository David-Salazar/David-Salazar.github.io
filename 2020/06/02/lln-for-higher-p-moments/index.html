<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>LLN for higher p Moments - David Salazar&#39;s blog</title>
<meta property="og:title" content="LLN for higher p Moments - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">LLN for higher p Moments</h1>

    
    <span class="article-date">2020-06-02</span>
    

    <div class="article-content">
      


<p>I have recently been exploring Nassim Taleb’s latest <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">technical book: Statistical Consequences of Fat Tails</a>. In it, we have seen how the Law of Large Numbers for different estimators simply does not work fast enough (in Extremistan) to be used in real life. For example, we have seen how the distribution of the <a href="2020-04-17-fat-vs-thin-does-lln-work.html">sample mean</a>, <a href="2020-04-27-spurious-pca-under-thick-tails.html">PCA</a>, <a href="2020-05-22-correlation-is-not-correlation.html">sample correlation</a> and <a href="2020-05-26-r-squared-and-fat-tails.html"><span class="math inline">\(R^2\)</span></a> turn into pure noise when we are dealing with fat-tails. Also, we have seen <a href="2020-05-30-central-limit-theorem-in-action.html">how the slow convergence of the CLT</a> for fat-tails: thus making the normal approximation for most samples sizes an unreliable on.</p>
<p>In this post, I’ll explore the slow workings of the Law of Large numbers for higher moments. <strong>Similar to what we did for the sample mean, we will check whether adding observations reduce the variability of our estimate or whether it causes occasional jumps that lead us to believe that large sub-samples will produce different averages, indicating that the theoretical moment does not exist.</strong> This problem will only be worsened with higher moments: higher moments are convex transforms of the original random variable. Thus, they are even more fatter than our original variable and our estimates will be more likely to suffer from large discontinuous jumps that will derail convergence.</p>
<div id="the-math" class="section level1">
<h1>The math</h1>
<p>A consequence of the Law of Large Numbers (LLN) is the following:</p>
<p><span class="math display">\[ E[X^p] &lt; \infty  \iff R_n^p = \dfrac{max(X_1^p, \dots, X_n^p)}{\sum_{i=1}^n X_i^p} \to 0, \ \text{as} \ n \to \infty\]</span>
That is, the theoretical moment <span class="math inline">\(p\)</span> exists if and only if the ratio of the partial max to the partial sum converges to <span class="math inline">\(0\)</span>. <em>Of course, with any sample at hand, the fact that the ratio has yet to converge isn’t proof that it will never converge</em>. It may be that we just simply haven’t enough data. However, it is proof that we are dealing with fat-tails, as these are the variables for which we observe the slow convergence of the Law of Large Numbers.</p>
</div>
<div id="monte-carlo-simulations" class="section level1">
<h1>Monte Carlo simulations</h1>
<p>As always, I’ll explore the meaning of this convergence with some Monte-Carlo experiments. Note, however, that I’ll abstract the computation of the ratio itself to <code>ggtails</code>, the <a href="https://github.com/David-Salazar/ggtails">package</a> that I’ve created to plot the fat-tail-specific diagnostics.</p>
<pre class="r"><code>library(ggtails)</code></pre>
<div id="fast-convergence-mediocristan" class="section level2">
<h2>Fast-Convergence: Mediocristan</h2>
<pre class="r"><code>gaussian &lt;- rnorm(500) 
data.frame(gaussian) %&gt;% 
  ggplot(aes(sample = gaussian)) +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Gaussian&quot;,
       subtitle = &quot;Fast convergence to 0&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/gaussian-1.gif" /><!-- --></p>
<p>For a Gaussian, all the moments exist and therefore the ratio of the max-to-sum for all the moments converge to <span class="math inline">\(0\)</span>. Notice that after 50 observations, the ratio stabilizes and adding more observations always results in the ratio converging to 0. However, note that just as we saw with the CLT, higher moments take longer to converge. Notice that this is not a feature of the sample, but a feature of the fast convergence of the LLN for the Gaussian. To see this, we can check the convergence for 10 different gaussian samples:</p>
<pre class="r"><code>rerun(10, rnorm(500)) %&gt;% bind_cols() %&gt;% 
  mutate(sim =1:500) %&gt;% 
  pivot_longer(-sim, names_to = &quot;simulation&quot;) %&gt;%  
  mutate(simulation = str_extract(simulation, &quot;\\d+&quot;)) %&gt;% 
  ggplot(aes(sample = value))  +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  facet_wrap(~simulation, nrow = 4) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Gaussian&quot;,
       subtitle = &quot;Fast convergence to 0&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/many-normals-1.gif" /><!-- --></p>
</div>
<div id="lognormal" class="section level2">
<h2>Lognormal</h2>
<p>Notice that all the moments also exist for a lognormal. However, the convergence is much slower: the extreme events observations cause sudden jumps that derail the convergence. Much more data is needed. Here we have an order of magnitude more.</p>
<pre class="r"><code>lognormal &lt;- rlnorm(5000, sd = 4) 
data.frame(lognormal) %&gt;% 
  ggplot(aes(sample = lognormal)) +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Lognormal (sd = 4)&quot;,
       subtitle = &quot;Slow convergence and even slower for higher moments&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/lognormal-1.gif" /><!-- --></p>
<p>Crucially, the higher moments are even more fat-tailed. Thus, they are much more susceptible to discontinuous jumps and therefore have slower convergence.</p>
<p>Notice that this behavior is not a property of our specific sample but of the distribution itself.</p>
<pre class="r"><code>rerun(10, rlnorm(500, sd = 4)) %&gt;% bind_cols() %&gt;% 
  mutate(sim =1:500) %&gt;% 
  pivot_longer(-sim, names_to = &quot;simulation&quot;) %&gt;%  
  mutate(simulation = str_extract(simulation, &quot;\\d+&quot;)) %&gt;% 
  ggplot(aes(sample = value))  +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  facet_wrap(~simulation) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Lognormal&quot;,
       subtitle = &quot;Slow convergence for higher moments&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/many-lognormals-1.gif" /><!-- --></p>
</div>
<div id="pareto-distribution" class="section level2">
<h2>Pareto Distribution</h2>
<p>Now it’s time to analyze a distribution that has no finite theoretical moments. For example, a Pareto with tail exponent <span class="math inline">\(\alpha = 1.16\)</span>, equivalent to the 80/20 rule, only has finite mean. All other higher moments are infinite. If you want an intuitive explanation of the tail exponent, check this <a href="2020-05-19-understanding-the-tail-exponent.html">other blogpost</a></p>
<pre class="r"><code>alpha &lt;- 1.16
rpareto &lt;- function(n) {
   (1/runif(n)^(1/alpha)) # inverse transform sampling
}

pareto &lt;- rpareto(5000)
data.frame(pareto) %&gt;% 
  ggplot(aes(sample = pareto)) +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Pareto (alpha = 1.16)&quot;,
       subtitle = &quot;Only the mean converges&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/pareto-1.gif" /><!-- --></p>
<p>Compare it for a Pareto with $ = 2.32$:</p>
<pre class="r"><code>alpha &lt;- 2.1
rpareto &lt;- function(n) {
   (1/runif(n)^(1/alpha)) # inverse transform sampling
}

pareto &lt;- rpareto(5000)
data.frame(pareto) %&gt;% 
  ggplot(aes(sample = pareto)) +
  stat_max_sum_ratio_plot(p = 4) +
  geom_hline(aes(yintercept = 0),
             linetype = 2) +
  transition_reveal(stat(x)) +
  scale_color_viridis_d() +
  labs(title = &quot;Max to Sum ratio for a Pareto (alpha = 2.1)&quot;,
       subtitle = &quot;The two first moments exist, yet only one converges. Second moment has alpha = 1.05&quot;)</code></pre>
<p><img src="/post/2020-06-02-lln-for-higher-p-moments_files/figure-html/unnamed-chunk-3-1.gif" /><!-- --></p>
<p>Indeed, the first two moments exist and should both converge to zero. However, the second moment has fatter tails and thus takes longer to converge.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>The LLN converges very slowly for fat-tailed variables. For higher moments, the problem is compounded: their convergence is even slower.</p>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

