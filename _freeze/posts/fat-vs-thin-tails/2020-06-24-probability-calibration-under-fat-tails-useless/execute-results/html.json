{
  "hash": "506cbc8809951b4b6fa396d7ca244171",
  "result": {
    "markdown": "---\ntitle: 'Probability Calibration under fat-tails: useless'\nauthor: ''\ndate: '2020-06-24'\nslug: probability-calibration-under-fat-tails-useless\ncategories: []\ntags: []\naliases: \n  - ../../2020/06/24/probability-calibration-under-fat-tails-useless/\n---\n\n\n\n\nProbability calibration refers to a manner of evaluating forecasts: the forecast frequency of an event should correspond to the correct frequency of the event happening in real life. Is this truly the mark of a *good analysis?* Under fat-tails, Nassim Taleb in [his book](https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf) answer with  **a categorical response NO!**\n\n## Probability calibration in the real world\n\nProbability calibration amounts, in the real world, to a binary payoff: a fixed sum is paid off if the event happens. If one wants to hedge the risk of a fat-tailed variable, the question is then: which lump sum?\n\n![](/images/mistracking.PNG)\n\nThe answer: there is no possible lump sum that can hedge the exposure to a fat-tailed variable. The reason is the same as to why single-point forecasts are useless:\n\n> There is no typical collapse or disaster, owing to the absence of characteristic scale\n\nTherefore, given that there is no characteristic scale for fat-tailed variables, one cannot know in advance the size of the collapse nor how much the lump sum of the binary payoff should be. \n\n## Monte Carlo simulation\n\nA quick Monte-Carlo Simulation should do the trick to understand why fat-tailed variables have no characteristic scale. Imagine you are exposed to certain losses. You take a lump-sum insurance. Let's simulate the possible losses that you may incur if the exposure is a lognormal. For low-values of $\\sigma$, the lognormal behaves as a Gaussian. For higher values, it behaves like a fat-tailed variable.\n\n### Log normal, sigma = 0.2\n\nWith a log normal of sigma = 0.2, a lump-sum of 2 will absolutely cover any of the losses. The reason: [the MDA is Gumbel that decays pretty rapidly](2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html). Therefore, the sample maxima is effectively bounded at large values from the mean. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-06-24-probability-calibration-under-fat-tails-useless_files/figure-html/lognormal-1.gif)\n:::\n:::\n\n\nWhereas if we are exposed to a Pareto 80/20, there's no lump sum that can covers us. The MDA is a [FrÃ©chet that decays as a power law](2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html): \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-06-24-probability-calibration-under-fat-tails-useless_files/figure-html/pareto-1.gif)\n:::\n:::\n\n\nGiven this lack of characteristic scale, there should not be any prize for saying that a there will be a loss larger than $K$. With fat-tailed variables, almost any larger value is likely. Indeed, fat-tailed variables are long-tailed variables and thus share the following property:\n\n$$\n\\lim_{x \\to \\infty} \\Pr[X>x+t\\mid X>x] =1\n$$\n\nThat is, reducing the variability of a fat-tailed random variable to a binary payoff makes no sense. Therefore, probability calibration with fat-tailed variables makes no sense. \n\n## Conclusion\n\n> You do not eat forecasts, most business have severly skewed payoffs, so being calibrated in probability is meaningless. \n\n",
    "supporting": [
      "2020-06-24-probability-calibration-under-fat-tails-useless_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}