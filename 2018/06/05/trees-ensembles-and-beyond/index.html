<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.37.1" />


<title>Trees, Ensembles and beyond, XGBoost and LGBM - Dilettanting Data Science</title>
<meta property="og:title" content="Trees, Ensembles and beyond, XGBoost and LGBM - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/%3cnil%3e"
         width=""
         height=""
         alt="">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Trees, Ensembles and beyond, XGBoost and LGBM</h1>

    
    <span class="article-date">2018/06/05</span>
    

    <div class="article-content">
      <div id="why" class="section level1">
<h1>Why?</h1>
<p><code>lightgbm</code> and <code>xgboost</code> appear in every single competition at Kaggle. Thus, these boosting techniques must be able to learn something that cannot be easily learned from intelligent bagging techniques like Random Forests. This is my attempt to understand <em>why</em> and <em>how</em> they can do that.</p>
<div id="set-up" class="section level2">
<h2>Set-up</h2>
<p>This will be kind of mathy, but I will try to keep the notation as clear as possible so it can be understood without much hassle. The traditional problem in machine learning, for a given algorithm, is as follows: learn a function, <span class="math inline">\(f(x; \theta)\)</span> such that <span class="math inline">\(\theta\)</span> minimizes the following:</p>
<p><span class="math display">\[ E_y (L(y, f(x; \theta)) ) \]</span>
Unpacked, it can be explained thus: given a loss function, <span class="math inline">\(L\)</span>, we want to find the <span class="math inline">\(\theta\)</span> such that the loss between the observations, <span class="math inline">\(y\)</span>, and our predictions, <span class="math inline">\(f(x;\theta)\)</span> is minimized across the values we expect to see of the observation. Note that this last condition precludes <em>train-set-overfitting</em>: we want something to generalizes even to not-yet seen observations.</p>
</div>
</div>
<div id="trees" class="section level1">
<h1>Trees</h1>
<p>From the freely available <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.305:</p>
<blockquote>
<p>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.</p>
</blockquote>
<p>Thus, a prediction with a tree is a matter of finding on <strong>which partition</strong> our new observation sits and predict <strong>the constant assigned</strong> to that region. For regression problems, the constant is the average of the observations in the regions; for classification problem, the constant is the modal class within the region.</p>
<div id="fitting-them" class="section level2">
<h2>Fitting them</h2>
<p>They sound, and are quite simple, but the big problem is <strong>how to find the partitions</strong> in the first place. Mathematically, the problem can be defined thus:</p>
<p>For <span class="math inline">\(J\)</span> partitions that we decide to perform, find where in the space to do the partition, <span class="math inline">\(R_j\)</span>, and what constant to assign to that partition, <span class="math inline">\(c_j\)</span>. We must find, for each partition <span class="math inline">\(j\)</span> the <span class="math inline">\(R_j\)</span>, <span class="math inline">\(c_j\)</span> pair that minimize the following:</p>
<p><span class="math display">\[ \sum_{j}^J \sum_{x_i \in R_j} E_y (L(y, c_j) ) \]</span></p>
<p>However, this a prohibitely expensive combinatorial optimization problem. To avoid this, we proceed with a <strong>greedy algorithm</strong>:</p>
<p>For any given node do the following:</p>
<ol style="list-style-type: decimal">
<li>Loop over each of the features.</li>
<li>For each feature, sort the observations and find every possible split using this feature. Compute the <strong>information gain</strong> of doing each of these splits. Keep only the one that gave the best information gain.</li>
</ol>
<blockquote>
<p>Information gain is defined as the difference between the loss in the node and the average loss across the nodes created as we do the proposed partition</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Compare the best information again across features. Finally, use the feature that had the best one.</li>
<li>Go over the next node and repeat.</li>
</ol>
<p>Thus, note that the algorithm will keep doing this partitions as long as there are no possible partitions in any of the nodes.</p>
</div>
<div id="interpretation" class="section level2">
<h2>Interpretation</h2>
<p>Besides the traditional tree visualization, notice that we can rank the importance of each feature according to their predictive power:</p>
<blockquote>
<p>For each node, annotate which feature was used and the resulting information gain from that partition. Do that across all the nodes. Sum for all the features that were used more than once. The features that have more of these values, are the ones that were more important for our tree.</p>
</blockquote>
<p><strong>This way of computing feature importances will be generalized to ensemble trees. As we will do the same procedure for each tree, and then average across trees. However, this procedure is not without its <a href="http://parrt.cs.usfca.edu/doc/rf-importance/index.html">perils</a>.</strong></p>
</div>
</div>
<div id="ensembles" class="section level1">
<h1>Ensembles</h1>
<p>Trees are simple and powerful. Yet, they are limited: they are extremely susceptible to noise. However, <strong>they can be much improved if they are combined together</strong>.</p>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p>Bagging consists on the idea of combining <strong>weak learners</strong>, that is, models that do not perform very well, and that the end result is greater than the sum of its parts. Why can we do this? When we have multiple and <strong>uncorrelated weak learner</strong>, they will make mistakes in their individual predictions, yes, but they will commit <strong>different types of mistakes</strong>. If they are <strong>roughly right</strong>, some models will overshoot and some will undershoot: the end result of combining them will be that we will make correct predictions as the <strong>errors will cancel each other</strong>.</p>
<p>In the statistical learning lingua, what we are trying to achieve is, by virtue of combining <strong>uncorrelated weak learners</strong>, a better position in the <strong>bias-variance trade-off</strong>. To do so, we need to enforce the following conditions:</p>
<ol style="list-style-type: decimal">
<li>The individual <strong>weak learners</strong> must have <strong>low correlation between each other</strong> and have <strong>low bias</strong>(i.e., they must able to learn complex data structures).</li>
<li>The combination of the models <strong>cannot increase the bias</strong> that the individual learner has.</li>
<li>The combination of the models must <strong>reduce the overall model variance</strong> with respect to the variance of the individual weak learners.</li>
</ol>
<p>If the three conditions hold, bagging will lead us into a better position in the <strong>bias-variance trade-off</strong>: we will have the same bias (which is already low, given how powerful indiviual trees can be) but we will have lower variance. Thus, we have a better model.</p>
<p>To make sure that three conditions mentioned hold, there are two smart methods for bagging: simple bagging and random forests. In both, we combine low bias trees and try to decorrelate them by using some clever tricks such that the trees that we construct end up being <strong>uncorrelated weak learners</strong>. Note that the key part here is “<strong>uncorrelated weak learners</strong>”. <strong>If they are weak learners, but are highly correlated, combining them won’t fix any of the problems the individual learners have</strong>.</p>
<p>Mathematically (without worrying about its derivation, just the interpretation), this can be seen thus:</p>
<ul>
<li>The variance of an average of <span class="math inline">\(B\)</span> <span class="math inline">\(i.i.d.\)</span> random variables (<span class="math inline">\(X\)</span> with variance <span class="math inline">\(\sigma^2\)</span>) is:</li>
</ul>
<p><span class="math display">\[ \frac{1}{B}\sigma^2\ \]</span></p>
<ul>
<li>However, if they are identically distributed but not independent, and with positive pairwise correlation <span class="math inline">\(\rho\)</span>, the variance of their sum average:</li>
</ul>
<p><span class="math display">\[ \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2  \]</span></p>
<p>Thus, as <span class="math inline">\(B\)</span> increases the right side term will disappear and the weak learners will become more like the <strong>uncorrelated weak learners</strong> we need for bagging to be effective.</p>
<p>Once we have multiple <strong>uncorrelated weak learners</strong>, combining them is a simple task. If the problem is a regression one, we average our predictions across trees. If it is a classification one, we use majority voting.</p>
<div id="bootstraping" class="section level3">
<h3>Bootstraping</h3>
<p>So we know how to construct high variance low bias trees:</p>
<ol style="list-style-type: decimal">
<li><p>Construct them such that they have a lot of depth and thus are able to capture complex structure in the data. This can be accomplished by setting a low value for <code>min_samples_leaf</code> in the <code>scikit-learn</code> implementation. Although each of the trees will tend to overfit as they have more depth, <strong>bagging will precisely solve this problem</strong> by reducing the overall model variance.</p></li>
<li><p>Construct them such that they are <strong>uncorrelated</strong> between them. How can we do so?</p></li>
</ol>
<p>One way making sure that the trees are uncorrelated is to make them to use slightly <strong>different samples</strong>. That is, for each tree that we construct, we are going to pass them over a bootstrap-resampled version (i.e., sampled <span class="math inline">\(n = |trainingdata|\)</span> times with replacement) of the training data. Thus, as each tree is going to work with different data, they are going to learn
<strong>different partitions</strong> and thus be <strong>uncorrelated weak learners</strong>.</p>
<p>To understand this, let’s make the following experiment. imagine one tree and one bootstrap-resampled version of the training data. For a given row, <strong>what is the probability that the tree will see this row in its data?</strong></p>
<blockquote>
<p>The probability of not choosing that row in the first iteration of the bootstrap is: <span class="math inline">\(\frac{n-1}{n}\)</span>. The probability of not choosing that row in any of the other iterations is: <span class="math inline">\((\frac{n-1}{n})^n\)</span> (remember that we are sampling n times from the n rows). Thus, the probability of choosing this row in at least of one the iterations, and thus the tree seeing it in its data, is: <span class="math inline">\(1- (\frac{n-1}{n})^n\)</span></p>
</blockquote>
<p>Using computer power, we can see that this expression converges to 63%:</p>
<pre class="r"><code>library(ggplot2)
library(dplyr)
samples &lt;- seq(10, 1000, 10)
probs &lt;- 1 - ((samples-1)/samples)^(samples)
df &lt;- data.frame(cbind(samples, probs))
df %&gt;% 
  ggplot(aes(x = samples, y = probs)) +
    geom_line() +
    hrbrthemes::theme_ipsum_rc() + 
    scale_y_continuous(labels = scales::percent) +
    labs(x = &quot;# of obs training and bootstrap iterations&quot;,
         y = &quot;Probability of seeing row in data&quot;,
         subtitle = &quot;How likely is that the tree will see this row?&quot;,
         title = &quot;Probability with iterations = # training data&quot;)</code></pre>
<p><img src="/post/2018-06-05-trees-ensembles-and-beyond_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Thus, <strong>if we perform a bootstrap with equal number of iterations as there is # of observations in the training data, the tree will see at least that row in its data with 63% of probability</strong>. Not bad right? <strong>That means that there’s a 47% chance that it will not see it, will not learn from it, and thus will arrive at a different partition</strong> than the trees that do see that observation. Notice that we are moving on the <strong>bias-variance tradeoff</strong>: if the tree does not see it, it will be biased; however, when averaging with others, this will be corrected <strong>and</strong> these differences will reduce the overall model variance.</p>
<p>However, we can be even more extreme with this procedure. <strong>We can perform a bootstrap by sampling even less times than there are training observations</strong>. If we do so, the probability that, for a given row, a given tree will not see it in its data is:</p>
<p><span class="math display">\[1- (\frac{n-1}{n})^k \]</span></p>
<p><strong>Remember that now <span class="math inline">\(k &lt; n\)</span></strong>. Evaluating this operation such that <span class="math inline">\(k \in {n/2, n/3, n/4}\)</span>:</p>
<p><img src="/post/2018-06-05-trees-ensembles-and-beyond_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Notice how the probability of <strong>seeing a given row in a given tree decreases as we reduce the number of iterations in the bootstrap</strong>. This means that more and more trees won’t see some observations, won’t learn from them and thus will learn different partitions from the trees that do see those observations. Thus, <strong>by reducing the number of iterations in the bootstrap we can reduce the correlation between the trees</strong>. However, notice that this will accomplish an even greater shift in the bias-variance tradeoff: the reduction in the variance will be greater, yes, but so will be the increase in the bias of each individual tree. Also, this technique has the benefit of reducing training time, as each tree takes less time in training due to lower number of observations (and consequently, # of splits) it considers.</p>
<p><strong>Note that the only way I know of setting the number of bootstrap iterations lower than the number of observations is by using the <code>fast.ai</code> library and the following two functions:</strong></p>
<pre class="python"><code>from sklearn.ensemble import forest
def set_rf_samples(n):
    &quot;&quot;&quot; Changes Scikit learn&#39;s random forests to give each tree a random sample of
    n random rows. from fast.ai: https://github.com/fastai/fastai/blob/master/fastai/structured.py
    &quot;&quot;&quot;
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n))
        
def reset_rf_samples():
    &quot;&quot;&quot; Undoes the changes produced by set_rf_samples.
    &quot;&quot;&quot;
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n_samples))</code></pre>
</div>
<div id="random-forests" class="section level3">
<h3>Random Forests</h3>
<p>Even though passing over different bootstraped-versions to each tree in a forest can effectively “decorrelate” the trees, we can improve on it. Let’s think of a situation where <strong>this procedure fails</strong>: no matter what rows each tree sees, the partitions are all the same. Why? Close to all of the <strong>observations share a predominant</strong> feature: one that always will yield the best information gain if split on it. Thus, the trees won’t give different predictions and bagging won’t help much. What can we do?</p>
<p>Enter <strong>random selection</strong>: every time any of the trees will evaluate which feature to use, we <strong>will tell it to only consider a subset of the features</strong>. Which subset? Well, <strong>every time any of the trees reach any of the nodes, we will draw a random sample of the subsets</strong>.</p>
<p>Now consider the former problem of the predominant feature. What will happen? Each time a node evaluates on which features to split, <strong>the predominant feature may or may not be within the subsets of features the tree is allowed to use in that node</strong>. If it is not, then the tree won’t use that feature: <strong>even then it would have yield the best information gain!!</strong>. Why? This loss in the information gain at a particular tree is <strong>countered by the overall model benefits of having trees that do not correlate between them and thus repaing the benefits of bagging</strong>.</p>
<p>From the <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.598, we can see that, effectively, tree correlation decreases as we are more stringent with the number of features that each node can use:</p>
<div class="figure">
<img src="/post/2018-06-05-trees-ensembles-and-beyond_files/esl.png" alt="esl" />
<p class="caption">esl</p>
</div>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

