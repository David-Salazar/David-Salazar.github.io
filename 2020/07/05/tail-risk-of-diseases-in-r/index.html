<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Tail Risk of diseases in R - Dilettanting Data Science</title>
<meta property="og:title" content="Tail Risk of diseases in R - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">15 min read</span>
    

    <h1 class="article-title">Tail Risk of diseases in R</h1>

    
    <span class="article-date">2020/07/05</span>
    
    

    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#the-data">The Data</a></li>
<li><a href="#the-plots">The plots</a><ul>
<li><a href="#max-to-sum-ratio">Max-to-Sum ratio</a></li>
<li><a href="#histogram">Histogram</a></li>
<li><a href="#the-zipf-plot">The Zipf plot</a></li>
<li><a href="#mean-excess-plot">Mean Excess Plot</a></li>
</ul></li>
<li><a href="#fitting-the-tail">Fitting the tail</a><ul>
<li><a href="#wait-a-moment-infinite-casualties">Wait a moment: infinite casualties?</a></li>
<li><a href="#extreme-value-theory-on-the-dual-observations">Extreme Value theory on the dual observations</a></li>
<li><a href="#maximum-likelihood-estimate">Maximum Likelihood estimate</a></li>
</ul></li>
<li><a href="#bayesian-model">Bayesian model</a><ul>
<li><a href="#simulating-fake-data">Simulating fake data</a></li>
<li><a href="#fitting-the-model">Fitting the model</a></li>
<li><a href="#posterior-predictive-checks">Posterior predictive checks</a></li>
<li><a href="#convergence-diagnostics">Convergence diagnostics</a></li>
</ul></li>
<li><a href="#stressing-the-data">Stressing the data</a><ul>
<li><a href="#measurement-error">Measurement error</a></li>
</ul></li>
<li><a href="#influential-observations">Influential observations</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<p>Pasquale Cirillo and Nassim Taleb published a short, interesting and important paper on the <a href="https://arxiv.org/abs/2004.08658">Tail Risk of contagious diseases</a>. In short, the distribution of fatalities is strongly fat-tailed: thus rendering any forecast, whether is pointwise or a distributional forecast, useless and dangerous. The <strong>distributional evidence is there</strong>: the lack of a characteristic scale makes our uncertainty at any point in the pandemic maximal: we can only say that it can always get worse.</p>
<p>Read the paper! It’s short and packed of ideas. In this blogpost, I’ll reproduce the main plots, the main model that uses Extreme Value Theory to model the tail of the distribution of casualties and, finally, I will re-implement the model in a Bayesian framework using Stan.</p>
<div id="the-data" class="section level2">
<h2>The Data</h2>
<p>Taleb and Cirillo collected data for 72 events with more than 1,000 estimated victims.</p>
<table>
<caption><span id="tab:unnamed-chunk-2">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">72</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">7</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">name</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">9</td>
<td align="right">34</td>
<td align="right">0</td>
<td align="right">69</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">start_year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1619.08</td>
<td align="right">517.26</td>
<td align="right">-429</td>
<td align="right">1595.00</td>
<td align="right">1813.0</td>
<td align="right">1916.50</td>
<td align="right">2019</td>
<td align="left">▁▁▁▁▇</td>
</tr>
<tr class="even">
<td align="left">end_year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1613.35</td>
<td align="right">520.65</td>
<td align="right">-426</td>
<td align="right">1593.00</td>
<td align="right">1813.5</td>
<td align="right">1923.75</td>
<td align="right">2020</td>
<td align="left">▁▁▁▁▇</td>
</tr>
<tr class="odd">
<td align="left">lower</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2660.74</td>
<td align="right">9915.03</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">75.5</td>
<td align="right">850.00</td>
<td align="right">75000</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">avg_est</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4877.66</td>
<td align="right">19132.36</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">82.0</td>
<td align="right">850.00</td>
<td align="right">137500</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">upper_est</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7094.56</td>
<td align="right">28705.00</td>
<td align="right">1</td>
<td align="right">10.00</td>
<td align="right">88.0</td>
<td align="right">850.00</td>
<td align="right">200000</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">rescaled_avg_est</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">84874.62</td>
<td align="right">409099.85</td>
<td align="right">2</td>
<td align="right">41.75</td>
<td align="right">738.0</td>
<td align="right">6112.25</td>
<td align="right">2678283</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="odd">
<td align="left">population</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2037.94</td>
<td align="right">2434.28</td>
<td align="right">50</td>
<td align="right">554.00</td>
<td align="right">990.0</td>
<td align="right">1817.25</td>
<td align="right">7643</td>
<td align="left">▇▂▁▁▂</td>
</tr>
</tbody>
</table>
<p>The estimates for the number of casualties are in the thousands. The <code>avg_est</code> is the estimate for the number of casualities that we are going to be working with.</p>
</div>
<div id="the-plots" class="section level2">
<h2>The plots</h2>
<p>Taleb and Cirillo begin the paper with a graphical analysis of the properties of the data. In it, they show that the data present all of the traits of fat-tailed random variables. To reproduce most of the figures, I use an <a href="https://github.com/David-Salazar/ggtails/">R package</a> that I wrote: <code>ggtails</code>.</p>
<div id="max-to-sum-ratio" class="section level3">
<h3>Max-to-Sum ratio</h3>
<p>Taleb and Cirillo begin by examining the <a href="https://david-salazar.github.io/2020/06/02/lln-for-higher-p-moments/">Max-to-Sum ratio plots</a>. A consequence of the Law of Large Numbers (LLN) is the following:</p>
<p><span class="math display">\[
E[X^p] &lt; \infty  \iff R_n^p = \dfrac{max(X_1^p, \dots, X_n^p)}{\sum_{i=1}^n X_i^p} \to 0, \ \text{as} \ n \to \infty
\]</span>
That is, the theoretical moment <span class="math inline">\(p\)</span> exists if and only if the ratio of the partial max to the partial sum converges to <span class="math inline">\(0\)</span>. Neither of the fourth moments converges for neither of the fatalities’ estimates</p>
<pre class="r"><code>data %&gt;% 
  ggplot(aes(sample = avg_est)) +
  ggtails::stat_max_sum_ratio_plot() +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1)) +
  scale_color_brewer(palette = 2, type = &quot;qual&quot;) +
  labs(title = &quot;Max-to-Sum ratio plot&quot;,
       subtitle = &quot;There&#39;s no convergence. No finite moment is likely to exist&quot;)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/maxtosum-1.png" width="768" /></p>
<p>Given that none of the moments converges, it is likely that we are dealing with such a fat-tailed random variable that all of the theoretical moments are undefined. Or if the theoretical moments exist, that the Law of Large Numbers works way too slowly for us to use it. In which case any method that relies on any sample moment estimator of the empirical distribution is useless.</p>
</div>
<div id="histogram" class="section level3">
<h3>Histogram</h3>
<p>Let’s check what exactly is the range of values that Cirillo and Taleb have collected:</p>
<pre class="r"><code>data %&gt;% 
  ggplot(aes(avg_est*1000)) +
  geom_histogram(binwidth = 10^8/80, fill = &quot;dodgerblue4&quot;, color = &quot;black&quot;, alpha = 0.5) +
  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^8)) +
  labs(x = TeX(&quot;Casualties (x $10^8$)&quot;),
       title = &quot;Histogram of casualties&quot;,
       subtitle = &quot;The data encodes a huge array of variation. No characteristic scale, typical of fat-tails&quot;) </code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/histogram-1.png" width="768" /></p>
<p>The data contains an incredible arrange of variation. This is typical for fat-tailed random variables.</p>
</div>
<div id="the-zipf-plot" class="section level3">
<h3>The Zipf plot</h3>
<p>For a Pareto random variable, the slope of the log of the Survival function in log space decays linearly. With the Zipf plot, we compare the decay of the log of the empirical survival function with the linear decay that we expect with a Pareto.</p>
<pre class="r"><code>data %&gt;% 
  ggplot(aes(sample = avg_est*1000)) +
  ggtails::stat_zipf(color = &quot;dodgerblue4&quot;, alpha = 0.7)  -&gt; p

dat &lt;- layer_data(p)
p +
  geom_smooth(data = dat, aes(x = x, y = survival), inherit.aes = FALSE,
              method = &quot;lm&quot;, se = F, linetype = 2, color = &quot;red&quot;) +
  scale_x_log10(label=scientific_10) +
  scale_y_log10() +
  labs(title = &quot;Zipf plot of casualties&quot;,
       subtitle = &quot;There&#39;s the linear decay we expect with fat-tailed variables&quot;,
       x = &quot;log(x)&quot;,
       y = &quot;log(Survival)&quot;)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/loglogplot-1.png" width="768" /></p>
<p>The empirical survival function indeed decays slowly: it’s almost linear. Thus, giving us a hint that we are dealing with a fat-tailed random variable.</p>
</div>
<div id="mean-excess-plot" class="section level3">
<h3>Mean Excess Plot</h3>
<p>For a given threshold <span class="math inline">\(v\)</span>, the Mean Excess for a random variable <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[
E[X - v | X &gt; v] 
\]</span>
For a Pareto, we expect this mean excess to scale linearly with the threshold.</p>
<pre class="r"><code>data %&gt;% 
  ggplot(aes(sample = avg_est*1000)) +
  ggtails::stat_mean_excess()-&gt; p 
dat &lt;- layer_data(p)

dat %&gt;%
  filter(y &lt; 3.95e07) %&gt;% 
  ggplot() +
  geom_point(aes(x, y), color = &quot;dodgerblue4&quot;, alpha = 0.7) +
  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^6)) +
  scale_y_continuous(labels = function(x) scales::number(x, scale = 1/10^7)) +
  expand_limits(y = 5e07) +
  labs(title = &quot;Mean Excess Plot for casualties&quot;,
       subtitle = &quot;There&#39;s the linear slope that we expect with fat-tailed variables&quot;,
       caption = &quot;More volatile observations were excluded.&quot;,
       x = TeX(&quot;Threshold (x $10^6$)&quot;),
       y = TeX(&quot; Mean Excess Funtion (x $10^7$)&quot;))</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/meplot-1.png" width="768" /></p>
<p>Given that the mean excess plot increases linearly, we are even more convinced that the number of casualties is indeed fat-tailed.</p>
</div>
</div>
<div id="fitting-the-tail" class="section level2">
<h2>Fitting the tail</h2>
<p>The graphical analysis tells us that we are likely dealing with a fat-tailed random variable: the survival function decays very slowly. Thus, the fat-tails make a) an extremely large array of possibilities relevant; b) thus, eliminating the possibility of a characteristic scale or “typical” catastrophe; c) and possibly making the theoretical moments undefined.</p>
<div id="wait-a-moment-infinite-casualties" class="section level3">
<h3>Wait a moment: infinite casualties?</h3>
<p>We know that the number of casualties is bounded by the total population. Thus, the variable only has the appearance of an infinite mean given its upper bound. Graphically, by ignoring the upper bound, we are positing a continuous tail thus:</p>
<p><img src="/images/apparenttail.PNG" /></p>
<p>The difference, thus, is only relevant in the vicinity of the upper bound <span class="math inline">\(H\)</span>. One could thus keep modeling ignoring the upper bound without too many practical consequences. Nevertheless, it would be epistemologically wrong. To solve this problem, Taleb and Cirillo introduce a log transformation that eliminates the upper bound:</p>
<p><span class="math display">\[
Z = \varphi(Y)=L-H \log \left(\frac{H-Y}{H-L}\right)
\]</span></p>
<p>Taleb and Cirillo call <span class="math inline">\(Z\)</span> the dual observations. <span class="math inline">\(Z\)</span> is the variable that we will model with Extreme Value theory.</p>
<pre class="r"><code>L &lt;- 1
H &lt;- 7700000
data_to_model &lt;- data %&gt;% 
  mutate(dual = L -  H* log( (H-avg_est) / (H-L) ) )</code></pre>
</div>
<div id="extreme-value-theory-on-the-dual-observations" class="section level3">
<h3>Extreme Value theory on the dual observations</h3>
<p>A logical question, then, is how fat-tailed is exactly the tail of casualties from contagious diseases? Extreme Value Theory offers an answer. Indeed, the Pickands–Balkema–de Haan theorem states that tail events (events larger than a large threshold) have as a limiting distribution a Generalized Pareto Distribution (GPD). In math, for large u, the conditional excess function is thus defined and approximated by a GPD:</p>
<p><span class="math display">\[
G_{u}(z)=P(Z \leq z \mid Z&gt;u)=\frac{G(z)-G(u)}{1-G(u)} \approx GPD(z; \xi, \beta, u)
\]</span>
Where <span class="math inline">\(\xi\)</span> is the crucial shape parameter that determines how slowly the tail decays. The larger <span class="math inline">\(\xi\)</span>, the more slowly it decays. For example, the variance is only defined for <span class="math inline">\(\xi &lt; 0.5\)</span>. For <span class="math inline">\(\xi &gt; 1\)</span>, the theoretical mean is not defined.</p>
<p>Crucially, we can approximate the tail of the original distribution <span class="math inline">\(G(z)\)</span> with <em>a <span class="math inline">\(GPD\)</span> with the same shape parameter</em> <span class="math inline">\(\xi\)</span>. Finally, Taleb and Cirillo use <span class="math inline">\(200,000\)</span> as a threshold <span class="math inline">\(u\)</span>. We can check both in the Mean Excess Plot and the Zipf plot that around this value we observe a power-law like behavior.</p>
</div>
<div id="maximum-likelihood-estimate" class="section level3">
<h3>Maximum Likelihood estimate</h3>
<p>We can fit the GPD via maximum likelihood.</p>
<pre class="r"><code>fit &lt;- evir::gpd(data_to_model$dual, threshold = 200)
round(fit$par.ests, 2)</code></pre>
<pre><code>##      xi    beta 
##    1.62 1174.74</code></pre>
<p>Which are the same estimates as Taleb and Cirillo. The standard errors are thus:</p>
<pre class="r"><code>round(fit$par.ses, 2)</code></pre>
<pre><code>##     xi   beta 
##   0.52 534.44</code></pre>
<p>Just as we saw with our graphical analysis, the variable is definitely fat-tailed: <span class="math inline">\(\xi &gt; 0\)</span>, which thanks to the relationship of the the Pickands–Balkema–de Haan with the Fisher-Tippet theorem, tells us that the Maximum Domain of Attraction is thus a Fréchet. From Taleb and Cirillo’s paper:</p>
<blockquote>
<p>As expected <span class="math inline">\(\xi\)</span> &gt; 1 once again supporting the idea of an infinite first moment… Looking at the standard error of <span class="math inline">\(\xi\)</span>, one could also argue that, with more data from the upper tail, the first moment could possibly become finite. Yet there is no doubt about the non-existence of
the second moment, and thus about the unreliability of the sample mean, which remains too volatile to be safely used.</p>
</blockquote>
<p>Let’s see if we can reproduce this conclusions in a bayesian framework.</p>
</div>
</div>
<div id="bayesian-model" class="section level2">
<h2>Bayesian model</h2>
<p>I’ll sample from the posterior using <a href="https://mc-stan.org/users/interfaces/rstan">Stan’s</a> incredible implementation of Hamiltonian Monte Carlo. Most of the heavy lifting in Stan has already been done by Aki Vehtari in a <a href="https://mc-stan.org/users/documentation/case-studies/gpareto_functions.html">case study</a> using the GPD.</p>
<p>Our bayesian model will be thus defined:</p>
<p><span class="math display">\[
y \sim GPD(\xi, \beta, u = 200) \\
\xi \sim Normal(1, 1) \\
\beta \sim Normal(1000, 300)
\]</span>
The prior for <span class="math inline">\(\xi\)</span> is weakly informative and yet still opens the opportunity for the data to move our posterior towards a finite mean and finite variance.</p>
<div id="simulating-fake-data" class="section level3">
<h3>Simulating fake data</h3>
<p>To verify that our code is correctly working, we’ll simulate data and assess parameter recovery.</p>
<pre class="r"><code>expose_stan_functions(&quot;gpd.stan&quot;)

fake_data &lt;- replicate(1e3, gpareto_rng(ymin = 200, 0.8, 1000))
ds&lt;-list(ymin=200, N=1e3, y=fake_data)
fit_gpd &lt;- stan(file=&#39;gpd.stan&#39;, data=ds, refresh=0,
                     chains=4, seed=100, cores = 4)
print(fit_gpd, pars = c(&quot;xi&quot;, &quot;beta&quot;))</code></pre>
<pre><code>## Inference for Stan model: gpd.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean    sd   2.5%     25%     50%     75%   97.5% n_eff Rhat
## xi      0.78    0.00  0.06   0.68    0.74    0.78    0.82    0.89  1883    1
## beta 1112.40    1.55 65.45 988.48 1067.52 1110.15 1155.93 1245.06  1783    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jul 06 12:27:37 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>The credible intervals are in line with the true parameters. Graphically:</p>
<pre class="r"><code>posterior &lt;- as.matrix(fit_gpd, pars = c(&quot;xi&quot;, &quot;beta&quot;))
true &lt;- c(0.8, 1000)
mcmc_recover_hist(posterior, true) +
  labs(title = &quot;Parameter recovery&quot;,
       subtitle = &quot;We successfully recover the parameters&quot;)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/par-recover-1.png" width="768" /></p>
<p>We can thus reasonably recover our parameter of interest <span class="math inline">\(\xi\)</span> with our current model. Therefore, we can follow along and fit our model to the real data.</p>
</div>
<div id="fitting-the-model" class="section level3">
<h3>Fitting the model</h3>
<p>Using the dual observations, we end up with <span class="math inline">\(25/72\)</span> observations, around <span class="math inline">\(34.7\)</span>% of the total number of observations</p>
<pre class="r"><code>data_to_model %&gt;% 
  filter(dual &gt; 200) %&gt;% 
  pull(dual) -&gt; dual_observations
summary(dual_observations)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     200     800    1500   14040    7504  138742</code></pre>
<p>Fitting the model is just as simple as fitting it to fake data:</p>
<pre class="r"><code>ds&lt;-list(ymin=200, N=25, y=dual_observations)
fit_gpd &lt;- stan(file=&#39;gpd.stan&#39;, data=ds,
                     chains=4, seed=100, cores = 4, iter = 5000)
print(fit_gpd, pars = c(&quot;xi&quot;, &quot;beta&quot;))</code></pre>
<pre><code>## Inference for Stan model: gpd.
## 4 chains, each with iter=5000; warmup=2500; thin=1; 
## post-warmup draws per chain=2500, total post-warmup draws=10000.
## 
##         mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat
## xi      1.70    0.01   0.43   0.98   1.39    1.65    1.96    2.65  6162    1
## beta 1079.97    3.31 247.72 616.34 909.10 1073.36 1242.28 1584.00  5598    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jul 06 12:28:10 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>For know, the <span class="math inline">\(\widehat R\)</span> values look OK, which indicates that there’s no much disagreement between the Markov Chains. The credible intervals rule out the possibility that <span class="math inline">\(\xi &lt; 0\)</span>.</p>
<p>We can intuitively interrogate the posterior for more precise questions in a Bayesian settings. For example, what is the probability that the theoretical mean is undefined, i.e., that <span class="math inline">\(\xi &gt; 1\)</span>:</p>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/theoretical_mean-1.png" width="768" /></p>
<p>Therefore, we conclude that it is very likely that <strong>the theoretical mean is undefined and we rule out definitely the possibility of a finite second moment</strong>. Thus, we reproduce the paper’s conclusions.</p>
<p>Given the fat-tailedness of the data and the resulting lack of characteristic scale from the huge array of variability that it’s possible, there’s no <em>possibility</em> of forecasting what we may face with either a pointwise prediction or a distributional forecast.</p>
</div>
<div id="posterior-predictive-checks" class="section level3">
<h3>Posterior predictive checks</h3>
<p>If the model fits well to the data, the replicated data under the model should look similar to the observed data. We can easily generate data from our model because our model is generative: we draw simulated values from the posterior predictive distribution of replicated data.</p>
<p>However, with <em>fat-tailed</em> data: what exactly does it mean for our replicated data to track our observed data. For example, what about the <strong>replicated maxima</strong>? These definitely should include the observed maxima: <strong>but also much larger values. That is the whole point of being in the MDA of a Fréchet.</strong></p>
<pre class="r"><code>yrep &lt;- extract(fit_gpd)$yrep 
ppc_stat(log(dual_observations), log(yrep), stat = &quot;max&quot;) +
  labs(title = &quot;Posterior predictive check (log scale)&quot;,
       subtitle = &quot;Maxima across replicated datasets track observed maximum and beyond, just like it should&quot;,
       x = &quot;log(maxima)&quot;)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/ppc-1.png" width="768" /></p>
<p>And this is exactly what our model does: a difference of 30 in a log-scale is huge. We can just as well expect maxima as large as observed, and even much larger.</p>
</div>
<div id="convergence-diagnostics" class="section level3">
<h3>Convergence diagnostics</h3>
<p>One of the main benefits of Stan’s implementation of HMC is that it yells at you when things have gone wrong. We can thus check a variety of diagnostics to check for convergence. Above, we examined that the <span class="math inline">\(\widehat R\)</span> values look OK. We can also check trace plots:</p>
<pre class="r"><code>posterior &lt;- as.array(fit_gpd)
color_scheme_set(&quot;viridis&quot;)
mcmc_trace(posterior, pars = c(&quot;xi&quot;, &quot;beta&quot;)) +
  labs(title = &quot;Traceplots&quot;,
       subtitle = &quot;Traceplots are stationary, well mixed and the chains converge&quot;)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/traceplot-1.png" width="768" /></p>
<p>We can check HMC specific diagnostics:</p>
<pre class="r"><code>check_divergences(fit_gpd)</code></pre>
<pre><code>## 0 of 10000 iterations ended with a divergence.</code></pre>
<pre class="r"><code>check_treedepth(fit_gpd)</code></pre>
<pre><code>## 0 of 10000 iterations saturated the maximum tree depth of 10.</code></pre>
</div>
</div>
<div id="stressing-the-data" class="section level2">
<h2>Stressing the data</h2>
<p>Taleb and Cirillo are well aware that they are not working with the more precise of data. Thus, they ‘stress’ the data and check whether the results still hold. Given that we are working with fat-tailed data, the tail wags the dog: data problems can only modify the results if they are in the tail.</p>
<div id="measurement-error" class="section level3">
<h3>Measurement error</h3>
<p>To account for measurement error, Taleb and Cirillo recreate 10,000 of their datasets, but this time where each observation is allowed to vary between 80% and 120% of its recorded values. They find that their results are robust to this modification.</p>
<p>In a bayesian setting, we can very easily extend our model to account for uncertainty around the true data. Indeed, in a bayesian model there’s no fundamental difference between a <em>parameter and an observation</em>: one is observed and the other is not. Thus, we formulate the true casualties being measured as <a href="https://mc-stan.org/docs/2_18/stan-users-guide/bayesian-measurement-error-model.html">missing data</a>.</p>
<p>Therefore, we specify that the measurement comes a normal with unknown mean, the true number of casualties, and that their standard deviation is 20% of the observed value.</p>
<p><span class="math display">\[
y \sim Normal(y_{true}, y*0.2)
\]</span>
We fit the model and let Bayes do the rest:</p>
<pre class="r"><code>ds&lt;-list(ymin=200, N=25, y=dual_observations, noise = 0.2*dual_observations)
fit_gpd_m &lt;- stan(file=&#39;gpd_measurementerror.stan&#39;, data=ds,
                     chains=4, seed=100, cores = 4, iter = 5000)
print(fit_gpd_m, pars = c(&quot;xi&quot;, &quot;beta&quot;))</code></pre>
<pre><code>## Inference for Stan model: gpd_measurementerror.
## 4 chains, each with iter=5000; warmup=2500; thin=1; 
## post-warmup draws per chain=2500, total post-warmup draws=10000.
## 
##         mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat
## xi      1.67    0.00   0.43   0.96   1.36    1.62    1.93    2.63 15658    1
## beta 1056.12    2.14 247.68 591.61 885.15 1050.35 1218.61 1563.95 13362    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jul 06 12:28:21 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>The model sampled remarkably well. We can thus check whether our inferences still hold:</p>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/infer-measurement-1.png" width="768" /></p>
</div>
</div>
<div id="influential-observations" class="section level2">
<h2>Influential observations</h2>
<p>Taleb and Cirillo also stress their data by recreating the dataset 10,000 times and then eliminating from 1 to 7 of the observations with a jacknife resampling procedure. Thus, they confirm that no single observation is driving the inference.</p>
<p>In a bayesian setting, we can check for influential observations by <em>comparing</em> the <strong>full-data predictive posterior distribution to the Leave-one-out predictive posterior for each left out point</strong>. That is: we compare <span class="math inline">\(p(y_i | y)\)</span> with <span class="math inline">\(p(y_i, | y_{-i})\)</span>.</p>
<p>We can quickly estimate <span class="math inline">\(p(\theta_i, | y_{-i})\)</span> for each <span class="math inline">\(i\)</span> by just sampling from <span class="math inline">\(p(\theta | y)\)</span> using <a href="https://mc-stan.org/loo/reference/psis.html">Pareto Smoothed Importance Sampling LOO</a>. By examining the distribution of the importance weights, we can compare how different the two distributions are. If the weights are too fat-tailed, and the variance is infinite for the <span class="math inline">\(j\)</span>th observation, then the <span class="math inline">\(j\)</span>th observation is highly <a href="https://arxiv.org/pdf/1709.01449.pdf">influential observation determining our posterior distribution</a>.</p>
<pre class="r"><code>loglik &lt;- extract(fit_gpd)$log_lixi
loopsis &lt;- loo::loo(loglik, loo::relative_eff(exp(loglik)))</code></pre>
<pre><code>## Warning: Relative effective sample sizes (&#39;r_eff&#39; argument) not specified.
## For models fit with MCMC, the reported PSIS effective sample sizes and 
## MCSE estimates will be over-optimistic.</code></pre>
<pre class="r"><code>plot(loopsis)</code></pre>
<p><img src="/post/2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Indeed, no single observation is driving our inference.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Taleb and Cirillo show that the number of casualties are patently fat-tailed. Thus, there’s no typical catastrophe nor characteristic scale: a huge array of possibilities are likely and relevant for any analysis. Thus, there’s no <em>possibility</em> of forecasting what we may face with either a pointwise prediction or a distributional forecast.</p>
<p>Indeed, we cannot even use our sample mean to do anything useful. Given the asymmetric risks involved and the huge uncertainty, the whole of Taleb’s work is very clear: we must kill the pandemic in the egg.</p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

