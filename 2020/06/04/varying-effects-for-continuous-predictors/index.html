<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Varying effects for continuous predictors -&gt; GP regression - Dilettanting Data Science</title>
<meta property="og:title" content="Varying effects for continuous predictors -&gt; GP regression - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">14 min read</span>
    

    <h1 class="article-title">Varying effects for continuous predictors -&gt; GP regression</h1>

    
    <span class="article-date">2020/06/04</span>
    
    

    <div class="article-content">
      


<p><a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> is a fabulous course on Bayesian Statistics (and much more). Following its presentation, I’ll give a succinct intuitive introduction to <strong>Gaussian Process (GP) regression as a method to extend the varying effects strategy to continuous predictors</strong>. This method is incredibly useful when assuming a linear functional relationship between a continuous predictor and the outcome variable is not enough to capture the variation in the data.</p>
<p>First, I’ll begin by motivating the varying effects strategy. Secondly, I’ll show the intuitive problems of <em>naively</em> extending it to continuous predictors. Thirdly, I’ll show intuitively how the Gaussian Process (GP) regression takes into account these problems when faced with a continuous category. Finally, I’ll show an example of GP regression with simulated data using <code>stan</code>.</p>
<div id="varying-effects-strategy" class="section level2">
<h2>Varying Effects Strategy</h2>
<p>The varying effects strategy consists in estimating a <em>unique parameter for each cluster in the data as coming from a common distribution</em>. This, in turn, allows us to <strong>pool information across clusters</strong> whose end result is to shrink our predictions toward the common distribution’s mean. The amount of shrinkage, then, will itself be learnt from the data. Here is a <a href="https://david-salazar.github.io/2020/05/28/simulating-into-understanding-multilevel-models/">blogpost</a> I wrote trying to understand this fundamental idea.</p>
<p>We can extend this idea by letting different parameter types vary by cluster. For example, an intercept and a slope. Then, we can model them (every pair of intercept and slope) as coming from a common multivariate distribution. The <strong>covariance in this distribution will let us pool not only across clusters but also across parameter types</strong>. Here is another <a href="2020-06-01-understanding-pooling-across-intercepts-and-slopes.html">blogpost</a> of mine where I simulated data to understand how this works.</p>
</div>
<div id="naively-extending-it-ignores-covariances" class="section level2">
<h2>Naively extending it ignores covariances</h2>
<p>Notice that so far we only vary the effects across discrete, exchangeable values that represent different clusters. For example, across classrooms in a school. There’s no meaningful order in the indices: classroom 1 and classroom 2 are just labels.</p>
<p>However, with a continuous variable there’s an order. For example, different ages: 20 years old, 21 years old and 80 years olds have an internal order with specific distances among them (i.e., the age difference); they are, unlike our clusters, not exchangeable. Therefore, <em>were we to estimate a unique parameter for each possible age as coming from a joint distribution</em>, we would be saying that 40 years olds should inform our estimate of the 20 years olds just as much 21 year olds. This is clearly not a good idea: <strong>the varying effects strategy should take into account the distance between the ages to determine how much information is pooled across different ages</strong>. The closer the ages, the more information we should pool among them; the farther they are, the less we should pool.</p>
</div>
<div id="covariances-as-function-of-distances" class="section level2">
<h2>Covariances as function of distances</h2>
<p>GP regression does precisely this. For example, continuing with the age example, Gaussian Process regression <strong>varies the effect of age</strong> for each unique age value in the sample <em>such that</em> the observations with <strong>similar ages have similar estimated effects</strong>. The heart of the method lies in positing a multivariate normal prior for each unique age effect and explicitly modelling the covariance among these effects as a function of the distance between the ages. In math:</p>
<p><span class="math display">\[ \begin{bmatrix} \mu_1 \\ \mu_2  \\ \cdots \\ \mu_N \end{bmatrix} = MultiNormal(\begin{bmatrix} 0 \\ 0  \\ 0 \\ 0 \end{bmatrix}, K)   \]</span></p>
<p>Where <span class="math inline">\(K\)</span> is the covariance matrix between each observation’s unique age effect. The covariances are modeled as function of the ages’ distance with a kernel function:</p>
<p><span class="math display">\[ K_{ij} = \eta^2 \text{exp}(-\rho^2 D^2_{ij}) \]</span></p>
<p>Where <span class="math inline">\(D_{ij}\)</span> is the age difference between the observations. Thus, <strong>the covariance between two unique age effects exponentially decreases with the distance between the ages</strong>. Therefore, we will <em>pool much more</em> for <strong>individuals with similar ages and pool much less for individuals with wider age gaps</strong>. Indeed, this is the key to avoid overfitting to the data. This is done through two parameters: <span class="math inline">\(\eta, \rho\)</span>. <span class="math inline">\(\eta\)</span> controls how strong the covariances between observations can be; <span class="math inline">\(\rho\)</span> controls how quickly the covariances can change. As usual with bayesian estimation, we’ll have a posterior distribution for both of them.</p>
<p>Notice that this varying effects strategy to each unique age value, whilst modeling the covariance between these unique effects, is equivalent to defining a functional relationship between age and the outcome variable at any age. Indeed, for any unseen age, we just let the covariances between all the seen ages’ effects (and the age gap to this new value) determine what should be the effect for this new unique value of age. That is, each unique value of <span class="math inline">\(\eta, \rho\)</span> defines a unique functional relationship between age and the outcome variable. Therefore, setting a Gaussian Process <strong>is equivalent to setting up a prior over infinitely different functions for the overall functional relationship between age and the outcome variable</strong>.</p>
<p>This is the reason why Gaussian Process regression is so useful. We do not prefer a linear or quadratic functional relationship for the relationship between an outcome variable and a continuous predictor. <strong>We let the data decide which functional relationship is the most consistent with the data</strong>.</p>
</div>
<div id="gp-regression-simulated-cohort-voting-for-two-regions" class="section level1">
<h1>GP Regression: simulated cohort voting for two regions</h1>
<div id="the-simulation" class="section level2">
<h2>The simulation</h2>
<p>Let’s check how these ideas work in practice with a simulated example. This will be loosely based on a <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/cohort_voting_20191017.pdf">paper</a> by Andrew Gelman et alter. Imagine you are modeling cohort voting. Therefore, you are trying <em>to estimate the probability of people voting</em> for the democratic party given their age. Intuitively, individuals with similar ages experienced the same events at the same preference defining moments and were therefore shaped by them in the same way. However, for different cohorts, the events that shaped them were different. Thus, some cohorts are biased towards voting for the democratic party, whilst others the opposite.</p>
<p>To make things a bit more interesting, there is a division in the data from two different regions. This means that the cohort effect varies by region. Although they experienced the same events in both regions, in the north some events were seen favorable whereas in the south the events may be seen negatively. Therefore, we will posit two different Gaussian Processes for each of the regions.</p>
<p>Here, I simulate the (two) GPs to determine the probability of voting for the democratic party for any given age with <code>stan</code>. The code is slightly altered from Michael Betancourt’s excellent <a href="https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html#33_simulating_from_a_gaussian_process_conditional_on_non-gaussian_observations">post</a> about Gaussian processes. Then, we simulate 5000 bernoulli r.v. according to these probabilities for each region. The <code>stan</code> code is the following:</p>
<pre class="r"><code>writeLines(readLines(&quot;logit_sim_regions.stan&quot;))</code></pre>
<pre><code>## 
## data {
##   int&lt;lower=1&gt; N_north;
##   int&lt;lower=1&gt; N_south;
##   real x_north[N_north];
##   real x_south[N_south];
## 
##   real&lt;lower=0&gt; rho_north;
##   real&lt;lower=0&gt; rho_south;
##   real&lt;lower=0&gt; eta_north;
##   real&lt;lower=0&gt; eta_south;
## }
## 
## transformed data {
##   matrix[N_north, N_north] cov_north = cov_exp_quad(x_north, eta_north, rho_north)
##                      + diag_matrix(rep_vector(1e-10, N_north));
##   matrix[N_north, N_north] L_cov_north = cholesky_decompose(cov_north);
##   
##   matrix[N_south, N_south] cov_south = cov_exp_quad(x_south, eta_south, rho_south)
##                      + diag_matrix(rep_vector(1e-10, N_south));
##                      
##   matrix[N_south, N_south] L_cov_south = cholesky_decompose(cov_south);
##   
## }
## 
## parameters {}
## model {}
## 
## generated quantities {
##   vector[N_north] f_north = multi_normal_cholesky_rng(rep_vector(0.5, N_north), L_cov_north);
##   vector[N_north] y_north;
##   
##   vector[N_south] f_south = multi_normal_cholesky_rng(rep_vector(-0.5, N_south), L_cov_south);
##   vector[N_south] y_south;
##   
##   for (n in 1:N_north) {
##     y_north[n] = bernoulli_logit_rng(f_north[n]);
##   }
##   
##   for (n in 1:N_north) {
##     y_south[n] = bernoulli_logit_rng(f_south[n]);
##   }
##   
## }</code></pre>
<p>We’ll simulate with the following parameters for the two different gaussian processes:</p>
<pre class="r"><code>eta_true_south &lt;- 0.5
rho_true_south &lt;- 10

eta_true_north &lt;- 2.2
rho_true_north &lt;- 1.5</code></pre>
<p>Notice the different combination of parameters between regions. For the south, the maximum covariance is much smaller as a result of the smaller <span class="math inline">\(\eta\)</span>. Also, the covariances will “wiggle” much less in the south as a result of the larger <span class="math inline">\(\rho\)</span>.</p>
<p>The results of the simulation are two different GP to determine the probability of voting for the democratic party at every possible age:</p>
<p><img src="/post/2020-06-04-varying-effects-for-continuous-predictors_files/figure-html/gplotlinear-1.png" width="768" /></p>
<p>Notice that in both regions there’s a non-linear effect of age. However, the cohort effects vary much more in the north. This is a direct result of the lower <span class="math inline">\(\rho\)</span> that we used to simulate the data. These different cohort effects across regions appear partially in our data thus:</p>
<p><img src="/post/2020-06-04-varying-effects-for-continuous-predictors_files/figure-html/stacked-1.png" width="672" /></p>
</div>
<div id="naive-multilevel-model" class="section level2">
<h2>Naive multilevel model</h2>
<p>A naive way to model this data would be to create a multilevel model where we model a region-specific intercept and a region specific slope for age. However, <strong>this would not take into account the highly non-linear responses to age across different cohorts</strong>, as it assumes a monotonic relationship between age and the probability of voting democrat. We can code this model thus:</p>
<pre class="r"><code>model_naive &lt;- ulam(
  alist(
    vote_dem ~ bernoulli(p),
    logit(p) &lt;- intercept[region] + age_slope[region]*age,
    c(intercept, age_slope)[region] ~ multi_normal(c(a, b), Rho, sigma_region),
    a ~ normal(0, 1),
    b ~ normal(0, 1),
    sigma_region ~ exponential(1),
    Rho ~ lkj_corr(2)
  ),
  data = data_model, chains = 4, cores = 4,
  iter = 2000, log_lik = TRUE
)</code></pre>
<p>Let’s check the posterior’s implied cohort effects against the original GP using the great <code>tidybayes</code> package:</p>
<p><img src="/post/2020-06-04-varying-effects-for-continuous-predictors_files/figure-html/poorfit-1.png" width="768" /></p>
<p>As expected, this naive model does not capture the highly non-linear cohort voting effects present in the data. That is, the fit is too general; <strong>we should let the effect of age on the probability of voting for the democratic party vary more across each unique age value.</strong> A <em>possible answer</em> would be to include square and cubic terms of age (and quartic, and … <em>where to stop?</em>) in the linear model. However, that is very likely to result in overfitting for the south. Then, we would have to let the linear, square and cubic terms vary by region? This strategy’s complexity is growing more and more. It is just neither scalable nor reliable. A cleaner answer is to let the <strong>influence of age on the probability of voting for the Democrats vary by each unique age and region</strong>. That is, we must <em>extend the varying effects strategy</em> to include a continuous predictor.</p>
</div>
<div id="positing-two-gaussian-processes" class="section level2">
<h2>Positing two Gaussian processes</h2>
<p>A better strategy than the naive multilevel model is to use two different Gaussian Processes (one for each region) to vary the cohort effects by age. Notice that by <em>letting the effect vary by age and constraining the amount of pooling across of observations by their age gap</em>, <strong>we are letting the data specify the functional relationship of cohort voting for each region</strong>. Instead of stating a pre-specified functional relationship, we let the data find the best functional relationship by modeling the covariance between each unique age-specific effect.</p>
<p>Our first step, then, is to set up two different age distance matrices: one for the north, other for the south.</p>
<pre class="r"><code>distance_matrix &lt;- function(x) {
  
  matrix_distance &lt;- matrix(nrow = 200, ncol = 200)
  
  for (row in 1:length(x)) {
    for (col in 1:length(x)) {
      matrix_distance[row, col] = abs(x[row] - x[col])
    }
  }
  matrix_distance
}

simulated_for_model %&gt;% 
  filter(region_int == 1) %&gt;% 
  pull(age) %&gt;% 
  distance_matrix() -&gt; dist_north

simulated_for_model %&gt;% 
  filter(region_int == 2) %&gt;% 
  pull(age) %&gt;% 
  distance_matrix() -&gt; dist_south</code></pre>
<p>Finally, to extend the varying effects strategy it’s a matter of simply setting up a unique parameter for each combination of age value and region, and let the modeling of the covariances do the rest. Using <code>rethinking::ulam</code> we can fit the two Gaussian Processes for both regions (with non centered parametrizations) thus:</p>
<pre class="r"><code>model_gps &lt;- ulam(
  alist(
    vote_dem ~ bernoulli(p),
    logit(p) &lt;- intercept[region] + (2-region)*k_north[age_id] + (region - 1)*k_south[age_id],
    # north gp
    transpars&gt; vector[200]:k_north &lt;&lt;- L_SIGMA_north *z,
    vector[200]: z ~ normal(0, 1),
    transpars&gt; matrix[200, 200]: L_SIGMA_north &lt;&lt;- cholesky_decompose(SIGMA_north),
    transpars &gt; matrix[200, 200]: SIGMA_north &lt;- cov_GPL2(dist_north, etasq_north, rhosq_north, 0.01),
    # south gp
    transpars&gt; vector[200]:k_south &lt;&lt;- L_SIGMA_south *z_s,
    vector[200]: z_s ~ normal(0, 1),
    transpars&gt; matrix[200, 200]: L_SIGMA_south &lt;&lt;- cholesky_decompose(SIGMA_south),
    transpars &gt; matrix[200, 200]: SIGMA_south &lt;- cov_GPL2(dist_south, etasq_south, rhosq_south, 0.01),
    # priors
    intercept[region] ~ normal(0, 1),
    c(etasq_north, etasq_south) ~ exponential(0.5),
    c(rhosq_north, rhosq_south) ~ exponential(0.5)
  ), data = data_model, chains = 4, cores = 4, iter = 200, log_lik = TRUE,
  sample = FALSE
)</code></pre>
<p>Let’s check the posterior’s implied cohort effects against the original Gaussian Processes from which we sampled:</p>
<p><img src="/post/2020-06-04-varying-effects-for-continuous-predictors_files/figure-html/gpsplot-1.png" width="768" /></p>
<p>Indeed, <strong>by letting each unique age value have a unique influence on the probability of voting democratic, and modeling the covariances between these effects</strong>, we have achieved a much better fit with respect to the true cohort effects. This is specially true for the cohort voting effects in the north. That is, <em>GP regression is more useful when the underlying functional relationship between the predictor and outcome variable is highly non-linear</em>.</p>
<p>An alternative way to visualize the different cohort effects across regions is to plot the <strong>posterior of the different covariance functions for each respective Gaussian Process</strong>. That is, we can visualize how quickly the covariances decrease in both regions. For the north, we expect <em>the covariances to very quickly decrease to zero</em>: this is why we can predict such different age effects for the different cohorts.</p>
<p><img src="/post/2020-06-04-varying-effects-for-continuous-predictors_files/figure-html/covplots-1.png" width="960" />
Finally, we can also compare the predictive accuracy out of sample using information criteria for both the naive model and the model using Gaussian Processes:</p>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#nxrlkkkwmi .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#nxrlkkkwmi .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nxrlkkkwmi .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#nxrlkkkwmi .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#nxrlkkkwmi .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nxrlkkkwmi .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#nxrlkkkwmi .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#nxrlkkkwmi .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#nxrlkkkwmi .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#nxrlkkkwmi .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#nxrlkkkwmi .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#nxrlkkkwmi .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#nxrlkkkwmi .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#nxrlkkkwmi .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#nxrlkkkwmi .gt_from_md > :first-child {
  margin-top: 0;
}

#nxrlkkkwmi .gt_from_md > :last-child {
  margin-bottom: 0;
}

#nxrlkkkwmi .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#nxrlkkkwmi .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#nxrlkkkwmi .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nxrlkkkwmi .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#nxrlkkkwmi .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#nxrlkkkwmi .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#nxrlkkkwmi .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#nxrlkkkwmi .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nxrlkkkwmi .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#nxrlkkkwmi .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#nxrlkkkwmi .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#nxrlkkkwmi .gt_left {
  text-align: left;
}

#nxrlkkkwmi .gt_center {
  text-align: center;
}

#nxrlkkkwmi .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#nxrlkkkwmi .gt_font_normal {
  font-weight: normal;
}

#nxrlkkkwmi .gt_font_bold {
  font-weight: bold;
}

#nxrlkkkwmi .gt_font_italic {
  font-style: italic;
}

#nxrlkkkwmi .gt_super {
  font-size: 65%;
}

#nxrlkkkwmi .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="nxrlkkkwmi" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">model</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">WAIC</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">SE</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">dWAIC</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">dSE</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">pWAIC</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">weight</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr>
      <td class="gt_row gt_left">model_gps</td>
      <td class="gt_row gt_right">508.16</td>
      <td class="gt_row gt_right">14.20</td>
      <td class="gt_row gt_right">0.00</td>
      <td class="gt_row gt_right">NA</td>
      <td class="gt_row gt_right">19.89</td>
      <td class="gt_row gt_right">1.00</td>
    </tr>
    <tr>
      <td class="gt_row gt_left">model_naive</td>
      <td class="gt_row gt_right">529.14</td>
      <td class="gt_row gt_right">10.88</td>
      <td class="gt_row gt_right">20.99</td>
      <td class="gt_row gt_right">9.96</td>
      <td class="gt_row gt_right">4.30</td>
      <td class="gt_row gt_right">0.00</td>
    </tr>
  </tbody>
  
  
</table></div>
<p>Indeed, we gain a lot of predictive accuracy by positing a unique effect for each unique age value and region combination. Even <strong>if we are creating around extra 400 parameters, we are not overfitting to the data</strong>. This is possible by virtue of modeling the covariances across the different age values’ unique effects as a function of the distance between the ages.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Gaussian Process Regression is a way to extend the varying effects strategy to continuous predictors. We do this by modeling the influence of each unique value in the continuous predictor with its own effect <em>and</em> simultaneously modeling the covariance of these unique effects as a function of the distance between the values in the continuous predictor scale. This is equivalent to let the data model the functional relationship between the continuous predictor and the outcome variable. Therefore, this is incredibly useful when assuming a functional linear relationship is not enough.</p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

