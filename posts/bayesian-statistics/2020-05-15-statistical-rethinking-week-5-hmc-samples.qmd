---
title: Statistical Rethinking Week 5 -> HMC samples
author: David Salazar
date: '2020-05-15'
slug: statistical-rethinking-week-5-hmc-samples
categories: []
tags: []
aliases: 
  - ../../2020/05/15/statistical-rethinking-week-5-hmc-samples/
---

```{r message=TRUE, warning=TRUE, include=FALSE}
library(rethinking)
library(ggridges)
library(tidyverse)
extrafont::loadfonts(device="win") 
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=corei7 -mtune=corei7')
set.seed(42)
```

# Statistical Rethinking: Week 5

After a quick tour around interactions, this week was a quick introduction to MCMC samplers and how they are the engine that powers current Bayesian modelling. We looked at Metropolis, Gibbs and finally HMC. Not only HMC is more efficient, but it also let us know when it fails. Let's tackle the homework with these new tools:

# Homework 5

### Problem Week 1

```{r}
data("Wines2012")

wines <- Wines2012
wines %>% 
  count(judge)
```
We have 9 judges and each of them gave 20 reviews. Let's check the scores

```{r}
wines %>% 
  ggplot(aes(score)) +
  geom_histogram(binwidth = 1, color = "black", fill = "dodgerblue4", alpha = 0.5) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = "Score original distribution")
```

0 should be a meaningful metric. As well as 1. Let's perform feature scaling on score:

```{r}
wines %>% 
  mutate(score_std = (score-min(score))/(max(score) - min(score)) ) -> wines_scaled
wines_scaled %>% 
  ggplot(aes(score_std)) +
  geom_histogram(binwidth = 0.1, color = "black", fill = "dodgerblue4", alpha = 0.5) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = "Score distribution scaled")
```

Let's analyze our predictor variables: judge and wine

```{r}
wines_scaled %>% 
  count(judge.amer)
```

```{r}
wines_scaled %>% 
  count(wine)
```

Let's create an index variable:

```{r}
wines_scaled %>% 
  mutate(judge = as.factor(judge.amer),
         judge = as.integer(judge)) %>%
  count(judge, judge.amer)
```

```{r}
wines_scaled %>% 
  mutate(wine_fct = as.factor(wine),
         wine_fct = as.integer(wine_fct)) %>% 
  count(wine_fct, wine)
```

```{r}
wines_scaled %>% 
  mutate(wine_fct = as.factor(wine),
         wine_fct = as.integer(wine_fct),
         judge = as.factor(judge),
         judge = as.integer(judge)) -> wines_to_model 
```

> Consider only variation among judges and wines

```{r}
wine_to_ulam <- list(
  score = wines_to_model$score_std,
  judge = wines_to_model$judge,
  wine = wines_to_model$wine_fct
)

model_judge_wine <- ulam(
  alist(
    score ~ dnorm(mu, sigma),
    mu <- j[judge] + w[wine],
    j[judge] ~ dnorm(0.5, 0.1),
    w[wine] ~ dnorm(0.5, 0.1),
    sigma ~ dexp(1)
  ),
  data = wine_to_ulam,
  chains = 4, cores = 4
)
```

Let's check how the sampling of the posterior went:

```{r}
traceplot_ulam(model_judge_wine)
```

Our chains have all the characteristics of healthy chains:

1. They are stationary.
2. They mix well the the entire parameter space.
3. They converge to explore the same parameter space across chains.

```{r}
precis(model_judge_wine, depth = 2)
```

The Rhat is ok for all the parameters. 

The table is just not informative. Let's do some ggridges for both judges and wines.

```{r message=FALSE, warning=FALSE}
levels_judges <- levels(as.factor(wines$judge))

samples <- extract.samples(model_judge_wine)
samples_judges <- samples$j
data.frame(samples_judges, sim = 1:2000) %>% 
  pivot_longer(-sim, names_to = "judge", values_to = "score") %>% 
  mutate(judge = str_extract(judge, "\\d"),
         judge = levels_judges[as.integer(judge)]) %>%
  ggplot(aes(score, judge)) +
  geom_density_ridges(fill = "deeppink4") +
  hrbrthemes::theme_ipsum_rc(grid = "x") +
  labs(title = "Expected Wine Score by Judge")
```

It seems that John Foy was, on average, according to our data and assumptions, gave the highest scores across wines. Whilst Robert Hodgson was the one who gave the least favorable scores. 

```{r message=FALSE, warning=FALSE, figure1, fig.width=10, fig.height=10}
samples_wines <- samples$w

levels_wines <- levels(as.factor(wines$wine))


data.frame(samples_wines, sim = 1:2000) %>% 
  pivot_longer(-sim, names_to = "wines", values_to = "score") %>% 
  mutate(wines = str_extract(wines, "\\d++"),
         wines = levels_wines[as.integer(wines)]) %>% 
  ggplot(aes(score, wines)) +
  geom_density_ridges(fill = "deeppink4") +
  hrbrthemes::theme_ipsum_rc(grid = "x") +
  labs(title = "Expected Wine Score by Wine")
```

It seems that we expect, on average, wines B2 and J2 to be scored the highest across all judges. Whilst the lowest we expect it to be I2. 

> Now, consider three features of the wines and judges:

1. flight
1. wine.amer
1. judge.amer


```{r}
wines_scaled %>% 
  mutate(flight_int = as.integer(flight),
         wine.amer_int = as.integer(as.factor(wine.amer)),
         judge.amer_int = as.integer(as.factor(judge.amer))) -> wines_scaled_problem_2
```


```{r}
wines_scaled_problem_2 %>% 
  count(flight, flight_int)
```

There are equal number of wines

```{r}
wines_scaled_problem_2 %>% 
  count(wine.amer, wine.amer_int)
```

There are more american wines.

```{r}
wines_scaled_problem_2 %>% 
  count(judge.amer, judge.amer_int)
```

There are more american judges' scores. 

```{r}
data_problem_two <- list(
  score = wines_scaled_problem_2$score_std,
  flight = wines_scaled_problem_2$flight_int,
  wineamer = wines_scaled_problem_2$wine.amer_int,
  judgeamer = wines_scaled_problem_2$judge.amer_int
)

model_flight_nationality <- ulam(
  alist(score ~ dnorm(mu, sigma),
        mu <- f[flight] + wa[wineamer] + ja[judgeamer],
        f[flight] ~ dnorm(0.5, 0.1),
        wa[wineamer] ~ dnorm(0.5, 0.1),
        ja[judgeamer] ~ dnorm(0.5, 0.1),
        sigma ~ dexp(1)
        ),
  data = data_problem_two,
  chains = 4, cores = 4,
  iter = 2000,
)

traceplot_ulam(model_flight_nationality)
```

These chains look healthy. They:

1. Are stationary.
2. Mix well.
3. All chains converge.

```{r}
precis(model_flight_nationality, depth = 2)
```

Let's visualize the differences between flights:

```{r message=FALSE, warning=FALSE}
levels_flights = levels(wines$flight)

samples_problem_2 <- extract.samples(model_flight_nationality)
samples_flights <- samples_problem_2$f

data.frame(sim = 1:2000, samples_flights) %>% 
  pivot_longer(-sim, names_to = "flight", values_to = "score") %>% 
  mutate(flight = str_extract(flight, "\\d+"),
         flight = levels_flights[as.integer(flight)]) %>% 
  ggplot(aes(score, flight, fill = flight)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = "X") +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("deeppink4", "lightgoldenrod2")) +
  labs(title = "Expected Red and White wines' score")
```

On expectation, there does not seem to be a difference between the red's score and the white's score. 

```{r message=FALSE, warning=FALSE}

samples_judge_amer <-samples_problem_2$ja

data.frame(sim = 1:2000, samples_judge_amer) %>% 
  pivot_longer(-sim, names_to = "judge_american", values_to = "score") %>% 
  mutate(judge_american = str_extract(judge_american, "\\d+"),
         judge_american = ifelse(judge_american == 1, "Not American", "American")) %>% 
  ggplot(aes(score, judge_american)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = "X") +
  labs(title = "Expected American and Non american judges' scores")
```

American judges, on average, tend to give higher scores. 

```{r}
data.frame(sim = 1:2000, samples_problem_2$wa) %>% 
  pivot_longer(-sim, names_to = "wine_american", values_to = "score") %>% 
  mutate(wine_american = str_extract(wine_american, "\\d+"),
         wine_american = ifelse(wine_american == 1, "Not American", "American")) %>% 
  ggplot(aes(score, wine_american, fill = wine_american)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = "X") +
  labs(title = "Expected American and Non american wine' scores",
       fill = "") +
  scale_fill_brewer(palette = "Set1")
```

Non-American wines tend to get higher scores on average. 

> Now consider two way intercations among the three features

```{r}
wines_scaled %>% 
  mutate(flight_int = as.integer(flight) - 1) -> wines_problem_3

data_problem_three = list(
  score = wines_problem_3$score_std,
  flight = wines_problem_3$flight_int,
  wa = wines_problem_3$wine.amer,
  ja = wines_problem_3$judge.amer
)
```


```{r}
model_interactions <- ulam(alist(
  score ~ dnorm(mu, sigma),
  mu <- a+ w*wa + j*ja + f*flight + wj*wa*ja + wf*wa*flight +jf*ja*flight,
  a ~ dnorm(0, 0.1),
  w ~ dnorm(0.5, 0.1),
  j ~ dnorm(0.5, 0.1),
  f ~ dnorm(0.5, 0.1),
  wj ~ dnorm(0, 0.1),
  wf ~ dnorm(0, 0.1),
  jf ~ dnorm(0, 0.1),
  sigma ~ dexp(1)
), data = data_problem_three,
chains = 4, cores = 4, iter = 2000)

traceplot_ulam(model_interactions)
```

These chains look healthy, i.e., they:

1. They are stationary
2. They mix well over the parameter space.
3. Different chains converge to explore the same ridges of parameter space. 

```{r}
precis(model_interactions)
```

The Rhat is ok for all the parameters. However, now that we have interactions, it is not easy nor intuitive to analyze parameters on their own scale. We must compare them on the outcome scale. Let's create predictions for all eight possible values

```{r}
all_combinations <- tribble(~wa, ~ja, ~flight,
                            1, 1, 1,
                            1, 1, 0,
                            1, 0, 1,
                            1, 0, 0,
                            0, 1, 1,
                            0, 1, 0,
                            0, 0, 1,
                            0, 0, 0)
all_combinations
```

Then, we can calculate our expected predictions for each of the cases:

```{r}
mu <- link(model_interactions, data = all_combinations)
```

```{r message=FALSE, warning=FALSE}
data.frame(sims = 1:4000, mu) %>% 
  pivot_longer(-sims, names_to = "interaction", values_to = "score") %>% 
  mutate(interaction = case_when(interaction == "X1" ~ "USA Wine, USA Judge, White",
                                 interaction == "X2" ~ "USA Wine, USA Judge, Red",
                                 interaction == "X3" ~ "USA Wine, Int Judge, White",
                                 interaction == "X4" ~ "USA Wine, Int Judge, Red",
                                 interaction == "X5" ~ "Int Wine, USA Judge, White",
                                 interaction == "X6" ~ "Int Wine, USA Judge, Red",
                                 interaction == "X7" ~ "Int Wine, Int Judge, White",
                                 interaction == "X8" ~ "Int Wine, Int Judge, Red"),
         red = str_detect(interaction, "Red")) %>%  
  ggplot(aes(score, interaction, fill = red)) +
  geom_density_ridges() +
  scale_fill_manual(values = c( "lightgoldenrod2", "deeppink4")) +
  hrbrthemes::theme_ipsum_rc(grid = "X") +
  theme(legend.position = "none") +
  labs(title = "Posterior Score Predictions")
```

Whites tend, on average, to be higher scored. Also, non American Judges tend to be harsher than their american counterparts, regardless of the origin of the wine. The worst rated wine, on average, are the red international wine. 


