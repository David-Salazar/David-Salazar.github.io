<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on David Salazar&#39;s blog</title>
    <link>https://david-salazar.github.io/post/</link>
    <description>Recent content in Posts on David Salazar&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://david-salazar.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Forecasting elections? Taleb says no</title>
      <link>https://david-salazar.github.io/2020/09/03/forecasting-elections-taleb-says-no/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/09/03/forecasting-elections-taleb-says-no/</guid>
      <description>With US elections around the corner, news outlets are constantly deploying new models to try to predict who will win the next presidential elections. Ever since Trump’s ‘surprising’ win in ’16, these (pre-election) polls based models have come under scrutiny. Indeed, there is a huge amount of uncertainty that these models do not seem to capture well enough.
Taleb’s and Dhruv Madeka’s point is the following: whilst forecasting an uncertain election, one cannot invoke probabilistic thinking unless one imposes severe constraints on how the forecast will move up to election day.</description>
    </item>
    
    <item>
      <title>Causality: Mediation Analysis</title>
      <link>https://david-salazar.github.io/2020/08/26/causality-mediation-analysis/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/08/26/causality-mediation-analysis/</guid>
      <description>library(tidyverse) library(ggdag) extrafont::loadfonts(device=&amp;quot;win&amp;quot;) theme_set(theme_dag(base_family = &amp;quot;Roboto Condensed&amp;quot;)) Motivation Kids are the prototypical question makers; they never stop asking questions. Just after you have answered a Why? question, they ask yet another Why? This is the problem of mediation analysis: if you answer that X causes Y, how does exactly the causal mechanism work? Is the causal effect direct or mediated through yet another variable M? Mediation analysis aims to disentangle the direct effect (which does not pass through the mediator) from the indirect effect (the part that passes through the mediator).</description>
    </item>
    
    <item>
      <title>Causality: Probabilities of Causation</title>
      <link>https://david-salazar.github.io/2020/08/20/causality-probabilities-of-causation/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/08/20/causality-probabilities-of-causation/</guid>
      <description>Why read this? Questions of attribution are everywhere: i.e., did \(X=x\) cause \(Y=y\)? From legal battles to personal decision making, we are obsessed by them. Can we give a rigorous answer to the problem of attribution?
One alternative to solve the problem of attribution is to reason in the following manner: if there is no possible alternative causal process, which does not involve \(X\),that can cause \(Y=y\), then \(X=x\) is necessary to produce the effect in question.</description>
    </item>
    
    <item>
      <title>Causality: Regret? Look at Effect of Treatment on the Treated</title>
      <link>https://david-salazar.github.io/2020/08/16/causality-effect-of-treatment-on-the-treated/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/08/16/causality-effect-of-treatment-on-the-treated/</guid>
      <description>Why read this? Regret about our actions stems from a counterfactual question: What if I had acted differently?. Therefore, to answer such question, we need a more elaborate language than the one we need to answer prediction or intervention questions. Why? Because we need to compare what happened with what would had happened if we had acted differently. We need to compute the Effect of Treatment on the Treated (ETT).</description>
    </item>
    
    <item>
      <title>Causality: Counterfactuals - Clash of Worlds</title>
      <link>https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/</link>
      <pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/08/10/causality-counterfactuals-clash-of-worlds/</guid>
      <description>Motivation We’ve seen how the language of causality require an exogenous intervention on the values of \(X\); so far we’ve studied interventions on all the population, represented by the expression \(do(X)\). Nevertheless, with this language, there are plenty of interventions that remain outside our realm: most notably, counterfactual expressions where the antecedent is in contradiction with the observed behavior: there’s a clash between the observed world and the hypothetical world of interest.</description>
    </item>
    
    <item>
      <title>Causality: Testing Identifiability</title>
      <link>https://david-salazar.github.io/2020/07/31/causality-testing-identifiability/</link>
      <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/31/causality-testing-identifiability/</guid>
      <description>Motivation We’ve defined causal effects as an interventional distribution and posit two identification strategies to estimate them: the back-door and the front-door criteria. However, we cannot always use these criteria; sometimes, we cannot measure the necessary variables to use either of them.
More generally, given a causal model and some incomplete set of measurements, when is the causal effect of interest identifiable? In this blog post, we will develop a graphical criterion to answer this question by exploiting the concept of c-components.</description>
    </item>
    
    <item>
      <title>Causality: The front-door criterion</title>
      <link>https://david-salazar.github.io/2020/07/30/causality-the-front-door-criterion/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/30/causality-the-front-door-criterion/</guid>
      <description>Motivation In a past blogpost, I’ve explore the backdoor criterion: a simple graphical algorithm, we can define which variables we must include in our analysis in order to cancel out all the information coming from different causal relationships than the one we are interested. However, these variables are not always measured. What else can we do?
In this blogpost, I’ll explore the front-door criterion: i) an intuitive proof of why it works; (ii) how to estimate it; (iii) what are its fundamental assumptions; finally, (iv) an experiment with monte-carlo samples.</description>
    </item>
    
    <item>
      <title>Causality: To adjust or not to adjust</title>
      <link>https://david-salazar.github.io/2020/07/25/causality-to-adjust-or-not-to-adjust/</link>
      <pubDate>Sat, 25 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/25/causality-to-adjust-or-not-to-adjust/</guid>
      <description>What is this blogpost about? In this blogpost, I’ll simulate data to show how conditioning on as many variables as possible is not a good idea. Sometimes, conditioning can lead to de-confound an effect; other times, however, conditioning on a variable can create unnecessary confounding and bias the effect that we are trying to understand.
It all depends on our causal story: by applying the backdoor-criterion to our Causal Graph, we can derive an unambiguous answer to decide which variables should we use as controls in our statistical analysis.</description>
    </item>
    
    <item>
      <title>Causality: Invariance under Interventions</title>
      <link>https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/</guid>
      <description>In the last post we saw how two causal models can yield the same testable implications and thus cannot be distinguished from data alone. That is, we cannot gain causal understanding from data alone. Does that mean that we cannot ever gain causal understanding? Far from it; it just means that we must have a causal model.
Thus, causal effects cannot be estimated from the data itself without a causal story.</description>
    </item>
    
    <item>
      <title>Causality: Bayesian Networks and Probability Distributions</title>
      <link>https://david-salazar.github.io/2020/07/18/causality-bayesian-networks/</link>
      <pubDate>Sat, 18 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/18/causality-bayesian-networks/</guid>
      <description>Motivation Stats people know that correlation coefficients do not imply causal effects. Yet, very often, partial correlation coefficients from regressions with an ever growing set of ‘control variables’ are unequivocally interpreted as a step in the right direction toward estimating a causal effect. This mistaken intuition was aptly named by Richard McElreath, in his fantastic Stats course, as Causal Salad: people toss a bunch of control variables and hope to get a casual effect out of it.</description>
    </item>
    
    <item>
      <title>BDA Week 9: Large Sample Theory for the Posterior</title>
      <link>https://david-salazar.github.io/2020/07/13/bda-week-9-large-sample-theory-for-the-posterior/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/13/bda-week-9-large-sample-theory-for-the-posterior/</guid>
      <description>As Richard McElreath says in his fantastic Statistics course, Frequentist statistics is more a framework to evaluate estimators than a framework for deriving them. Therefore, we can use frequentist tools to evaluate the posterior. In particular, what happens to the posterior as more and more data arrive from the same sampling distribution?
In this blogpost, I’ll follow chapter 4 of Bayesian Data Analysis and the material in week 9 of Aki Vehtari’s course to study the Posterior Distribution under the framework of Large Sample Theory.</description>
    </item>
    
    <item>
      <title>BDA Week 8: Bayesian Decision Analysis</title>
      <link>https://david-salazar.github.io/2020/07/10/bda-week-8-bayesian-decision-analysis/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/10/bda-week-8-bayesian-decision-analysis/</guid>
      <description>Many if not most statistical analyses are performed for the ultimate goal of decision making. Bayesian Statistics has the advantage of direct use of probability to quantify the uncertainty around unobserved quantities of interest: whether those are parameters or predictions, we end up with a posterior distribution.
Indeed, our posterior predictions interact uniquely for each possible action \(d\) we can take to create a unique distribution of our utility \(U(x)|d\).</description>
    </item>
    
    <item>
      <title>BDA week 7: LOO and its diagnostics</title>
      <link>https://david-salazar.github.io/2020/07/08/bda-week-7-loo-and-its-diagnostics/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/08/bda-week-7-loo-and-its-diagnostics/</guid>
      <description>Once Stan’s implementation of HMC has run its magic, we finally have samples from the posterior distribution \(\pi (\theta | y))\). We can then run posterior predictive checks and hopefully our samples looks plausible under our posterior. Nevertheless, this is just an internal validation check: we expect more from our model. We expect it to hold under an external validation check: never seen observations, once predicted, should also look plausible under our posterior.</description>
    </item>
    
    <item>
      <title>Tail Risk of diseases in R</title>
      <link>https://david-salazar.github.io/2020/07/05/tail-risk-of-diseases-in-r/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/05/tail-risk-of-diseases-in-r/</guid>
      <description>The Data The plots Max-to-Sum ratio Histogram The Zipf plot Mean Excess Plot  Fitting the tail Wait a moment: infinite casualties? Extreme Value theory on the dual observations Maximum Likelihood estimate  Bayesian model Simulating fake data Fitting the model Posterior predictive checks Convergence diagnostics  Stressing the data Measurement error  Influential observations Conclusion   Pasquale Cirillo and Nassim Taleb published a short, interesting and important paper on the Tail Risk of contagious diseases.</description>
    </item>
    
    <item>
      <title>BDA Week 6: MCMC in High Dimensions, Hamiltonian Monte Carlo</title>
      <link>https://david-salazar.github.io/2020/07/02/bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/07/02/bda-week-6-mcmc-in-high-dimensions-hamiltonian-monte-carlo/</guid>
      <description>In the last couple of weeks, We’ve seen how the most difficult part of Bayesian Statistics is computing the posterior distribution. In particular, in the last week, we’ve studied the Metropolis Algorithm. In this blogpost, I’ll study why Metropolis does not scale well enough to high dimensions and give an intuitive explanation of our best alternative: Hamiltonian Monte Carlo (HMC).
This blogpost is my personal digestion of the excellent content that Michael Betancourt has put out there to explain HMC.</description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis: Week 5 -&gt; Metropolis </title>
      <link>https://david-salazar.github.io/2020/06/29/bayesian-data-analysis-week-5-metropolis/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/29/bayesian-data-analysis-week-5-metropolis/</guid>
      <description>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.
So far in the course, we have seen how the main obstacle in the way of performing Bayesian Statistics is the computation of the posterior.</description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis: Week 4 -&gt; Importance Sampling</title>
      <link>https://david-salazar.github.io/2020/06/27/bayesian-data-analysis-week-4-importance-sampling/</link>
      <pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/27/bayesian-data-analysis-week-4-importance-sampling/</guid>
      <description>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.
In this blogpost, I’ll go over one of the main topics of Week 4: Importance Sampling; I’ll also solve a couple of the exercises for Chapter 10 of the book.</description>
    </item>
    
    <item>
      <title>Gini Index under Fat-Tails</title>
      <link>https://david-salazar.github.io/2020/06/26/gini-index-under-fat-tails/</link>
      <pubDate>Fri, 26 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/26/gini-index-under-fat-tails/</guid>
      <description>I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In this blogpost, I’ll follow Taleb’s exposition of the Gini Index under fat-tails in Chapter 13 of his book.
Intuitively, if we use the “empirical distribution” to estimate the Gini Index, under fat-tails, we underestimate the tail of the distribution and thus underestimate the Gini index. This is yet another example of how we fool ourselves when we are using the “empirical” distribution.</description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis: Week 3-&gt; Exercises</title>
      <link>https://david-salazar.github.io/2020/06/25/bayesian-data-analysis-week-3-exercises/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/25/bayesian-data-analysis-week-3-exercises/</guid>
      <description>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.
In this blogpost, I’ll go over a couple of the selected exercises for week 3: exercise number 2 and exercise number 3.</description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis: Week 3 -&gt; Fitting a Gaussian probability model</title>
      <link>https://david-salazar.github.io/2020/06/24/bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/24/bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model/</guid>
      <description>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.
Instead of going through the homeworks (due to the fear of ruining the fun for future students of Aki’s), I’ll go through some of the examples of the book as case studies.</description>
    </item>
    
    <item>
      <title>Probability Calibration under fat-tails: useless</title>
      <link>https://david-salazar.github.io/2020/06/24/probability-calibration-under-fat-tails-useless/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/24/probability-calibration-under-fat-tails-useless/</guid>
      <description>Probability calibration refers to a manner of evaluating forecasts: the forecast frequency of an event should correspond to the correct frequency of the event happening in real life. Is this truly the mark of a good analysis? Under fat-tails, Nassim Taleb in his book answer with a categorical response NO!
Probability calibration in the real world Probability calibration amounts, in the real world, to a binary payoff: a fixed sum is paid off if the event happens.</description>
    </item>
    
    <item>
      <title>Bayesian Data Analysis: Week 2</title>
      <link>https://david-salazar.github.io/2020/06/22/bayesian-data-analysis-week-2/</link>
      <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/22/bayesian-data-analysis-week-2/</guid>
      <description>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s freely available online. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.
In this series of blogposts, I’ll go over the homeworks that Aki has kindly made available online.</description>
    </item>
    
    <item>
      <title>Extreme Value Theory for Time Series</title>
      <link>https://david-salazar.github.io/2020/06/17/extreme-value-theory-for-time-series/</link>
      <pubDate>Wed, 17 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/17/extreme-value-theory-for-time-series/</guid>
      <description>The Fisher-Tippet theorem (a type of CLT for the tail events) rests on the assumption that the observed values are independent and identically distributed. However, in any non trivial example, time series will reflect an underlying structure that will create dependence among the observations. Indeed, tail events tend to occur in clusters. Does this mean that we cannot use the Extreme Value Theory (EVT) to model the maxima of a time series?</description>
    </item>
    
    <item>
      <title>When are GARCH (and friends) models warranted?</title>
      <link>https://david-salazar.github.io/2020/06/14/when-are-garch-and-friends-models-warranted/</link>
      <pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/14/when-are-garch-and-friends-models-warranted/</guid>
      <description>In this blogpost, I’ll answer the question, following Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails, when can we use GARCH (and firends) models? As an example, also following Taleb, I’ll check the resulting conditions with the S&amp;amp;P500.
What are the obstacles? The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) family of models attempt to model a given time series by exploiting “volatility” clustering (i.e., for some periods volatility is consistently high, for other periods is consistently low).</description>
    </item>
    
    <item>
      <title>How to not get fooled by the &#34;Empirical Distribution&#34;</title>
      <link>https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/</link>
      <pubDate>Thu, 11 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/</guid>
      <description>With fat-tailed random variables, as Nassim Taleb says, the tail wags the dogs. That is, “the tails (the rare events) play a disproportionately large role in determining the properties”. Following the presentation given by Taleb in his latest technical book: Statistical Consequences of Fat Tails, I’ll show:
 Why using the empirical distribution for estimating the moments of a fat-tailed random variable is a terrible idea. A less “unreliable” alternative to estimating the moments.</description>
    </item>
    
    <item>
      <title>Fisher Tippet Th: a &#34;CLT&#34; for the sample maxima</title>
      <link>https://david-salazar.github.io/2020/06/10/fisher-tippet-th-a-clt-for-the-sample-maxima/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/10/fisher-tippet-th-a-clt-for-the-sample-maxima/</guid>
      <description>For fat-tailed random variables, the statistical properties are determined by a few observations in the tail. In Nassim Taleb’s words, “the tail wags the dog”. Therefore, it is vital to study the distribution of these few observations. A logical question to ask, then, is: is there a limiting distribution for the sample maxima as the number of samples grows? This is precisely what the Fisher Tippet Theorem states: the limiting distribution of (a normalized) sample maxima is the Generalized Extreme distribution (GED).</description>
    </item>
    
    <item>
      <title>Statistical Rethinking Week 10</title>
      <link>https://david-salazar.github.io/2020/06/09/statistical-rethinking-week-10/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/09/statistical-rethinking-week-10/</guid>
      <description>This is the final week of the best Statistics course out there. It showed the benefits of being ruthless with conditional probabilities: replace everything you don’t know with a distribution conditioned on what you do know. Bayes will do the rest. This holds for both measurement error and missing data.
1st problem Consider the relationship between brain volume (brain) and body mass (body) in the data(Primates301). These values are presented as single values for each species.</description>
    </item>
    
    <item>
      <title>Varying effects for continuous predictors -&gt; GP regression</title>
      <link>https://david-salazar.github.io/2020/06/04/varying-effects-for-continuous-predictors/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/04/varying-effects-for-continuous-predictors/</guid>
      <description>Statistical Rethinking is a fabulous course on Bayesian Statistics (and much more). Following its presentation, I’ll give a succinct intuitive introduction to Gaussian Process (GP) regression as a method to extend the varying effects strategy to continuous predictors. This method is incredibly useful when assuming a linear functional relationship between a continuous predictor and the outcome variable is not enough to capture the variation in the data.
First, I’ll begin by motivating the varying effects strategy.</description>
    </item>
    
    <item>
      <title>Bayesian Instrumental Variable Regression</title>
      <link>https://david-salazar.github.io/2020/06/03/bayesian-instrumental-variable-regression/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/03/bayesian-instrumental-variable-regression/</guid>
      <description>Statistical Rethinking is a fabulous course on Bayesian Statistics (and much more). In what follows, I’ll give a succinct presentation of Instrumental Variable Regression in a Bayesian setting using simulated data.
I had already seen the traditional econometrics formulation and yet found Richard’s presentation both illuminating and fun. It’s a testament of his incredible achievement with this book.
The problem The start of every instrumental variable setting is the following.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 9</title>
      <link>https://david-salazar.github.io/2020/06/03/statistical-rethinking-week-9/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/03/statistical-rethinking-week-9/</guid>
      <description>Week 9 was all about fitting models with multivariate distributions in them. For example, a multivariate likelihood helps us use an instrumental variable to estimate the true causal effect of a predictor. But also as an adaptive prior for some of the predictors. In both cases, we found out that the benefit comes from modelling the resulting var-cov matrix. In the instrumental variable case, the resulting joint distribution for the residuals was the key to capture the statistical information of the confounding variable.</description>
    </item>
    
    <item>
      <title>LLN for higher p Moments</title>
      <link>https://david-salazar.github.io/2020/06/02/lln-for-higher-p-moments/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/02/lln-for-higher-p-moments/</guid>
      <description>I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In it, we have seen how the Law of Large Numbers for different estimators simply does not work fast enough (in Extremistan) to be used in real life. For example, we have seen how the distribution of the sample mean, PCA, sample correlation and \(R^2\) turn into pure noise when we are dealing with fat-tails.</description>
    </item>
    
    <item>
      <title>Understanding Pooling across Intercepts and Slopes</title>
      <link>https://david-salazar.github.io/2020/06/01/understanding-pooling-across-intercepts-and-slopes/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/06/01/understanding-pooling-across-intercepts-and-slopes/</guid>
      <description>Statistical Rethinking is a fabulous course on Bayesian Statistics (and much more). By following simulations in the book, I recently tried to understand why pooling is the process and shrinkage is the result. In this post, I’ll try to do the same for a model where we pool across intercepts and slopes. That is, we will posit a multivariate common distribution for both intercept and slopes to impose adaptive regularization on our predictions.</description>
    </item>
    
    <item>
      <title>Central Limit Theorem in Action</title>
      <link>https://david-salazar.github.io/2020/05/30/central-limit-theorem-in-action/</link>
      <pubDate>Sat, 30 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/30/central-limit-theorem-in-action/</guid>
      <description>I have recently been exploring Nassim Taleb’s latest technical book: Statistical Consequences of Fat Tails. In it, we have seen how the Law of Large Numbers for different estimators simply does not work fast enough (in Extremistan) to be used in real life. For example, we have seen how the distribution of the sample mean, PCA, sample correlation and \(R^2\) turn into pure noise when we are dealing with fat-tails.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking Week 8</title>
      <link>https://david-salazar.github.io/2020/05/29/statistical-rethinking-week-8/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/29/statistical-rethinking-week-8/</guid>
      <description>Statistical Rethinking Week 8 This week was our first introduction to Multilevel models. Models where we explicitly model a family of parameters as coming from a common distribution: with each sample, we simultaneously learn each parameter and the parameters of the common distribution. This process of sharing information is called pooling. The end result is shrinkage: each parameter gets pulled towards the estimated mean of the common distribution. I tried my best to understand this process and result by simulating in this post</description>
    </item>
    
    <item>
      <title>Simulating into understanding Multilevel Models</title>
      <link>https://david-salazar.github.io/2020/05/28/simulating-into-understanding-multilevel-models/</link>
      <pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/28/simulating-into-understanding-multilevel-models/</guid>
      <description>Simulating into Understanding Multilevel Models  Pooling is the process and shrinkning is the result
 Pooling and Shrinking are not easy concepts to understand. In the lectures, Richard, as always, does an excellent job of creating metaphors and examples to help us gain intuition around what Multilevel models do. Multilevel models are models mnesic models.
Imagine a cluster of observations: it can be different classrooms in a school. Pooling means using the information from other classrooms to inform our estimates for each classroom.</description>
    </item>
    
    <item>
      <title>R-squared and fat tails</title>
      <link>https://david-salazar.github.io/2020/05/26/r-squared-and-fat-tails/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/26/r-squared-and-fat-tails/</guid>
      <description>R-squared and Fat-tails This post continues to explore how common statistical methods are unreliable and dangerous when we are dealing with fat-tails. So far, we have seen how the distribution of the sample mean, PCA and sample correlation turn into pure noise when we are dealing with fat-tails. In this post, I’ll show the same for \(R^2\) (i.e., coefficient of determination). Remember, it is a random variable that we are estimating and thefore has its own distribution.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 7</title>
      <link>https://david-salazar.github.io/2020/05/24/statistical-rethinking-week-7/</link>
      <pubDate>Sun, 24 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/24/statistical-rethinking-week-7/</guid>
      <description>Statistical Rethinking: Week 7 This week paid off. All the hard work of understanding link functions, HMC flavored Monte-Carlo, and GLM allowed to study more complex models. To keep using Richard’s metaphor: it allowed us to study monsters: models with different parts made out of different models. In particular, Zero Inflated Models and Ordered Categories.
 Homework  In the Trolley data—data(Trolley)—we saw how education level (modeled as an ordered category) is associated with responses.</description>
    </item>
    
    <item>
      <title>Correlation is not Correlation</title>
      <link>https://david-salazar.github.io/2020/05/22/correlation-is-not-correlation/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/22/correlation-is-not-correlation/</guid>
      <description>Correlation is not correlation Small sample effects with Sample Correlation Sample Correlation in Mediocristan Sample correlations from Extremistan  Misused Correlation Quadrant’s Correlation Berkson’s paradox Simpson’s Paradox Correlation under non linearities  Misunderstanding of Correlation: its signal is non linear Entropy and Mutual Information  Conclusion   Correlation is not correlation To the usual phrase of correlation is not causation, Nassim Taleb often answers: correlation is not correlation.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 6</title>
      <link>https://david-salazar.github.io/2020/05/20/statistical-rethinking-week-6/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/20/statistical-rethinking-week-6/</guid>
      <description>Statistical Rethinking: Week 6 Quick summary of the week The week was a whirlwind tour of:
Maximum entropy and introduction to GLMs. The problems that come when using link functions. The perils of relative effects when studying binomial regression and how complicated it is to directly calculate probabilities with GLMs: all the parameters interact among themselves.  This week was an introduction to GLMs and the principle of Maximum Entropy.</description>
    </item>
    
    <item>
      <title>Understanding the tail exponent</title>
      <link>https://david-salazar.github.io/2020/05/19/understanding-the-tail-exponent/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/19/understanding-the-tail-exponent/</guid>
      <description>Understanding the tail exponent Power Laws are ubiquitous to describe fat tails, a topic that I’ve been trying to wrap my head around for the last couple of weeks. However, up until now, I haven’t had a visceral understanding of what exactly is the function of their main parameter: the tail exponent \(\alpha\). This blogpost is my attempt at gaining understanding. To do so, I will be replicating some of the plots and derivations from two sources:</description>
    </item>
    
    <item>
      <title>Statistical Rethinking Week 5 -&gt; HMC samples</title>
      <link>https://david-salazar.github.io/2020/05/15/statistical-rethinking-week-5-hmc-samples/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/15/statistical-rethinking-week-5-hmc-samples/</guid>
      <description>Statistical Rethinking: Week 5 After a quick tour around interactions, this week was a quick introduction to MCMC samplers and how they are the engine that powers current Bayesian modelling. We looked at Metropolis, Gibbs and finally HMC. Not only HMC is more efficient, but it also let us know when it fails. Let’s tackle the homework with these new tools:
 Homework 5 Problem Week 1 data(&amp;quot;Wines2012&amp;quot;) wines &amp;lt;- Wines2012 wines %&amp;gt;% count(judge) ## # A tibble: 9 x 2 ## judge n ## &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; ## 1 Daniele Meulder 20 ## 2 Francis Schott 20 ## 3 Jamal Rayyis 20 ## 4 Jean-M Cardebat 20 ## 5 John Foy 20 ## 6 Linda Murphy 20 ## 7 Olivier Gergaud 20 ## 8 Robert Hodgson 20 ## 9 Tyler Colman 20 We have 9 judges and each of them gave 20 reviews.</description>
    </item>
    
    <item>
      <title>Standard Deviation and Fat Tails</title>
      <link>https://david-salazar.github.io/2020/05/13/standard-deviation-and-fat-tails/</link>
      <pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/13/standard-deviation-and-fat-tails/</guid>
      <description>Statistical Consequences of Fat Tails In this post, I’ll continue to explore with Monte-Carlo simulations the ideas in Nassim Taleb’s latest book: Statistical Consequences of Fat Tails. In other posts, I have look at the persistent small sample effect that plagues the mean estimates under fat tails as a consequence of the loooong pre-asymptotics of the law of large numbers. Also, how this in turn plagues other statistical techniques such as PCA.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 5 -&gt; Interactions</title>
      <link>https://david-salazar.github.io/2020/05/12/statistical-rethinking-week-5-interactions/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/12/statistical-rethinking-week-5-interactions/</guid>
      <description>Statisical Rethinking: Week 5 -&amp;gt; Interactions As Richard says in class, interactions are easy to code but incredibly difficult to interpret. By going through the problems in Chapter 8, I hope to gain a bit of practice working with them.
Chapter 8 Problems Problem 1 Let’s run the tulips model but this time with the bed variable. Given that this is a categorical variable, it will create a different intercept for each of the beds in the sample.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 4</title>
      <link>https://david-salazar.github.io/2020/05/11/statistical-rethinking-week-4/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/11/statistical-rethinking-week-4/</guid>
      <description>Statistical Rethinking: Week 4 This week was a marathon of content. Richard introduced beautifully the trade-off between overfitting and underfitting and prescribed two complimentary methods to help us navigate this trade-off:
Regularizing priors Information criteria and Cross-Validation estimates of the risk of overfitting.  Regularizing priors reduces the risk of overfitting of any model by introducing skepticisim into the priors. Whereas information criteria and Cross-Validation help us to estimate whether we have overfitted or not.</description>
    </item>
    
    <item>
      <title>What does it mean to fatten the tails?</title>
      <link>https://david-salazar.github.io/2020/05/09/what-does-it-mean-to-fatten-the-tails/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/09/what-does-it-mean-to-fatten-the-tails/</guid>
      <description>What does it mean to fatten the tails? First, let’s define what we mean by fatter tails.
What are fatter tails? Intuitively, fat tails distribution are distributions for which their PDFs decay to zero very slowly. So slowly, that extreme values start gaining traction in the determination of the whole distribution. Thus, a distribution is fatter than another one if its PDF takes longer to decay to zero.
 Fattening the tails Thus, if we wanted to fatten the tails, the intuitive response is to add more mass at the tails such that the PDF takes more time to decay.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 3</title>
      <link>https://david-salazar.github.io/2020/05/03/statistical-rethinking-week-3/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/05/03/statistical-rethinking-week-3/</guid>
      <description>Statistical Rethinking: Week 3 Week 3 gave the most interesting discussion of multiple regression. Why isn’t it enough with univariate regression? It allows us to disentagle two types of mistakes:
 Spurious correlation between the predictor and independent variable. A masking relationship between two explanatory variables.  It also started to introduce DAGs and how they are an incredible tool for thinking before fitting. Specially, it managed to convince me the frequent strategy of tossing everything into a multiple regression and hoping for the ebst is a recipe for disaster.</description>
    </item>
    
    <item>
      <title>Wittgenstein&#39;s Ruler: Fat or Thin?</title>
      <link>https://david-salazar.github.io/2020/04/30/wittgenstein-s-ruler-fat-or-thin/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/30/wittgenstein-s-ruler-fat-or-thin/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 2</title>
      <link>https://david-salazar.github.io/2020/04/28/statistical-rethinking-week-2/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/28/statistical-rethinking-week-2/</guid>
      <description>library(rethinking) library(tidyverse) library(ggridges) extrafont::loadfonts(device=&amp;quot;win&amp;quot;) set.seed(24) data(&amp;quot;Howell1&amp;quot;) precis(Howell1) ## mean sd 5.5% 94.5% ## height 138.2635963 27.6024476 81.108550 165.73500 ## weight 35.6106176 14.7191782 9.360721 54.50289 ## age 29.3443934 20.7468882 1.000000 66.13500 ## male 0.4724265 0.4996986 0.000000 1.00000 ## histogram ## height &amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2581&amp;gt; ## weight &amp;lt;U+2581&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt; ## age &amp;lt;U+2587&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt; ## male &amp;lt;U+2587&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2587&amp;gt; Week 2 Week 2 has gotten us to start exploring linear regression from a bayesian perspective. I found it the most interesting to propagate uncertainty through the model.</description>
    </item>
    
    <item>
      <title>Spurious PCA under Thick Tails</title>
      <link>https://david-salazar.github.io/2020/04/27/spurious-pca-under-thick-tails/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/27/spurious-pca-under-thick-tails/</guid>
      <description>Spurious PCA under Thick Tails PCA is a dimensionality reduction technique. It seeks to project the data onto a lower dimensional hyperplane such that as much of the original data variance is preserved. The underlying idea is that the vectors creating these lower dimensional hyperplanes reflect a latent structure in the data. However, what happens when there is no structure at all?
In his most recently published technical book, Taleb examines this question under two different regimes: Mediocristan and Extremistan.</description>
    </item>
    
    <item>
      <title>Pareto 80/20 and Maximum Likelihood</title>
      <link>https://david-salazar.github.io/2020/04/23/pareto-80-20-and-maximum-likelihood/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/23/pareto-80-20-and-maximum-likelihood/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 1</title>
      <link>https://david-salazar.github.io/2020/04/19/statistical-rethinking-week-1/</link>
      <pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/19/statistical-rethinking-week-1/</guid>
      <description>Week 1 Week 1 tries to go as deep as possible in the intuition and the mechanics of a very simple model. As always with McElreath, he goes on with both clarity and erudition.
 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.
 # define grid p_grid &amp;lt;- seq(from = 0, to = 1, length.</description>
    </item>
    
    <item>
      <title>Fat vs Thin: does LLN work?</title>
      <link>https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://david-salazar.github.io/2020/04/17/fat-vs-thin-does-lln-work/</guid>
      <description>Fat tails are a different beast  Statistical estimation is based on the LLN and CLT. The CLT states that the sampling distribution will look like a normal. The LLN that the variance of the normal will decrease as our sampling size increases.
 Or so does Nassim Nicholas Taleb says in his recently published technical book, wherein he explains how common practice statistical methodology breaks down under the Extremistan regime.</description>
    </item>
    
  </channel>
</rss>
