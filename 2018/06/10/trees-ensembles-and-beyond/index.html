<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Trees, Ensembles and beyond, XGBoost and LGBM - Dilettanting Data Science</title>
<meta property="og:title" content="Trees, Ensembles and beyond, XGBoost and LGBM - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">22 min read</span>
    

    <h1 class="article-title">Trees, Ensembles and beyond, XGBoost and LGBM</h1>

    
    <span class="article-date">2018/06/10</span>
    
    

    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#why">Why?</a><ul>
<li><a href="#set-up">Set-up</a></li>
</ul></li>
<li><a href="#trees">Trees</a><ul>
<li><a href="#fitting-them">Fitting them</a></li>
<li><a href="#interpretation">Interpretation</a></li>
</ul></li>
<li><a href="#ensembles">Ensembles</a><ul>
<li><a href="#bagging">Bagging</a><ul>
<li><a href="#bootstraping">Bootstraping</a></li>
<li><a href="#random-forests">Random Forests</a></li>
<li><a href="#conclusions-for-bagging">Conclusions for Bagging</a></li>
</ul></li>
<li><a href="#boosting">Boosting</a><ul>
<li><a href="#directional-derivative">Directional Derivative</a></li>
<li><a href="#gradient-boosting-back-to-our-problem">Gradient Boosting: Back to our problem</a></li>
<li><a href="#conclusions-for-boosting">Conclusions for Boosting</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="why" class="section level1">
<h1>Why?</h1>
<p><code>lightgbm</code> and <code>xgboost</code> appear in every single competition at Kaggle. Thus, these boosting techniques must be able to learn something that cannot be easily learned from intelligent bagging techniques like Random Forests. This is my attempt to understand <em>why</em> and <em>how</em> they can do that.</p>
<div id="set-up" class="section level2">
<h2>Set-up</h2>
<p>This will be kind of <em>mathy</em>, but I will try to keep the notation as clear as possible so it can be understood without much hassle. The traditional problem in machine learning, for a given algorithm, is as follows: learn a function, <span class="math inline">\(f(x; \theta)\)</span> such that <span class="math inline">\(\theta\)</span> minimizes the following:</p>
<p><span class="math display">\[ E_{y,x} (L(y, f(x; \theta)) ) \]</span>
Unpacked, it can be explained thus: given a loss function, <span class="math inline">\(L\)</span>, we want to find the <span class="math inline">\(\theta\)</span> such that the loss between the observations, <span class="math inline">\(y\)</span>, and our predictions, <span class="math inline">\(f(x;\theta)\)</span> is minimized across the values we expect to see of the observations <span class="math inline">\(\{x, y\}\)</span>. Note that this last condition precludes <em>train-set-overfitting</em>: we want something to generalizes even to not-yet seen observations.</p>
</div>
</div>
<div id="trees" class="section level1">
<h1>Trees</h1>
<p>From the freely available <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.305:</p>
<blockquote>
<p>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.</p>
</blockquote>
<p>Thus, a prediction with a tree is a matter of finding on <strong>which partition</strong> our new observation sits and predict <strong>the constant assigned</strong> to that region. For regression problems, the constant is the average of the observations in the regions; for classification problem, the constant is the modal class within the region.</p>
<div id="fitting-them" class="section level2">
<h2>Fitting them</h2>
<p>They sound, and are quite simple, but the big problem is <strong>how to find the partitions</strong> in the first place. Mathematically, the problem can be defined thus:</p>
<p>For <span class="math inline">\(J\)</span> partitions that we decide to perform, find where in the space to do the partition, <span class="math inline">\(R_j\)</span>, and what constant to assign to that partition, <span class="math inline">\(c_j\)</span>. We must find, for each partition <span class="math inline">\(j\)</span> the <span class="math inline">\(R_j\)</span>, <span class="math inline">\(c_j\)</span> pair that minimize the following:</p>
<p><span class="math display">\[ \sum_{j}^J \sum_{x_i \in R_j} E_y (L(y, c_j) ) \]</span></p>
<p>However, this a prohibitively expensive combinatorial optimization problem. To avoid this, we proceed with a <strong>greedy algorithm</strong>:</p>
<p>For any given node do the following:</p>
<ol style="list-style-type: decimal">
<li>Loop over each of the features.</li>
<li>For each feature, sort the observations and find every possible split using this feature. Compute the <strong>information gain</strong> of doing each of these splits. Keep only the one that gave the best information gain.</li>
</ol>
<blockquote>
<p>Information gain is defined as the difference between the loss in the node and the average loss across the nodes created as we do the proposed partition</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Compare the best information again across features. Finally, use the feature that had the best one.</li>
<li>Go over the next node and repeat.</li>
</ol>
<p>Thus, note that, unless it is told otherwise, the algorithm will keep doing this partitions as long as there are no possible partitions in any of the nodes.</p>
</div>
<div id="interpretation" class="section level2">
<h2>Interpretation</h2>
<p>Besides the traditional tree visualization, notice that we can rank the importance of each feature according to their predictive power:</p>
<blockquote>
<p>For each node, annotate which feature was used and the resulting information gain from that partition. Do that across all the nodes. Sum for all the features that were used more than once. The features that have more of these values, are the ones that were more important for our tree.</p>
</blockquote>
<p><strong>This way of computing feature importances will be generalized to ensemble trees. As we will do the same procedure for each tree in the ensemble, and then average across trees. However, this procedure is not without its <a href="http://parrt.cs.usfca.edu/doc/rf-importance/index.html">perils</a>.</strong></p>
</div>
</div>
<div id="ensembles" class="section level1">
<h1>Ensembles</h1>
<p>Trees are simple and powerful. Yet, they are limited: they are extremely susceptible to noise: i.e., they have a high variance. From <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a>, p.312:</p>
<blockquote>
<p>The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it.</p>
</blockquote>
<p>However, <strong>they can be much improved if they are combined together</strong>.</p>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p>Bagging consists on the idea of combining <strong>weak learners</strong>, that is, models that do not perform very well, and that the end result is greater than the sum of its parts. Why can we do this? When we have multiple and <strong>uncorrelated weak learners</strong>, they will make mistakes in their individual predictions, yes, but they will commit <strong>different types of mistakes</strong>. If they are <strong>roughly right</strong>, some models will overshoot and some will undershoot: the end result of combining them will be that we will make correct predictions as the <strong>errors will cancel each other</strong>.</p>
<p>In the statistical learning lingua, what we are trying to achieve is, by virtue of combining <strong>uncorrelated weak learners</strong>, a better position in the <strong>bias-variance trade-off</strong>. To do so, we need to enforce the following conditions:</p>
<ol style="list-style-type: decimal">
<li>The individual <strong>weak learners</strong> must have <strong>low correlation between each other</strong> and have <strong>low bias</strong>(i.e., they must able to learn complex data structures).</li>
<li>The combination of the models <strong>cannot increase the bias</strong> that the individual learner has.</li>
<li>The combination of the models must <strong>reduce the overall model variance</strong> with respect to the variance of the individual weak learners.</li>
</ol>
<p>If the three conditions hold, bagging will lead us into a better position in the <strong>bias-variance trade-off</strong>: we will have the same bias (which is already low, given how powerful individual trees can be) but we will have lower variance. Thus, we have a better model.</p>
<p>To make sure that three conditions mentioned hold, there are two smart methods for bagging: <strong>simple bagging and random forests</strong>. In both, we combine low bias trees and try to decorrelate them by using some clever tricks such that the trees that we construct end up being <strong>uncorrelated weak learners</strong>. Note that the key part here is “<strong>uncorrelated weak learners</strong>”. <strong>If they are weak learners, but are highly correlated, combining them won’t fix any of the problems the individual learners have</strong>.</p>
<p>Mathematically (without worrying about its derivation, just the interpretation), this can be seen thus:</p>
<ul>
<li>The variance of an average of <span class="math inline">\(B\)</span> <span class="math inline">\(i.i.d.\)</span> random variables (<span class="math inline">\(X\)</span> with variance <span class="math inline">\(\sigma^2\)</span>) is:</li>
</ul>
<p><span class="math display">\[ \frac{1}{B}\sigma^2\ \]</span></p>
<ul>
<li>However, if they are identically distributed but not independent, and with positive pairwise correlation <span class="math inline">\(\rho\)</span>, the variance of their sum average:</li>
</ul>
<p><span class="math display">\[ \rho \sigma^2 + \frac{1 - \rho}{B} \sigma^2  \]</span></p>
<p>Thus, as <span class="math inline">\(B\)</span> increases and <span class="math inline">\(\rho\)</span> decreases, our individual weak learners will become more like the <strong>uncorrelated weak learners</strong> we need for bagging to be effective. Then, the conditions for effective bagging are clear: <strong>we must be able to create a large number of weak learners (large <span class="math inline">\(B\)</span>) and we must be able to create decorrelated versions of them</strong>. It just so happens that, with some ingenuity, trees are the perfect candidate! Both <strong>Bootstraping and Random Forests</strong> are ways of achieving this successful ensemble with trees.</p>
<p>Once we have multiple <strong>uncorrelated weak learners</strong>, combining them is a simple task. If the problem is a regression one, we average our predictions across trees. If it is a classification one, we use majority voting.</p>
<div id="bootstraping" class="section level3">
<h3>Bootstraping</h3>
<p>So we know how to construct high variance low bias trees:</p>
<blockquote>
<p>Construct them such that they have a lot of depth and thus are able to capture complex structure in the data. This can be accomplished by setting a low value for <code>min_samples_leaf</code> in the <code>scikit-learn</code> implementation. Although each of the trees will tend to overfit as they have more depth, <strong>bagging will precisely solve this problem</strong> by reducing the overall model variance.</p>
</blockquote>
<p><strong>But how to construct them such that they are uncorrelated between them?</strong></p>
<p>One way of making sure that the trees are uncorrelated is to make them use slightly <strong>different samples</strong> in their respective training. That is, for each tree that we construct, we are going to pass them over a bootstrap-resampled version (i.e., n sampled <span class="math inline">\(rows\)</span> with replacement, where <span class="math inline">\(n = |trainingdata|\)</span>) of the training data. Thus, as each tree is going to work with different data, they are going to learn
<strong>different partitions</strong> and thus predict different values and behave like <strong>uncorrelated weak learners</strong> we need.</p>
<p>To understand this, let’s make the following experiment. Imagine one tree and one bootstrap-resampled version of the training data. For a given row, <strong>what is the probability that the tree will see this row in this bootstrap-resampled version of the training data?</strong></p>
<blockquote>
<p>The probability of not choosing that row in the first iteration of the bootstrap is: <span class="math inline">\(\frac{n-1}{n}\)</span>. The probability of not choosing that row in any of the other iterations is: <span class="math inline">\((\frac{n-1}{n})^n\)</span> (remember that we are sampling n times from the n rows). Thus, the probability of the tree seeing that row in its data, is: <span class="math inline">\(1- (\frac{n-1}{n})^n\)</span></p>
</blockquote>
<p>Using computer power, we can see that this expression converges to 63%:</p>
<p><img src="/post/2018-06-05-trees-ensembles-and-beyond_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Thus, <strong>if we perform a bootstrap with equal number of iterations as there is # of observations in the training data, the tree will see at least that row in its data with 63% of probability</strong>. Not bad right? <strong>That means that there’s a 37% chance that it will not see it, will not learn from it, and thus will arrive at a different partition</strong> than the trees that do see that observation. Notice that we are moving on the <strong>bias-variance tradeoff</strong>: if the tree does not see it, it will be biased; however, when averaging with others, this will be corrected <strong>and</strong> these differences will reduce the overall model variance.</p>
<p>However, we can be even more extreme with this procedure. <strong>We can perform a bootstrap by sampling even less times than there are training observations</strong>. If we do so, the probability that, for a given row, a given tree will see it in its bootstrap-resampled version of the training data is:</p>
<p><span class="math display">\[1- (\frac{n-1}{n})^k \]</span></p>
<p><strong>Remember that now <span class="math inline">\(k &lt; n\)</span></strong>. Evaluating this operation such that <span class="math inline">\(k \in \{n/2, n/3, n/4\}\)</span>:</p>
<p><img src="/post/2018-06-05-trees-ensembles-and-beyond_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Notice how the probability of <strong>seeing a given row in a given tree decreases as we reduce the number of iterations in the bootstrap</strong>. This means that more and more trees won’t see some observations, won’t learn from them and thus will learn different partitions from the trees that do see those observations. Thus, <strong>by reducing the number of iterations in the bootstrap we can reduce the correlation between the trees</strong>. However, notice that this will accomplish an even greater shift in the bias-variance tradeoff: the reduction in the variance will be greater, yes, but so will be the increase in the bias of each individual tree. Also, this technique has the benefit of reducing training time, as each tree takes less time in training due to lower number of observations (and consequently, # of splits) it considers.</p>
<p><strong>Note that the only way I know of setting the number of bootstrap iterations lower than the number of observations is by using the <a href="https://github.com/fastai/fastai/blob/master/fastai/structured.py"><code>fast.ai</code></a> library and the following two functions:</strong></p>
<pre class="python"><code>from sklearn.ensemble import forest
def set_rf_samples(n):
    &quot;&quot;&quot; Changes Scikit learn&#39;s random forests to give each tree a random sample of
    n random rows. from fast.ai: https://github.com/fastai/fastai/blob/master/fastai/structured.py
    &quot;&quot;&quot;
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n))
        
def reset_rf_samples():
    &quot;&quot;&quot; Undoes the changes produced by set_rf_samples.
    &quot;&quot;&quot;
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n_samples))</code></pre>
</div>
<div id="random-forests" class="section level3">
<h3>Random Forests</h3>
<p>Even though passing over different bootstrap-resampled versions to each tree in a forest can effectively “decorrelate” the trees, we can improve on it. Let’s think of a situation where <strong>this procedure fails</strong>: no matter what rows each tree sees, the partitions are all the same. Why? Close to all of the <strong>observations share a predominant</strong> feature: one that always will yield the best information gain if split on it, no matter what subset of the data the tree is seeing. Thus, the trees won’t give different predictions and bagging won’t help much. What can we do?</p>
<p>Enter <strong>random selection</strong>: every time any of the trees will evaluate which feature to use, we <strong>will tell it to only consider a subset of the features</strong>. Which subset? Well, <strong>every time any of the trees reach any of the nodes, we will draw a random subset of the features</strong> and the tree will only be allowed to look within that subset for the best split.</p>
<p>Now consider the former problem of the predominant feature. What will happen? Each time a node evaluates on which features to split, <strong>the predominant feature may or may not be within the subsets of features the tree is allowed to use in that node</strong>. If it is not, then the tree won’t use that feature: even if it could have used that feature and improve the information gain from that split. <strong>Why do we choose to sacrifice individual tree performance?</strong> This loss in the information gain at a particular split in a tree is <strong>countered by the overall model benefits of having trees that are uncorrelated between them and thus truly reaping the benefits of bagging</strong>. That is, we are choosing a different position in the bias-variance trade-off.</p>
<p>From the <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.598, we can see that, effectively, tree correlation decreases as we are more stringent with the number of features that each node can use:</p>
<p><img src="/post/2018-06-05-trees-ensembles-and-beyond_files/esl.png" /></p>
</div>
<div id="conclusions-for-bagging" class="section level3">
<h3>Conclusions for Bagging</h3>
<p><em>Ensembling</em> trees into a bigger model will be a good idea when the single trees are enough to understand the complexity of the structure in the data and when we can construct the trees such that the correlation between them is low enough. <strong>One way of tackling the first issue is constructing trees with enough depth. One way of tackling the second issue is by using bagging and random forests.</strong></p>
<p>Thus, there are <strong>four hyperparameters that you have to set to define in which position of the bias-variance trade-off</strong> you want to be when using the tree ensembles discussed above:</p>
<ul>
<li><span class="math inline">\(B\)</span>: The number of trees you want to use to average predictions. In <code>scikit-learn</code>: <code>n_estimators</code>.</li>
</ul>
<blockquote>
<p>This type of ensemble is pretty difficult to get to overfit with this hyperparameter. You will see that as you change this number, predictions won’t change that much.</p>
</blockquote>
<ul>
<li><p><span class="math inline">\(k\)</span>: How many samples of the training data you want to pass to each individual tree. Use fast.ai’s function <code>set_rf_samples()</code>.</p></li>
<li><p>The size of the random subset of features that each node can use to determine which is the best split. The less, the more “decorrelated” the trees will be. In <code>scikit-learn</code>: <code>max_features</code>.</p></li>
<li><p>The individual tree depth that each tree can go to. This is easily determined by <code>min_samples_leaf</code> in <code>scikit-learn</code>, which determines the minimum number observations required to be at a leaf node.</p></li>
</ul>
<p>In code this will look like:</p>
<pre class="python"><code>from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
set_rf_samples(k)
model = RandomForestRegressor(n_estimators=B, min_samples_leaf=3, 
                      max_features=0.5, n_jobs=-1)</code></pre>
</div>
</div>
<div id="boosting" class="section level2">
<h2>Boosting</h2>
<p>Boosting is also an ensemble of weak learners technique, but <strong>a fundamentally different one than Bagging</strong>. The way it works is by <strong>sequentially fitting weak learners models to sequentially modified versions of the data</strong>. The key is in <em>sequentially modified versions of the data</em>. With boosting, we want each <strong>model to focus on the structure within the data that, given the others’ learners predictions, can helps us the most to improve our predictions</strong>. Thus, each learner will be directed to focus on different structures within the data. By combining this different types of <em>insights</em> into an ensemble of trees, we can improve the overall model performance.</p>
<p>Putting this more formally. <strong>Sequentially means that we will repeat one process for a given number of iterations. Those are the number of boosting iterations</strong>. For the first iteration we will treat it as any other supervised learning task. That is, we will, fit a tree such that we minimize the following:</p>
<p><span class="math display">\[ E_{y,x} (L(y, T_1(\{R_{j,1}\}, \{c_{j,1}\},x)) ) \]</span></p>
<p>Were <span class="math inline">\(\{R_{j,1}\}\)</span>, <span class="math inline">\(\{c_{j,1}\}\)</span> are the sets of regions and predictions we create by fitting this first tree.</p>
<p>However, for any further iteration, let’s say the second iteration, we will be:</p>
<ol style="list-style-type: decimal">
<li>trying to find another function,<span class="math inline">\(T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)\)</span> such that we minimize our iteration’s loss function.</li>
<li>We are treating the trees we fitted at the other iterations as set in stone.</li>
</ol>
<p>Thus, our optimization problem for the second boosting iteration is:</p>
<p><span class="math display">\[E_{y,x}(L(y, T_1(\{R_{j,1} \}, \{c_{j,1} \},x) + T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)))\]</span></p>
<p>Notice the big change, <strong>within the loss now there are terms set in stone at both sides of the comma:</strong> <span class="math inline">\(y\)</span> and <span class="math inline">\(T_1(\{R_{j,1}\}, \{c_{j,1}\},x)\)</span>. Notice <strong>how the complexity of finding the best splits for</strong> <span class="math inline">\(T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)\)</span> <strong>changed with respect to the first tree</strong>: for any given split we are considering, we must first find what predictions we gave before for the observations at either side of the split. With more boosting iterations and thus more trees, this problem grows even more complex.</p>
<p>To solve this problem, we will take a detour into <strong>directional derivatives and steepest descent</strong>. Then, we will be able to solve the above problem with a very clever approximation.</p>
<div id="directional-derivative" class="section level3">
<h3>Directional Derivative</h3>
<p>Let’s make an experiment for a second and forget the above. Let’s think of the problem of only predicting one row, <span class="math inline">\(row\)</span>. Let’s forget <span class="math inline">\(T_{2}( \{R_{j,2}\}, \{c_{j,2}\},x)\)</span>, let’s treat <span class="math inline">\(y\)</span> as a parameter and let’s treat <span class="math inline">\(T_1(\{R_{j,1}\}, \{c_{j,1}\},x)\)</span> as a variable <span class="math inline">\(z\)</span>. And finally, let’s forget the expectation. Then, we end up with a function in one variable:</p>
<p><span class="math display">\[ L(z ; y_{row}) \]</span></p>
<p>Imagine we have to minimize this function by taking little steps around a starting point. That is, for a starting point, <span class="math inline">\(z_0\)</span>, which <span class="math inline">\(addition\)</span> is the one that minimizes: <span class="math display">\[  L(z_0 + addition; y_{row}) \]</span></p>
<p>From a bit of calculus (do not worry about the derivation, just the interpretation), we know that:</p>
<p><span class="math display">\[ L(z_0 + addition; y_{row}) \approx L(z_0; y_{row}) + \nabla  L(z_0; y_{row}) \cdot addition \]</span>
<span class="math inline">\(\nabla L(y_{row}, z)\)</span> is the gradient of the function. Remember: the gradient is just a vector, each component tells you, for a given axis, the rate of change of the function in the direction of that axis. By taking the dot product with <span class="math inline">\(addition\)</span>, which can also be seen as a one dimensional vector, we have the rate of change of the function in the direction of that vector. <strong>This is called the directional derivative</strong> of the function in the direction of the vector <span class="math inline">\(addition\)</span>.</p>
<p>Thus, recoginize the true nature of our minimization problem: <strong>to find the best addition we can make, such that <span class="math inline">\(L(z_0 + addition; y_{row})\)</span> will be eventually minimized</strong>. That is given, a starting point, in which direction we have to move. Because, <span class="math inline">\(L(z_0; y_{row})\)</span> is a constant, our problem is minimizing the dot product <span class="math inline">\(\nabla L(z_0; y_{row})\cdot addition\)</span>. For our problem to make sense (i.e., we cannot say our direction is minus infinite across any of the axes), we will restrict ourselves to close by vectors. Were we to solve this problem sequentially, that is, <strong>find the <span class="math inline">\(addition^*\)</span> that minimizes our function around some point, move to that point, and then solve the problem again</strong>, we would be performing <strong>The Steepest Descent method</strong> and end up minimizing our function <span class="math inline">\(L(z; y_{row})\)</span>.</p>
<p>Summarizing, <strong>Steepest Descent</strong> consists of:</p>
<ol style="list-style-type: decimal">
<li><p>Start at some point <span class="math inline">\(z_0\)</span>.</p></li>
<li><p>Calculate the vector <strong>addition</strong> that minimizes the <strong>directional derivative</strong> <span class="math inline">\(\nabla L(z_0; y_{row}) \cdot addition\)</span>. Move the point <span class="math inline">\(z_0\)</span> in the direction of <span class="math inline">\(addition^*\)</span>. To do so: <span class="math inline">\(z_0:= z_0 + addition\)</span>.</p></li>
<li><p>Repeat (2) many times. Then, you end up performing <strong>Stepping Descent</strong>.</p></li>
</ol>
</div>
<div id="gradient-boosting-back-to-our-problem" class="section level3">
<h3>Gradient Boosting: Back to our problem</h3>
<p>Notice that the problem of boosting is equivalent to the problem we just solved. <strong>Given an initial point (i.e., our first prediction), what is the ideal addition (i.e., our second prediction) we can make to our current prediction? The addition that minimizes the directional derivative!</strong>. To make it even more clear:</p>
<ol style="list-style-type: decimal">
<li>Start around a given point in a function you want to minimize. <strong>That would be the tree we fit in the first boosting iteration</strong> and the loss function, respectively.</li>
<li>We want to add a term to our prediction <strong>such that we move in the direction that minimizes our loss function</strong>. We know in which direction the function minimizes if we use <strong>directional derivatives and Steepest descent</strong>. Thus, we must make that the term we are adding, <strong>the predictions of our second tree</strong>, are as close as possible as to this value. <strong>The best way to do so? Direct this second tree to predict these values!</strong> <em>Note that each tree is working to predict different data: each tree is trying to predict values that give the best chance of overall model improvement, given the predictions of the tree that came before it.</em></li>
<li>Repeat (2) for further boosting iterations. Because we are essentially performing <strong>Steepest Descent</strong>, we will end up minimizing our loss function.</li>
</ol>
<p>Using math notation:</p>
<ol style="list-style-type: decimal">
<li>Fit a tree in the first boosting iteration <span class="math inline">\(T_1(\{R_{j,1} \}, \{c_{j,1} \},x)\)</span>.</li>
<li>Evaluate loss function around the point that is our prediction with the first tree. <strong>We want to move that point in the direction such that we minimize our loss function</strong> (which is the same as adding <span class="math inline">\(addition^*\)</span> to our first prediction). We already know which vector solves this problem!! <strong>The vector</strong> <span class="math inline">\(addition^*\)</span> <strong>that minimizes</strong> <span class="math inline">\(\nabla L(y_{row}, T_1(\{R_{j,1} \}, \{c_{j,1} \},x_{row})) \cdot addition\)</span>. <strong>Now that we know the solution to our problem the question is: can we arrive at this</strong> <span class="math inline">\(addition^*\)</span> <strong>by using a single tree</strong>.</li>
</ol>
<blockquote>
<p>Well the only way we can know is by trying. <strong>Let’s repeat the process for each row such that we have an <span class="math inline">\(addition^*\)</span> for each row and then put them all in a vector</strong>, <span class="math inline">\([addition_i^*]\)</span>. Then, let’s fit our second boosting iteration tree, <span class="math inline">\(T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)\)</span>, but <strong>this time, instead of trying to fit the tree to <span class="math inline">\(y\)</span> given our first prediction, we want to fit the tree to this new vector of</strong> <span class="math inline">\([addition_i^*]\)</span>. Once we have predicted these with the tree, add those predictions to our first tree predictions. The result of this addition is now our current prediction and our current point the in the Steepest Descent algorithm.</p>
</blockquote>
<blockquote>
<p>Note that each tree predicts a different quantity: <strong>a quantity that direct us where the biggest improvement, given the past trees’ predictions, can be done for each of the rows</strong>. That is, of all the quantities that we can try to predict, we are predicting the quantity that will help us the most into minimize our loss function. This is the “sequentially fitting weak learners models to sequentially modified versions of the data”. Given that we are always trying to predict a real-valued vector, <span class="math inline">\(addition*\)</span>, this will always be a regression problem that will be solved using a least squares loss function. <strong>That is, find the splits such that our predictions are as close to these <span class="math inline">\(additions*\)</span>’s as possible.</strong></p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><strong>If we do a good job across boosting iterations repeating (2), we would be updating</strong> <span class="math inline">\(L(y_{row}, T_1(\{R_{j,1} \}, \{c_{j,1} \},x) + T_{2}(\{R_{j,2} \}, \{c_{j,2} \} + ...,x_0))\)</span> by a value close enough to <span class="math inline">\(addition^*\)</span> at each iteration. Then, each boosting iteration is really a Steepest Descent iteartion and we ** will be end up minimizing our loss function**.</li>
</ol>
<p>Notice that it wasn’t a coincidence that we used the term <strong>addition</strong> when describing <strong>directional derivatives and Steepest descent</strong>. When performing step (2), we are literally adding to our predictions up to that point the predicted <span class="math inline">\(addition\)</span>’s. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. And that’s it: that’s how you perform Boosting by using Steepest descent to direct each iteration on the most promising direct to predict with your trees.</p>
</div>
<div id="conclusions-for-boosting" class="section level3">
<h3>Conclusions for Boosting</h3>
<ul>
<li><strong>Tree Depth</strong>: Notice how different Boosting is from Bagging. Whereas in the latter each of the individual learners was supposed to learn the whole complexity of the data, and we were trying to make each of these trees as uncorrelated as possible, in <strong>Boosting we are doing the complete opposite</strong>. Each tree is dependent from the other: each tree focuses on predicting that which, given the trees from former boosting iterations, has the best chances of helping us into minimizing our loss function. <strong>Thus, the complexity that each tree in Bagging was supposed to learn is now cleverly divided across trees when we are using Boosting.</strong></li>
</ul>
<blockquote>
<p>This change in the level of complexity we are assuming each of our trees must learn has big consequences. <strong>For our purposes, it is enough that a model truly learns a small part of the complexity in the data well enough, and that leaves that which cannot be easily learned from the data is seeing in this boosting iteration to other trees</strong>. In terms of parameters, this means that we will enforce the trees to have much lower levels of depth. <strong>We now care the most that each of the individual trees is as far from overfitting as possible. Otherwise, further boosting iterations will be wasted.</strong> Thus, we won’t care much about each individual tree’s bias and will require to them low levels of depth. From <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.363:</p>
</blockquote>
<blockquote>
<p>“Experience so far indicates that [# of final nodes in each tree] 4 ≤ J ≤ 8 works well in the context of boosting, with results being fairly insensitive to particular choices in this range”.</p>
</blockquote>
<ul>
<li><strong>Shrinkage</strong>: Until now, we have assumed that each of the trees will learn part of the complexity on the data. <strong>However, it is unlikely that each of them will be equally successful in learning its part</strong>. Some will learn it well, some will learn it bad. To avoid trusting each of the models too much, we can, instead of a simple sum, shrink the contribution of each tree: e.g.:</li>
</ul>
<p><span class="math display">\[v*T_1(\{R_{j,1} \}, \{c_{j,1} \},x) + v*T_{2}(\{R_{j,2} \}, \{c_{j,2} \},x)  + ... +v* T_{n}(\{R_{j,n} \}, \{c_{j,n} \},x)\]</span></p>
<blockquote>
<p>Where <span class="math inline">\(v\)</span> is called a learning rate: how quickly or slowly we learn from each boosting iteration.</p>
</blockquote>
<ul>
<li><p><strong>Subsampling</strong>: Much like we did with Bagging, we can pass different versions of the data to each tree. Here we won’t pass a bootstrap-version, but a random subset of the data without replacement. The size of the subset will be our hyperparameter.</p></li>
<li><p><strong>Random subset of features</strong>: Much like we did with Random Forests, we can pass over that at each tree, at each node, only a random subset of features to split on.</p></li>
<li><p><strong>Boosting iterations</strong>: Of course, <strong>the biggest hyperparameter to set is the number of Boosting iterations.</strong> This will determine the overall level of complexity the model can learn and how likely we are to overfit. Contrary to the number of Bagging trees, it is quite easy to overfit with this hyperparameter.</p></li>
</ul>
<p>Summarizing what we’ve said and putting it into <code>sklearn</code> code:</p>
<ul>
<li><strong>Boosting iterations</strong> = <code>n_estimators</code>.</li>
<li><strong>Tree depth</strong>: now it is better to control it with <code>max_depth</code>. A <code>max_depth</code> of 3 will set the maximum number of leaves equal to 3.</li>
<li><strong>Learning Rate</strong>: Shrinkage applied to each tree in the boosting. <code>learning_rate</code>.</li>
<li><strong>Subsampling</strong>: Size of the random subsample to pass to each tree. <code>subsample</code>.</li>
<li><strong>Random subset of features</strong>: Size of the random subset of features each tree can consider at each node. <code>max_features</code>.</li>
</ul>
<pre class="python"><code>from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
model = GradientBoostingClassifier(n_estimators = 200, max_depth = 3, subsample = 0.5,
                                   max_features = 0.5, learning_rate = 0.1)</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>When we are in a regression problem, this makes perfect sense: we are adding real-valued terms one after another. However, in classification problems, we cannot continually add to our first prediction, a class (or probability), the real-valued terms that we start to predict from the second iteration onward. To solve this problem, I recommend you look up <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">Elements of Statistical Learning</a> p.360 or the source code of the <a href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/gradient_boosting.py"><code>scikit-learn</code></a> function: <code>GradientBoostingClassifier</code>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

