<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>Causality: Counterfactuals - Clash of Worlds - David Salazar&#39;s blog</title>
<meta property="og:title" content="Causality: Counterfactuals - Clash of Worlds - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Causality: Counterfactuals - Clash of Worlds</h1>

    
    <span class="article-date">2020-08-10</span>
    

    <div class="article-content">
      


<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>We’ve seen how the language of causality require an exogenous intervention on the values of <span class="math inline">\(X\)</span>; so far we’ve studied interventions on all the population, represented by the expression <span class="math inline">\(do(X)\)</span>. Nevertheless, with this language, there are plenty of interventions that remain outside our realm: most notably, <strong>counterfactual expressions where the antecedent is in contradiction with the observed behavior</strong>: there’s a <em>clash</em> between the observed world and the <strong>hypothetical world of interest</strong>.</p>
<p>To solve this conundrum we will need to set up a more elaborate language whereby we leverage the <strong>invariant information</strong> from the observed world into the <em>hypothetical world</em>.</p>
<p>These type of counterfactual queries are fundamental in our study of causality; as they enable us to answer such questions as: interventions on sub-populations; additive interventions; mediation analysis through ascertaining direct and indirect effects; and to study probability of causation (sufficient and necessary causes).</p>
<p>In this post, all citations come from Pearl’s book: Causality.</p>
</div>
<div id="game-plan" class="section level2">
<h2>Game Plan</h2>
<p>In this blogpost, we will define <strong>Structural Causal Models</strong> (SCM) and explore how they encapsulate all the information we need in order to <em>study counterfactuals</em>. First, we’ll analyze why we cannot use the do-calculus to study counterfactuals where the antecedent contradicts the observed world. Secondly, we will define counterfactuals as <strong>derived properties</strong> of SCM and realize how interventional data undetermines counterfactual information. Thirdly, we will formulate a SCM to put what we have learned into action.</p>
</div>
<div id="we-change-by-taking-the-road-less-traveled-by" class="section level2">
<h2>We change by taking the road less traveled by</h2>
<p>Let’s play with Frost’s famous poem:</p>
<blockquote>
<p>Two roads diverged in a wood, and I—
I took the one less traveled by,
And that has made all the difference.</p>
</blockquote>
<p>What if Frost <strong>hadn’t taken</strong> the road less traveled by? Let’s say that by taking the road less traveled by, it took Frost <span class="math inline">\(Y=1\)</span> hour of driving time. Given how long it took him on the road less traveled by, how long would it had taken him on the other road?</p>
<p><span class="math display">\[
E[ Y| \  \text{do(Other road)}, Y = 1]
\]</span>
There’s a <strong>clash</strong> between the <span class="math inline">\(Y\)</span> we are trying to estimate and the observed <span class="math inline">\(Y = 1\)</span>. Unfortunately, the do-operator does not offer us the possibility of distinguishing between the two variables themselves: one standing for the <span class="math inline">\(Y\)</span> if we take the road less traveled by, the other <span class="math inline">\(Y\)</span> for the <strong>hypothetical</strong> <span class="math inline">\(Y\)</span> if Frost had taken the other road. That is, the <strong>different <span class="math inline">\(y\)</span>’s are events occurring in different worlds</strong>.</p>
<p>Because the do-calculus offers <em>no way of connecting the information across the different worlds</em>, it means that <strong>we cannot use interventional experiments to estimate the counterfactuals</strong>. Indeed, the Frost <em>after</em> taking the road less traveled by is a very different Frost <em>than he was before</em> taking any of the roads.</p>
</div>
<div id="the-ladder-of-causation" class="section level2">
<h2>The Ladder of Causation</h2>
<p>Before, we had seen that observational information <a href="https://david-salazar.github.io/2020/07/22/causality-invariance-under-interventions/">is not enough to distinguish between different causal diagrams</a>. We’ll show that the same thing happens with counterfactuals: information from interventions is not enough to distinguish between different Structural Causal Diagrams. Indeed, prediction, intervention and counterfactuals represent a <strong>natural hierarchy</strong> of <em>reasoning tasks</em>, with increasing levels of refinement and increasing demands on the knowledge required to accomplish them. Pearl calls this hierarchy the <strong>Ladder of Causation</strong>:</p>
<p><img src="/images/ladder.png" /></p>
<p>Whereas for prediction one only needs a joint distribution function, the analysis of intervention requires a causal structure; finally, <strong>processing counterfactuals</strong> requires information about the <strong>functional relationships</strong> that determine that determine the variables and/or the distribution of the <strong>omitted factors</strong>. We will encode all this necessary information with <strong>Structural Causal Models (SCM)</strong>.</p>
</div>
<div id="defining-counterfactuals" class="section level2">
<h2>Defining Counterfactuals</h2>
<p>A Structural Causal Model is a triplet of Unobserved Exogenous Variables (<span class="math inline">\(U\)</span>) called <strong>background variables</strong>, Observed Endogenous Variables (<span class="math inline">\(V\)</span>) and Functional relationships (<span class="math inline">\(F\)</span>) that map for each <span class="math inline">\(V_i\)</span> from their respective domain <span class="math inline">\(U_i \cup Pa_i\)</span> (<span class="math inline">\(Pa_i\)</span> are the parents of <span class="math inline">\(i\)</span>) into <span class="math inline">\(V_i\)</span> thus:</p>
<p><span class="math display">\[
v_i = f_i(Pa_i, u_i)
\]</span></p>
<p>Every SCM can be associated with a causal DAG. However, the graph merely identifies the endogenous and background variables; it does not specify the functional form of <span class="math inline">\(f_i\)</span> nor the distribution of the background variables.</p>
<p><strong>A counterfactual</strong> is defined by a submodel, <span class="math inline">\(M_x\)</span>, where the the functional relationship for <span class="math inline">\(X\)</span> is replaced to make <span class="math inline">\(X=x\)</span> hold true under any <span class="math inline">\(u\)</span>. Thus, the potential response of <span class="math inline">\(Y\)</span> to action <span class="math inline">\(do(X=x)\)</span> denoted by <span class="math inline">\(Y_x(u)\)</span> is the solution for <span class="math inline">\(Y\)</span> on the set of equations <span class="math inline">\(F_x\)</span> in <span class="math inline">\(M_x\)</span>.</p>
<p>If we define a probability function over the background variables, <span class="math inline">\(P(u)\)</span>, we can define the probability over the endogenous variables thus:</p>
<p><span class="math display">\[
P(Y = y) := \sum_{u | Y(u) = y} P(u)
\]</span></p>
<p>Therefore, the probability of counterfactual statements is thus derived:</p>
<p><span class="math display">\[
P(Y_x = y) := \sum_{u | Y_x(u) = y} P(u)
\]</span></p>
<p>Note that we can define <span class="math inline">\(P(Y = y | do(X=x)) = P(Y_x = y)\)</span>. This solution to the SCM coincides with the truncated factorization obtained by pruning arrows from a causal DAG.</p>
<div id="connecting-different-worlds-through-background-variables" class="section level3">
<h3>Connecting different worlds through background variables</h3>
<p>The determining feature of most counterfactuals is that we are interested in a <strong>conditional probability</strong> such that the information we are updating on is in contradiction with the counterfactual antecedent. In math terms:</p>
<p><span class="math display">\[
P(Y_{x&#39;} = y&#39; | X = x, Y = y) = \sum_u P(Y_{x&#39;}(u) = y&#39;) P(u | x, y)
\]</span>
First, notice that we are using the information from one causal world (<span class="math inline">\(&lt;M, u&gt;\)</span>) where we observe <span class="math inline">\((X=x, Y = y)\)</span> to find out the probability of a statement <span class="math inline">\(Y_x\)</span> in a different causal world (<span class="math inline">\(&lt;M_x, u&gt;\)</span>). That is, the <em>counterfactual antecedent</em> “<strong>must be evaluated under the same background conditions</strong> as those prevailing in the observed world”.</p>
<p>“The background variables are thus the main carriers of information from the actual world to the hypothetical world; they serve as the guardians of invariance.” To do so, we must first update our knowledge of <span class="math inline">\(P(u)\)</span> to obtain <span class="math inline">\(P(u|x, y)\)</span>. Therefore, to be able to answer counterfactual queries we must have a <strong>distribution for the background variables</strong>. Indeed, this key step is known as <strong>abduction</strong>: reasoning from evidence (<span class="math inline">\((x, y)\)</span> observed) to explanation (the background variables).</p>
<p>This is the fundamental characteristic counterfactual statements: we need to <em>route the impact of known facts</em> through U".</p>
</div>
<div id="after-abduction-comes-action-and-prediction" class="section level3">
<h3>After abduction comes action and prediction</h3>
<p>Once we have evaluated the prevailing background conditions, we use these in the sub-model <span class="math inline">\(M_{x&#39;}\)</span>, where <span class="math inline">\(x&#39;\)</span> is the antecedent of the counterfactual. Finally, we use the equations in this modified SCM to predict the probability of <span class="math inline">\(Y_{x&#39;}\)</span>, the consequence of the counterfactual.</p>
<p>Pearl has a great temporal metaphor for this whole process:</p>
<blockquote>
<p>Abduction explains the past (U) in light of the current evidence. The action bends the course of history to comply with the hypothetical condition <span class="math inline">\(X= x&#39;\)</span>. Finally, we predict the future based on our new understanding of the past and our newly established condition, <span class="math inline">\(X=x&#39;\)</span>.</p>
</blockquote>
</div>
</div>
<div id="probabilities-for-the-dead" class="section level2">
<h2>Probabilities for the dead</h2>
<p>Let’s formulate the following example that will show why information from interventions undetermines counterfactuals and that will serve as practice in computing counterfactuals. The example is taken from the excellent paper from <a href="https://causalai.net/r60.pdf">Bareinboim (et alter) (PDF)</a>:</p>
<p>Let <span class="math inline">\(\mathcal{M}^{*}=\left\langle\mathbf{U}=\left\{U_{1}, U_{2}\right\}, \mathbf{V}=\{X, Y\}, \mathcal{F}^{*}, P(U)\right\rangle,\)</span> where
<span class="math display">\[
\mathcal{F}^{*}=\left\{\begin{array}{ll}
X &amp; \leftarrow U_{1} \\
Y &amp; \leftarrow U_{2}
\end{array}\right.
\]</span></p>
<p>and <span class="math inline">\(U_1, U_2\)</span> are binary.</p>
<p>Notice that we expect that any intervention will lead us to conclude that the treatment <span class="math inline">\(X\)</span> is not effective: <span class="math inline">\(P(Y| do(X)) = P(Y)\)</span>. Suppose that we conclude exactly this with a RCT.</p>
<p><strong>Is this intervention evidence</strong> enough to argue for <span class="math inline">\(\mathcal{M}^{*}\)</span>? No! Interventional information undetermines counterfactual information. Notice that other SCM, <span class="math inline">\(\mathcal{M}^{&#39;}\)</span>, is also consistent with such causal effects and yet leads to a different counterfactual answer:</p>
<p><span class="math display">\[
\mathcal{F}^{\prime}=\left\{\begin{array}{ll}
X &amp; \leftarrow U_{1} \\
Y &amp; \leftarrow X U_{2}+(1-X)\left(1-U_{2}\right)
\end{array}\right.
\]</span></p>
<p>In both <span class="math inline">\(\mathcal{M}^{&#39;}\)</span> and <span class="math inline">\(\mathcal{M}^{*}\)</span>, we expect an intervention on <span class="math inline">\(X\)</span> to lead to no causal effect: <span class="math inline">\(P(Y| do(X)) = P(Y)\)</span>.</p>
<p><img src="https://cdn.mathpix.com/snip/images/iQHSNdrTUmxs6kJrz577MAtWDUpuP1GqchRppNY_lXo.original.fullsize.png" /></p>
<p>However, notice that they lead to very different answers for counterfactual queries. Suppose, then, that you have a patient <span class="math inline">\(S\)</span> that took the treatment and died: what is the probability that <span class="math inline">\(S\)</span> would have survived had they not been treated? We write this as <span class="math inline">\(P\left(Y_{X=0}=1 \mid X=1, Y=0\right),\)</span></p>
<blockquote>
<p>In <span class="math inline">\(\mathcal{M}^{*},\)</span> we have <span class="math inline">\(P^{\mathscr{M}^{*}}\left(Y_{X=0}=1 \mid X=1, Y=0\right)=0,\)</span> whereas in <span class="math inline">\(\mathcal{M}^{\prime}\)</span> we have the exact opposite pattern, <span class="math inline">\(P^{\mathscr{M}^{\prime}}\left(Y_{X=0}=1 \mid X=1, Y=0\right)=1\)</span>. These two models thus make diametrically opposed predictions about whether <span class="math inline">\(S\)</span> would have survived had they not taken the treatment.</p>
</blockquote>
<p>In other words, the best explanation for <span class="math inline">\(S\)</span> ’s death may be completely different depending on whether the world is like <span class="math inline">\(\mathcal{M}^{*}\)</span> or <span class="math inline">\(\mathcal{M}^{\prime}\)</span>. In <span class="math inline">\(\mathcal{M}^{*}, S\)</span> would hav died anyway, while in <span class="math inline">\(\mathcal{M}^{\prime}, S\)</span> would actually have survived, if only they had not been give the treatment.</p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>Counterfactual queries are a crucial part of our reasoning tools. Yet they pose a fundamental challenge: most of the time, the counterfactual antecedent contradicts the observed evidence. Thus, creating a clashing of worlds between the observed world and the hypothetical world that is our object of study.</p>
<p>To reconcile these two worlds, we must posit a SCM that <strong>leverages the invariant information across causal worlds</strong>: the background variables. Once we have this information, we can answer counterfactual queries. Finally, we saw how interventional information is far from being sufficient to deliver answer to these queries.</p>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

