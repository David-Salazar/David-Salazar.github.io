{
  "hash": "f17b6d6a47f74fa00dba7cb179356516",
  "result": {
    "markdown": "---\ntitle: 'Causality: Invariance under Interventions'\nauthor: ''\ndate: '2020-07-22'\nslug: causality-invariance-under-interventions\ncategories: []\ntags: []\n---\n\n\n\n\nIn the [last post](https://david-salazar.github.io/2020/07/18/causality-bayesian-networks/) we saw how two causal models can yield the same testable implications and thus cannot be distinguished from data alone. That is, we cannot gain causal understanding from data alone. Does that mean that we cannot ever gain causal understanding? Far from it; it just means that we must have a causal model. \n\n**Thus, causal effects cannot be estimated from the data itself without a causal story.** In this blogpost, I'll show how exactly the combination between causal models and observational data can lead us into estimating causal effects. In short, causal effects can be estimated by leveraging the invariant information that the pre-intervention distribution can provide. Doing so, we connect pre-intervention probabilities with the post-intervention probabilities that define the causal effect.  \n\n## Defining the causal effect with the do-operator\n\nFundamentally, we cannot gain causal understanding with data because **the data we see could have been generated by many a causal models**. That is, the associations we see, $P(Y | X)$, can be the result of many interactions; **some of them causal and some purely observational**. We can say that any statistically meaningful association is *the result of a causal relationship* **somewhere in the system**, but *not necessarily* of the causal effect of interest, $X \\rightarrow Y$.\n\nTo disentangle this confusion, then, let's define a causal effect. [Following Pearl](https://fabiandablander.com/r/Causal-Inference.html), we will take an **interventionist position** and say that a variable $X$ has a causal influence on $Y$ if intervening to change $X$ leads to changes in $Y$. Intervening on $X$ means lifting $X$ from whatever mechanism previously defined its value and now set it to a particular value $X=x$ in an exogenous way. \n\nThus, the **causal effect** is defined as a *function* from the values $X$ can take to the space of *probability distributions* on $Y$. For example, if $X := x$, then we arrive at the **interventional distribution** $P(Y| \\text{do}(x))$: the population distribution of $Y$ if *everyone* in the population had their $X$ value fixed at $x$.\n\nThe $\\text{do}$ operator defines the exogenous process through which we have intervened to set the value of $X := x$. Finally, we derive $P(Y| \\text{do}(x))$ for every possible $x$ and test whether the distribution changes as we change the value $X$ takes. \n\nTherefore, to study the causal effect of $X$ is to change the system by determining the value of $X$ outside of it and seeing how the effects cascade thorough the system. However, before we change a system we must define it. How to represent the system? With a Causal Graph!\n\n## Causal Graphs\n\nThe question, then, becomes: **how can we simulate the effects of intervening in the causal system?**. First, however, we must define the system in question. \n\nLet each node represent one of the variables of interest. We will draw an arrow from $X$ to $Y$ if there is a **direct causal effect** from $X$ to $Y$ for at least one individual. Alternatively, **the lack of an arrow** means that *there's no* causal effect for any individual in the population. We will assume that the system is adequately written if **all common causes** of *any pair* of variables on the graph are **themselves on the graph**. Finally, we'll say that a variable is always a cause of its descendants. \n\nWe will link Causal Graphs to Bayesian graphs by assuming that each variable, conditional on its parents, is independent of any variable for which it is not a cause (i.e., all its predecessors). In turn, this will imply that the **Graph defines the same recursive decomposition of the joint distribution** as a *Bayesian Graph*:\n\n$$\nP\\left(x_{1}, \\ldots, x_{n}\\right)=\\prod_{j} P\\left(x_{j} \\mid pa_j\\right)\n$$\n\nThereby, we can derive, using the d-separation criterion, **testable implications** of our causal models. \n\nTo make things more concrete, let's work with the following fork: let's say that a new treatment is developed to reduce cholesterol. However, women take the treatment more/less than men and have higher/lower levels of cholesterol. How to compute the causal effect of the treatment on cholesterol?  \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-22-causality-invariance-under-interventions_files/figure-html/drug, coffee-1.png){width=672}\n:::\n:::\n\n\n## Interventions: Eliminating incoming arrows\n\nIntervening on $X$ such that $\\text{do(X = 1)}$ amounts to curtailing the previous mechanism that defined $X$. In Graph lingo: **eliminate the incoming arrows into $X$**: *gender no longer cause $X$*. Therefore, we eliminate the arrow from Gender into treatment. Thus, an intervention is equivalent to eliminating arrows in a Causal Graph. Let's label this new graph $G_m$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-22-causality-invariance-under-interventions_files/figure-html/drug-eliminated-1.png){width=672}\n:::\n:::\n\n\n## Invariant probabilities under intervention\n\nThe mutilated graph is still a Causal Graph. Thus, it implies a *particular decomposition of the joint probability* ($P_m$) of it's own. With respect to this post-intervention distribution, we can define the causal effect: $P(Y=y|\\text{do}(X=x)) := P_m (Y=y|X=x)$. However, this new post-intervention distribution $P_m$ is **not totally disconnected** from the pre-intervention distribution ($P$) that we can study with observational data. \n\nThere are two **invariant qualities** that are the same in the *pre-intervention and post-intervention* distribution:\n\n- Our intervention is **atomic**: there are no side effects that alter the way non-descendants of $X$ are determined. Thus, $P_m(Z=z| X=x) = P(Z=z)$.\n\n- The conditional probability $Y$ is invariant, because the mechanism by which Y responds to $X$ and $Z$ remains the same, *regardless* of whether $X$ **changes spontaneously or by deliberate manipulation**. Thus; $P_m(Y| X=x, Z = z) = P(Y|X=x, Z=z)$.\n\n### Connecting pre-intervention probabilities with post-treament\n\nTherefore, using probability laws and our independence assumption between $X$ and $Z$ in the mutilated graph, we define the causal effect in terms of post-intervention distribution thus:\n\n$$\n\\begin{array}{l}\nP(Y = y | do(X=x)) := P_m (Y=y|X=x) \\\\\n=\\sum_{z} P_{m}(Y=y \\mid X=x, Z=z) P_{m}(Z=z \\mid X=x) \\\\\n=\\sum_{z} P_{m}(Y=y \\mid X=x, Z=z) P_{m}(Z=z)\n\\end{array}\n$$\nLuckily, all the terms invariant: both terms can be **connected to the original pre-intervention** probability distribution:\n\n$$\nP(Y=y \\mid d o(X=x))=\\sum_{z} P(Y=y \\mid X=x, Z=z) P(Z=z)\n$$\nTherefore, we arrive at a definition of the **causal effect in terms of the pre-treatment distribution**. Thus, we can **estimate the causal effect from observational studies** without the need of *actually carrying out* the intervention. \n\n## The Adjustment Formula\n\nMore generally, we define the causal effect in terms of pre-intervention probability thus. Given a graph $G$ in which a set of variables $pa$ are designated as the parents of $X$, the causal effect of $X$ on $Y$ is given by:\n\n$$\nP(Y=y|\\text{do}(X=x)) = \\sum_{z} P(Y=y | X=x, P A=z) P(pa=z)\n$$\nTherefore, we can conclude why it is necessary to have a causal story to be able to estimate the causal effect: **to identify the parents of $X$ and adjust for them**: first condition $P(Y=y| X =x)$ on $PA$ and then average the result, weighted the prior probability of $pa = z$.\n\n## An example\n\nLet's follow our thought experiment with our previous graph. In the [experiment](https://fabiandablander.com/r/Causal-Inference.html), we observe both men and women who decide whether they take the drug or not. The results are the following:\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div id=\"apqbwdcovh\" style=\"overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>html {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#apqbwdcovh .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#apqbwdcovh .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#apqbwdcovh .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#apqbwdcovh .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#apqbwdcovh .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#apqbwdcovh .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#apqbwdcovh .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#apqbwdcovh .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#apqbwdcovh .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#apqbwdcovh .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#apqbwdcovh .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#apqbwdcovh .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#apqbwdcovh .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#apqbwdcovh .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#apqbwdcovh .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#apqbwdcovh .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#apqbwdcovh .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#apqbwdcovh .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#apqbwdcovh .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#apqbwdcovh .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#apqbwdcovh .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#apqbwdcovh .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#apqbwdcovh .gt_left {\n  text-align: left;\n}\n\n#apqbwdcovh .gt_center {\n  text-align: center;\n}\n\n#apqbwdcovh .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#apqbwdcovh .gt_font_normal {\n  font-weight: normal;\n}\n\n#apqbwdcovh .gt_font_bold {\n  font-weight: bold;\n}\n\n#apqbwdcovh .gt_font_italic {\n  font-style: italic;\n}\n\n#apqbwdcovh .gt_super {\n  font-size: 65%;\n}\n\n#apqbwdcovh .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#apqbwdcovh .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#apqbwdcovh .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#apqbwdcovh .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#apqbwdcovh .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#apqbwdcovh .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#apqbwdcovh .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\">\n  \n  <thead class=\"gt_col_headings\">\n    <tr>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">Recovered</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">N</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">Treatment</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\">Gender</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td class=\"gt_row gt_right\">81</td>\n<td class=\"gt_row gt_right\">87</td>\n<td class=\"gt_row gt_right\">1</td>\n<td class=\"gt_row gt_left\">Male</td></tr>\n    <tr><td class=\"gt_row gt_right\">234</td>\n<td class=\"gt_row gt_right\">270</td>\n<td class=\"gt_row gt_right\">0</td>\n<td class=\"gt_row gt_left\">Male</td></tr>\n    <tr><td class=\"gt_row gt_right\">192</td>\n<td class=\"gt_row gt_right\">263</td>\n<td class=\"gt_row gt_right\">1</td>\n<td class=\"gt_row gt_left\">Female</td></tr>\n    <tr><td class=\"gt_row gt_right\">55</td>\n<td class=\"gt_row gt_right\">80</td>\n<td class=\"gt_row gt_right\">0</td>\n<td class=\"gt_row gt_left\">Female</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n:::\n:::\n\nWhen we study the data across genders, we find out that the patients who didn't take the drug had a higher rate of recovery:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nHowever, once we separate the data by gender, the opposite picture arises:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-22-causality-invariance-under-interventions_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\nWe have a case of Simpson's Paradox! Let's use the causal knowledge embedded in our graph to estimate the true causal effect of the treatment. Given that Gender is the only parent of Treatment, we will adjust for it:\n\n\\[\nP(Y=1 \\mid d o(X=1))=\\frac{0.93(87+270)}{700}+\\frac{0.73(263+80)}{700}=0.832\n\\]\nwhile, similarly,\n\\[\nP(Y=1 \\mid d o(X=0))=\\frac{0.87(87+270)}{700}+\\frac{0.69(263+80)}{700}=0.7818\n\\]\nThus, comparing the effect of drug-taking \\( (X=1) \\) to the effect of nontaking \\( (X=0), \\) we obtain\n\\[\nA C E=P(Y=1 \\mid d o(X=1))-P(Y=1 \\mid d o(X=0))=0.832-0.7818=0.0502\n\\]\n\nHowever, if Gender had not been a parent of Treatment (i.e., if both Genders decide to take the treatment equally), our Causal effect would be different because we would adjust for Gender in the first place. \n\n## Identifiable\n\nWe've estimated causal effects with a pretty simple strategy: adjust for the parents of the exposure and average those effects weighted by the probability of the parents.  \n\nTherefore, according to our strategy, the causal effect will be **identifiable** whenever both $X, Y$ and the parents of $X$, $pa$ are measured. Whenever measurements for some of them are missing, we must use other techniques to estimate the causal effect.\n\n## Addendum: RCT\n\nRandomized Control Trials are sometimes referred to as the gold standard in causal inference. However, in our framework, they are nothing more than a **different graph surgery**. Whereas before we cut all the incoming arrows into treatment, now we replace all the incoming arrows with only with one arrow that signifies the randomization of the treatment:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-22-causality-invariance-under-interventions_files/figure-html/rct-1.png){width=768}\n:::\n:::\n\n\nTherefore, now we must simply adjust by randomization to estimate the causal effect of treatment. Does that mean that they are not useful? No, they will always have the upper hand when we are uncertain about our causal model. If there is another parent of treatment that we are not accounting for, Randomization will offer a clean solution. ",
    "supporting": [
      "2020-07-22-causality-invariance-under-interventions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}