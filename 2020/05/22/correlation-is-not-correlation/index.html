<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>Correlation is not Correlation - David Salazar&#39;s blog</title>
<meta property="og:title" content="Correlation is not Correlation - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">15 min read</span>
    

    <h1 class="article-title">Correlation is not Correlation</h1>

    
    <span class="article-date">2020-05-22</span>
    

    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#correlation-is-not-correlation">Correlation is not correlation</a></li>
<li><a href="#small-sample-effects-with-sample-correlation">Small sample effects with Sample Correlation</a><ul>
<li><a href="#sample-correlation-in-mediocristan">Sample Correlation in Mediocristan</a></li>
<li><a href="#sample-correlations-from-extremistan">Sample correlations from Extremistan</a></li>
</ul></li>
<li><a href="#misused-correlation">Misused Correlation</a><ul>
<li><a href="#quadrants-correlation">Quadrant’s Correlation</a></li>
<li><a href="#berksons-paradox">Berkson’s paradox</a></li>
<li><a href="#simpsons-paradox">Simpson’s Paradox</a></li>
<li><a href="#correlation-under-non-linearities">Correlation under non linearities</a></li>
</ul></li>
<li><a href="#misunderstanding-of-correlation-its-signal-is-non-linear">Misunderstanding of Correlation: its signal is non linear</a><ul>
<li><a href="#entropy-and-mutual-information">Entropy and Mutual Information</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<div id="correlation-is-not-correlation" class="section level1">
<h1>Correlation is not correlation</h1>
<p>To the usual phrase of correlation is not causation, Nassim Taleb often answers: correlation is not correlation. First, just like the <a href="2020-04-17-fat-vs-thin-does-lln-work.html">mean</a> and <a href="2020-04-27-spurious-pca-under-thick-tails.html">PCA</a>, the sample correlation coefficient has persistent small sample effects when variables from Extremistan are involved. These topics are analyized in his latest book: <a href="https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf">Statistical Consequences of Fat Tails</a>. I’ll explore this with some Monte-Carlo simulations from both Mediocristan and Extremsitan.</p>
<p>Secondly, however, as Taleb says, even if we are in Mediocristan, the correlation coefficient is commonly <strong>misued and/or misunderstood</strong>. Commonly <strong>misused</strong> because <em>people take non-random subsamples and expect the same correlation</em>. Thereby arriving at some paradoxes: Berkson’s paradox (Collider Bias) and Simpson’s Paradox. I’ll take non random subsamples from a underlying random sample to show how you can get both paradoxes. Also, it is commonly misused because Correlation should not be used when there’s a non-linear relationship between the variables; otherwise, you get deceiving results. <strong>Commonly misunderstood</strong> because the amount of information it conveys is not linear. That is, a correlation of 0.9 conveys much, much more information than 0.7. Taleb analyzes these topics in a terrific, short paper called <a href="https://www.dropbox.com/s/18pjy7gmz0hl6q7/Correlation.pdf?dl=0">Fooled by Correlation: Common Misinterpretations in Social “Science”</a></p>
</div>
<div id="small-sample-effects-with-sample-correlation" class="section level1">
<h1>Small sample effects with Sample Correlation</h1>
<div id="sample-correlation-in-mediocristan" class="section level2">
<h2>Sample Correlation in Mediocristan</h2>
<p>Let’s do some Monte-Carlo simulations in Mediocristan. I’ll be sampling from two independent gaussians; that is, we know the “true” correlation is 0 and then we will compare it with the sample-correlation.</p>
<pre class="r"><code>correlation_mediocristan &lt;- function(n, rho = 0) {
  # sample from multivariate normal
  data &lt;- rnorm2d(n = n, rho = rho)
  
  # compute sample correlation
  sample_correlation &lt;- cor(data)[1,2]
  sample_correlation
}

rerun(10000, correlation_mediocristan(20, 0)) %&gt;% 
  unlist() -&gt; mediocristan_correlations_20_obs

rerun(10000, correlation_mediocristan(1000, 0)) %&gt;% 
  unlist() -&gt; mediocristan_correlations_1000_obs

data.frame(sim = 1:10000, small_sample = mediocristan_correlations_20_obs,
           large_sample = mediocristan_correlations_1000_obs) %&gt;% 
  pivot_longer(-sim, names_to = &quot;sample&quot;, values_to = &quot;sample_correlation&quot;) %&gt;% 
  mutate(sample = if_else(sample == &quot;large_sample&quot;, &quot;B: Sample with 1,000 observations&quot;, 
                          &quot;A: Sample with 20 observations&quot;)) -&gt; gaussian_corr</code></pre>
<p>Now we plot them:</p>
<pre class="r"><code>gaussian_corr %&gt;% 
  ggplot(aes(sim, sample_correlation)) +
  geom_col(aes(fill = sample)) +
  facet_wrap(~sample) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;none&quot;) +
  scale_y_continuous(limits = c(-1, 1)) +
  scale_x_continuous(labels = scales::comma_format()) +
  labs(title = &quot;Sample correlations across different simulations&quot;,
       subtitle = &quot;Sample correlation quickly converges. Variables are independent&quot;,
       y = &quot;sample correlation&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>gaussian_corr %&gt;% 
  ggplot(aes(sample_correlation, fill = sample)) +
  geom_histogram(binwidth = 0.05, color = &quot;black&quot;, alpha = 0.5) +
  facet_wrap(~sample) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Distribution of Sample correlations in Mediocristan&quot;,
       subtitle = &quot;Sample correlation quickly converges. Variables are independent&quot;,
       x = &quot;sample correlation&quot;,
       caption = &quot;Histogram binwidth = 0.05&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Therefore, we can use our sample correlation coefficients in Mediocristan as they quickly converge. That is, with randomness coming from Mediocristan, <strong>noise quickly washes out with relatively small sample size</strong></p>
</div>
<div id="sample-correlations-from-extremistan" class="section level2">
<h2>Sample correlations from Extremistan</h2>
<p>To examine the slow convergence of the sample correlation coefficient, Taleb proposes a bivariate t-student distribution with exponent 2/3. Notice that the mean and variance are undefined. Yet, there is a finite correlation. Let’s replicate the same experiment as we did in mediocristan:</p>
<pre class="r"><code>correlation_bivariate_t &lt;- function(n, rho = 0) {
  # sample from multivariate normal
  data &lt;- rt2d(n, rho = rho, nu = 2/3)
  
  # compute sample correlation
  sample_correlation &lt;- cor(data)[1,2]
  sample_correlation
}

rerun(10000, correlation_bivariate_t(20, rho = 0)) %&gt;% 
  unlist() -&gt; bivariate_t_20

rerun(10000, correlation_bivariate_t(1000, rho = 0)) %&gt;% 
  unlist() -&gt; bivariate_t_thousand

data.frame(sim = 1:10000, bivariate_t_20, bivariate_t_thousand) %&gt;% 
  pivot_longer(-sim, names_to = &quot;sample_size&quot;, values_to = &quot;sample_correlation&quot;) %&gt;% 
  mutate(sample_size = if_else(sample_size == &quot;bivariate_t_20&quot;, 
                               &quot;A: Sample with 20 observations&quot;, &quot;B: Sample with 1,000 observations&quot;)) -&gt; t_corr </code></pre>
<p>Now, let’s plot the results:</p>
<pre class="r"><code>t_corr %&gt;% 
  ggplot(aes(sim, sample_correlation)) +
  geom_col(aes(fill = sample_size)) +
  facet_wrap(~sample_size) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;none&quot;) +
  scale_y_continuous(limits = c(-1, 1)) +
  scale_x_continuous(labels = scales::comma_format()) +
  labs(title = &quot;Sample correlations across different simulations&quot;,
       subtitle = &quot;Sample correlation is just as erratic, regardless of sample size. True correlation is zero. &quot;,
       y = &quot;sample correlation&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
<pre class="r"><code>t_corr %&gt;% 
  ggplot(aes(sample_correlation, fill = sample_size)) +
  geom_histogram(color = &quot;black&quot;, alpha = 0.5, binwidth = 0.05) +
  facet_wrap(~sample_size) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Distribution of Sample correlations from Extremistan&quot;,
       subtitle = &quot;Sample correlation suffers from small sample effect. True correlation is zero.&quot;,
         x = &quot;sample correlation&quot;,
       caption = &quot;Histogram binwidth = 0.05&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>As Taleb writes: “finite correlation doesn’t mean low [estimator’s] variance: it exists, but may not be useful for statistical purposes owing to the noise and slow convergence.”</p>
<p>Finally, let’s experiment what happens if we make our samples 10k big:</p>
<pre class="r"><code>rerun(10000, correlation_bivariate_t(10000, rho = 0)) %&gt;% 
  unlist() -&gt; bivariate_t_10k</code></pre>
<p>Let’s plot the results:</p>
<pre class="r"><code>data.frame(sample_size_10k = bivariate_t_10k) %&gt;% 
  ggplot(aes(sample_size_10k)) +
  geom_histogram(color = &quot;black&quot;, alpha = 0.5, binwidth = 0.05) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  labs(title = &quot;Distribution of Sample correlations from Extremistan&quot;,
       subtitle = &quot;Sample (n = 10k) correlation suffers from small sample effect. True correlation is zero.&quot;,
         x = &quot;sample correlation&quot;,
       caption = &quot;Histogram binwidth = 0.05&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-9-1.png" width="768" /></p>
<p>That is, the sample correlations suffers from persistent small sample effect. Thus, there’s to much noise to use the sample correlation for fat-tailed distributions.</p>
</div>
</div>
<div id="misused-correlation" class="section level1">
<h1>Misused Correlation</h1>
<p>There are two famous statistical paradoxes regarding correlation: Berkson’s paradox and Simpson’s Paradox. Both arise from the wrong expectation that the total correlation will be preserved when taking non-random subsamples. Indeed, it is quite the opposite. <a href="https://twitter.com/nntaleb/status/1108541435140366336">Taleb</a>: “all non-random subsamples will yield a correlation below the total one”</p>
<div id="quadrants-correlation" class="section level2">
<h2>Quadrant’s Correlation</h2>
<p>To get some intuition about the problem, let’s sample <span class="math inline">\(10^6\)</span> from a multivariate normal with correlation of <span class="math inline">\(0.75\)</span>. Then, we will group the data into geometrically intuitive non-random partitions: into quadrants. Then, we will calculate the correlation between the observations within each quadrant.</p>
<pre class="r"><code>n &lt;- 10^6
rho &lt;- 3/4 
data &lt;- rnorm2d(n = n, rho = rho) 

data.frame(data) %&gt;% 
  rename(x = X1, y = X2) %&gt;% 
  mutate(quadrant = case_when(
    x &gt; 0 &amp; y &gt; 0 ~ &quot;II&quot;,
    x &lt; 0 &amp; y &gt; 0 ~ &quot;I&quot;,
    x &lt; 0 &amp; y &lt; 0 ~ &quot;IV&quot;,
    x &gt; 0 &amp; y &lt; 0 ~ &quot;III&quot; 
    )) %&gt;% 
  group_by(quadrant) %&gt;% 
  summarise(correlation = cor(x, y)) %&gt;% 
  gt::gt() %&gt;% 
  gt::fmt_number(vars(correlation))</code></pre>
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#xsyojmmzur .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#xsyojmmzur .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xsyojmmzur .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#xsyojmmzur .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#xsyojmmzur .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xsyojmmzur .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xsyojmmzur .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#xsyojmmzur .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#xsyojmmzur .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#xsyojmmzur .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#xsyojmmzur .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#xsyojmmzur .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#xsyojmmzur .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#xsyojmmzur .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#xsyojmmzur .gt_from_md > :first-child {
  margin-top: 0;
}

#xsyojmmzur .gt_from_md > :last-child {
  margin-bottom: 0;
}

#xsyojmmzur .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#xsyojmmzur .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#xsyojmmzur .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xsyojmmzur .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#xsyojmmzur .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xsyojmmzur .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#xsyojmmzur .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xsyojmmzur .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xsyojmmzur .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#xsyojmmzur .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xsyojmmzur .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#xsyojmmzur .gt_left {
  text-align: left;
}

#xsyojmmzur .gt_center {
  text-align: center;
}

#xsyojmmzur .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#xsyojmmzur .gt_font_normal {
  font-weight: normal;
}

#xsyojmmzur .gt_font_bold {
  font-weight: bold;
}

#xsyojmmzur .gt_font_italic {
  font-style: italic;
}

#xsyojmmzur .gt_super {
  font-size: 65%;
}

#xsyojmmzur .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
</style>
<div id="xsyojmmzur" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;"><table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">quadrant</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">correlation</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr>
      <td class="gt_row gt_left">I</td>
      <td class="gt_row gt_right">0.19</td>
    </tr>
    <tr>
      <td class="gt_row gt_left">II</td>
      <td class="gt_row gt_right">0.53</td>
    </tr>
    <tr>
      <td class="gt_row gt_left">III</td>
      <td class="gt_row gt_right">0.18</td>
    </tr>
    <tr>
      <td class="gt_row gt_left">IV</td>
      <td class="gt_row gt_right">0.53</td>
    </tr>
  </tbody>
  
  
</table></div>
<p>Therefore, we should not expect to have the same correlation in non-random subsamples of the data as we have in the whole sample.</p>
</div>
<div id="berksons-paradox" class="section level2">
<h2>Berkson’s paradox</h2>
<p>In Berkson’s paradox, there appears to be a negative correlation between 2 variables in a subsample; when in fact there is no correlation between them considering the whole. The bias is introduced by sub-sampling our observations based on another variable that the 2 variables in question jointly determine. In DAG slang, it is collider bias.</p>
<p>Imagine, then, that we have samples from two gaussian variables that are independent.</p>
<pre class="r"><code>n &lt;- 10^4
rho &lt;- 0 
data &lt;- rnorm2d(n = n, rho = rho) 

data.frame(data) %&gt;% 
  rename(x = X1, y = X2) -&gt; data_berkson

data_berkson %&gt;% 
  ggplot(aes(x, y)) +
  geom_point(alpha = 0.1) +
  geom_smooth(linetype = 2, color = &quot;red&quot;, se = FALSE) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;Variables are uncorrelated in the whole sample&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>However, they jointly determine another variable <span class="math inline">\(Z\)</span> thus: <span class="math inline">\(Z\)</span> is zero except when either <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> are greater than <span class="math inline">\(0\)</span>, in which case we have <span class="math inline">\(Z = 1\)</span>. Then, sampling conditioning on <span class="math inline">\(Z = 1\)</span> is non a random subsample. Therefore, as we know that Correlation is subadditive, we know that we do not expect the same correlation as in the whole sample.</p>
<pre class="r"><code>data_berkson %&gt;% 
  mutate(z = if_else(x &gt; 0 | y &gt; 0, 1, 0))-&gt; data_berkson_z

data_berkson_z %&gt;% 
  ggplot(aes(x, y, color = factor(z))) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, linetype = 2) +
  hrbrthemes::theme_ipsum_rc() +
  scale_color_viridis_d(begin = 1, end = 0) +
  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = &quot;red&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Berkson&#39;s paradox: Negative relation between independent vars&quot;,
       subtitle = &quot;Correlation does weird things on non-random sub samples.&quot;,
       caption = &quot;Red line is the whole sample trend&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
<p>Intuitively, if we know that <span class="math inline">\(X\)</span> is negative, due to our conditioning on <span class="math inline">\(Z\)</span>, then it must be the case that <span class="math inline">\(Y\)</span> is positive. Therefore, conditioning on <span class="math inline">\(Z\)</span> creates a negative correlation between two independent variables.</p>
<p>Therefore, if we condition on <span class="math inline">\(Z=1\)</span>, a non-random subsample (thereby opening a collider), we get a negative correlation from two independent variables: Berkson’s paradox. <strong>Again, we should not expect the global correlation to stay constant under non-random sub samples.</strong></p>
</div>
<div id="simpsons-paradox" class="section level2">
<h2>Simpson’s Paradox</h2>
<p>To consider Simpson’s paradox, let’s consider 3 random variables. <span class="math inline">\(X\)</span> is going to determine the value of <span class="math inline">\(Z\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are jointly going to determine the value <span class="math inline">\(Y\)</span></p>
<p>On the whole sample, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are positively related</p>
<pre class="r"><code>n &lt;- 10000
x &lt;- rnorm(n)
z &lt;- rnorm(n, 1.2*x) 
y &lt;- rnorm(n, -0.8*x + z)

data.frame(x, y, z) %&gt;% 
  ggplot(aes(x, y)) +
  geom_point(alpha = 0.1) +
  geom_smooth(se = FALSE, linetype = 2) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;X and Y are positively related in the whole sample&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>However, once we condition on <span class="math inline">\(Z\)</span>, that is, take non-random sub-samples according to the values of <span class="math inline">\(Z\)</span>, we will see a reversal of the sign of the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<pre class="r"><code>data.frame(x, y, z) %&gt;% 
  mutate(z = cut(z, 10)) %&gt;% 
  ggplot(aes(x, y, color = z)) +
  geom_point(alpha = 0.1)  +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = &quot;red&quot;) +
  hrbrthemes::theme_ipsum_rc() + 
  theme(legend.position = &quot;none&quot;) +
  scale_color_viridis_d() + 
  labs(title = &quot;Simpson&#39;s Paradox: Reversal of correlation sign&quot;,
       subtitle = &quot;Correlation does weird things on non-random sub samples.&quot;,
       caption = &quot;Red line is whole sample trend&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-14-1.png" width="768" /></p>
<p>We are doing something extremely similar to what linear regression does. Once we adjust for <span class="math inline">\(Z\)</span>, what’s left is the negative effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. <strong>Again, we should not expect the global correlation to stay constant under non-random sub-samples.</strong></p>
</div>
<div id="correlation-under-non-linearities" class="section level2">
<h2>Correlation under non linearities</h2>
<p>Sometimes, the association between two variables depends on the levels of the variables themselves. That is, there is a non-linear relationship between the two variables. In these types of situations it’s a mistake to use total correlation. Taleb proposes the following experiment, which he calls “Dead Man Bias”:</p>
<blockquote>
<p>You administer IQ tests to 10k people, then give them a “performance test” for anything, any task. 2000 of them are dead. Dead people score 0 on IQ and 0 on performance. The rest have the IQ uncorrelated to the performance to the performance. What is the spurious correlation IQ/Performance?</p>
</blockquote>
<p>Let’s perform this thought experiment:</p>
<pre class="r"><code>n &lt;- 10000
p_dead &lt;- 0.2

dead &lt;- rbernoulli(n, p_dead)

performance &lt;- runif(n)
iq &lt;- runif(n)

performance[dead] &lt;- 0
iq[dead] &lt;- 0</code></pre>
<p>Let’s plot our results:</p>
<pre class="r"><code>data.frame(iq, performance, dead) %&gt;% 
  ggplot(aes(iq, performance, color = dead)) +
  geom_point(alpha = 0.1) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;You&#39;ll never guess a positive relationship&quot;,
       subtitle = &quot;Until you notice the dark spot at the origin&quot;) +
  scale_color_viridis_d(begin = 1, end = 0) +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Looking at the data, you’ll never believe there’s a positive correlation. However, the dark spot in the origin changes the whole history. Let’s calculate the correlation coefficient:</p>
<pre class="r"><code>cor(iq, performance)</code></pre>
<pre><code>## [1] 0.3849505</code></pre>
<p>That is, iq (in this thought experiment, at least) is not a predictor of good performance. However, it does matter: the dark spot (the dead people) fools correlation into thinking that if <span class="math inline">\(iq=0\)</span> means <span class="math inline">\(performance=0\)</span>, then the relationship must hold for the entire range of observations. That is, the extremely non-linear relationship between iq and performance fools the correlation coefficient.</p>
<p>Moral of the story, <strong>never use correlation when there’s an indication of non-linear relationship between the variables</strong>.</p>
</div>
</div>
<div id="misunderstanding-of-correlation-its-signal-is-non-linear" class="section level1">
<h1>Misunderstanding of Correlation: its signal is non linear</h1>
<p>Although we’ve just said that correlation can only model linear relationships, it cannot be interpreted linearly. That is, the information that correlation picks up about the association between two variables does not scale linearly with correlation.</p>
<div id="entropy-and-mutual-information" class="section level2">
<h2>Entropy and Mutual Information</h2>
<p>From information theory, we get a measure of the inherent uncertainty in a distribution. Information Entropy, which is just: <span class="math inline">\(-E[log(p_i)]\)</span>. Also, there is divergence: the extra uncertainity that is induced when we approximate the distribution p with the distribution q: <span class="math inline">\(E[log(\dfrac{p_i}{q_i})]\)</span>.</p>
<p>In the same way, there is the concept of Mutual Information: the extra uncertainity that is induced when we approximate a joint probability distribution with the product of their marginal distributions. If we don’t increase the uncertainty, then we do not lose information by modelling them independently. Therefore, <strong>it is a measure of dependence: the higher, the more dependence between the variables.</strong></p>
<p>For the Gaussian case, Taleb shows that the Mutual information is:</p>
<p><span class="math display">\[ MI = -\dfrac{1}{2} log (1 - \rho^2) \]</span></p>
<p>Therefore, the information about the association between two variables that Correlation conveys does not scale linearly with the correlation coefficient. Indeed, MI has a convex response to <span class="math inline">\(\rho\)</span>:</p>
<pre class="r"><code>rho &lt;- seq(-1, 1, length.out = 100)
mutual_information = -1/2* log(1 - rho^2)

data.frame(rho, mutual_information) %&gt;%
  mutate(association = if_else(rho &lt; 0, &quot;negative&quot;, &quot;positive&quot;)) %&gt;% 
  ggplot(aes(rho, mutual_information, fill = association)) +
  geom_area(alpha = 0.5) +
  scale_fill_viridis_d() +
  hrbrthemes::theme_ipsum_rc() +
  theme(legend.position = &quot;none&quot;) +
  labs(subtitle = &quot;Information conveyed by correlation does not scale linearly&quot;,
       title = &quot;Mutual information as a function of correlation&quot;,
       caption = &quot;Gaussian case&quot;, 
       y = &quot;Mutual Information&quot;,
       x = &quot;correlation coefficient&quot;)</code></pre>
<p><img src="/post/2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>For example, the change in the information conveyed from a correlation of <span class="math inline">\(0\)</span> to <span class="math inline">\(0.5\)</span> is much, much smaller than a change from <span class="math inline">\(0.7\)</span> to <span class="math inline">\(0.99\)</span>. Therefore, correlation coefficient should not be interpreted linearly.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>As expected by now, sample correlation suffers from persistent small sample effect when the type of randomness we are dealing with comes from Extremistan. However, even if we are in Mediocristan, using and interpreting the correlation coefficient correctly is a tricky endeavour. For example, Simpson’s and Berkson’s paradoxes arise from mistakingly expecting the same correlation coefficient from the whole sample as in non-random subsamples from it. Also, non-linear relationships are, by definition, the worst case to use a correlation coefficient. Finally, the signal that correlation convey does not scale linearly with its coefficient: a correlation coefficient of 0.7 conveys much less than a coefficient of 0.9; however, a correlation 0.3 and 0.5 convey just about the same about the dependence.</p>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

