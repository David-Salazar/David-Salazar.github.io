{
  "hash": "e6d849720f09cc5b35889f37470f9097",
  "result": {
    "markdown": "---\ntitle: Tail Risk of diseases in R\nauthor: ''\ndate: '2020-07-05'\nslug: tail-risk-of-diseases-in-r\ncategories: []\ntags: []\noutput:\n  blogdown::html_page:\n    toc: true\naliases: \n  - ../../2020/07/05/tail-risk-of-diseases-in-r/\n---\n\n\n\n\nPasquale Cirillo and Nassim Taleb published a short, interesting and important paper on the [Tail Risk of contagious diseases](https://arxiv.org/abs/2004.08658). In short, the distribution of fatalities is strongly fat-tailed: thus rendering any forecast, whether is pointwise or a distributional forecast, useless and dangerous. The **distributional evidence is there**: the lack of a characteristic scale makes our uncertainty at any point in the pandemic maximal: we can only say that it can always get worse. \n\nRead the paper! It's short and packed of ideas. In this blogpost, I'll reproduce the main plots, the main model that uses Extreme Value Theory to model the tail of the distribution of casualties and, finally, I will re-implement the model in a Bayesian framework using Stan. \n\n## The Data\n\nTaleb and Cirillo collected data for 72 events with more than 1,000 estimated victims. \n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |data |\n|Number of rows           |72   |\n|Number of columns        |8    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|character                |1    |\n|numeric                  |7    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|name          |         0|             1|   9|  34|     0|       69|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable    | n_missing| complete_rate|     mean|        sd|   p0|     p25|    p50|     p75|    p100|hist  |\n|:----------------|---------:|-------------:|--------:|---------:|----:|-------:|------:|-------:|-------:|:-----|\n|start_year       |         0|             1|  1619.08|    517.26| -429| 1595.00| 1813.0| 1916.50|    2019|▁▁▁▁▇ |\n|end_year         |         0|             1|  1613.35|    520.65| -426| 1593.00| 1813.5| 1923.75|    2020|▁▁▁▁▇ |\n|lower            |         0|             1|  2660.74|   9915.03|    1|   10.00|   75.5|  850.00|   75000|▇▁▁▁▁ |\n|avg_est          |         0|             1|  4877.66|  19132.36|    1|   10.00|   82.0|  850.00|  137500|▇▁▁▁▁ |\n|upper_est        |         0|             1|  7094.56|  28705.00|    1|   10.00|   88.0|  850.00|  200000|▇▁▁▁▁ |\n|rescaled_avg_est |         0|             1| 84874.62| 409099.85|    2|   41.75|  738.0| 6112.25| 2678283|▇▁▁▁▁ |\n|population       |         0|             1|  2037.94|   2434.28|   50|  554.00|  990.0| 1817.25|    7643|▇▂▁▁▂ |\n:::\n:::\n\n\nThe estimates for the number of casualties are in the thousands. The `avg_est` is the estimate for the number of casualities that we are going to be working with. \n\n## The plots\n\nTaleb and Cirillo begin the paper with a graphical analysis of the properties of the data. In it, they show that the data present all of the traits of fat-tailed random variables. To reproduce most of the figures, I use an [R package](https://github.com/David-Salazar/ggtails/) that I wrote: `ggtails`. \n\n### Max-to-Sum ratio\n\nTaleb and Cirillo begin by examining the [Max-to-Sum ratio plots](https://david-salazar.github.io/2020/06/02/lln-for-higher-p-moments/). A consequence of the Law of Large Numbers (LLN) is the following:\n\n$$\nE[X^p] < \\infty  \\iff R_n^p = \\dfrac{max(X_1^p, \\dots, X_n^p)}{\\sum_{i=1}^n X_i^p} \\to 0, \\ \\text{as} \\ n \\to \\infty\n$$\nThat is, the theoretical moment $p$ exists if and only if the ratio of the partial max to the partial sum converges to $0$. Neither of the fourth moments converges for neither of the fatalities' estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot(aes(sample = avg_est)) +\n  ggtails::stat_max_sum_ratio_plot() +\n  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1)) +\n  scale_color_brewer(palette = 2, type = \"qual\") +\n  labs(title = \"Max-to-Sum ratio plot\",\n       subtitle = \"There's no convergence. No finite moment is likely to exist\")\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/maxtosum-1.png){width=768}\n:::\n:::\n\n\nGiven that none of the moments converges, it is likely that we are dealing with such a fat-tailed random variable that all of the theoretical moments are undefined. Or if the theoretical moments exist, that the Law of Large Numbers works way too slowly for us to use it. In which case any method that relies on any sample moment estimator of the empirical distribution is useless. \n\n### Histogram \n\nLet's check what exactly is the range of values that Cirillo and Taleb have collected:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot(aes(avg_est*1000)) +\n  geom_histogram(binwidth = 10^8/80, fill = \"dodgerblue4\", color = \"black\", alpha = 0.5) +\n  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^8)) +\n  labs(x = TeX(\"Casualties (x $10^8$)\"),\n       title = \"Histogram of casualties\",\n       subtitle = \"The data encodes a huge array of variation. No characteristic scale, typical of fat-tails\") \n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/histogram-1.png){width=768}\n:::\n:::\n\n\nThe data contains an incredible arrange of variation. This is typical for fat-tailed random variables. \n\n### The Zipf plot\n\nFor a Pareto random variable, the slope of the log of the Survival function in log space decays linearly. With the Zipf plot, we compare the decay of the log of the empirical survival function with the linear decay that we expect with a Pareto.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot(aes(sample = log10(avg_est*1000))) +\n  ggtails::stat_zipf(color = \"dodgerblue4\", alpha = 0.7)  +\n  scale_x_log10(label=scientific_10) +\n  scale_y_log10() +\n  labs(title = \"Zipf plot of casualties\",\n       subtitle = \"There's the linear decay we expect with fat-tailed variables\",\n       x = \"log(x)\",\n       y = \"log(Survival)\")\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/loglogplot-1.png){width=768}\n:::\n:::\n\n\nThe empirical survival function indeed decays slowly: it's almost linear. Thus, giving us a hint that we are dealing with a fat-tailed random variable. \n\n### Mean Excess Plot\n\nFor a given threshold $v$, the Mean Excess for a random variable $X$ is:\n\n$$\nE[X - v | X > v] \n$$\nFor a Pareto, we expect this mean excess to scale linearly with the threshold.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n  ggplot(aes(sample = avg_est*1000)) +\n  ggtails::stat_mean_excess()-> p \ndat <- layer_data(p)\n\ndat %>%\n  filter(y < 3.95e07) %>% \n  ggplot() +\n  geom_point(aes(x, y), color = \"dodgerblue4\", alpha = 0.7) +\n  scale_x_continuous(labels = function(x) scales::number(x, scale = 1/10^6)) +\n  scale_y_continuous(labels = function(x) scales::number(x, scale = 1/10^7)) +\n  expand_limits(y = 5e07) +\n  labs(title = \"Mean Excess Plot for casualties\",\n       subtitle = \"There's the linear slope that we expect with fat-tailed variables\",\n       caption = \"More volatile observations were excluded.\",\n       x = TeX(\"Threshold (x $10^6$)\"),\n       y = TeX(\" Mean Excess Funtion (x $10^7$)\"))\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/meplot-1.png){width=768}\n:::\n:::\n\n\nGiven that the mean excess plot increases linearly, we are even more convinced that the number of casualties is indeed fat-tailed. \n\n## Fitting the tail\n\nThe graphical analysis tells us that we are likely dealing with a fat-tailed random variable: the survival function decays very slowly. Thus, the fat-tails make a) an extremely large array of possibilities relevant; b) thus, eliminating the possibility of a characteristic scale or \"typical\" catastrophe; c) and possibly making the theoretical moments undefined.\n\n### Wait a moment: infinite casualties?\n\nWe know that the number of casualties is bounded by the total population. Thus, the variable only has the appearance of an infinite mean given its upper bound. Graphically, by ignoring the upper bound, we are positing a continuous tail thus:\n\n![](/images/apparenttail.PNG)\n\nThe difference, thus, is only relevant in the vicinity of the upper bound $H$. One could thus keep modeling ignoring the upper bound without too many practical consequences. Nevertheless, it would be epistemologically wrong. To solve this problem, Taleb and Cirillo introduce a log transformation that eliminates the upper bound:\n\n$$\nZ = \\varphi(Y)=L-H \\log \\left(\\frac{H-Y}{H-L}\\right)\n$$\n\nTaleb and Cirillo call $Z$ the dual observations. $Z$ is the variable that we will model with Extreme Value theory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL <- 1\nH <- 7700000\ndata_to_model <- data %>% \n  mutate(dual = L -  H* log( (H-avg_est) / (H-L) ) )\n```\n:::\n\n\n### Extreme Value theory on the dual observations\n\nA logical question, then, is how fat-tailed is exactly the tail of casualties from contagious diseases? Extreme Value Theory offers an answer. Indeed, the Pickands–Balkema–de Haan theorem states that tail events (events larger than a large threshold) have as a limiting distribution a Generalized Pareto Distribution (GPD). In math, for large u, the conditional excess function is thus defined and approximated by a GPD:\n\n$$\nG_{u}(z)=P(Z \\leq z \\mid Z>u)=\\frac{G(z)-G(u)}{1-G(u)} \\approx GPD(z; \\xi, \\beta, u)\n$$\nWhere $\\xi$ is the crucial shape parameter that determines how slowly the tail decays. The larger $\\xi$, the more slowly it decays. For example, the variance is only defined for $\\xi < 0.5$. For $\\xi > 1$, the theoretical mean is not defined.\n\nCrucially, we can approximate the tail of the original distribution $G(z)$ with *a $GPD$ with the same shape parameter* $\\xi$. Finally, Taleb and Cirillo use $200,000$ as a threshold $u$. We can check both in the Mean Excess Plot and the Zipf plot that around this value we observe a power-law like behavior.  \n\n### Maximum Likelihood estimate\n\nWe can fit the GPD via maximum likelihood. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- evir::gpd(data_to_model$dual, threshold = 200)\nround(fit$par.ests, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     xi    beta \n   1.62 1174.74 \n```\n:::\n:::\n\n\nWhich are the same estimates as Taleb and Cirillo. The standard errors are thus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(fit$par.ses, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    xi   beta \n  0.52 534.44 \n```\n:::\n:::\n\n\nJust as we saw with our graphical analysis, the variable is definitely fat-tailed: $\\xi > 0$, which thanks to the relationship of the the Pickands–Balkema–de Haan with the Fisher-Tippet theorem, tells us that the Maximum Domain of Attraction is thus a Fréchet. From Taleb and Cirillo's paper:\n\n> As expected $\\xi$ > 1 once again supporting the idea of an infinite first moment... Looking at the standard error of $\\xi$, one could also argue that, with more data from the upper tail, the first moment could possibly become finite. Yet there is no doubt about the non-existence of\nthe second moment, and thus about the unreliability of the sample mean, which remains too volatile to be safely used.\n\nLet's see if we can reproduce this conclusions in a bayesian framework. \n\n## Bayesian model\n\nI'll sample from the posterior using [Stan's](https://mc-stan.org/users/interfaces/rstan) incredible implementation of Hamiltonian Monte Carlo. Most of the heavy lifting in Stan has already been done by Aki Vehtari in a [case study](https://mc-stan.org/users/documentation/case-studies/gpareto_functions.html) using the GPD.  \n\nOur bayesian model will be thus defined:\n\n$$\ny \\sim GPD(\\xi, \\beta, u = 200) \\\\\n\\xi \\sim Normal(1, 1) \\\\\n\\beta \\sim Normal(1000, 300)\n$$\nThe prior for $\\xi$ is weakly informative and yet still opens the opportunity for the data to move our posterior towards a finite mean and finite variance. \n\n### Simulating fake data\n\nTo verify that our code is correctly working, we'll simulate data and assess parameter recovery. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpose_stan_functions(\"gpd.stan\")\n\nfake_data <- replicate(1e3, gpareto_rng(ymin = 200, 0.8, 1000))\nds<-list(ymin=200, N=1e3, y=fake_data)\nfit_gpd <- stan(file='gpd.stan', data=ds, refresh=0,\n                     chains=4, seed=100, cores = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 1.3 seconds.\nChain 2 finished in 1.3 seconds.\nChain 3 finished in 1.3 seconds.\nChain 4 finished in 1.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.3 seconds.\nTotal execution time: 1.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nprint(fit_gpd, pars = c(\"xi\", \"beta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInference for Stan model: gpd-202211241219-1-288048.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n        mean se_mean    sd   2.5%     25%     50%     75%   97.5% n_eff Rhat\nxi      0.78    0.00  0.06   0.67    0.74    0.78    0.82    0.90   942    1\nbeta 1111.34    2.24 65.43 983.84 1068.06 1112.49 1153.74 1241.58   856    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:38 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n:::\n:::\n\n\nThe credible intervals are in line with the true parameters. Graphically: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior <- as.matrix(fit_gpd, pars = c(\"xi\", \"beta\"))\ntrue <- c(0.8, 1000)\nmcmc_recover_hist(posterior, true) +\n  labs(title = \"Parameter recovery\",\n       subtitle = \"We successfully recover the parameters\")\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/par-recover-1.png){width=768}\n:::\n:::\n\n\nWe can thus reasonably recover our parameter of interest $\\xi$ with our current model. Therefore, we can follow along and fit our model to the real data. \n\n### Fitting the model\n\nUsing the dual observations, we end up with $25/72$ observations, around $34.7$% of the total number of observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_to_model %>% \n  filter(dual > 200) %>% \n  pull(dual) -> dual_observations\nsummary(dual_observations)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    200     800    1500   14040    7504  138742 \n```\n:::\n:::\n\n\nFitting the model is just as simple as fitting it to fake data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds<-list(ymin=200, N=25, y=dual_observations)\nfit_gpd <- stan(file='gpd.stan', data=ds,\n                     chains=4, seed=100, cores = 4, iter = 5000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 1 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 2 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 3 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 4 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 4 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 1 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 2 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 3 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n```\n:::\n\n```{.r .cell-code}\nprint(fit_gpd, pars = c(\"xi\", \"beta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInference for Stan model: gpd-202211241219-1-6efc8a.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat\nxi      1.69    0.01   0.43   0.99   1.39    1.65    1.95    2.66  5847    1\nbeta 1077.46    3.32 246.93 624.37 902.09 1069.18 1240.92 1579.44  5519    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:51 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n:::\n:::\n\n\nFor know, the $\\widehat R$ values look OK, which indicates that there's no much disagreement between the Markov Chains. The credible intervals rule out the possibility that $\\xi < 0$.\n\nWe can intuitively interrogate the posterior for more precise questions in a Bayesian settings. For example, what is the probability that the theoretical mean is undefined, i.e., that $\\xi > 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/theoretical_mean-1.png){width=768}\n:::\n:::\n\n\nTherefore, we conclude that it is very likely that **the theoretical mean is undefined and we rule out definitely the possibility of a finite second moment**. Thus, we reproduce the paper's conclusions. \n\nGiven the fat-tailedness of the data and the resulting lack of characteristic scale from the huge array of variability that it's possible, there's no *possibility* of forecasting what we may face with either a pointwise prediction or a distributional forecast. \n\n### Posterior predictive checks \n\nIf the model fits well to the data, the replicated data under the model should look similar to the observed data. We can easily generate data from our model because our model is generative: we draw simulated values from the posterior predictive distribution of replicated data. \n\nHowever, with *fat-tailed* data: what exactly does it mean for our replicated data to track our observed data. For example, what about the **replicated maxima**? These definitely should include the observed maxima: **but also much larger values. That is the whole point of being in the MDA of a Fréchet.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyrep <- extract(fit_gpd)$yrep \nppc_stat(log(dual_observations), log(yrep), stat = \"max\") +\n  labs(title = \"Posterior predictive check (log scale)\",\n       subtitle = \"Maxima across replicated datasets track observed maximum and beyond, just like it should\",\n       x = \"log(maxima)\")\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/ppc-1.png){width=768}\n:::\n:::\n\n\nAnd this is exactly what our model does: a difference of 30 in a log-scale is huge. We can just as well expect maxima as large as observed, and even much larger. \n\n### Convergence diagnostics\n\nOne of the main benefits of Stan's implementation of HMC is that it yells at you when things have gone wrong. We can thus check a variety of diagnostics to check for convergence. Above, we examined that the $\\widehat R$ values look OK. We can also check trace plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior <- as.array(fit_gpd)\ncolor_scheme_set(\"viridis\")\nmcmc_trace(posterior, pars = c(\"xi\", \"beta\")) +\n  labs(title = \"Traceplots\",\n       subtitle = \"Traceplots are stationary, well mixed and the chains converge\")\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/traceplot-1.png){width=768}\n:::\n:::\n\n\nWe can check HMC specific diagnostics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_divergences(fit_gpd)\ncheck_treedepth(fit_gpd)\n```\n:::\n\n\n## Stressing the data\n\nTaleb and Cirillo are well aware that they are not working with the more precise of data. Thus, they 'stress' the data and check whether the results still hold. Given that we are working with fat-tailed data, the tail wags the dog: data problems can only modify the results if they are in the tail. \n\n### Measurement error\n\nTo account for measurement error, Taleb and Cirillo recreate 10,000 of their datasets, but this time where each observation is allowed to vary between 80% and 120% of its recorded values. They find that their results are robust to this modification.\n\nIn a bayesian setting, we can very easily extend our model to account for uncertainty around the true data. Indeed, in a bayesian model there's no fundamental difference between a *parameter and an observation*: one is observed and the other is not. Thus, we formulate the true casualties being measured as [missing data](https://mc-stan.org/docs/2_18/stan-users-guide/bayesian-measurement-error-model.html). \n\nTherefore, we specify that the measurement comes a normal with unknown mean, the true number of casualties, and that their standard deviation is 20% of the observed value. \n\n$$\ny \\sim Normal(y_{true}, y*0.2)\n$$\nWe fit the model and let Bayes do the rest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds<-list(ymin=200, N=25, y=dual_observations, noise = 0.2*dual_observations)\nfit_gpd_m <- stan(file='gpd_measurementerror.stan', data=ds,\n                     chains=4, seed=100, cores = 4, iter = 5000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 1 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 1 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 1 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 1 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 1 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 1 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 1 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 1 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 1 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 1 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 2 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 2 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 2 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 2 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 2 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 2 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 2 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 2 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 2 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 2 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 2 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 2 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 3 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 3 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 3 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 3 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 3 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 3 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 3 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 3 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 3 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration:    1 / 5000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 5000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 5000 [  4%]  (Warmup) \nChain 4 Iteration:  300 / 5000 [  6%]  (Warmup) \nChain 4 Iteration:  400 / 5000 [  8%]  (Warmup) \nChain 4 Iteration:  500 / 5000 [ 10%]  (Warmup) \nChain 4 Iteration:  600 / 5000 [ 12%]  (Warmup) \nChain 4 Iteration:  700 / 5000 [ 14%]  (Warmup) \nChain 4 Iteration:  800 / 5000 [ 16%]  (Warmup) \nChain 4 Iteration:  900 / 5000 [ 18%]  (Warmup) \nChain 4 Iteration: 1000 / 5000 [ 20%]  (Warmup) \nChain 1 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 1 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 1 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 1 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 1 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 1 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 1 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 1 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 1 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 1 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 1 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 1 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 1 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 2 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 2 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 2 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 2 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 2 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 2 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 2 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 2 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 2 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 2 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 3 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 3 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 3 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 3 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 3 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 3 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 3 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 3 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 3 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 3 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 1100 / 5000 [ 22%]  (Warmup) \nChain 4 Iteration: 1200 / 5000 [ 24%]  (Warmup) \nChain 4 Iteration: 1300 / 5000 [ 26%]  (Warmup) \nChain 4 Iteration: 1400 / 5000 [ 28%]  (Warmup) \nChain 4 Iteration: 1500 / 5000 [ 30%]  (Warmup) \nChain 4 Iteration: 1600 / 5000 [ 32%]  (Warmup) \nChain 4 Iteration: 1700 / 5000 [ 34%]  (Warmup) \nChain 4 Iteration: 1800 / 5000 [ 36%]  (Warmup) \nChain 4 Iteration: 1900 / 5000 [ 38%]  (Warmup) \nChain 4 Iteration: 2000 / 5000 [ 40%]  (Warmup) \nChain 1 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 1 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 1 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 1 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 1 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 1 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 1 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 1 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 1 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 2 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 2 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 2 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 2 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 2 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 2 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 2 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 2 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 2 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 2 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 3 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 3 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 3 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 3 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 3 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 3 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 3 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 3 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 3 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 4 Iteration: 2100 / 5000 [ 42%]  (Warmup) \nChain 4 Iteration: 2200 / 5000 [ 44%]  (Warmup) \nChain 4 Iteration: 2300 / 5000 [ 46%]  (Warmup) \nChain 4 Iteration: 2400 / 5000 [ 48%]  (Warmup) \nChain 4 Iteration: 2500 / 5000 [ 50%]  (Warmup) \nChain 4 Iteration: 2501 / 5000 [ 50%]  (Sampling) \nChain 4 Iteration: 2600 / 5000 [ 52%]  (Sampling) \nChain 4 Iteration: 2700 / 5000 [ 54%]  (Sampling) \nChain 4 Iteration: 2800 / 5000 [ 56%]  (Sampling) \nChain 4 Iteration: 2900 / 5000 [ 58%]  (Sampling) \nChain 4 Iteration: 3000 / 5000 [ 60%]  (Sampling) \nChain 4 Iteration: 3100 / 5000 [ 62%]  (Sampling) \nChain 1 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 1 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 1 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 1 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 1 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 1 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 1 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 2 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 2 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 2 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 2 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 2 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 2 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 2 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 3 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 3 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 3 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 3 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 3 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 3 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 3 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 3 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 3200 / 5000 [ 64%]  (Sampling) \nChain 4 Iteration: 3300 / 5000 [ 66%]  (Sampling) \nChain 4 Iteration: 3400 / 5000 [ 68%]  (Sampling) \nChain 4 Iteration: 3500 / 5000 [ 70%]  (Sampling) \nChain 4 Iteration: 3600 / 5000 [ 72%]  (Sampling) \nChain 4 Iteration: 3700 / 5000 [ 74%]  (Sampling) \nChain 4 Iteration: 3800 / 5000 [ 76%]  (Sampling) \nChain 4 Iteration: 3900 / 5000 [ 78%]  (Sampling) \nChain 1 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 1 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 1 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 1 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 1 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 2 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 2 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 2 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 2 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 2 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 2 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 2 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 2 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 3 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 3 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 3 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 3 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 3 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 3 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 3 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 3 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 4000 / 5000 [ 80%]  (Sampling) \nChain 4 Iteration: 4100 / 5000 [ 82%]  (Sampling) \nChain 4 Iteration: 4200 / 5000 [ 84%]  (Sampling) \nChain 4 Iteration: 4300 / 5000 [ 86%]  (Sampling) \nChain 4 Iteration: 4400 / 5000 [ 88%]  (Sampling) \nChain 4 Iteration: 4500 / 5000 [ 90%]  (Sampling) \nChain 4 Iteration: 4600 / 5000 [ 92%]  (Sampling) \nChain 4 Iteration: 4700 / 5000 [ 94%]  (Sampling) \nChain 4 Iteration: 4800 / 5000 [ 96%]  (Sampling) \nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 4 Iteration: 4900 / 5000 [ 98%]  (Sampling) \nChain 4 Iteration: 5000 / 5000 [100%]  (Sampling) \nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nprint(fit_gpd_m, pars = c(\"xi\", \"beta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInference for Stan model: gpd_measurementerror-202211241219-1-98c1f4.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%     50%     75%   97.5% n_eff Rhat\nxi      1.66    0.00   0.43   0.95   1.35    1.62    1.92    2.61 13151    1\nbeta 1056.99    1.99 248.82 583.47 887.32 1049.45 1219.83 1562.54 15706    1\n\nSamples were drawn using NUTS(diag_e) at Thu Nov 24 12:19:57 2022.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n:::\n:::\n\n\nThe model sampled remarkably well. We can thus check whether our inferences still hold:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/infer-measurement-1.png){width=768}\n:::\n:::\n\n\n## Influential observations\n\nTaleb and Cirillo also stress their data by recreating the dataset 10,000 times and then eliminating from 1 to 7 of the observations with a jacknife resampling procedure. Thus, they confirm that no single observation is driving the inference. \n\nIn a bayesian setting, we can check for influential observations by *comparing* the **full-data predictive posterior distribution to the Leave-one-out predictive posterior for each left out point**. That is: we compare $p(y_i | y)$ with $p(y_i, | y_{-i})$. \n\nWe can quickly estimate $p(\\theta_i, | y_{-i})$ for each $i$ by just sampling from $p(\\theta | y)$ using [Pareto Smoothed Importance Sampling LOO](https://mc-stan.org/loo/reference/psis.html). By examining the distribution of the importance weights, we can compare how different the two distributions are. If the weights are too fat-tailed, and the variance is infinite for the $j$th observation, then the $j$th observation is highly [influential observation determining our posterior distribution](https://arxiv.org/pdf/1709.01449.pdf).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik <- extract(fit_gpd)$log_lixi\nloopsis <- loo::loo(loglik, loo::relative_eff(exp(loglik)))\nplot(loopsis)\n```\n\n::: {.cell-output-display}\n![](2020-07-05-tail-risk-of-diseases-in-r_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nIndeed, no single observation is driving our inference.\n\n# Conclusion\n\nTaleb and Cirillo show that the number of casualties are patently fat-tailed. Thus, there's no typical catastrophe nor characteristic scale: a huge array of possibilities are likely and relevant for any analysis. Thus, there's no *possibility* of forecasting what we may face with either a pointwise prediction or a distributional forecast. \n\nIndeed, we cannot even use our sample mean to do anything useful. Given the asymmetric risks involved and the huge uncertainty, the whole of Taleb's work is very clear: we must kill the pandemic in the egg. \n\n\n",
    "supporting": [
      "2020-07-05-tail-risk-of-diseases-in-r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}