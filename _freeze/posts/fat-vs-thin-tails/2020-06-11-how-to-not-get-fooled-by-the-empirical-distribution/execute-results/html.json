{
  "hash": "6f7e81cb580bf9fa0d57bd8bbf482fe8",
  "result": {
    "markdown": "---\ntitle: How to not get fooled by the \"Empirical Distribution\"\nauthor: ''\ndate: '2020-06-11'\nslug: how-to-not-get-fooled-by-the-empirical-distribution\ncategories: []\ntags: []\naliases: \n  - ../../2020/06/11/how-to-not-get-fooled-by-the-empirical-distribution/\n---\n\n\n\n\n\nWith fat-tailed random variables, as Nassim Taleb says, [the tail wags the dogs](https://twitter.com/nntaleb/status/1245399854710509574/photo/1). That is, \"the tails (the rare events) play a disproportionately large role in determining the properties\". Following the presentation given by Taleb in his latest [technical book: Statistical Consequences of Fat Tails](https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf), I'll show:\n\n- Why using the empirical distribution for estimating the moments of a fat-tailed random variable is a terrible idea.\n- A less \"unreliable\" alternative to estimating the moments. \n\n## Why does the empirical distribution fool us?\n\nThe tails play a disproportionate role in defining the theoretical moments for fat-tailed distributions. However, if we are working with the non-parametric *\"empirical distribution\"*, we are effectively **cutting the tail at our sample maximum**. The rest of the tail, the possible values larger than our sample maximum, **are taken out of the equation when estimating any moment through the \"empirical\" distribution**. This *hidden contribution* to the theoretical mean that does not appear in the sample, however, is **precisely** the most important to define the theoretical moment that we are trying to estimate. Thus, our estimates with the \"empirical\" distribution will be terrible. \n\nInstead of using the \"empirical distribution\", what one should attempt is an *intelligent* extrapolation to take into consideration [future maxima](2020-06-10-fisher-tippet-th-a-clt-for-the-sample-maxima.html) and their influence in our estimate. This can be done, in the case of a Pareto distribution, by estimating the [tail exponent $\\alpha$](2020-05-19-understanding-the-tail-exponent.html) and then **plug-in** our estimated alpha to estimate the mean. \n\n## Visualizing the invisible tail\n\nThe tails contribute the most for any theoretical moment of any fat-tailed variable. However, when we work with the \"empirical\" distribution, we are ignoring the contribution of the tail beyond our sample maximum. Graphically, Taleb shows it thus:\n\n![Hidden contribution to the p-moment](/images/hiddentail.PNG)\n\nTaleb also shows how this ignorance of the tail is most worrisome the fatter the distribution:\n\n![The fatter, the worse is the mistake of the empirical](/images/worrysomethefatter.PNG)\n\n## Estimating the tail first, then the mean\n\n[By definition](2020-05-19-understanding-the-tail-exponent.html) the tail exponent tells us information about the tail. Specifically, about the survival's function rate of decay. Therefore:\n\n> The tail exponent $\\alpha$ captures, by extrapolation, the low probability deviation not seen in the data, but that plays a disproportionately large share in determining the mean. \n\nThus, once one has taken into account the hidden tail's influence with the estimated $\\widehat \\alpha$, we can produce a less unreliable estimate of the mean (or other higher moments). However, care must be taken: **with a Pareto, the mean is hardly what matters**. What is really important here is the **idea of first figuring out the properties of the fat-tailed distribution** and *then* trying to estimate things. \n\n## Maximum Likelihood\n\nFor a Pareto with known minimum observation 1, things are pretty straightforward. As it is often the case in statistics, the answer is maximum likelihood. Just posit a likelihood for your data, take the log, differentiate w.r.t $\\alpha$ and you have your estimate:\n\n$$ L(\\alpha) = \\prod_{i=1}^n \\alpha \\frac {1} {x_i^{\\alpha+1}} = \\alpha^n \\prod_{i=1}^n \\frac {1}{x_i^{\\alpha+1}}. $$\n\n$$ \\ell(\\alpha) = n \\ln \\alpha  - (\\alpha + 1) \\sum_{i=1} ^n \\ln x_i. $$\n$$ \\widehat \\alpha = \\frac{n}{\\sum _i  \\ln (x_i) }$$\nLuckily, this maximum likelihood estimate for $\\alpha$ works reasonably well with relatively small amounts of data. Why? Because $\\widehat \\alpha$ follows an Inverse gamma distribution with shape parameter equal to $n$ and scale parameter equal to $\\alpha n$. Although biased, the distribution of the estimator rapidly converges to a normal distribution tightly *around* the true $\\alpha$. Therefore, one can reliably estimate the tail exponent of the Pareto and thus understand the properties of the distribution with relatively few data.\n\nOnce we have an estimate for $\\widehat \\alpha$, our estimate for the mean will be $\\dfrac{\\widehat \\alpha}{ \\widehat \\alpha - 1 }$. This is the **plug-in** estimator for the mean. \n\n### Maximum likelihood in practice\n\nTo demonstrate the *superiority* of the **maximum likelihood and plug in estimator** approach to the sample mean of an empirical distribution, I'll simulate 10^5 Monte-Carlo experiments. For each experiment, I'll sample $n$ observations from a Pareto with $\\alpha = 1.2$ and theoretical mean $\\dfrac{1.2}{1.2 - 1} = 6$. Then, I'll produce the **maximum likelihood estimate for the tail exponent** and an estimate of the mean using our **plug-in** estimator. At the same time, I'll produce the regular sample mean for each experiment considering the *\"empirical distribution\"*. Finally, I'll compare the resulting distribution of both the sample mean and the mean from the plug-in estimator. \n\nI'll repeat this for both $n = 100, 1000$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 1.2\nrpareto <- function(n) {\n   (1/runif(n)^(1/alpha)) # inverse transform sampling\n}\n\nestimate_alpha_ml <- function(observations) {\n  alpha <- length(observations)/sum(log(observations))\n  if (alpha < 1) {\n    alpha <- 1.0005 \n  }\n  alpha\n}\n\ncrossing(experiment = 1:10^5, sample_size = c(100, 1000)) %>% \n  mutate(data = map(sample_size, ~ rpareto(.)),\n         mean_sample = map_dbl(data, ~ mean(.)),\n         alpha_ml = map_dbl(data, ~ estimate_alpha_ml(.)),\n         mean_ml = alpha_ml / (alpha_ml - 1) ) -> simulations_result\n```\n:::\n\n\n## Maximum likelihood's alpha distribution\n\nFrom relatively few observations, we can reliably estimate the tail exponent of the distribution. \n\nThis goes against the usual comment that with fat-tailed variables we *need more* and more observations; **the information about the properties of the distribution** is already there with *some* data. Let's check our Monte-Carlo distribution for our maximum likelihood alpha estimates for both values of $n = 100$ and $n= 1000$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## From ML estimator for alpha to plug-in mean\n\nOnce we have convinced ourselves that we can **reliably estimate $\\alpha$**, we can then use this alpha estimate to *estimate* the **mean of the distribution**. However, one must be cautious. To prepare the reader, there are going to be *crazy* large observations that will confuse both methods at some Monte-Carlo experiments: this is just the nature of fat-tails and the precise reason why forecasting **just a single variable is so dangerous**. Therefore, the mean, or *any other* single estimate, **cannot possibly prepare us** for the enormous variation that a fat-tailed variable encodes. Thus, these problems haunt even our Maximum Likelihood estimator for the mean; *just less* than they haunt our estimate for the mean when we use the \"empirical distribution\". \n\nThese problems show themselves in the form of a large mean for the distribution of the estimates according to each method. Alongside these means, the median and other percentiles of the distributions for both type of estimation methods and both $n$'s appear in the table below:\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div id=\"cipbnscuuz\" style=\"overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>html {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#cipbnscuuz .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#cipbnscuuz .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#cipbnscuuz .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#cipbnscuuz .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#cipbnscuuz .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#cipbnscuuz .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#cipbnscuuz .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#cipbnscuuz .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#cipbnscuuz .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#cipbnscuuz .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#cipbnscuuz .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#cipbnscuuz .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#cipbnscuuz .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cipbnscuuz .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#cipbnscuuz .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#cipbnscuuz .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cipbnscuuz .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#cipbnscuuz .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cipbnscuuz .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#cipbnscuuz .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cipbnscuuz .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cipbnscuuz .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cipbnscuuz .gt_left {\n  text-align: left;\n}\n\n#cipbnscuuz .gt_center {\n  text-align: center;\n}\n\n#cipbnscuuz .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#cipbnscuuz .gt_font_normal {\n  font-weight: normal;\n}\n\n#cipbnscuuz .gt_font_bold {\n  font-weight: bold;\n}\n\n#cipbnscuuz .gt_font_italic {\n  font-style: italic;\n}\n\n#cipbnscuuz .gt_super {\n  font-size: 65%;\n}\n\n#cipbnscuuz .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#cipbnscuuz .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#cipbnscuuz .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#cipbnscuuz .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#cipbnscuuz .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#cipbnscuuz .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#cipbnscuuz .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\">\n  \n  <thead class=\"gt_col_headings\">\n    <tr>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\">method</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">n</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">mean</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">percentile_25</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">median</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">percentile_75</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">maximum_value</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td class=\"gt_row gt_left\">Empirical Distribution</td>\n<td class=\"gt_row gt_right\">100</td>\n<td class=\"gt_row gt_right\">5.74</td>\n<td class=\"gt_row gt_right\">3.34</td>\n<td class=\"gt_row gt_right\">3.99</td>\n<td class=\"gt_row gt_right\">5.12</td>\n<td class=\"gt_row gt_right\">21,870.11</td></tr>\n    <tr><td class=\"gt_row gt_left\" style=\"background-color: #F7EFB2;\">Maximum Likelihood</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">100</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">67.14</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">4.47</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">5.93</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">8.95</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">29,762.42</td></tr>\n    <tr><td class=\"gt_row gt_left\">Empirical Distribution</td>\n<td class=\"gt_row gt_right\">1000</td>\n<td class=\"gt_row gt_right\">7.66</td>\n<td class=\"gt_row gt_right\">4.16</td>\n<td class=\"gt_row gt_right\">4.64</td>\n<td class=\"gt_row gt_right\">5.42</td>\n<td class=\"gt_row gt_right\">189,816.61</td></tr>\n    <tr><td class=\"gt_row gt_left\" style=\"background-color: #F7EFB2;\">Maximum Likelihood</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">1000</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">6.16</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">5.42</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">6.00</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">6.71</td>\n<td class=\"gt_row gt_right\" style=\"background-color: #F7EFB2;\">47.60</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n:::\n:::\n\n\nFor what it's worth, the medians of the maximum likelihood method's distributions demonstrate its superiority versus using the mean of the empirical distribution.  Once we zoom in on the majority of the sample, the histogram also shows the superiority of the maximum likelihood over the empirical distribution way of estimating the mean using the sample mean:  \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution_files/figure-html/comparison_dist-1.png){width=960}\n:::\n:::\n\n\nJust remember, superiority over the \"empirical distribution\" is not that big of a compliment. \n\n# Conclusion\n\nThe \"empirical distribution\" is a patently bad approach for fat-tailed distributions. It cuts the tail at the sample maxima. Thus, a portion of the tail is hidden and ignored in our mean estimation. A better approach is to take advantage of what one can possibly know from the data: the tail properties. For a Pareto, we can estimate its $\\alpha$ and then exploit the knowledge we gain about the tail to estimate the mean. This latter approach is less \"unreliable\": \n",
    "supporting": [
      "2020-06-11-how-to-not-get-fooled-by-the-empirical-distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}