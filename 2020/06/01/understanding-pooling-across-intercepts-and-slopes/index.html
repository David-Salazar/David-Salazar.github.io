<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>Understanding Pooling across Intercepts and Slopes - David Salazar&#39;s blog</title>
<meta property="og:title" content="Understanding Pooling across Intercepts and Slopes - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">Understanding Pooling across Intercepts and Slopes</h1>

    
    <span class="article-date">2020-06-01</span>
    

    <div class="article-content">
      


<p><a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> is a fabulous course on Bayesian Statistics (and much more). By following simulations in the book, I recently tried to understand why <a href="https://david-salazar.github.io/2020/05/28/simulating-into-understanding-multilevel-models/">pooling is the process and shrinkage is the result</a>. In this post, I’ll try to do the same for a model where we pool across intercepts and slopes. That is, we will posit a multivariate common distribution for both intercept and slopes to impose adaptive regularization on our predictions.</p>
<div id="the-model" class="section level1">
<h1>The model</h1>
<p>In the book, Richard proposes the following model for the morning and afternoon waiting times at different cafés.</p>
<p><span class="math display">\[ W_i \sim Normal(\mu, \sigma) \]</span></p>
<p><span class="math display">\[ \mu_i = \alpha_{ café_{[i]} } + \beta_{ café_{[i]} } A_i \]</span>
Intuitively, we expect the waiting time at mornings to be longer than the afternoon waiting times. Therefore, we expect a negative <span class="math inline">\(\beta\)</span>. However, the interesting part is <strong>that each café gets its own pair of parameters.</strong> That is what multilevel models are, after all:</p>
<blockquote>
<p>[Multilevel models] are massive interaction machines. They allow every unit in the data to have its own unique response to any treatment… while also improving estimates via pooling.</p>
</blockquote>
<p>That is, each café has its own average waiting time and slope for the afternoon (<span class="math inline">\(A_i\)</span>). <em>We could, after all, perform pooling independently: we would model one common distribution for the intercepts and one common distribution for the slopes</em>. Note, however, that it would imply that knowing the intercept does not help us to predict the slope. A strong assumption that, in this case, it’s wrong: if a café has no lines neither in the morning, it won’t have lines in the afternoon: knowing the average waiting time in the morning(the intercept) adjusts our expectations for the waiting time in the afternoon (slope). <strong>There’s information that can and should be pooled across parameter types. Therefore, we are better off by setting up a joint common distribution for both the intercepts and slopes</strong>, which means modeling their covariance.</p>
<p><span class="math display">\[ \begin{bmatrix}
           \alpha_{café_j} \\
           \beta_{cafe_j}
         \end{bmatrix}  =  MVNormal ( \begin{bmatrix}
           \alpha \\
           \beta
         \end{bmatrix} , S)\]</span></p>
<p>Where <span class="math inline">\(S\)</span> will be the var-cov matrix for the joint common distribution of intercepts and slopes. Intuitively, if the absolute value of the covariance is large, shrinking in one dimension (intercept) will lead to shrinking in the other dimension (slope).</p>
</div>
<div id="simulating-the-dataset" class="section level1">
<h1>Simulating the dataset</h1>
<div id="the-joint-distribution" class="section level2">
<h2>The joint distribution</h2>
<p>Remember that “priors are not ontology, but epistemology”. Therefore, we never include the priors on our simulations. Let’s begin by setting up the parameters of the common joint distribution of intercepts and slopes.</p>
<pre class="r"><code>a &lt;- 3.5 # average morning wait time
b &lt;- -1 # average difference afternoon wait time
sigma_a &lt;- 1 # std dev in intercepts
sigma_b &lt;- 0.5 # std dev in slopes
rho &lt;- -0.7  # correlation between intercepts and slopes</code></pre>
<p>We then construct the var-cov matrix:</p>
<pre class="r"><code>Mu &lt;- c(a, b)
sigmas &lt;- c(sigma_a, sigma_b)
Rho &lt;- matrix(c(1, rho, rho, 1), nrow = 2) # correlation matrix

Sigma &lt;- diag(sigmas) %*% Rho %*% diag(sigmas)
Sigma # var-cov</code></pre>
<pre><code>##       [,1]  [,2]
## [1,]  1.00 -0.35
## [2,] -0.35  0.25</code></pre>
</div>
<div id="simulate-for-each-cafe" class="section level2">
<h2>Simulate for each cafe</h2>
<pre class="r"><code>N_cafes &lt;- 20
vary_effects &lt;- mvrnorm(N_cafes, Mu, Sigma) 
a_cafe &lt;- vary_effects[, 1]
b_cafe &lt;- vary_effects[, 2]
glimpse(vary_effects)</code></pre>
<pre><code>##  num [1:20, 1:2] 4.22 2.01 4.57 3.34 1.7 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : NULL</code></pre>
<p>That is, for each of the cafés we have one intercept and one slope. Let’s check how our simulated intercepts and slopes fit in the overall joint distribution:</p>
<pre class="r"><code>contour_level &lt;- function(level) {
  ellipse::ellipse(Sigma, centre = Mu, level = level) %&gt;% 
    data.frame() %&gt;% 
    mutate(level = level)
} 

title &lt;- TeX(&quot;Sampling from a Multivariate Normal ($ \\alpha $, $ \\beta $)&quot;)

purrr::map(c(0.1, 0.3, 0.5, 0.8, 0.99), contour_level) %&gt;% 
  bind_rows() %&gt;% 
  ggplot(aes(x, y)) +
  geom_path(aes(group = level, color = level), linetype = 2) + 
  geom_point(data = data.frame(x = a_cafe, y = b_cafe), alpha = 0.4) +
  geom_point(data = data.frame(x = a, y = b), color = &quot;red&quot;) +
  labs(title = title,
       color = &quot;Confidence level&quot;,
       subtitle = &quot;Center of the Distribution in Red&quot;,
       x = expression(alpha),
       y = expression(beta)) +
  scale_colour_viridis_c(begin = 1, end = 0) +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-5-1.png" width="768" /></p>
<p>The points that lie in the farthest ellipses are the “outliers”. Note that most of the samples surround the center of the distribution.</p>
</div>
<div id="simulate-observations" class="section level2">
<h2>Simulate observations</h2>
<p>Once we have an <span class="math inline">\(\alpha, \beta\)</span> for each café, we can simulate the waiting times for each of them:</p>
<pre class="r"><code>set.seed(22)

data.frame(cafe_ids = 1:N_cafes, a_cafe, b_cafe) %&gt;% 
  mutate(N_visits = rep(c(6, 10, 16, 20), times = 5)) %&gt;% 
  group_by(cafe_ids) %&gt;% 
  rowwise() %&gt;% 
  mutate(afternoon = list( rep(0:1, N_visits/ 2) )) %&gt;%  # repeat data for morning and afternoon
  unnest() -&gt; data_for_simulation</code></pre>
<pre><code>## Warning: `cols` is now required.
## Please use `cols = c(afternoon)`</code></pre>
<pre class="r"><code># different visits number of visits for each cafe
data_for_simulation %&gt;% 
  count(cafe_ids)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    cafe_ids     n
##       &lt;int&gt; &lt;int&gt;
##  1        1     6
##  2        2    10
##  3        3    16
##  4        4    20
##  5        5     6
##  6        6    10
##  7        7    16
##  8        8    20
##  9        9     6
## 10       10    10
## 11       11    16
## 12       12    20
## 13       13     6
## 14       14    10
## 15       15    16
## 16       16    20
## 17       17     6
## 18       18    10
## 19       19    16
## 20       20    20</code></pre>
<p>Now, we are ready to simulate:</p>
<pre class="r"><code>data_for_simulation %&gt;% 
  transmute(cafe_ids, mu = a_cafe + b_cafe*afternoon, afternoon) %&gt;% 
  rowwise() %&gt;% 
  transmute(cafe_ids, afternoon, wait = rnorm(1, mu, sd = 0.5)) -&gt; model_data
glimpse(model_data)</code></pre>
<pre><code>## Rows: 260
## Columns: 3
## Rowwise: 
## $ cafe_ids  &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ...
## $ afternoon &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, ...
## $ wait      &lt;dbl&gt; 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3....</code></pre>
<p>In Richard’s words:</p>
<blockquote>
<p>Exactly the sort of data that is well-suited to a varying slopes model. There are multiple clusters in the data. These are the cafés. And each cluster is observed under differened conditions. So it’s possible to estimate both an individual intercept for each cluster, as well as an individua slope.</p>
</blockquote>
</div>
<div id="fixed-effects-maximal-overfit" class="section level2">
<h2>Fixed effects: Maximal Overfit</h2>
<p><strong>When we do not allow any pooling at all, we maximally overfit</strong>. The prediction for both afternoon and morning, for each café, will be the observed mean at each of the times.</p>
<pre class="r"><code>model_data %&gt;% 
  group_by(cafe_ids, afternoon) %&gt;% 
  summarise(avg_wait = mean(wait)) %&gt;%
  ggplot(aes(factor(afternoon, labels = c(&quot;M&quot;, &quot;A&quot;)), y = avg_wait)) +
  geom_point() +
  geom_path(aes(group = cafe_ids), linetype = 2, color = &quot;dodgerblue4&quot;) +
  facet_wrap(~cafe_ids) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;Y&quot;) +
  labs(title = &quot;Fixed Effects estimates&quot;,
       x = &quot;Time of the day&quot;,
       y = &quot;Average waiting time&quot;,
       subtitle = &quot;Average waiting time per cafe and per time of the day&quot;)</code></pre>
<pre><code>## `summarise()` regrouping output by &#39;cafe_ids&#39; (override with `.groups` argument)</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="partial-pooling-multilevel-model" class="section level2">
<h2>Partial Pooling: Multilevel model</h2>
<p>Instead of ignoring the rest of the cafes when doing our predictions, let’s pool the information across cafes and across parameter types. To do so, let’s finish setting up the model that we had at the beginning:</p>
<p><span class="math display">\[ W_i \sim Normal(\mu, \sigma) \]</span></p>
<p><span class="math display">\[ \mu_i = \alpha_{ café_{[i]} } + \beta_{ café_{[i]} } A_i \]</span></p>
<p><span class="math display">\[ \begin{bmatrix}
           \alpha_{café_j} \\
           \beta_{cafe_j}
         \end{bmatrix}  =  MVNormal ( \begin{bmatrix}
           \alpha \\
           \beta
         \end{bmatrix} , S)\]</span></p>
<p>The only thing left is to posit priors for the parameters. First, we write the var-cov matrix thus:</p>
<p><span class="math display">\[ S = \begin{bmatrix} 
  \sigma_{\alpha} \  0 \\
  0 \ \sigma_{\beta}
\end{bmatrix} R \begin{bmatrix} 
  \sigma_{\alpha} \  0 \\
  0 \ \sigma_{\beta}
\end{bmatrix} \]</span></p>
<p>We do this to set up the prior for the correlation matrix thus:</p>
<p><span class="math display">\[ R \sim \text{LKJcorr}(2) \]</span></p>
<p>The rest will be traditional priors. Let’s focus for a second on the LKJcorr.</p>
<div id="lkjcorr-interlude" class="section level3">
<h3>LKJcorr interlude</h3>
<p>For our purposes, what we need to do is to understand how our prior changes with the parameter of the distribution <span class="math inline">\(\eta\)</span>:</p>
<pre class="r"><code>rlkj &lt;- function(eta) {
  rlkjcorr(1e4, K = 2, eta = eta)[,,2][,1] %&gt;% 
    data.frame(value = .) %&gt;% 
    mutate(sims = 1:1e4, eta = eta)
}

purrr::map(c(1, 2, 4, 10), rlkj) %&gt;% 
  bind_rows() %&gt;% 
  ggplot(aes(value, fill = factor(eta))) +
  geom_density(aes(group = NA)) +
  gganimate::transition_states(factor(eta)) +
  labs(subtitle = &quot;Eta =  {closest_state}&quot;,
       title = &quot;LKJ prior for correlation matrix&quot;,
       fill = expression(eta))</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-9-1.gif" /><!-- --></p>
</div>
<div id="fit-the-model" class="section level3">
<h3>Fit the model</h3>
<p>Finally, we are ready to fit our partial pooling model.</p>
<pre class="r"><code>model_data_list = list(
  wait = model_data$wait,
  cafe_ids = as.integer(model_data$cafe_ids),
  afternoon = as.integer(model_data$afternoon)
)

partial_pooling &lt;- ulam(
  alist(
    wait ~ normal(mu, sigma),
    mu &lt;- a_cafe[cafe_ids] + b_cafe[cafe_ids]*afternoon, # each cafe has own intercept and slope
    # outcome sd prior
    sigma ~ exponential(1),
    # adaptive-prior
    c(a_cafe, b_cafe)[cafe_ids] ~ multi_normal(c(a, b), Rho, sigma_cafe), # joint common distribution, ensures pooling
    # hyper-priors
    a ~ normal(5, 2),
    b ~ normal(-1, 0.5),
    sigma_cafe ~ exponential(1), # ulam converts it to 2d vector
    Rho ~ lkj_corr(1.5)
  ), data = model_data_list,
  chains = 4, cores = 4, iter = 10000)</code></pre>
<p>Let’s check our chains’ health:</p>
<pre class="r"><code>traceplot_ulam(partial_pooling)</code></pre>
<pre><code>## [1] 10000
## [1] 1
## [1] 10000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-11-3.png" width="672" /><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-11-4.png" width="672" /></p>
<p>Our chains look healthy enough:</p>
<ol style="list-style-type: decimal">
<li>They mix well</li>
<li>They are stationary.</li>
<li>Different chains converge to explore the same parameter space.</li>
</ol>
<p>Let’s look at the <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(partial_pooling, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  dplyr::select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>## 4 matrix parameters hidden. Use depth=3 to show them.</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9998  
##  1st Qu.:0.9999  
##  Median :0.9999  
##  Mean   :1.0000  
##  3rd Qu.:1.0000  
##  Max.   :1.0002</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look OK. Let’s check our inferences for the joint common distribution:</p>
<pre class="r"><code>samples &lt;- extract.samples(partial_pooling)
precis(partial_pooling) </code></pre>
<pre><code>## 46 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##             mean        sd       5.5%      94.5%    n_eff     Rhat4
## sigma  0.4968132 0.0240167  0.4600468  0.5362058 22153.22 0.9999910
## a      3.6296587 0.2209041  3.2804992  3.9814312 23391.26 0.9999139
## b     -1.0874893 0.1453769 -1.3157815 -0.8535724 23084.93 0.9998701</code></pre>
<p>It seems that we’ve been able to recover the common joint distribution’s parameters that adaptively regularizes our individual estimates for each café. For example, the marginal distribution of <span class="math inline">\(a\)</span>:</p>
<pre class="r"><code>data.frame(a = samples$a) %&gt;% 
  ggplot(aes(a)) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.6) +
  geom_vline(aes(xintercept = 3.5), linetype = 2, color = &quot;red&quot;) +
  labs(title = &quot;Posterior probability for a&quot;)</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Now for the posterior marginal distribution of b:</p>
<pre class="r"><code>data.frame(b = samples$b) %&gt;% 
  ggplot(aes(b)) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.6) +
  geom_vline(aes(xintercept = -1), linetype = 2, color = &quot;red&quot;) +
  labs(title = &quot;Posterior probability for b&quot;)</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Finally, we can plot the joint distribution with contours:</p>
<pre class="r"><code>Mu_est &lt;- c(mean(samples$a), mean(samples$b))
rho_est &lt;- mean(samples$Rho[,1,2])
sa_est &lt;- mean(samples$sigma_cafe[,1])
sb_est &lt;- mean(samples$sigma_cafe[, 2])
cov_ab &lt;- sa_est*sb_est*rho_est
Sigma_est &lt;- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol = 2)

contour_level &lt;- function(level) {
  ellipse::ellipse(Sigma_est, centre = Mu_est, level = level) %&gt;% 
    data.frame() %&gt;% 
    mutate(level = level)
} 

purrr::map(c(0.1, 0.3, 0.5, 0.8, 0.99), contour_level) %&gt;% 
  bind_rows() %&gt;% 
  ggplot(aes(x, y)) +
  geom_path(aes(group = level), linetype = 2) +
  geom_point(data = data.frame(x = Mu_est[1]), y = Mu_est[2], color = &quot;red&quot;) +
  labs(title = &quot;Esimated joint common distribution of intercepts and slopes&quot;,
       color = &quot;Confidence level&quot;,
       subtitle = &quot;Center of the Distribution in Red&quot;,
       x = expression(alpha),
       y = expression(beta))  -&gt; joint_ellipses
joint_ellipses</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>As we can see, our joint common distribution captures the negative correlation between intercepts and slopes.</p>
</div>
<div id="visualizing-the-shrinkage" class="section level3">
<h3>Visualizing the shrinkage</h3>
<p>We’ve seen how and why it’s sensible to pool information across clusters and across parameter types. We’ve estimated with our multilevel model the common joint distribution. Now it’s time to visualize the shrinkage: <strong>how our estimates are pulled towards the estimated joint common distribution toward its mean.</strong></p>
<p>To do, we must first average our estimates for each café over the posterior distribution. Then, we can compare with our fixed effects estimates from the beginning.</p>
<pre class="r"><code># fixed effects
model_data %&gt;% 
  group_by(cafe_ids, afternoon) %&gt;% 
  summarise(avg_wait = mean(wait)) %&gt;% 
  mutate(afternoon = if_else(afternoon == 1, &quot;afternoon&quot;, &quot;morning&quot;)) %&gt;% 
  pivot_wider(cafe_ids, names_from = afternoon, values_from = avg_wait) %&gt;% 
  mutate(b_fixed = afternoon - morning) %&gt;% 
  dplyr::select(-afternoon) %&gt;% 
  rename(a_fixed = morning) -&gt; fixed_effects </code></pre>
<pre><code>## `summarise()` regrouping output by &#39;cafe_ids&#39; (override with `.groups` argument)</code></pre>
<pre class="r"><code># partial pooling
a_pooled &lt;- apply(samples$a_cafe, 2, mean)
b_pooled &lt;- apply(samples$b_cafe, 2, mean)

data.frame(cafe_ids = 1:N_cafes, a_partial_pooling = a_pooled) %&gt;% 
  left_join(fixed_effects %&gt;% 
              dplyr::select(cafe_ids, a_fixed)) %&gt;% 
  pivot_longer(-cafe_ids, names_to = &quot;method&quot;, names_prefix = &quot;a_&quot;, values_to =&quot;intercept&quot;) -&gt; comparison_intercepts</code></pre>
<pre><code>## Joining, by = &quot;cafe_ids&quot;</code></pre>
<pre class="r"><code>data.frame(cafe_ids = 1:N_cafes, b_partial_pooling = b_pooled) %&gt;% 
  left_join(fixed_effects %&gt;% 
              dplyr::select(cafe_ids, b_fixed)) %&gt;% 
  pivot_longer(-cafe_ids, names_to = &quot;method&quot;, names_prefix = &quot;b_&quot;, values_to = &quot;slope&quot;) -&gt; comparison_slopes</code></pre>
<pre><code>## Joining, by = &quot;cafe_ids&quot;</code></pre>
<pre class="r"><code>comparison_intercepts %&gt;% 
  left_join(comparison_slopes) -&gt; comparisons</code></pre>
<pre><code>## Joining, by = c(&quot;cafe_ids&quot;, &quot;method&quot;)</code></pre>
<pre class="r"><code>glimpse(comparisons)</code></pre>
<pre><code>## Rows: 40
## Columns: 4
## $ cafe_ids  &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10,...
## $ method    &lt;chr&gt; &quot;partial_pooling&quot;, &quot;fixed&quot;, &quot;partial_pooling&quot;, &quot;fixed&quot;, &quot;...
## $ intercept &lt;dbl&gt; 4.270203, 4.271750, 1.990772, 1.902532, 4.562014, 4.66699...
## $ slope     &lt;dbl&gt; -0.9600074, -0.8844626, -0.7998550, -0.7224460, -2.097964...</code></pre>
<p>Finally, we can plot our points over our posterior joint common distribution to visualize how our estimates are pooled over towards the joint common distribution’s mean:</p>
<pre class="r"><code>joint_ellipses +
  geom_point(data = comparisons, mapping = aes(x =intercept, y = slope, color = method),
             inherit.aes = FALSE, size = 3) +
  geom_path(data = comparisons, mapping = aes(x =intercept, y = slope, group = cafe_ids),
             inherit.aes = FALSE) +
  labs(title = &quot;Esimated joint common distribution of intercepts and slopes&quot;,
       color = &quot;Method&quot;,
       subtitle = &quot;Estimates are shrunk towards the grand mean of the common distribution&quot;,
       x = expression(alpha),
       y = expression(beta),
       caption = &quot;Lines joint the predictions for predictions that belong to the same café&quot;) +
  scale_colour_viridis_d(begin = 1, end = 0) +
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2020-06-01-understanding-pooling-across-intercepts-and-slopes_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>As we can see, the partial pooling estimates are always closer to the center of the distribution than the fixed effects estimates. <strong>This is the direct result of pooling the information with a joint common distribution that shrinks our estimates towards the grand mean.</strong> Not only that, <strong>shrinking in one dimension entails shrinking in the other dimension</strong>. This is the direct result of the pooling across parameter types.</p>
</div>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

