{
  "hash": "396cb98160e6ede67ea60c5e36102e4e",
  "result": {
    "markdown": "---\ntitle: Bayesian Instrumental Variable Regression\nauthor: ''\ndate: '2020-06-03'\nslug: bayesian-instrumental-variable-regression\ncategories: []\ntags: []\naliases: \n  - ../../2020/06/03/bayesian-instrumental-variable-regression/\n---\n\n\n\n\n[Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) is a fabulous course on Bayesian Statistics (and much more). In what follows, I'll give a succinct presentation of Instrumental Variable Regression in a Bayesian setting using simulated data.\n\nI had already seen the traditional econometrics formulation and yet found Richard's presentation both illuminating and fun. It's a testament of his incredible achievement with this book. \n\n# The problem\n\nThe start of every instrumental variable setting is the following. We want to estimate the causal effect of $X$ on $Y$. However, there's a fork between $X$ and $Y$: an unobserved variable $U$ that has an effect on both of them. In DAG form:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_confound <- dagitty::dagitty('dag{\n                        X -> Y\n                        X <- U\n                        U -> Y\n                        }')\ndrawdag(dag_confound)\n```\n\n::: {.cell-output-display}\n![](2020-06-03-bayesian-instrumental-variable-regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nTherefore, there's a backdoor path from $X$, through $U$, toward $Y$ that will bias our estimates. One alternative would be to statistically adjust by $U$; however, we don't observe $U$. \n\n# Creating a collider as a solution\n\nColliders are dangerous and scary, as Richard has said a many times. However, they can also be useful. They can create statistical relationships between certain variables that allows us to introduce certain otherwise unavailable statistical information into our models. This is the case with an instrument, $I$, that is only related to $X$ in this DAG. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag_instrument <- dagitty::dagitty('dag{\n                        X -> Y\n                        X <- U\n                        U -> Y\n                        I -> X\n                        }')\ndrawdag(dag_instrument)\n```\n\n::: {.cell-output-display}\n![](2020-06-03-bayesian-instrumental-variable-regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNoticed that we've created a collider out of $X$. Therefore, if we open the collider by statistically adjusting simultaneously by $X$ and $I$, there will be a statistical relationship (not causal) between $I$ and $U$. Thus, a collider opens a path that can help us adjust by $U$. **Our goal, then, is to create a model that simultaneously opens the collider and estimates the effefct of $X$ on $Y$.** \n\n## Opening collider and estimating simultaneously\n\nThe model then must be simultaneous. It must open the collider **and** regress $Y$ on $X$. The solution is thu:\n\n$$ \\begin{bmatrix} \nY_i \\\\\nX_i\n\\end{bmatrix} \\sim MVNormal(\\begin{bmatrix} \n\\mu_{Y,i} \\\\\n\\mu_{X, y}\n\\end{bmatrix}, S) $$\n\n$$ \\mu_{Y,i} = \\alpha_y + \\beta X_i $$\n\n$$ \\mu_{X, i} = \\alpha_x + \\gamma I $$\n\nWe are modelling $X, Y$ simultaneously with a joint error structure represented by $S$. Notice, then, that at both linear models we should be adjusting by $U$. Therefore, the errors of our each of our linear regressions, represented by $S$, will be correlated; this is what a fork does and what creates the original bias in our estimates. **However, we are opening simultaneously the collider on $X$ by adjusting with $I$. Therefore, statistical information about $U$ is entering into our model in the form of a correlated error structure (represented by $S$) between the two linear regressions**. This statistical information of $U$ will then allow us to causally estimate the effect of $X$ on $Y$.\n\nNote that, given a DAG, we can algorithmically compute if there is an instrument that we can use. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstrumentalVariables(dag_instrument, exposure = 'X', outcome = 'Y')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n I\n```\n:::\n:::\n\n\n# Simulated data\n\nIn this case we will simulate data where the true effect of $X$ on $Y$ is null. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 1000\nU <- rnorm(N)\nI <- rnorm(N)\nX <- rnorm(N, U + I)\nY <- rnorm(N, U)\n\ndata_sim <- list(\n  Y = standardize(Y),\n  X = standardize(X),\n  I = standardize(I)\n)\n```\n:::\n\n\n## Naive regression\n\nA naive regression won't account by the confounding effect of $U$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_naive <- ulam(\n  alist(\n    Y ~ normal(mu, sigma),\n    mu <- alpha + beta*X,\n    alpha ~ normal(0, 1),\n    beta ~ normal(0, 1),\n    sigma ~ exponential(1)\n  ),\n  chains = 4, cores = 4,\n  data = data_sim\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.4 seconds.\nChain 3 finished in 0.4 seconds.\nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nprecis(model_naive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               mean         sd        5.5%      94.5%    n_eff     Rhat4\nalpha -0.0007866032 0.02848885 -0.04462621 0.04554595 1496.227 1.0004385\nbeta   0.4439785015 0.02900968  0.39840345 0.49061714 2087.683 1.0016685\nsigma  0.8969762870 0.02008460  0.86616765 0.92862533 1929.604 0.9991075\n```\n:::\n:::\n\n\nIndeed, we have an estimate with a 87% compatibility interval of (0.40, 0.49) when we know that the true effect is zero. We can plot the expected relationship:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(data_sim) %>% \n  data_grid(X = seq_range(X, 50)) %>% \n  add_predicted_draws(model_naive) %>% \n  ggplot(aes(X, Y)) +\n  stat_lineribbon(aes(y = .prediction), alpha = 1/4, fill = \"dodgerblue4\") +\n  geom_point(data = data.frame(data_sim), alpha = 0.4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  labs(title = \"Naive model finds a relationship\",\n       subtitle = \"Confounding effect is unaccounted for. True effect of X on Y is null\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-bayesian-instrumental-variable-regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Instrumental Variable Regression\n\nGiven our DAG and our data, we can do better. We can fit a multivariate model that, by virtue of opening a collider on $X$, will allows us to statistical adjust by the confounding factor $U$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_instrumental <- ulam(\n  alist(\n    c(Y, X) ~ multi_normal(c(muY, muX), Rho, Sigma),\n    muY <- alphaY + beta*X,\n    muX <- alphaX + gamma*I,\n    c(alphaY, alphaX) ~ normal(0, 0.2),\n    c(beta, gamma) ~ normal(0, 0.5),\n    Rho ~ lkj_corr(2),\n    Sigma ~ exponential(1)\n  ),\n  data = data_sim, chains = 4, cores = 4\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 finished in 11.8 seconds.\nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 12.1 seconds.\nChain 4 finished in 12.1 seconds.\nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 finished in 12.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 12.2 seconds.\nTotal execution time: 12.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nprecis(model_instrumental)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                mean         sd        5.5%      94.5%     n_eff    Rhat4\nalphaX -0.0007029589 0.02612907 -0.04213007 0.04238940 1514.2057 1.000685\nalphaY -0.0007681863 0.03114677 -0.04975118 0.04891647 1311.7424 1.000845\ngamma   0.5686533460 0.02628153  0.52624149 0.61137419 1414.5350 1.001943\nbeta    0.0594884285 0.05408430 -0.02861268 0.14559261  970.4573 1.000913\n```\n:::\n:::\n\nWhereas before we posited a positive and relatively large effect of $X$ on $Y$, now we correctly infer that the true effect is null. Because $\\beta$ has lots of its mass around zero. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_instrumental %>% \n  spread_draws(beta) %>% \n  ggplot(aes(beta)) +\n  geom_histogram(color = \"black\", fill = \"dodgerblue4\", alpha = 4/10,\n                 binwidth = 0.05) +\n  geom_vline(aes(xintercept = 0), linetype = 2, color = \"red\") +\n  labs(title = \"Instrumental Variable Regression\",\n       subtitle = \"Accounting for the confounding through IV, finds true null effect\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-bayesian-instrumental-variable-regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "2020-06-03-bayesian-instrumental-variable-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}