<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>BDA week 7: LOO and its diagnostics - Dilettanting Data Science</title>
<meta property="og:title" content="BDA week 7: LOO and its diagnostics - Dilettanting Data Science">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">BDA week 7: LOO and its diagnostics</h1>

    
    <span class="article-date">2020/07/08</span>
    
    

    <div class="article-content">
      


<p>Once Stan’s implementation of HMC has run its magic, we finally have samples from the posterior distribution <span class="math inline">\(\pi (\theta | y))\)</span>. We can then run posterior predictive checks and hopefully our samples looks plausible under our posterior. Nevertheless, this is just an internal validation check: <strong>we expect more from our model</strong>. We expect it to hold under an external validation check: never seen observations, once predicted, should also look plausible under our posterior.</p>
<p><strong>Leave-One-Out (LOO) log pointwise predictive density</strong> is the preferred Bayesian way to do this. In this blogpost, I’ll explain how we can <strong>approximate</strong> this metric <em>without the need of refitting the model <span class="math inline">\(n\)</span> times</em> with <strong>LOO Pareto Smoothed Importance Sampling (PSIS)</strong>. Also, I’ll explain how PSIS diagnostics tells us, not unlike HMC, when the algorithm is a poor approximation. As a byproduct, we can also derive a metric to identify if there are influential observations that are driving the inference. Finally, I’ll simulate data to show how we can perform all this with real data.</p>
<p>All of this is based on this <a href="https://arxiv.org/abs/1507.04544">great paper</a> by Vehtari, Gelman and Gabry.</p>
<div id="what-is-our-metric-log-pointwise-predictive-density" class="section level2">
<h2>What is our metric? Log pointwise predictive density</h2>
<p>Given an observation <span class="math inline">\(y_i\)</span>, we define our metric to evaluate how well we have predicted <span class="math inline">\(y_i\)</span> as its log likelihood according to our model. Given our uncertainty over the parameters, we integrate over our posterior distribution for our model. We call this the <strong>log pointwise predictive density (lpd)</strong>:</p>
<p><span class="math display">\[
lpd = \log \int \pi (y_i | \theta) \pi (\theta | y) d\theta
\]</span></p>
<p>The fundamental problem comes when we use <span class="math inline">\(y_i\)</span> to compute the full posterior <span class="math inline">\(\pi (\theta | y)\)</span>: we are performing an internal check, not an external validation. A solution is to use the resulting posterior without using the observation <span class="math inline">\(y_i\)</span> in the fitting. <strong>This is the Leave-One-Out (LOO) posterior</strong>: <span class="math inline">\(\pi(\theta, y_{-i})\)</span></p>
<p>The problem is that evaluating the LOO posterior is just as computationally expensive as fitting the model all over again. If we then want to use all our observations to perform the external validation check, this amounts to fitting the probability model <span class="math inline">\(n\)</span> times.</p>
</div>
<div id="approximating-the-loo-posterior" class="section level2">
<h2>Approximating the LOO posterior</h2>
<p>Not being able to compute from a distribution is an awfully familiar problem in Bayesian Statistics. Which in this case comes in handy. We can use <a href="https://david-salazar.github.io/2020/06/27/bayesian-data-analysis-week-4-importance-sampling/">Importance Sampling</a> to use <strong>the samples from the full posterior to approximate the LOO posterior</strong>. Thus, our Importance Weights for each sample <span class="math inline">\(s\)</span> from the posterior is the ratio of the densities.</p>
<p><span class="math display">\[
r_i^s = \frac{\pi (\theta^s | y_{-i})}{\pi(\theta^s|y_i)}
\]</span></p>
<p>If we correct, then, our original full posterior samples by these weights, we get equivalent samples from the LOO posterior. Thus, we can compute the <strong>log pointwise predictive density (lpd)</strong> that can track the out-of-sample performance of our model.</p>
</div>
<div id="the-approximation-is-likely-to-fail" class="section level2">
<h2>The approximation is likely to fail</h2>
<p>Sadly, this approximation to the LOO posterior using the full posterior is likely to fail. Importance Sampling only works when all of the weights are roughly equal. When the weights are very small with a large probability, and very, very large with a small probability, Importance Sampling fails: our computations end up <em>effectively</em> using <strong>only the large weights samples</strong>, thus drastically reducing our <strong>effective number of samples from the LOO posterior</strong>. That is, Importance Sampling is likely to <strong>fail when the distribution of the weights is fat-tailed</strong>.</p>
<p>Sadly, this is very likely to happen with our approximation: the LOO posterior is likely to have a <em>larger variance and fatter tails</em> than the full posterior. Thus, samples from the tails of the full posterior will have large weights to compensate for this fact. Therefore, the distribution of importance weights is likely gonna be fat-tailed.</p>
</div>
<div id="correcting-the-approximation-with-psis" class="section level2">
<h2>Correcting the approximation with PSIS</h2>
<p>Vehtari, Gelman and Gabry correct the distribution of Importance Weights and thereby improve the approximation to the LOO posterior. First, they use Extreme Value Theory to fit the tail of the distribution with a Generalized Pareto Distribution with tail shape parameter <span class="math inline">\(k\)</span> (<span class="math inline">\(GPD(k)\)</span>).</p>
<p>Secondly, they replace the large weights with smoothed over versions of the weights according to expected order statistics of the fitted <span class="math inline">\(GPD(k)\)</span>. This in turn gives the name to the method: <strong>Pareto Smoothed Importance Sampling (PSIS)</strong> Thirdly, they truncate large weights at <span class="math inline">\(\dfrac{3}{4}\)</span> of the mean of the smoothed weights.</p>
<p>Therefore, we arrive at a new vector of importance weights <span class="math inline">\(w_i^s\)</span> which, in general, behaves better than the original importance weights <span class="math inline">\(r_i^s\)</span> and thus allow us to perform a better approximation of the LOO posterior.</p>
<div id="its-about-the-diagnostics-we-made-along-the-way" class="section level3">
<h3>It’s about the diagnostics we made along the way</h3>
<p>The great thing about PSIS, besides creating better importance weights, it’s the diagnostics that it creates along the way. By fitting a <span class="math inline">\(GPD(k)\)</span>, the tail shape parameter becomes a diagnostic to assess the reliability of our approximation. When is Pareto Smoothed Importance Sampling (PSIS) a valid approximation to the LOO posterior?</p>
<p>The smoothing and the truncating can only do so much. If <span class="math inline">\(k &gt; 0.7\)</span>, the importance weights are probably too fat-tailed to begin with and PSIS-LOO will be a poor approximation the LOO posterior. Not only that, it is also a diagnostic that tells us about a fundamental disagreement bewteen the full posterior and the LOO posterior: that is, <strong>about observations that are highly influential in determining the posterior.</strong></p>
<p>Therefore, by performing PSIS-LOO, we also arrive at a diagnostic for highly influential observations that are driving our inference and are thus surprising observations to our model.</p>
</div>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

