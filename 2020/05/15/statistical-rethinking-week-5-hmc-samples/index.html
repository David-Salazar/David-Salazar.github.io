<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Statistical Rethinking Week 5 -&gt; HMC samples - Dilettanting Data Science</title>
<meta property="og:title" content="Statistical Rethinking Week 5 -&gt; HMC samples - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Statistical Rethinking Week 5 -&gt; HMC samples</h1>

    
    <span class="article-date">2020/05/15</span>
    
    

    <div class="article-content">
      


<div id="statistical-rethinking-week-5" class="section level1">
<h1>Statistical Rethinking: Week 5</h1>
<p>After a quick tour around interactions, this week was a quick introduction to MCMC samplers and how they are the engine that powers current Bayesian modelling. We looked at Metropolis, Gibbs and finally HMC. Not only HMC is more efficient, but it also let us know when it fails. Let’s tackle the homework with these new tools:</p>
</div>
<div id="homework-5" class="section level1">
<h1>Homework 5</h1>
<div id="problem-week-1" class="section level3">
<h3>Problem Week 1</h3>
<pre class="r"><code>data(&quot;Wines2012&quot;)

wines &lt;- Wines2012
wines %&gt;% 
  count(judge)</code></pre>
<pre><code>## # A tibble: 9 x 2
##   judge               n
##   &lt;fct&gt;           &lt;int&gt;
## 1 Daniele Meulder    20
## 2 Francis Schott     20
## 3 Jamal Rayyis       20
## 4 Jean-M Cardebat    20
## 5 John Foy           20
## 6 Linda Murphy       20
## 7 Olivier Gergaud    20
## 8 Robert Hodgson     20
## 9 Tyler Colman       20</code></pre>
<p>We have 9 judges and each of them gave 20 reviews. Let’s check the scores</p>
<pre class="r"><code>wines %&gt;% 
  ggplot(aes(score)) +
  geom_histogram(binwidth = 1, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;Score original distribution&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>0 should be a meaningful metric. As well as 1. Let’s perform feature scaling on score:</p>
<pre class="r"><code>wines %&gt;% 
  mutate(score_std = (score-min(score))/(max(score) - min(score)) ) -&gt; wines_scaled
wines_scaled %&gt;% 
  ggplot(aes(score_std)) +
  geom_histogram(binwidth = 0.1, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  hrbrthemes::theme_ipsum_rc() +
  labs(title = &quot;Score distribution scaled&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Let’s analyze our predictor variables: judge and wine</p>
<pre class="r"><code>wines_scaled %&gt;% 
  count(judge.amer)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   judge.amer     n
##        &lt;int&gt; &lt;int&gt;
## 1          0    80
## 2          1   100</code></pre>
<pre class="r"><code>wines_scaled %&gt;% 
  count(wine)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    wine      n
##    &lt;fct&gt; &lt;int&gt;
##  1 A1        9
##  2 A2        9
##  3 B1        9
##  4 B2        9
##  5 C1        9
##  6 C2        9
##  7 D1        9
##  8 D2        9
##  9 E1        9
## 10 E2        9
## 11 F1        9
## 12 F2        9
## 13 G1        9
## 14 G2        9
## 15 H1        9
## 16 H2        9
## 17 I1        9
## 18 I2        9
## 19 J1        9
## 20 J2        9</code></pre>
<p>Let’s create an index variable:</p>
<pre class="r"><code>wines_scaled %&gt;% 
  mutate(judge = as.factor(judge.amer),
         judge = as.integer(judge)) %&gt;%
  count(judge, judge.amer)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   judge judge.amer     n
##   &lt;int&gt;      &lt;int&gt; &lt;int&gt;
## 1     1          0    80
## 2     2          1   100</code></pre>
<pre class="r"><code>wines_scaled %&gt;% 
  mutate(wine_fct = as.factor(wine),
         wine_fct = as.integer(wine_fct)) %&gt;% 
  count(wine_fct, wine)</code></pre>
<pre><code>## # A tibble: 20 x 3
##    wine_fct wine      n
##       &lt;int&gt; &lt;fct&gt; &lt;int&gt;
##  1        1 A1        9
##  2        2 A2        9
##  3        3 B1        9
##  4        4 B2        9
##  5        5 C1        9
##  6        6 C2        9
##  7        7 D1        9
##  8        8 D2        9
##  9        9 E1        9
## 10       10 E2        9
## 11       11 F1        9
## 12       12 F2        9
## 13       13 G1        9
## 14       14 G2        9
## 15       15 H1        9
## 16       16 H2        9
## 17       17 I1        9
## 18       18 I2        9
## 19       19 J1        9
## 20       20 J2        9</code></pre>
<pre class="r"><code>wines_scaled %&gt;% 
  mutate(wine_fct = as.factor(wine),
         wine_fct = as.integer(wine_fct),
         judge = as.factor(judge),
         judge = as.integer(judge)) -&gt; wines_to_model </code></pre>
<blockquote>
<p>Consider only variation among judges and wines</p>
</blockquote>
<pre class="r"><code>wine_to_ulam &lt;- list(
  score = wines_to_model$score_std,
  judge = wines_to_model$judge,
  wine = wines_to_model$wine_fct
)

model_judge_wine &lt;- ulam(
  alist(
    score ~ dnorm(mu, sigma),
    mu &lt;- j[judge] + w[wine],
    j[judge] ~ dnorm(0.5, 0.1),
    w[wine] ~ dnorm(0.5, 0.1),
    sigma ~ dexp(1)
  ),
  data = wine_to_ulam,
  chains = 4, cores = 4
)</code></pre>
<p>Let’s check how the sampling of the posterior went:</p>
<pre class="r"><code>traceplot_ulam(model_judge_wine)</code></pre>
<pre><code>## Waiting to draw page 2 of 2</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>Our chains have all the characteristics of healthy chains:</p>
<ol style="list-style-type: decimal">
<li>They are stationary.</li>
<li>They mix well the the entire parameter space.</li>
<li>They converge to explore the same parameter space across chains.</li>
</ol>
<pre class="r"><code>precis(model_judge_wine, depth = 2)</code></pre>
<pre><code>##            mean         sd       5.5%     94.5%    n_eff     Rhat4
## j[1]  0.1805921 0.04091305 0.11566774 0.2471122 2299.151 0.9998222
## j[2]  0.2833373 0.04227225 0.21611715 0.3503847 3240.061 0.9992340
## j[3]  0.2824585 0.04151775 0.21596181 0.3488334 2773.753 0.9988249
## j[4]  0.1274560 0.04176254 0.06159551 0.1933727 2571.588 0.9997362
## j[5]  0.4039394 0.04236707 0.33702778 0.4735205 2505.136 0.9990441
## j[6]  0.3387293 0.04209175 0.27130067 0.4069038 2653.255 0.9984949
## j[7]  0.2668106 0.04208220 0.19947769 0.3346740 2508.067 0.9989037
## j[8]  0.1035261 0.04287512 0.03672145 0.1714558 2533.110 0.9989672
## j[9]  0.1689490 0.04344599 0.09853704 0.2385020 2825.264 0.9986152
## w[1]  0.4053683 0.05490867 0.31903752 0.4914570 3093.179 0.9982881
## w[2]  0.3996293 0.05696134 0.31118522 0.4939138 3996.439 0.9989594
## w[3]  0.4301058 0.05263637 0.34779865 0.5145053 3365.640 0.9994366
## w[4]  0.4772571 0.05639534 0.38543711 0.5657950 3328.664 0.9996901
## w[5]  0.3625673 0.05297973 0.28026231 0.4493293 3144.681 0.9991020
## w[6]  0.3200490 0.05689325 0.22846814 0.4123886 4049.297 0.9986973
## w[7]  0.4312860 0.05393923 0.34597426 0.5166673 3096.703 0.9992070
## w[8]  0.4275013 0.05530349 0.34037447 0.5148452 3368.509 0.9987926
## w[9]  0.3961048 0.05317565 0.31035625 0.4847095 3230.705 0.9998353
## w[10] 0.4030388 0.05433764 0.31384677 0.4890742 3241.326 0.9997066
## w[11] 0.3799495 0.05345839 0.29561544 0.4664896 4086.487 0.9984919
## w[12] 0.3775945 0.05718174 0.29025339 0.4710384 3901.093 0.9987077
## w[13] 0.3644185 0.05397834 0.27942131 0.4459239 3509.452 0.9989017
## w[14] 0.3831099 0.05534547 0.29830973 0.4721013 3835.587 0.9993570
## w[15] 0.3445391 0.05690449 0.25146759 0.4336893 3208.133 0.9988576
## w[16] 0.3485301 0.05537065 0.26031113 0.4390524 3682.070 0.9990115
## w[17] 0.3583830 0.05584977 0.26729778 0.4496521 3831.488 1.0001293
## w[18] 0.2365040 0.05676113 0.14795307 0.3274455 3305.646 0.9989720
## w[19] 0.3547211 0.05610315 0.26606830 0.4445293 3605.686 0.9986847
## w[20] 0.4483619 0.05710119 0.35912396 0.5378830 4617.340 0.9989631
## sigma 0.1873238 0.01141797 0.17004813 0.2071004 2534.095 0.9985393</code></pre>
<p>The Rhat is ok for all the parameters.</p>
<p>The table is just not informative. Let’s do some ggridges for both judges and wines.</p>
<pre class="r"><code>levels_judges &lt;- levels(as.factor(wines$judge))

samples &lt;- extract.samples(model_judge_wine)
samples_judges &lt;- samples$j
data.frame(samples_judges, sim = 1:2000) %&gt;% 
  pivot_longer(-sim, names_to = &quot;judge&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(judge = str_extract(judge, &quot;\\d&quot;),
         judge = levels_judges[as.integer(judge)]) %&gt;%
  ggplot(aes(score, judge)) +
  geom_density_ridges(fill = &quot;deeppink4&quot;) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;x&quot;) +
  labs(title = &quot;Expected Wine Score by Judge&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>It seems that John Foy was, on average, according to our data and assumptions, gave the highest scores across wines. Whilst Robert Hodgson was the one who gave the least favorable scores.</p>
<pre class="r"><code>samples_wines &lt;- samples$w

levels_wines &lt;- levels(as.factor(wines$wine))


data.frame(samples_wines, sim = 1:2000) %&gt;% 
  pivot_longer(-sim, names_to = &quot;wines&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(wines = str_extract(wines, &quot;\\d++&quot;),
         wines = levels_wines[as.integer(wines)]) %&gt;% 
  ggplot(aes(score, wines)) +
  geom_density_ridges(fill = &quot;deeppink4&quot;) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;x&quot;) +
  labs(title = &quot;Expected Wine Score by Wine&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/figure1-1.png" width="960" /></p>
<p>It seems that we expect, on average, wines B2 and J2 to be scored the highest across all judges. Whilst the lowest we expect it to be I2.</p>
<blockquote>
<p>Now, consider three features of the wines and judges:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>flight</li>
<li>wine.amer</li>
<li>judge.amer</li>
</ol>
<pre class="r"><code>wines_scaled %&gt;% 
  mutate(flight_int = as.integer(flight),
         wine.amer_int = as.integer(as.factor(wine.amer)),
         judge.amer_int = as.integer(as.factor(judge.amer))) -&gt; wines_scaled_problem_2</code></pre>
<pre class="r"><code>wines_scaled_problem_2 %&gt;% 
  count(flight, flight_int)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   flight flight_int     n
##   &lt;fct&gt;       &lt;int&gt; &lt;int&gt;
## 1 red             1    90
## 2 white           2    90</code></pre>
<p>There are equal number of wines</p>
<pre class="r"><code>wines_scaled_problem_2 %&gt;% 
  count(wine.amer, wine.amer_int)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   wine.amer wine.amer_int     n
##       &lt;int&gt;         &lt;int&gt; &lt;int&gt;
## 1         0             1    72
## 2         1             2   108</code></pre>
<p>There are more american wines.</p>
<pre class="r"><code>wines_scaled_problem_2 %&gt;% 
  count(judge.amer, judge.amer_int)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   judge.amer judge.amer_int     n
##        &lt;int&gt;          &lt;int&gt; &lt;int&gt;
## 1          0              1    80
## 2          1              2   100</code></pre>
<p>There are more american judges’ scores.</p>
<pre class="r"><code>data_problem_two &lt;- list(
  score = wines_scaled_problem_2$score_std,
  flight = wines_scaled_problem_2$flight_int,
  wineamer = wines_scaled_problem_2$wine.amer_int,
  judgeamer = wines_scaled_problem_2$judge.amer_int
)

model_flight_nationality &lt;- ulam(
  alist(score ~ dnorm(mu, sigma),
        mu &lt;- f[flight] + wa[wineamer] + ja[judgeamer],
        f[flight] ~ dnorm(0.5, 0.1),
        wa[wineamer] ~ dnorm(0.5, 0.1),
        ja[judgeamer] ~ dnorm(0.5, 0.1),
        sigma ~ dexp(1)
        ),
  data = data_problem_two,
  chains = 4, cores = 4,
  iter = 2000,
)

traceplot_ulam(model_flight_nationality)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>These chains look healthy. They:</p>
<ol style="list-style-type: decimal">
<li>Are stationary.</li>
<li>Mix well.</li>
<li>All chains converge.</li>
</ol>
<pre class="r"><code>precis(model_flight_nationality, depth = 2)</code></pre>
<pre><code>##            mean         sd       5.5%     94.5%    n_eff     Rhat4
## f[1]  0.1992518 0.06094876 0.09994076 0.2951653 1755.354 1.0016159
## f[2]  0.2010000 0.06087327 0.10330666 0.2959497 1758.204 1.0008723
## wa[1] 0.2182282 0.06016613 0.12050249 0.3152866 1792.231 0.9993482
## wa[2] 0.1733102 0.05990938 0.07639588 0.2681538 1821.992 0.9994857
## ja[1] 0.1742472 0.05659828 0.08461239 0.2654446 1801.749 1.0015110
## ja[2] 0.2210856 0.05662540 0.13109968 0.3111868 1715.139 1.0017330
## sigma 0.2141573 0.01153783 0.19663471 0.2330322 2262.258 1.0010303</code></pre>
<p>Let’s visualize the differences between flights:</p>
<pre class="r"><code>levels_flights = levels(wines$flight)

samples_problem_2 &lt;- extract.samples(model_flight_nationality)
samples_flights &lt;- samples_problem_2$f

data.frame(sim = 1:2000, samples_flights) %&gt;% 
  pivot_longer(-sim, names_to = &quot;flight&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(flight = str_extract(flight, &quot;\\d+&quot;),
         flight = levels_flights[as.integer(flight)]) %&gt;% 
  ggplot(aes(score, flight, fill = flight)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;X&quot;) +
  theme(legend.position = &quot;none&quot;) +
  scale_fill_manual(values = c(&quot;deeppink4&quot;, &quot;lightgoldenrod2&quot;)) +
  labs(title = &quot;Expected Red and White wines&#39; score&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>On expectation, there does not seem to be a difference between the red’s score and the white’s score.</p>
<pre class="r"><code>samples_judge_amer &lt;-samples_problem_2$ja

data.frame(sim = 1:2000, samples_judge_amer) %&gt;% 
  pivot_longer(-sim, names_to = &quot;judge_american&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(judge_american = str_extract(judge_american, &quot;\\d+&quot;),
         judge_american = ifelse(judge_american == 1, &quot;Not American&quot;, &quot;American&quot;)) %&gt;% 
  ggplot(aes(score, judge_american)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;X&quot;) +
  labs(title = &quot;Expected American and Non american judges&#39; scores&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>American judges, on average, tend to give higher scores.</p>
<pre class="r"><code>data.frame(sim = 1:2000, samples_problem_2$wa) %&gt;% 
  pivot_longer(-sim, names_to = &quot;wine_american&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(wine_american = str_extract(wine_american, &quot;\\d+&quot;),
         wine_american = ifelse(wine_american == 1, &quot;Not American&quot;, &quot;American&quot;)) %&gt;% 
  ggplot(aes(score, wine_american, fill = wine_american)) +
  geom_density_ridges(scale = 0.9) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;X&quot;) +
  labs(title = &quot;Expected American and Non american wine&#39; scores&quot;,
       fill = &quot;&quot;) +
  scale_fill_brewer(palette = &quot;Set1&quot;)</code></pre>
<pre><code>## Picking joint bandwidth of 0.0102</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Non-American wines tend to get higher scores on average.</p>
<blockquote>
<p>Now consider two way intercations among the three features</p>
</blockquote>
<pre class="r"><code>wines_scaled %&gt;% 
  mutate(flight_int = as.integer(flight) - 1) -&gt; wines_problem_3

data_problem_three = list(
  score = wines_problem_3$score_std,
  flight = wines_problem_3$flight_int,
  wa = wines_problem_3$wine.amer,
  ja = wines_problem_3$judge.amer
)</code></pre>
<pre class="r"><code>model_interactions &lt;- ulam(alist(
  score ~ dnorm(mu, sigma),
  mu &lt;- a+ w*wa + j*ja + f*flight + wj*wa*ja + wf*wa*flight +jf*ja*flight,
  a ~ dnorm(0, 0.1),
  w ~ dnorm(0.5, 0.1),
  j ~ dnorm(0.5, 0.1),
  f ~ dnorm(0.5, 0.1),
  wj ~ dnorm(0, 0.1),
  wf ~ dnorm(0, 0.1),
  jf ~ dnorm(0, 0.1),
  sigma ~ dexp(1)
), data = data_problem_three,
chains = 4, cores = 4, iter = 2000)

traceplot_ulam(model_interactions)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>These chains look healthy, i.e., they:</p>
<ol style="list-style-type: decimal">
<li>They are stationary</li>
<li>They mix well over the parameter space.</li>
<li>Different chains converge to explore the same ridges of parameter space.</li>
</ol>
<pre class="r"><code>precis(model_interactions)</code></pre>
<pre><code>##              mean         sd        5.5%       94.5%    n_eff    Rhat4
## a      0.35221452 0.04013806  0.28730279  0.41532758 1886.028 1.001263
## w      0.15170475 0.04778621  0.07493928  0.22871217 1795.938 1.000506
## j      0.26571472 0.04705942  0.19148927  0.34214161 2217.374 1.002029
## f      0.17353089 0.04903890  0.09508363  0.25235229 2155.015 1.000570
## wj    -0.16417911 0.05278299 -0.24906395 -0.07961976 2416.349 1.002076
## wf    -0.06983734 0.05209965 -0.15400171  0.01243461 2278.673 1.000318
## jf    -0.12941423 0.05145773 -0.21049314 -0.04751127 3030.571 1.001019
## sigma  0.22838249 0.01349830  0.20801075  0.25130852 2579.459 1.000596</code></pre>
<p>The Rhat is ok for all the parameters. However, now that we have interactions, it is not easy nor intuitive to analyze parameters on their own scale. We must compare them on the outcome scale. Let’s create predictions for all eight possible values</p>
<pre class="r"><code>all_combinations &lt;- tribble(~wa, ~ja, ~flight,
                            1, 1, 1,
                            1, 1, 0,
                            1, 0, 1,
                            1, 0, 0,
                            0, 1, 1,
                            0, 1, 0,
                            0, 0, 1,
                            0, 0, 0)
all_combinations</code></pre>
<pre><code>## # A tibble: 8 x 3
##      wa    ja flight
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1     1     1      1
## 2     1     1      0
## 3     1     0      1
## 4     1     0      0
## 5     0     1      1
## 6     0     1      0
## 7     0     0      1
## 8     0     0      0</code></pre>
<p>Then, we can calculate our expected predictions for each of the cases:</p>
<pre class="r"><code>mu &lt;- link(model_interactions, data = all_combinations)</code></pre>
<pre class="r"><code>data.frame(sims = 1:4000, mu) %&gt;% 
  pivot_longer(-sims, names_to = &quot;interaction&quot;, values_to = &quot;score&quot;) %&gt;% 
  mutate(interaction = case_when(interaction == &quot;X1&quot; ~ &quot;USA Wine, USA Judge, White&quot;,
                                 interaction == &quot;X2&quot; ~ &quot;USA Wine, USA Judge, Red&quot;,
                                 interaction == &quot;X3&quot; ~ &quot;USA Wine, Int Judge, White&quot;,
                                 interaction == &quot;X4&quot; ~ &quot;USA Wine, Int Judge, Red&quot;,
                                 interaction == &quot;X5&quot; ~ &quot;Int Wine, USA Judge, White&quot;,
                                 interaction == &quot;X6&quot; ~ &quot;Int Wine, USA Judge, Red&quot;,
                                 interaction == &quot;X7&quot; ~ &quot;Int Wine, Int Judge, White&quot;,
                                 interaction == &quot;X8&quot; ~ &quot;Int Wine, Int Judge, Red&quot;),
         red = str_detect(interaction, &quot;Red&quot;)) %&gt;%  
  ggplot(aes(score, interaction, fill = red)) +
  geom_density_ridges() +
  scale_fill_manual(values = c( &quot;lightgoldenrod2&quot;, &quot;deeppink4&quot;)) +
  hrbrthemes::theme_ipsum_rc(grid = &quot;X&quot;) +
  theme(legend.position = &quot;none&quot;) +
  labs(title = &quot;Posterior Score Predictions&quot;)</code></pre>
<p><img src="/post/2020-05-15-statistical-rethinking-week-5-hmc-samples_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Whites tend, on average, to be higher scored. Also, non American Judges tend to be harsher than their american counterparts, regardless of the origin of the wine. The worst rated wine, on average, are the red international wine.</p>
</div>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

