{
  "hash": "9ef83a4c519f4b799cad40ccbd4202d4",
  "result": {
    "markdown": "---\ntitle: Correlation is not Correlation\nauthor: ''\ndate: '2020-05-22'\nslug: correlation-is-not-correlation\ncategories: []\ntags: []\noutput:\n  blogdown::html_page:\n    toc: true\n---\n\n\n\n\n\n# Correlation is not correlation\n\nTo the usual phrase of correlation is not causation, Nassim Taleb often answers: correlation is not correlation. First, just like the [mean](2020-04-17-fat-vs-thin-does-lln-work.html) and [PCA](2020-04-27-spurious-pca-under-thick-tails.html), the sample correlation coefficient has persistent small sample effects when variables from Extremistan are involved. These topics are analyized in his latest book: [Statistical Consequences of Fat Tails](https://www.researchers.one/media/documents/260-m-Technical%20Incerto%20Vol%201.pdf). I'll explore this with some Monte-Carlo simulations from both Mediocristan and Extremsitan. \n\nSecondly, however, as Taleb says, even if we are in Mediocristan, the correlation coefficient is commonly **misued and/or misunderstood**. Commonly **misused** because *people take non-random subsamples and expect the same correlation*. Thereby arriving at some paradoxes: Berkson's paradox (Collider Bias) and Simpson's Paradox. I'll take non random subsamples from a underlying random sample to show how you can get both paradoxes. Also, it is commonly misused because Correlation should not be used when there's a non-linear relationship between the variables; otherwise, you get deceiving results. **Commonly misunderstood** because the amount of information it conveys is not linear. That is, a correlation of 0.9 conveys much, much more information than 0.7. Taleb analyzes these topics in a terrific, short paper called [Fooled by Correlation: Common Misinterpretations in Social \"Science\"](https://www.dropbox.com/s/18pjy7gmz0hl6q7/Correlation.pdf?dl=0)\n\n# Small sample effects with Sample Correlation\n\n## Sample Correlation in Mediocristan\n\nLet's do some Monte-Carlo simulations in Mediocristan. I'll be sampling from two independent gaussians; that is, we know the \"true\" correlation is 0 and then we will compare it with the sample-correlation. \n\n\n::: {.cell hash='2020-05-22-correlation-is-not-correlation_cache/html/unnamed-chunk-2_df4f2b1b31ba0441ed89a7cdacf2983e'}\n\n```{.r .cell-code}\ncorrelation_mediocristan <- function(n, rho = 0) {\n  # sample from multivariate normal\n  data <- rnorm2d(n = n, rho = rho)\n  \n  # compute sample correlation\n  sample_correlation <- cor(data)[1,2]\n  sample_correlation\n}\n\nrerun(10000, correlation_mediocristan(20, 0)) %>% \n  unlist() -> mediocristan_correlations_20_obs\n\nrerun(10000, correlation_mediocristan(1000, 0)) %>% \n  unlist() -> mediocristan_correlations_1000_obs\n\ndata.frame(sim = 1:10000, small_sample = mediocristan_correlations_20_obs,\n           large_sample = mediocristan_correlations_1000_obs) %>% \n  pivot_longer(-sim, names_to = \"sample\", values_to = \"sample_correlation\") %>% \n  mutate(sample = if_else(sample == \"large_sample\", \"B: Sample with 1,000 observations\", \n                          \"A: Sample with 20 observations\")) -> gaussian_corr\n```\n:::\n\n\nNow we plot them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngaussian_corr %>% \n  ggplot(aes(sim, sample_correlation)) +\n  geom_col(aes(fill = sample)) +\n  facet_wrap(~sample) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  labs(title = \"Sample correlations across different simulations\",\n       subtitle = \"Sample correlation quickly converges. Variables are independent\",\n       y = \"sample correlation\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='2020-05-22-correlation-is-not-correlation_cache/html/unnamed-chunk-4_151d24927c7c03f98018526daa0474ef'}\n\n```{.r .cell-code}\ngaussian_corr %>% \n  ggplot(aes(sample_correlation, fill = sample)) +\n  geom_histogram(binwidth = 0.05, color = \"black\", alpha = 0.5) +\n  facet_wrap(~sample) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Sample correlations in Mediocristan\",\n       subtitle = \"Sample correlation quickly converges. Variables are independent\",\n       x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nTherefore, we can use our sample correlation coefficients in Mediocristan as they quickly converge. That is, with randomness coming from Mediocristan, **noise quickly washes out with relatively small sample size**\n\n## Sample correlations from Extremistan\n\nTo examine the slow convergence of the sample correlation coefficient, Taleb proposes a bivariate t-student distribution with exponent 2/3. Notice that the mean and variance are undefined. Yet, there is a finite correlation.  Let's replicate the same experiment as we did in mediocristan:\n\n\n::: {.cell hash='2020-05-22-correlation-is-not-correlation_cache/html/unnamed-chunk-5_70e681948964fc345ed8186a05d81c6f'}\n\n```{.r .cell-code}\ncorrelation_bivariate_t <- function(n, rho = 0) {\n  # sample from multivariate normal\n  data <- rt2d(n, rho = rho, nu = 2/3)\n  \n  # compute sample correlation\n  sample_correlation <- cor(data)[1,2]\n  sample_correlation\n}\n\nrerun(10000, correlation_bivariate_t(20, rho = 0)) %>% \n  unlist() -> bivariate_t_20\n\nrerun(10000, correlation_bivariate_t(1000, rho = 0)) %>% \n  unlist() -> bivariate_t_thousand\n\ndata.frame(sim = 1:10000, bivariate_t_20, bivariate_t_thousand) %>% \n  pivot_longer(-sim, names_to = \"sample_size\", values_to = \"sample_correlation\") %>% \n  mutate(sample_size = if_else(sample_size == \"bivariate_t_20\", \n                               \"A: Sample with 20 observations\", \"B: Sample with 1,000 observations\")) -> t_corr \n```\n:::\n\n\nNow, let's plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_corr %>% \n  ggplot(aes(sim, sample_correlation)) +\n  geom_col(aes(fill = sample_size)) +\n  facet_wrap(~sample_size) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_x_continuous(labels = scales::comma_format()) +\n  labs(title = \"Sample correlations across different simulations\",\n       subtitle = \"Sample correlation is just as erratic, regardless of sample size. True correlation is zero. \",\n       y = \"sample correlation\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nt_corr %>% \n  ggplot(aes(sample_correlation, fill = sample_size)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  facet_wrap(~sample_size) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Distribution of Sample correlations from Extremistan\",\n       subtitle = \"Sample correlation suffers from small sample effect. True correlation is zero.\",\n         x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nAs Taleb writes: \"finite correlation doesn't mean low [estimator's] variance: it exists, but may not be useful for statistical purposes owing to the noise and slow convergence.\"\n\nFinally, let's experiment what happens if we make our samples 10k big:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrerun(10000, correlation_bivariate_t(10000, rho = 0)) %>% \n  unlist() -> bivariate_t_10k\n```\n:::\n\n\nLet's plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(sample_size_10k = bivariate_t_10k) %>% \n  ggplot(aes(sample_size_10k)) +\n  geom_histogram(color = \"black\", alpha = 0.5, binwidth = 0.05) +\n  hrbrthemes::theme_ipsum_rc(grid = \"Y\") +\n  labs(title = \"Distribution of Sample correlations from Extremistan\",\n       subtitle = \"Sample (n = 10k) correlation suffers from small sample effect. True correlation is zero.\",\n         x = \"sample correlation\",\n       caption = \"Histogram binwidth = 0.05\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\nThat is, the sample correlations suffers from persistent small sample effect. Thus, there's to much noise to use the sample correlation for fat-tailed distributions. \n\n# Misused Correlation\n\nThere are two famous statistical paradoxes regarding correlation: Berkson's paradox and Simpson's Paradox. Both arise from the wrong expectation that the total correlation will be preserved when taking non-random subsamples. Indeed, it is quite the opposite. [Taleb](https://twitter.com/nntaleb/status/1108541435140366336): \"all non-random subsamples will yield a correlation below the total one\"\n\n## Quadrant's Correlation\n\nTo get some intuition about the problem, let's sample $10^6$ from a multivariate normal with correlation of $0.75$. Then, we will group the data into geometrically intuitive non-random partitions: into quadrants. Then, we will calculate the correlation between the observations within each quadrant. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10^6\nrho <- 3/4 \ndata <- rnorm2d(n = n, rho = rho) \n\ndata.frame(data) %>% \n  rename(x = X1, y = X2) %>% \n  mutate(quadrant = case_when(\n    x > 0 & y > 0 ~ \"II\",\n    x < 0 & y > 0 ~ \"I\",\n    x < 0 & y < 0 ~ \"IV\",\n    x > 0 & y < 0 ~ \"III\" \n    )) %>% \n  group_by(quadrant) %>% \n  summarise(correlation = cor(x, y)) %>% \n  gt::gt() %>% \n  gt::fmt_number(vars(correlation))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"xsyojmmzur\" style=\"overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>html {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#xsyojmmzur .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#xsyojmmzur .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#xsyojmmzur .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#xsyojmmzur .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#xsyojmmzur .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#xsyojmmzur .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#xsyojmmzur .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#xsyojmmzur .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#xsyojmmzur .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#xsyojmmzur .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#xsyojmmzur .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#xsyojmmzur .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#xsyojmmzur .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xsyojmmzur .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#xsyojmmzur .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#xsyojmmzur .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xsyojmmzur .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#xsyojmmzur .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xsyojmmzur .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#xsyojmmzur .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-left: 4px;\n  padding-right: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xsyojmmzur .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#xsyojmmzur .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#xsyojmmzur .gt_left {\n  text-align: left;\n}\n\n#xsyojmmzur .gt_center {\n  text-align: center;\n}\n\n#xsyojmmzur .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#xsyojmmzur .gt_font_normal {\n  font-weight: normal;\n}\n\n#xsyojmmzur .gt_font_bold {\n  font-weight: bold;\n}\n\n#xsyojmmzur .gt_font_italic {\n  font-style: italic;\n}\n\n#xsyojmmzur .gt_super {\n  font-size: 65%;\n}\n\n#xsyojmmzur .gt_footnote_marks {\n  font-style: italic;\n  font-weight: normal;\n  font-size: 75%;\n  vertical-align: 0.4em;\n}\n\n#xsyojmmzur .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#xsyojmmzur .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#xsyojmmzur .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#xsyojmmzur .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#xsyojmmzur .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#xsyojmmzur .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\">\n  \n  <thead class=\"gt_col_headings\">\n    <tr>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\">quadrant</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\">correlation</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td class=\"gt_row gt_left\">I</td>\n<td class=\"gt_row gt_right\">0.19</td></tr>\n    <tr><td class=\"gt_row gt_left\">II</td>\n<td class=\"gt_row gt_right\">0.53</td></tr>\n    <tr><td class=\"gt_row gt_left\">III</td>\n<td class=\"gt_row gt_right\">0.18</td></tr>\n    <tr><td class=\"gt_row gt_left\">IV</td>\n<td class=\"gt_row gt_right\">0.53</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n:::\n:::\n\n\nTherefore, we should not expect to have the same correlation in non-random subsamples of the data as we have in the whole sample.  \n\n## Berkson's paradox\n\nIn Berkson's paradox, there appears to be a negative correlation between 2 variables in a subsample; when in fact there is no correlation between them considering the whole. The bias is introduced by sub-sampling our observations based on another variable that the 2 variables in question jointly determine. In DAG slang, it is collider bias.\n\nImagine, then, that we have samples from two gaussian variables that are independent. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10^4\nrho <- 0 \ndata <- rnorm2d(n = n, rho = rho) \n\ndata.frame(data) %>% \n  rename(x = X1, y = X2) -> data_berkson\n\ndata_berkson %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(linetype = 2, color = \"red\", se = FALSE) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"Variables are uncorrelated in the whole sample\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nHowever, they jointly determine another variable $Z$ thus: $Z$ is zero except when either $X$ or $Y$ are greater than $0$, in which case we have $Z = 1$. Then, sampling conditioning on $Z = 1$ is non a random subsample. Therefore, as we know that Correlation is subadditive, we know that we do not expect the same correlation as in the whole sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_berkson %>% \n  mutate(z = if_else(x > 0 | y > 0, 1, 0))-> data_berkson_z\n\ndata_berkson_z %>% \n  ggplot(aes(x, y, color = factor(z))) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = 2) +\n  hrbrthemes::theme_ipsum_rc() +\n  scale_color_viridis_d(begin = 1, end = 0) +\n  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = \"red\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"Berkson's paradox: Negative relation between independent vars\",\n       subtitle = \"Correlation does weird things on non-random sub samples.\",\n       caption = \"Red line is the whole sample trend\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\nIntuitively, if we know that $X$ is negative, due to our conditioning on $Z$, then it must be the case that $Y$ is positive. Therefore, conditioning on $Z$ creates a negative correlation between two independent variables.  \n\nTherefore, if we condition on $Z=1$, a non-random subsample (thereby opening a collider), we get a negative correlation from two independent variables: Berkson's paradox. **Again, we should not expect the global correlation to stay constant under non-random sub samples.** \n\n## Simpson's Paradox\n\nTo consider Simpson's paradox, let's consider 3 random variables. $X$ is going to determine the value of $Z$. $X$ and $Z$ are jointly going to determine the value $Y$ \n\nOn the whole sample, $X$ and $Y$ are positively related\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10000\nx <- rnorm(n)\nz <- rnorm(n, 1.2*x) \ny <- rnorm(n, -0.8*x + z)\n\ndata.frame(x, y, z) %>% \n  ggplot(aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(se = FALSE, linetype = 2) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"X and Y are positively related in the whole sample\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHowever, once we condition on $Z$, that is, take non-random sub-samples according to the values of $Z$, we will see a reversal of the sign of the correlation between $X$ and $Y$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(x, y, z) %>% \n  mutate(z = cut(z, 10)) %>% \n  ggplot(aes(x, y, color = z)) +\n  geom_point(alpha = 0.1)  +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(aes(x, y), linetype = 2, inherit.aes = FALSE, se = FALSE, color = \"red\") +\n  hrbrthemes::theme_ipsum_rc() + \n  theme(legend.position = \"none\") +\n  scale_color_viridis_d() + \n  labs(title = \"Simpson's Paradox: Reversal of correlation sign\",\n       subtitle = \"Correlation does weird things on non-random sub samples.\",\n       caption = \"Red line is whole sample trend\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n\nWe are doing something extremely similar to what linear regression does. Once we adjust for $Z$, what's left is the negative effect of $X$ on $Y$. **Again, we should not expect the global correlation to stay constant under non-random sub-samples.**\n\n## Correlation under non linearities\n\nSometimes, the association between two variables depends on the levels of the variables themselves. That is, there is a non-linear relationship between the two variables. In these types of situations it's a mistake to use total correlation. Taleb proposes the following experiment, which he calls \"Dead Man Bias\":\n\n> You administer IQ tests to 10k people, then give them a \"performance test\" for anything, any task. 2000 of them are dead. Dead people score 0 on IQ and 0 on performance. The rest have the IQ uncorrelated to the performance to the performance. What is the spurious correlation IQ/Performance?\n\nLet's perform this thought experiment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10000\np_dead <- 0.2\n\ndead <- rbernoulli(n, p_dead)\n\nperformance <- runif(n)\niq <- runif(n)\n\nperformance[dead] <- 0\niq[dead] <- 0\n```\n:::\n\n\nLet's plot our results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(iq, performance, dead) %>% \n  ggplot(aes(iq, performance, color = dead)) +\n  geom_point(alpha = 0.1) +\n  hrbrthemes::theme_ipsum_rc() +\n  labs(title = \"You'll never guess a positive relationship\",\n       subtitle = \"Until you notice the dark spot at the origin\") +\n  scale_color_viridis_d(begin = 1, end = 0) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nLooking at the data, you'll never believe there's a positive correlation. However, the dark spot in the origin changes the whole history. Let's calculate the correlation coefficient:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(iq, performance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3849505\n```\n:::\n:::\n\n\nThat is, iq (in this thought experiment, at least) is not a predictor of good performance. However, it does matter: the dark spot (the dead people) fools correlation into thinking that if $iq=0$ means $performance=0$, then the relationship must hold for the entire range of observations. That is, the extremely non-linear relationship between iq and performance fools the correlation coefficient. \n\nMoral of the story, **never use correlation when there's an indication of non-linear relationship between the variables**. \n\n# Misunderstanding of Correlation: its signal is non linear\n\nAlthough we've just said that correlation can only model linear relationships, it cannot be interpreted linearly. That is, the information that correlation picks up about the association between two variables does not scale linearly with correlation. \n\n## Entropy and Mutual Information\n\nFrom information theory, we get a measure of the inherent uncertainty in a distribution. Information Entropy, which is just: $-E[log(p_i)]$. Also, there is divergence: the extra uncertainity that is induced when we approximate the distribution p with the distribution q: $E[log(\\dfrac{p_i}{q_i})]$. \n\nIn the same way, there is the concept of Mutual Information: the extra uncertainity that is induced when we approximate a joint probability distribution with the product of their marginal distributions. If we don't increase the uncertainty, then we do not lose information by modelling them independently. Therefore, **it is a measure of dependence: the higher, the more dependence between the variables.**\n\nFor the Gaussian case, Taleb shows that the Mutual information is:\n\n$$ MI = -\\dfrac{1}{2} log (1 - \\rho^2) $$\n\nTherefore, the information about the association between two variables that Correlation conveys does not scale linearly with the correlation coefficient. Indeed, MI has a convex response to $\\rho$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrho <- seq(-1, 1, length.out = 100)\nmutual_information = -1/2* log(1 - rho^2)\n\ndata.frame(rho, mutual_information) %>%\n  mutate(association = if_else(rho < 0, \"negative\", \"positive\")) %>% \n  ggplot(aes(rho, mutual_information, fill = association)) +\n  geom_area(alpha = 0.5) +\n  scale_fill_viridis_d() +\n  hrbrthemes::theme_ipsum_rc() +\n  theme(legend.position = \"none\") +\n  labs(subtitle = \"Information conveyed by correlation does not scale linearly\",\n       title = \"Mutual information as a function of correlation\",\n       caption = \"Gaussian case\", \n       y = \"Mutual Information\",\n       x = \"correlation coefficient\")\n```\n\n::: {.cell-output-display}\n![](2020-05-22-correlation-is-not-correlation_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nFor example, the change in the information conveyed from a correlation of $0$ to $0.5$ is much, much smaller than a change from $0.7$ to $0.99$. Therefore, correlation coefficient should not be interpreted linearly. \n\n# Conclusion\n\nAs expected by now, sample correlation suffers from persistent small sample effect when the type of randomness we are dealing with comes from Extremistan. However, even if we are in Mediocristan, using and interpreting the correlation coefficient correctly is a tricky endeavour. For example, Simpson's and Berkson's paradoxes arise from mistakingly expecting the same correlation coefficient from the whole sample as in non-random subsamples from it. Also, non-linear relationships are, by definition, the worst case to use a correlation coefficient. Finally, the signal that correlation convey does not scale linearly with its coefficient: a correlation coefficient of 0.7 conveys much less than a coefficient of 0.9; however, a correlation 0.3 and 0.5 convey just about the same about the dependence. \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}