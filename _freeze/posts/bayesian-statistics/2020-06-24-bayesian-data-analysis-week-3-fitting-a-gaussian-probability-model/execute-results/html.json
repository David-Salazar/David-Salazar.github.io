{
  "hash": "13068df925da5c1135404d076701f049",
  "result": {
    "markdown": "---\ntitle: 'Bayesian Data Analysis: Week 3 -> Fitting a Gaussian probability model'\nauthor: ''\ndate: '2020-06-24'\nslug: bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model\ncategories: []\ntags: []\naliases: \n  - ../../2020/06/24/bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model/\n---\n\n\n\n\nBayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it's [freely available online](http://www.stat.columbia.edu/~gelman/book/). To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.\n\nInstead of going through the homeworks (due to the fear of ruining the fun for future students of Aki's), I'll go through some of the examples of the book as case studies. In this blogpost, I'll (wrongly) estimate the speed of light from the measurements of Simon Newcomb's 1882 experiments. \n\n## The Gaussian Probability model\n\nWhen fitting a Gaussian probability model, there are to parameters to estimate: $\\mu, \\sigma$. Therefore, we arrive at a joint posterior distribution:\n\n$$\ny | \\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2) \\\\\np(\\mu, \\sigma | y) \\propto p (y | \\mu, \\sigma) p(\\mu, \\sigma)\n$$\nIn this case, $\\sigma$ is a nuisance parameter: we are only really interested in knowing $\\mu$. The following we assume the non-informative prior:\n\n$$\np(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1}\n$$\n\n### Posterior marginal of sigma\n\nWe can show that the marginal posterior distribution for $\\sigma$ is:\n\n$$\n\\sigma^2 | y \\sim Inv -\\chi^2 (n - 1, s^2)\n$$\nWhere $s^2$ is the sample variance of the $y_i$'s.\n\n### Marginal Conditional posterior for mu\n\nThen:\n\n$$\n\\mu | \\sigma^2, y \\sim N(\\bar y, \\sigma^2 / n)\n$$\n> The posterior distribution of $\\mu$ can be regarded as a mixture of normal distributions, mixed over the scaled inverse $\\chi^2$ distribution for the variance, $\\sigma^2$.\n\n### Marginal posterior for mu\n\n$$\n\\dfrac{\\mu  - \\bar y}{s/\\sqrt{n}} | y \\sim t_{n-1}\n$$\n\nWhich has a nice correspondence with the distribution used for the mean estimator in frequentist statistics in the case of small samples.  \n## Fitting it with data\n\nThen, let's read the measurements from Newcomb's experiment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_delim(\"http://www.stat.columbia.edu/~gelman/book/data/light.asc\", \n                     delim = \" \", skip = 3,col_names = F) %>% \n  pivot_longer(everything()) %>% \n  select(value) %>% \n  drop_na()-> measurements\n\nmeasurements %>% \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"dodgerblue4\", alpha = 0.5) +\n  labs(title = \"Newcomb's measurements for the speed of light\",\n       subtitle = \"There are clearly problems with the data\",\n       x = \"measurement\")\n```\n\n::: {.cell-output-display}\n![](2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThere are clearly some problems with the data. Garbage in, garbage out.\n\n### Marginal for sigma\n\nWe therefore can derive the marginal distribution for $\\sigma$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasurements%>% \n  skimr::skim()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |66         |\n|Number of columns        |1          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |1          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd|  p0| p25| p50|   p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|---:|---:|---:|-----:|----:|:-----|\n|value         |         0|             1| 26.21| 10.75| -44|  24|  27| 30.75|   40|▁▁▁▂▇ |\n:::\n:::\n\n\nThus:\n\n$$\n\\sigma^2 | y \\sim Inv -\\chi^2 (65, 10.7^2)\n$$\n\nWhereas for mu we have :\n\n$$\n\\dfrac{\\mu  - 26.2}{10.7/\\sqrt{66}} | y \\sim t_{n-1}\n$$\nIn code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsinvchisq <- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)\n\nsigma_draw = rinvchisq(1000, 65, 10.7^2)\nmu_draw = rtnew(10000, df = 65, mean = 26.2, scale = 10.7/sqrt(66) )\n```\n:::\n\n\nThe 95% credible interval for our $\\mu$ is thus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninterval <- rethinking::PI(mu_draw, prob = 0.95)\nlower <- interval[[1]]\nupper <- interval[[2]]\nglue::glue(\"The 95% credible interval is thus: {round(lower, 1)}, {round(upper, 1)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe 95% credible interval is thus: 23.6, 28.8\n```\n:::\n:::\n\n\nA visualization may help:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(mu_draw) %>% \n  ggplot(aes(mu_draw)) +\n  geom_histogram(binwidth = 0.1, color = \"black\", fill = \"dodgerblue4\", alpha = 0.5) +\n  geom_vline(aes(xintercept = lower), color = \"red\", linetype = 2) +\n  geom_vline(aes(xintercept = upper), color = \"red\", linetype = 2) +\n  geom_vline(aes(xintercept = 33), color = \"black\", linetype = 1) +\n  labs(title = \"Speed of light: Posterior draws for mu\",\n       subtitle = \"Contemporary estimates for the speed of light in the experiment is 33. Garbage in, garbage...\")\n```\n\n::: {.cell-output-display}\n![](2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model_files/figure-html/mu-1.png){width=768}\n:::\n:::\n",
    "supporting": [
      "2020-06-24-bayesian-data-analysis-week-3-fitting-a-gaussian-probability-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}