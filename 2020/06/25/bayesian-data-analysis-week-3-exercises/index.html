<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Bayesian Data Analysis: Week 3-&gt; Exercises - Dilettanting Data Science</title>
<meta property="og:title" content="Bayesian Data Analysis: Week 3-&gt; Exercises - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">8 min read</span>
    

    <h1 class="article-title">Bayesian Data Analysis: Week 3-&gt; Exercises</h1>

    
    <span class="article-date">2020/06/25</span>
    
    

    <div class="article-content">
      


<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>In this blogpost, I’ll go over a couple of the selected exercises for week 3: exercise number 2 and exercise number 3.</p>
<div id="exercise-2" class="section level2">
<h2>Exercise 2</h2>
<p>Comparison of two multinomial observations: on September
<span class="math inline">\(25,1988,\)</span> the evening of a presidential campaign debate, ABC News conducted a survey of registered voters in the United States; 639 persons were polled before the debate, and 639 different persons were polled after. Assume the surveys are independent simple random samples from the population of registered voters. Model the data with two different multinomial distributions. For <span class="math inline">\(t=1,2,\)</span> let <span class="math inline">\(\alpha_{t}\)</span> be the proportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey
<span class="math inline">\(t .\)</span> <strong>Plot a histogram of the posterior density for <span class="math inline">\(\alpha_{2}-\alpha_{1} .\)</span> What is the posterior probability that there was a shift toward Bush?</strong></p>
<p>The results of the surveys are thus:</p>
<p><span class="math display">\[
\begin{array}{c|ccc|c}
\text { Survey } &amp; \text { Bush } &amp; \text { Dukakis } &amp; \text { No opinion/other } &amp; \text { Total } \\
\hline \text { pre-debate } &amp; 294 &amp; 307 &amp; 38 &amp; 639 \\
\text { post-debate } &amp; 288 &amp; 332 &amp; 19 &amp; 639
\end{array}
\]</span></p>
<div id="solution" class="section level3">
<h3>Solution</h3>
<p>Therefore, for the pre-debate we posit a multinomial model. A multinomial model is nothing more than the extension of the binomial model to more than 2 categories. Here we have 3: Bush, Dukakis and other. For both models, <strong>we assume that the 639 observations are independent and exchangeable</strong>. The likelihood for each survey is thus:</p>
<p><span class="math display">\[
p(y \mid \theta) \propto \prod_{j=1}^{k} \theta_{j}^{y_{j}}
\]</span></p>
<p>Where <span class="math inline">\(\theta_j\)</span> is the probability of choosing the <span class="math inline">\(j\)</span> option. The conjugate prior for the distribution is a multivariate generalization of the beta distribution known as Dirichlet:</p>
<p><span class="math display">\[
p(\theta \mid \beta) \propto \prod_{j=1}^{k} \theta_{j}^{\beta_{j}-1}
\]</span></p>
<p>If we set all <span class="math inline">\(\beta_j = 1\)</span>, we get an uniform distribution on the possible distributions for the <span class="math inline">\(\theta\)</span>’s. That is, <strong>just as the beta distribution, the Dirichlet distribution is a distribution of distributions.</strong></p>
<p>The resulting posterior distribution for the <span class="math inline">\(\theta_j\)</span>’s is a Dirichlet with parameters <span class="math inline">\(\beta_j + y_j\)</span>. The question, then, is how to go from the <span class="math inline">\(\theta_j\)</span>, the proportion that favors the option <span class="math inline">\(j\)</span>, to the requested <span class="math inline">\(\alpha_t\)</span>:</p>
<blockquote>
<p>Proportion of voters who preferred Bush, out of those who had a preference for either Bush or Dukakis at the time of survey t.</p>
</blockquote>
<p>Note that given the inherent restriction on the Dirichlet, we can rewrite the distribution of the <span class="math inline">\(\theta_j\)</span>’s as <span class="math inline">\((\theta_1, \theta_2, 1 - \theta_1 - \theta_2)\)</span>. We can then perform a change of variables: <span class="math inline">\((\alpha, \gamma) = (\dfrac{\theta_1}{\theta_1 + \theta_2}, \theta_1 + \theta_2)\)</span>. Which it can be shown that <span class="math inline">\(\alpha\)</span> is then distributed thus:</p>
<p><span class="math display">\[
\alpha | y \sim Beta(y_1 + \beta_1, y_2 + \beta_2)
\]</span>
### Pre-Debate</p>
<p>Therefore, setting an uniform prior (<span class="math inline">\(\beta_j = 1 \ \forall j\)</span>) on the possible distribution of the <span class="math inline">\(\theta_j\)</span>’s, the posterior distribution is:</p>
<p><span class="math display">\[
(\theta_{bush}, \theta_{dukakis}, \theta_{neither}) | y \sim Dirichlet(295, 308, 39)
\]</span>
Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the pre-debate, that is, <span class="math inline">\(\alpha_1\)</span> is thus:</p>
<p><span class="math display">\[
\alpha_1 | y \sim Beta(295, 308)
\]</span>
Which we can visualize thus:</p>
<pre class="r"><code>alpha1 = rbeta(10000, 295, 308)
data.frame(alpha1) -&gt; simulations_alpha1

simulations_alpha1 %&gt;% 
  ggplot(aes(alpha1)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  geom_vline(aes(xintercept = 0.5), linetype = 2, color = &quot;red&quot;) +
  labs(title = TeX(&quot;Posterior distribution for $\\alpha_1$&quot;),
       subtitle = &quot; Proportion of voters who preferred Bush, out of those who had a preference 
       for either Bush or Dukakis at pre-debate&quot;,
       x = TeX(&quot;$\\alpha_1$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/alpha1-1.png" width="768" /></p>
<p>That is, our posterior distribution points that at the pre-debate, there was already a majority of people (among the already decided) who favored Dukakis. Indeed:</p>
<pre class="r"><code>predebate &lt;- pbeta(0.5, 295, 308)
glue::glue(&quot;There&#39;s a {round(predebate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in
           the pre-debate.&quot;)</code></pre>
<pre><code>## There&#39;s a 70% posterior probability that among decided voters Dukakis had a majority in
## the pre-debate.</code></pre>
</div>
<div id="post-debate" class="section level3">
<h3>Post-Debate</h3>
<p>Therefore, setting an uniform prior (<span class="math inline">\(\beta_j = 1 \ \forall j\)</span>) on the possible distribution of the <span class="math inline">\(\theta_j\)</span>’s, the posterior distribution is:</p>
<p><span class="math display">\[
(\theta_{bush}, \theta_{dukakis}, \theta_{neither}) | y \sim Dirichlet(289, 333, 39)
\]</span>
Which then amounts that the proportion that favor Bush, out of those who had a preference for either Bush or Dukakis in the post-debate, that is, <span class="math inline">\(\alpha_2\)</span> is thus:</p>
<p><span class="math display">\[
\alpha_1 | y \sim Beta(289, 333)
\]</span></p>
<p>Which we can visualize thus:</p>
<pre class="r"><code>alpha2 = rbeta(10000, 289, 333)
data.frame(alpha2) -&gt; simulations_alpha2

simulations_alpha2 %&gt;% 
  ggplot(aes(alpha2)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  geom_vline(aes(xintercept = 0.5), linetype = 2, color = &quot;red&quot;) +
  labs(title = TeX(&quot;Posterior distribution for $\\alpha_2$&quot;),
       subtitle = &quot; Proportion of voters who preferred Bush, out of those who had a preference 
       for either Bush or Dukakis at post-debate&quot;,
       x = TeX(&quot;$\\alpha_2$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/alpha2-1.png" width="768" /></p>
<p>After the debate, Dukakis won an even larger majority among the decided voters:</p>
<pre class="r"><code>postdebeate &lt;- pbeta(0.5, 289, 333)
glue::glue(&quot;There&#39;s a {round(postdebeate, 2)*100}% posterior probability that among decided voters Dukakis had a majority in
           the pre-debate.&quot;)</code></pre>
<pre><code>## There&#39;s a 96% posterior probability that among decided voters Dukakis had a majority in
## the pre-debate.</code></pre>
</div>
<div id="a-shift-toward-bush" class="section level3">
<h3>A shift toward Bush?</h3>
<p>We have the posterior probability for both <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\alpha_2\)</span>. Sampling form these posteriors, we can then arrive at a posterior distribution for <span class="math inline">\(\alpha_2 - \alpha_1\)</span></p>
<pre class="r"><code>difference &lt;- alpha2 - alpha1

data.frame(difference) %&gt;% 
  ggplot(aes(difference)) +
  geom_vline(aes(xintercept = 0), color = &quot;red&quot;, linetype = 2) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7)  +
  labs(title = TeX(&quot;Posterior distribution for $\\alpha_2 - \\alpha_1$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/difference-1.png" width="768" /></p>
<p>The posterior probability that there was a shift toward Bush is the probability that <span class="math inline">\(\alpha_2 - \alpha_1 &gt; 0\)</span></p>
<pre class="r"><code>shift &lt;- sum(difference &gt; 0) / length(difference)
glue::glue(&quot;The posterior probability that there was a shift toward Bush is thus {round(shift, 2)*100}%&quot;)</code></pre>
<pre><code>## The posterior probability that there was a shift toward Bush is thus 19%</code></pre>
</div>
</div>
<div id="exercise-3" class="section level2">
<h2>Exercise 3</h2>
<p>Estimation from two independent experiments: an experiment was performed on the effects of magnetic fields on the flow of calcium out of chicken brains. Two groups of chickens were involved: a control group of 32 chickens and an exposed group of 36 chickens. One measurement was taken on each chicken, and the purpose of the experiment was to measure the average flow <span class="math inline">\(\mu_{c}\)</span> in untreated (control) chickens and the average flow <span class="math inline">\(\mu_{t}\)</span> in treated chickens. The 32 measurements on the control group had a sample mean of 1.013 and a sample standard deviation of <span class="math inline">\(0.24 .\)</span> The 36 measurements on the treatment group had a sample mean of 1.173 and a sample standard deviation of 0.20</p>
<ol style="list-style-type: lower-alpha">
<li><p>Assuming the control measurements were taken at random from a normal distribution with mean <span class="math inline">\(\mu_{c}\)</span> and variance <span class="math inline">\(\sigma_{c}^{2},\)</span> what is the posterior distribution of <span class="math inline">\(\mu_{c} ?\)</span> Similarly, use the treatment group measurements to determine the marginal posterior distribution of <span class="math inline">\(\mu_{t} .\)</span> Assume a uniform prior distribution on <span class="math inline">\(\left(\mu_{c}, \mu_{t}, \log \sigma_{c}, \log \sigma_{t}\right)\)</span></p></li>
<li><p>What is the posterior distribution for the difference, <span class="math inline">\(\mu_{t}-\mu_{c} ?\)</span> To get this, you may sample from the independent <span class="math inline">\(t\)</span> distributions you obtained in part(a) above. Plot a histogram of your samples and give an approximate <span class="math inline">\(95 \%\)</span> posterior interval for <span class="math inline">\(\mu_{t}-\mu_{c}\)</span></p></li>
</ol>
</div>
<div id="solution-1" class="section level2">
<h2>Solution</h2>
<p>Let’s posit two normal probability models for both the control measurements and the treatment measurements, assuming exchangeability among these two groups.</p>
<div id="control-group" class="section level3">
<h3>Control group</h3>
<p>Therefore:</p>
<p><span class="math display">\[
y_c | \mu, \sigma^2 \sim N(\mu_c, \sigma_c^2) \\
p(\mu_c, \sigma_c | y) \propto p (y | \mu_c, \sigma_c) p(\mu_c, \sigma_c)
\]</span>
If we posit an uniform prior on <span class="math inline">\((\mu_c, log \sigma_c)\)</span></p>
<p><span class="math display">\[
p(\mu_c, \sigma_c^2) \propto (\sigma_c^2)^{-1}
\]</span></p>
<p>Then, the marginal posterior distribution for <span class="math inline">\(\mu_c\)</span> is a t-distribution:</p>
<p><span class="math display">\[
\dfrac{\mu_c  - \bar y_c}{s_c/\sqrt{n_c}} | y \sim t_{n_c-1}
\]</span></p>
<p>For the control group, we have <span class="math inline">\(n_c = 32\)</span>, <span class="math inline">\(\bar y_c = 1.013\)</span> and <span class="math inline">\(s_c = 0.24\)</span></p>
<pre class="r"><code>mu_c &lt;- rtnew(10000, df = 31, mean = 1.013, scale = 0.24/sqrt(32) )

data.frame(mu_control = mu_c) %&gt;% 
  ggplot(aes(mu_control)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  labs(title = TeX(&quot;Posterior distribution for $\\mu_c$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/muc-1.png" width="768" /></p>
</div>
<div id="treatment-group" class="section level3">
<h3>Treatment Group</h3>
<p>The same likelihood and prior are valid for the treatment measurements. Therefore, the marginal posterior for <span class="math inline">\(\mu_t\)</span>:</p>
<p><span class="math display">\[
\dfrac{\mu_t  - \bar y_t}{s_t/\sqrt{n_t}} | y \sim t_{n_t-1}
\]</span></p>
<p>For the treatment group, we have <span class="math inline">\(n_t = 36\)</span>, <span class="math inline">\(\mu_t = 1.173\)</span>, <span class="math inline">\(s_t = 0.2\)</span>:</p>
<pre class="r"><code>mu_t &lt;- rtnew(10000, df = 35, mean = 1.173, scale = 0.2/sqrt(36) )

data.frame(mu_treatment = mu_t) %&gt;% 
  ggplot(aes(mu_treatment)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  labs(title = TeX(&quot;Posterior distribution for $\\mu_t$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/mut-1.png" width="768" /></p>
</div>
<div id="posterior-difference-between-mu_c-and-mu_t" class="section level3">
<h3>Posterior difference between mu_c and mu_t</h3>
<p>To get the posterior distribution of the difference, we compare the samples from the marginal posterior of <span class="math inline">\(\mu_c, \mu_t\)</span>. Therefore, the 95% posterior credibility interval on the different is thus</p>
<pre class="r"><code>different_mu &lt;- mu_t - mu_c
interval &lt;- rethinking::PI(different_mu, prob = 0.95)
lower &lt;- interval[[1]]
upper &lt;- interval[[2]]
glue::glue(&quot;The 95% posterior credibility interval for the difference is {round(lower, 2)}, {round(upper, 2)}&quot;)</code></pre>
<pre><code>## The 95% posterior credibility interval for the difference is 0.05, 0.27</code></pre>
<p>And the full posterior of the difference is thus:</p>
<pre class="r"><code>data.frame(different_mu) %&gt;% 
  ggplot(aes(different_mu)) +
  geom_histogram(binwidth = 0.01, color = &quot;black&quot;, fill = &quot;dodgerblue4&quot;, alpha = 0.7) +
  labs(title = TeX(&quot;Posterior distribution for $\\mu_t - \\mu_c$&quot;))</code></pre>
<p><img src="/post/2020-06-25-bayesian-data-analysis-week-3-exercises_files/figure-html/differentmu-1.png" width="768" /></p>
</div>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

