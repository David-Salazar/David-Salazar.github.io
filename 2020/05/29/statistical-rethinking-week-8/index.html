<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Statistical Rethinking Week 8 - Dilettanting Data Science</title>
<meta property="og:title" content="Statistical Rethinking Week 8 - Dilettanting Data Science">



  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/turner.jpg"
         width="200"
         height="200"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://www.kaggle.com/davidsalazarv95">Kaggle</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir?lang=en">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">18 min read</span>
    

    <h1 class="article-title">Statistical Rethinking Week 8</h1>

    
    <span class="article-date">2020/05/29</span>
    
    

    <div class="article-content">
      


<div id="statistical-rethinking-week-8" class="section level1">
<h1>Statistical Rethinking Week 8</h1>
<p>This week was our first introduction to Multilevel models. Models where we explicitly model a family of parameters as coming from a common distribution: with each sample, we simultaneously learn each parameter and the parameters of the common distribution. <strong>This process of sharing information is called pooling. The end result is shrinkage: each parameter gets pulled towards the estimated mean of the common distribution.</strong> I tried my best to understand this process and result by simulating in <a href="https://david-salazar.github.io/2020/05/28/simulating-into-understanding-multilevel-models/">this post</a></p>
</div>
<div id="homework" class="section level1">
<h1>Homework</h1>
</div>
<div id="st-problem" class="section level1">
<h1>1st Problem</h1>
<p>Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size treatment variables to the varying intercepts model. Consider models with either predictor alone, both predictors, as well as a model including their interaction. What do you infer about the causal influence of these predictor variables? Also focus on the inferred variation across tanks (the <span class="math inline">\(\sigma\)</span> across tanks). Explain why it changes
as it does across models with different predictors included.</p>
<pre class="r"><code>data(&quot;reedfrogs&quot;)
glimpse(reedfrogs)</code></pre>
<pre><code>## Rows: 48
## Columns: 5
## $ density  &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...
## $ pred     &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pr...
## $ size     &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, ...
## $ surv     &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23,...
## $ propsurv &lt;dbl&gt; 0.90, 1.00, 0.70, 1.00, 0.90, 0.90, 1.00, 0.90, 0.40, 0.90...</code></pre>
<div id="model-with-only-predation" class="section level2">
<h2>Model with only predation</h2>
<p>Let’s check how the predation variable is encoded:</p>
<pre class="r"><code>reedfrogs %&gt;% 
  count(pred)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   pred      n
##   &lt;fct&gt; &lt;int&gt;
## 1 no       24
## 2 pred     24</code></pre>
<pre class="r"><code>reedfrogs %&gt;% 
  mutate(predation_int = factor(pred),
         predation_int = as.integer(predation_int) - 1) -&gt; reedfrogs
reedfrogs %&gt;% 
  count(predation_int, pred)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   predation_int pred      n
##           &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;
## 1             0 no       24
## 2             1 pred     24</code></pre>
<p>Now, let’s propose the model with varying intercept for tanks and taking into account whether there were predators or not.</p>
<pre class="r"><code>reedfrogs$tank &lt;- 1:nrow(reedfrogs)

data_only_predation &lt;- list(
  S = reedfrogs$surv,
  N = reedfrogs$density,
  tank = reedfrogs$tank,
  predators = reedfrogs$predation_int
)

model_only_predators &lt;- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) &lt;- a_bar + a[tank]*sigma + predators * pred, # non-center prior for a[tank]
    # priors
    a[tank] ~ dnorm(0, 1),
    pred ~ dnorm(0, 0.5),
    # hyper priors
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dhalfnorm(0, 1)
  ),
  chains = 4, cores = 4, log_lik = TRUE, data = data_only_predation,
  iter = 2000
)</code></pre>
<p>Let’s check our chains:</p>
<pre class="r"><code>traceplot_ulam(model_only_predators)</code></pre>
<pre><code>## [1] 2000
## [1] 1
## [1] 2000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-6-3.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-6-4.png" width="672" /></p>
<p>Our chains look healthy. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well across the parameter space.</li>
<li>They are stationary.</li>
<li>Different chains converge to explore the same parameter space.</li>
</ol>
<p>Let’s check the posterior and the Rhat values:</p>
<pre class="r"><code>precis(model_only_predators, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9991  
##  1st Qu.:0.9994  
##  Median :0.9996  
##  Mean   :0.9998  
##  3rd Qu.:0.9999  
##  Max.   :1.0046</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look good enough, all are close to 0. There appear to not be signs of transient like behavior.</p>
<pre class="r"><code>precis(model_only_predators)</code></pre>
<pre><code>## 48 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##            mean        sd       5.5%     94.5%    n_eff    Rhat4
## pred  -1.827650 0.3078508 -2.2990549 -1.328904 1828.193 1.002348
## a_bar  2.197806 0.2327457  1.8235608  2.563913 2492.716 1.000783
## sigma  0.910383 0.1635299  0.6738623  1.194726 1402.057 1.004567</code></pre>
<p>As expected, tanks with predators have, on average, lower log odds of probability of surviving.</p>
</div>
<div id="model-with-only-size" class="section level2">
<h2>Model with only size</h2>
<p>Let’s prepare size to the model:</p>
<pre class="r"><code>reedfrogs %&gt;% 
  count(size)</code></pre>
<pre><code>## # A tibble: 2 x 2
##   size      n
##   &lt;fct&gt; &lt;int&gt;
## 1 big      24
## 2 small    24</code></pre>
<pre class="r"><code>reedfrogs %&gt;% 
  mutate(size_int = as.integer( factor(size)) - 1,
         size_int = as.integer(size_int)) -&gt; reedfrogs
reedfrogs %&gt;% 
  count(size_int, size)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   size_int size      n
##      &lt;int&gt; &lt;fct&gt; &lt;int&gt;
## 1        0 big      24
## 2        1 small    24</code></pre>
<p>Now, let’s add the size to our model with varying intercepts:</p>
<pre class="r"><code>data_only_size &lt;- list(
  S = reedfrogs$surv,
  N = reedfrogs$density,
  tank = reedfrogs$tank,
  size_i = reedfrogs$size_int
)

model_only_size &lt;- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) &lt;- a_bar + a[tank]*sigma + s * size_i, # non-center prior for a[tank]
    # priors
    a[tank] ~ dnorm(0, 1),
    s ~ dnorm(0, 0.5),
    # hyper priors
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dhalfnorm(0, 1)
  ),
  chains = 4, cores = 4, log_lik = TRUE, data = data_only_size,
  iter = 2000
)</code></pre>
<p>Let’s check our chains:</p>
<pre class="r"><code>traceplot_ulam(model_only_size)</code></pre>
<pre><code>## [1] 2000
## [1] 1
## [1] 2000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-12-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-12-3.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-12-4.png" width="672" /></p>
<p>Our chains look healthy. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well across the parameter space.</li>
<li>They are stationary.</li>
<li>Different chains converge to explore the same parameter space.</li>
</ol>
<p>Let’s check our <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_only_size, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9992  
##  1st Qu.:1.0003  
##  Median :1.0011  
##  Mean   :1.0012  
##  3rd Qu.:1.0019  
##  Max.   :1.0050</code></pre>
<p>Let’s check our precis ouptut:</p>
<pre class="r"><code>precis(model_only_size)</code></pre>
<pre><code>## 48 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##            mean        sd       5.5%     94.5%     n_eff    Rhat4
## s     0.2603811 0.3599186 -0.3142855 0.8374926 1056.8822 1.000290
## a_bar 1.2176508 0.3042200  0.7394854 1.7094029  810.2809 1.005003
## sigma 1.5956966 0.2021985  1.3016637 1.9412529 1050.1196 1.003385</code></pre>
<p>It seems that the size is not that relevant in the log-odds scale. Its 89% PI covers zero and a prety wide interval.</p>
</div>
<div id="model-with-size-and-predators" class="section level2">
<h2>Model with size and predators</h2>
<p>Let’s include both variables:</p>
<pre class="r"><code>data_both &lt;- list(
  S = reedfrogs$surv,
  N = reedfrogs$density,
  tank = reedfrogs$tank,
  size_i = reedfrogs$size_int,
  predators = reedfrogs$predation_int
)

model_both &lt;- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) &lt;- a_bar + a[tank]*sigma + s * size_i + predators * pred, # non-center prior for a[tank]
    # priors
    a[tank] ~ dnorm(0, 1),
    s ~ dnorm(0, 0.5),
    pred ~ dnorm(0, 0.5),
    # hyper priors
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dhalfnorm(0, 1)
  ),
  chains = 4, cores = 4, log_lik = TRUE, data = data_both,
  iter = 2000
)</code></pre>
<p>Now we can check our chains:</p>
<pre class="r"><code>traceplot_ulam(model_both)</code></pre>
<pre><code>## [1] 2000
## [1] 1
## [1] 2000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-16-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-16-3.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-16-4.png" width="672" /></p>
<p>Our chains look healthy. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well across the parameter space.</li>
<li>They are stationary.</li>
<li>Different chains converge to explore the same parameter space.</li>
</ol>
<p>Let’s check our <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_both, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9991  
##  1st Qu.:0.9994  
##  Median :0.9996  
##  Mean   :0.9997  
##  3rd Qu.:0.9999  
##  Max.   :1.0028</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look OK. Let’s check the <code>precis</code> output:</p>
<pre class="r"><code>precis(model_both)</code></pre>
<pre><code>## 48 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##             mean        sd        5.5%      94.5%    n_eff     Rhat4
## s      0.3948816 0.2575251 -0.01385201  0.8087681 2424.152 0.9994517
## pred  -1.8594984 0.2989954 -2.31879456 -1.3647253 2086.701 0.9993767
## a_bar  2.0052279 0.2573061  1.58597875  2.4162837 2407.533 0.9999848
## sigma  0.8680613 0.1629388  0.62412904  1.1415600 1435.159 1.0028449</code></pre>
<p>Predators’ effect is still large and negative on the log-odds scale. Also, size’s effect has shifted and, once we have statistically adjusted by the presence of predators, now has most of its posterior mass to the right of zero. Presumably, this arises because size and the presence of predators are related; unless we adjust by the presence of predators, the coefficient for size will pick up some of the predators’ effect.</p>
</div>
<div id="model-with-an-interaction" class="section level2">
<h2>Model with an interaction</h2>
<pre class="r"><code>model_interaction &lt;- ulam(
  alist(
    S ~ dbinom(N, p),
    logit(p) &lt;- a_bar + a[tank]*sigma + s * size_i + predators * pred + # non-center prior for a[tank]
      predators * size_i * interaction,
    # priors
    a[tank] ~ dnorm(0, 1),
    s ~ dnorm(0, 0.5),
    pred ~ dnorm(0, 0.5),
    interaction ~ dnorm(0, 0.25),
    # hyper priors
    a_bar ~ dnorm(0, 1.5),
    sigma ~ dhalfnorm(0, 1)
  ),
  chains = 4, cores = 4, log_lik = TRUE, data = data_both,
  iter = 2000
)</code></pre>
<p>Let’s check on our chains:</p>
<pre class="r"><code>traceplot_ulam(model_interaction)</code></pre>
<pre><code>## [1] 2000
## [1] 1
## [1] 2000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-20-3.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-20-4.png" width="672" /></p>
<p>The chains look healthy enough. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well</li>
<li>They are stationary</li>
<li>Different chains converge to explore the same parameter spaces.</li>
</ol>
<p>Let’s check on the <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_interaction, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9993  
##  1st Qu.:0.9996  
##  Median :0.9997  
##  Mean   :0.9998  
##  3rd Qu.:1.0000  
##  Max.   :1.0013</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look OK. Let’s check the <code>precis</code> output:</p>
<pre class="r"><code>precis(model_interaction)</code></pre>
<pre><code>## 48 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##                    mean        sd        5.5%      94.5%     n_eff    Rhat4
## s            0.38448534 0.2889756 -0.08378407  0.8389638 2604.1411 1.000701
## pred        -1.85617885 0.3103303 -2.33634042 -1.3473898 1889.9968 1.000411
## interaction -0.01647299 0.2281880 -0.38381028  0.3484448 4520.8782 1.000197
## a_bar        2.01372866 0.2567239  1.60957072  2.4191303 2414.4320 1.000372
## sigma        0.87815075 0.1706440  0.63020451  1.1657547  942.2315 1.001288</code></pre>
<p>Prediction is still large and negative on the log-odds scale. Size also does not appear to change much with the interaction. The interaction has a large standard error and most of its mass lies largely symmetric around zero.</p>
<p>Now it’s the turn to check how the estimated variation of tanks has changed with the different models:</p>
<pre class="r"><code>only_predation &lt;- extract.samples(model_only_predators)$sigma
only_size &lt;- extract.samples(model_only_size)$sigma
both_variables &lt;- extract.samples(model_both)$sigma
variables_interacted &lt;- extract.samples(model_interaction)$sigma

data.frame(sim =1:4000, only_predation, only_size, both_variables, variables_interacted) %&gt;% 
  pivot_longer(-sim, names_to = &quot;model&quot;, values_to = &quot;sigma&quot;) %&gt;% 
  mutate(predators_present = if_else(model == &quot;only_size&quot;, FALSE, TRUE)) %&gt;% 
  ggplot(aes(sigma, model, fill = predators_present)) +
  geom_density_ridges() +
  scale_fill_viridis_d() +
  theme(legend.position = &quot;bottom&quot;) +
  labs(title = &quot;Predicted variation across tanks&quot;,
       subtitle = &quot;Presence of predators explains some of the variation across tanks.&quot;)</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-23-1.png" width="768" /></p>
<pre class="r"><code>ggsave(&quot;predators.png&quot;)</code></pre>
<p>All the models that include predators have almost identical estimates for the variation across tanks. That is, the presence of predators explain some of the variation across tanks.</p>
<p>Finally, let’s compare the models according to information criteria:</p>
<pre class="r"><code>compare(model_only_predators, model_only_size, model_both, model_interaction)</code></pre>
<pre><code>##                          WAIC       SE     dWAIC       dSE    pWAIC    weight
## model_interaction    198.8377 7.702396 0.0000000        NA 19.13124 0.3610071
## model_only_predators 199.4225 8.246242 0.5847874 1.6632677 19.62986 0.2694826
## model_both           200.0687 7.965842 1.2310446 0.6508432 19.58692 0.1950733
## model_only_size      200.2924 7.203980 1.4546679 4.0668889 20.96438 0.1744370</code></pre>
<p>According to information criteria, all of the models make essentially have the same expected predictive performance out-of-sample.</p>
</div>
</div>
<div id="nd-problem" class="section level1">
<h1>2nd problem</h1>
<ol start="2" style="list-style-type: decimal">
<li><p>In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in data(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:</p></li>
<li><p>district</p></li>
<li><p>use.contraception</p></li>
</ol>
<pre class="r"><code>data(&quot;bangladesh&quot;)
glimpse(bangladesh)</code></pre>
<pre><code>## Rows: 1,934
## Columns: 6
## $ woman             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15...
## $ district          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ use.contraception &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0...
## $ living.children   &lt;int&gt; 4, 1, 3, 4, 1, 1, 4, 4, 2, 4, 1, 1, 2, 4, 4, 4, 1...
## $ age.centered      &lt;dbl&gt; 18.4400, -5.5599, 1.4400, 8.4400, -13.5590, -11.5...
## $ urban             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</code></pre>
<p>Let’s fix the district:</p>
<pre class="r"><code>bangladesh %&gt;% 
  mutate(district_id = as.integer( as.factor(district) ) ) -&gt; bangladesh</code></pre>
<blockquote>
<p>Now, focus on predicting use.contraception, clustered by district_id. Fit both (1) a traditional fixed-effects model that uses an index variable for district and (2) a multilevel model with varying intercepts for district.</p>
</blockquote>
<div id="traditional-fixed-effects" class="section level2">
<h2>Traditional fixed-effects</h2>
<pre class="r"><code>data_bangladesh &lt;- list(
  contraception = bangladesh$use.contraception,
  d_id = bangladesh$district_id
)

model_fixed_effects &lt;- ulam(
  alist(
    contraception ~ dbinom(1, p),
    logit(p) &lt;- d[d_id],
    d[d_id] ~ dnorm(0, 1.5)
  ),
  chains = 4, cores = 4, log_lik = TRUE,
  data = data_bangladesh
)</code></pre>
<p>Let’s check our chains’ health:</p>
<pre class="r"><code>traceplot_ulam(model_fixed_effects)</code></pre>
<pre><code>## [1] 1000
## [1] 1
## [1] 1000</code></pre>
<pre><code>## Waiting to draw page 2 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 4</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-28-3.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-28-4.png" width="672" /></p>
<p>The chains look healthy enough. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well across the parameter space.</li>
<li>They are stationary</li>
<li>Different chains converge to explore the same areas.</li>
</ol>
<p>Let’s check the <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_fixed_effects, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9983  
##  1st Qu.:0.9985  
##  Median :0.9986  
##  Mean   :0.9988  
##  3rd Qu.:0.9990  
##  Max.   :1.0001</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look OK. Let’s fit the multilevel model:</p>
<pre class="r"><code>model_multilevel &lt;- ulam(
  alist(
    contraception ~ dbinom(1, p),
    logit(p) &lt;- alpha + d[d_id]*sigma, # non-centered version
    d[d_id] ~ dnorm(0, 1),
    alpha ~ dnorm(0, 1),
    sigma ~ dhalfnorm(0, 1)
  ),
  chains = 4, cores = 4, log_lik = TRUE,
  data = data_bangladesh
)</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<p>Let’s check on our chains:</p>
<pre class="r"><code>traceplot_ulam(model_multilevel)</code></pre>
<pre><code>## [1] 1000
## [1] 1
## [1] 1000</code></pre>
<pre><code>## Waiting to draw page 2 of 5</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre><code>## Waiting to draw page 3 of 5</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-31-2.png" width="672" /></p>
<pre><code>## Waiting to draw page 4 of 5</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-31-3.png" width="672" /></p>
<pre><code>## Waiting to draw page 5 of 5</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-31-4.png" width="672" /><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-31-5.png" width="672" /></p>
<p>Our chains look healthy enough. They:</p>
<ol style="list-style-type: decimal">
<li>They mix well.</li>
<li>They are stationary</li>
<li>Different chains converge.</li>
</ol>
<p>Let’s check on the <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_multilevel, depth = 2) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9981  
##  1st Qu.:0.9989  
##  Median :0.9992  
##  Mean   :0.9995  
##  3rd Qu.:0.9996  
##  Max.   :1.0042</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look OK.</p>
<p>Now let’s inspect the values for the distribution of varying intercepts for each district:</p>
<pre class="r"><code>precis(model_multilevel)</code></pre>
<pre><code>## 60 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##             mean         sd       5.5%      94.5%    n_eff    Rhat4
## alpha -0.5407752 0.08881390 -0.6750478 -0.4009461 916.9953 1.001599
## sigma  0.5227560 0.08146068  0.3994696  0.6587127 707.6119 1.004206</code></pre>
<p>The overall use of contraceptives seems unlikely across districts, thus the negative alpha.</p>
<blockquote>
<p>Plot the predicted proportions of women in each district using contraception, for both the fixed-effects
model and the varying-effects model.</p>
</blockquote>
<p>Notice that each women, within a same district, has the same prediction.</p>
<pre class="r"><code># fixed effects
link_fixed_effects &lt;- link(model_fixed_effects) # probs
fixed_effects_prop_per_district &lt;- unique( apply((link_fixed_effects), 2, mean)) #average over posterior
# multilevel
link_multilevel &lt;- link(model_multilevel)
multilevel_prop_per_district &lt;- unique( apply((link_multilevel), 2, mean)) # average over posterior</code></pre>
<p>Let’s average over the posterior the alpha of the distribution of varying intercepts per district</p>
<pre class="r"><code>grand_average &lt;- mean( inv_logit(extract.samples(model_multilevel)$alpha) )</code></pre>
<p>Let’s plot the requested graph:</p>
<pre class="r"><code>data.frame(district_id = 1:60, fixed_effects = fixed_effects_prop_per_district, multilevel = multilevel_prop_per_district) %&gt;% 
  pivot_longer(-district_id, names_to = &quot;method&quot;, values_to = &quot;predicted_proportion&quot;) %&gt;% 
  left_join(bangladesh) %&gt;% 
  ggplot(aes(district_id, predicted_proportion, color = method)) +
  geom_point() +
  geom_hline(aes(yintercept = grand_average), linetype = 2, color = &quot;red&quot;) +
  scale_color_viridis_d() +
  theme(legend.position = &quot;bottom&quot;) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = &quot;Estimated proportion of Contraceptive usage per district&quot;,
       subtitle = &quot;Shrinkage in action&quot;,
       y = &quot;Estimated proportion of contraceptive usage &quot;)</code></pre>
<pre><code>## Joining, by = &quot;district_id&quot;</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;district.png&quot;)</code></pre>
<pre><code>## Saving 7 x 5.8 in image</code></pre>
<p>We are seeing the consequences of pooling information from the common distribution of districts: each district’s prediction is overall much closer to the estimated common distribution’s mean than the predictions from the fixed effects model. Therefore, each yellow point is closer to the red line than its corresponding purple point. There are a couple of districts where the difference in predictions between the two models is huge. This are the places where most outside information was used. From what we’ve known about Pooling, these must be the places that were most likely to overfit and had fewer data points. Let’s confirm this intuition:</p>
<pre class="r"><code>bangladesh %&gt;% 
  count(district_id) %&gt;% 
  left_join(data.frame(district_id = 1:60, fixed_effects = fixed_effects_prop_per_district, multilevel = multilevel_prop_per_district)) %&gt;% 
  mutate(absolute_difference = abs(fixed_effects - multilevel)) %&gt;% 
  ggplot(aes(n, absolute_difference)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(title = &quot;Difference in predictions per district vs Sample size in district&quot;,
       subtitle = &quot;Largest differences were present in districts with very small sample sizes.&quot;,
       y = &quot;Absolute difference in prediction&quot;)</code></pre>
<pre><code>## Joining, by = &quot;district_id&quot;</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&quot;errors_ban.png&quot;)</code></pre>
<pre><code>## Saving 7 x 5 in image
## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p>Finally, let’s compare their expected out of sample performance:</p>
<pre class="r"><code>compare(model_fixed_effects, model_multilevel)</code></pre>
<pre><code>##                         WAIC       SE    dWAIC      dSE    pWAIC      weight
## model_multilevel    2513.907 25.07935 0.000000       NA 35.63472 0.992797468
## model_fixed_effects 2523.759 28.97305 9.852188 7.641156 53.85069 0.007202532</code></pre>
</div>
</div>
<div id="rd-problem" class="section level1">
<h1>3rd Problem</h1>
<p>Return to the Trolley data, data(Trolley), from Chapter 12. Define and fit a varying intercepts model for these data. By this I mean to add an intercept parameter for the individual to the linear model. Cluster the varying intercepts on individual
participants, as indicated by the unique values in the id variable. Include action, intention, and contact as before.</p>
<pre class="r"><code>data(&quot;Trolley&quot;)
trolley_data &lt;- Trolley 
glimpse(trolley_data)</code></pre>
<pre><code>## Rows: 9,930
## Columns: 12
## $ case      &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, f...
## $ response  &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, ...
## $ order     &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 1...
## $ id        &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 9...
## $ age       &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1...
## $ male      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ edu       &lt;fct&gt; Middle School, Middle School, Middle School, Middle Schoo...
## $ action    &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, ...
## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...
## $ contact   &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ story     &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, sp...
## $ action2   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, ...</code></pre>
<p>Therefore, we will run an ordered logistic model where Action, Intention and Contact interact between each other.</p>
<pre class="r"><code># data 
data_list_model &lt;- list(
  R = trolley_data$response,
  action = trolley_data$action,
  intention = trolley_data$intention,
  contact = trolley_data$contact
)
# model

model_no_varying &lt;- ulam(
  alist(
    R ~ ordered_logistic(phi, kappa),
    phi &lt;-  bA*action + BI*intention + bC*contact,
    BI &lt;- bI + bIC*contact + bIA*action,
    kappa ~ normal(0, 0.5),
    c(bA, bC, bI, bIA, bIC) ~ normal(0, 0.5)
),
  data = data_list_model,
chains = 4, cores = 4, log_lik = TRUE
)</code></pre>
<p>Let’s check our chains’ health:</p>
<pre class="r"><code>traceplot_ulam(model_no_varying)</code></pre>
<pre><code>## [1] 1000
## [1] 1
## [1] 1000</code></pre>
<p><img src="/post/2020-05-29-statistical-rethinking-week-8_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>Our chains, in sampling, look healthy enough.</p>
<p>Let’s check our <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_no_varying) %&gt;% 
  data.frame() %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>## 6 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9992  
##  1st Qu.:0.9994  
##  Median :0.9995  
##  Mean   :0.9994  
##  3rd Qu.:0.9995  
##  Max.   :0.9996</code></pre>
<p>The <span class="math inline">\(\hat{R}\)</span> values look good enough.</p>
<pre class="r"><code>precis(model_no_varying)</code></pre>
<pre><code>## 6 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##           mean         sd       5.5%      94.5%     n_eff     Rhat4
## bIC -1.2528890 0.09883680 -1.4084661 -1.0975365 1058.2420 0.9993676
## bIA -0.4575741 0.08032935 -0.5873706 -0.3327920 1131.7405 0.9994861
## bI  -0.2645041 0.05750868 -0.3525047 -0.1682167 1015.7471 0.9992129
## bC  -0.3158187 0.06918817 -0.4294523 -0.2051602  995.2410 0.9996491
## bA  -0.4438125 0.05520635 -0.5330668 -0.3579424  983.3086 0.9995128</code></pre>
<p>Now, let’s fit the model with varying intercepts by individual:</p>
<pre class="r"><code># data 
data_list_model &lt;- list(
  R = trolley_data$response,
  action = trolley_data$action,
  intention = trolley_data$intention,
  contact = trolley_data$contact,
  individual = coerce_index(trolley_data$id)
)
# model

model_varying &lt;- ulam(
  alist(
    R ~ ordered_logistic(phi, kappa),
    phi &lt;- a[individual] +  bA*action + BI*intention + bC*contact,
    BI &lt;- bI + bIC*contact + bIA*action,
    # priors
    a[individual] ~ normal(0, sigma),
    kappa ~ normal(0, 1.5),
    c(bA, bC, bI, bIA, bIC) ~ normal(0, 0.5),
    # hyper-prior
    sigma ~ dhalfnorm(0, 1.5)
),
  data = data_list_model,
chains = 4, cores = 4, log_lik = TRUE,
iter = 1500
)</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>Let’s check the <span class="math inline">\(\hat{R}\)</span> values:</p>
<pre class="r"><code>precis(model_varying, depth = 2) %&gt;% 
  select(Rhat4) %&gt;% 
  summary()</code></pre>
<pre><code>##      Rhat4       
##  Min.   :0.9992  
##  1st Qu.:1.0011  
##  Median :1.0021  
##  Mean   :1.0026  
##  3rd Qu.:1.0030  
##  Max.   :1.0286</code></pre>
<p>They look OK.</p>
<blockquote>
<p>Compare the varying intercepts model and a model that ignores individuals, using both WAIC/LOO and posterior predictions.</p>
</blockquote>
<pre class="r"><code>precis(model_varying)</code></pre>
<pre><code>## 337 vector or matrix parameters hidden. Use depth=2 to show them.</code></pre>
<pre><code>##             mean         sd       5.5%      94.5%    n_eff    Rhat4
## bIC   -1.6680594 0.09988534 -1.8285236 -1.5130859 1508.711 1.002179
## bIA   -0.5589894 0.07917900 -0.6881672 -0.4335060 1368.869 1.000711
## bI    -0.3845842 0.05845552 -0.4791339 -0.2901760 1324.981 1.003448
## bC    -0.4522620 0.06886962 -0.5618426 -0.3433608 1524.535 1.001790
## bA    -0.6467616 0.05406882 -0.7355495 -0.5623498 1378.175 1.002583
## sigma  1.9156609 0.08116139  1.7910125  2.0506740 2474.462 1.001052</code></pre>
<p>We estimate the sigma to be very large, indicating lots of variations in the responses among the individuals. Once we control for this average response per individual, we can estimate all the other parameters much more easily.</p>
<pre class="r"><code>compare(model_no_varying, model_varying)</code></pre>
<pre><code>##                      WAIC        SE    dWAIC      dSE     pWAIC weight
## model_varying    31055.86 179.29419    0.000       NA 355.26164      1
## model_no_varying 36930.29  79.88502 5874.431 173.2331  11.09972      0</code></pre>
<p>It seems that the expected performance out of sample, for the model with varying intercepts, is a lot better than the expected for the model with no varying intercept.</p>
</div>

    </div>
  

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  
  </article>

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

