{
  "hash": "41f75aa3566a8df157066ff1a4fff7d0",
  "result": {
    "markdown": "---\ntitle: 'Statistical Rethinking: Week 9'\nauthor: ''\ndate: '2020-06-03'\nslug: statistical-rethinking-week-9\ncategories: []\ntags: []\naliases: \n  - ../../2020/06/03/statistical-rethinking-week-9/\n---\n\n\n\n\n\nWeek 9 was all about fitting models with multivariate distributions in them. For example, a multivariate likelihood helps us use an instrumental variable to estimate the true causal effect of a predictor. But also as an adaptive prior for some of the predictors. In both cases, we found out that the benefit comes from modelling the resulting var-cov matrix. In the [instrumental variable](2020-06-03-bayesian-instrumental-variable-regression.html) case, the resulting joint distribution for the residuals was the key to capture the statistical information of the confounding variable. In the adaptive prior case, it helps understand the relationship between different parameter types. \n\n# Homework\n\n# 1st question\n\nRevisit the Bangladesh fertility data,`data(bangladesh)`. Fit a model with both varying intercepts by district_id and varying slopes of urban (as a 0/1 indicator variable) by district_id. You are still predicting use.contraception. Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, in each district, with urban women on one axis and rural on the other, might also help.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"bangladesh\")\n\n# Fix the district id\nbangladesh %>% \n  mutate(district_id = as.integer( as.factor(district) ) ) -> bangladesh\nglimpse(bangladesh)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,934\nColumns: 7\n$ woman             <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ district          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ use.contraception <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ living.children   <int> 4, 1, 3, 4, 1, 1, 4, 4, 2, 4, 1, 1, 2, 4, 4, 4, 1, 4…\n$ age.centered      <dbl> 18.4400, -5.5599, 1.4400, 8.4400, -13.5590, -11.5600…\n$ urban             <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ district_id       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n:::\n:::\n\n\nLet's fit the varying effects models for each district to have its average contraception use its own the differential between urban and rural areas. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban\n)\n\n\nmodel_varying <- ulam(\n  alist(\n    contraception ~ binomial(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban,\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  data = data_varying,\n  chains = 4, cores = 4,\n  iter = 2000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 25.4 seconds.\nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 29.4 seconds.\nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 30.7 seconds.\nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 32.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 29.5 seconds.\nTotal execution time: 33.0 seconds.\n```\n:::\n:::\n\n\nLet's check our chains' health:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot_ulam(model_varying)\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-4-9.png){width=672}\n:::\n:::\n\nThe chains look healthy enough. They are:\n\n1. They are stationary\n1. They mix well across the parameter space.\n1. Different chains converge to explore the same parameter space.\n\nLet's check the $\\hat{R}$ values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- precis(model_varying, depth = 3)\nresults %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9995  \n Median :0.9999  \n Mean   :1.0001  \n 3rd Qu.:1.0002  \n Max.   :1.0051  \n NA's   :2       \n```\n:::\n:::\n\n\nThe $\\hat{R}$ look OK, indicating that the Markov chains are in close agreement with each other. Let's check the parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_varying, depth = 2, pars = c(\"sigma\", \"a\", \"b\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               mean         sd       5.5%      94.5%     n_eff    Rhat4\nsigma[1]  0.5732478 0.09736082  0.4268930  0.7362033 1117.7376 1.000141\nsigma[2]  0.7767925 0.20201103  0.4672885  1.1105892  503.5447 1.005126\na        -0.7055373 0.10293818 -0.8725771 -0.5428108 2976.0407 1.000136\nb         0.6969825 0.16603119  0.4444045  0.9658206 2375.1974 1.000239\n```\n:::\n:::\n\n\nThe contraceptive use is not that likely, thus the negative (in log-odds scale) average value in the adaptive prior for $a$. The positive value for $b$, on the other hand, indicates that the average distribution of slopes is positive. That is, women in urban areas are, on average, more likely to use contraception. Finally, the variances. Both indicate quite a bit of variation in the multivariate population for intercepts and slopes.   \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_varying, pars = \"Rho\", depth = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               mean        sd      5.5%      94.5%    n_eff    Rhat4\nRho[1,1]  1.0000000 0.0000000  1.000000  1.0000000      NaN      NaN\nRho[1,2] -0.6585362 0.1612475 -0.868516 -0.3679604 710.9425 1.003216\nRho[2,1] -0.6585362 0.1612475 -0.868516 -0.3679604 710.9425 1.003216\nRho[2,2]  1.0000000 0.0000000  1.000000  1.0000000      NaN      NaN\n```\n:::\n:::\n\n\nThere's a negative correlation between the parameter types: i.e., for districts with higher contraceptive usage overall, the correlation informs us that we should predict a lower than average differential in the use of contraceptives between rural and urban areas. \n\nWe can follow Richard's advice and plot both types of parameters for each district. We can even overlay the ellipses that determine the levels of the multivariate adaptive prior: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples <- extract.samples(model_varying)\n\nMu_est <- c(mean(samples$a), mean(samples$b))\nrho_est <- mean(samples$Rho[,1,2])\nsa_est <- mean(samples$sigma[,1])\nsb_est <- mean(samples$sigma[, 2])\ncov_ab <- sa_est*sb_est*rho_est\nSigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol = 2)\n\ncontour_level <- function(level) {\n  ellipse::ellipse(Sigma_est, centre = Mu_est, level = level) %>% \n    data.frame() %>% \n    mutate(level = level)\n} \n\npurrr::map(c(0.1, 0.3, 0.5, 0.8, 0.99), contour_level) %>% \n  bind_rows() -> data_elipses\ndata_elipses %>% \n  ggplot(aes(x, y)) +\n  geom_path(aes(group = level), linetype = 2) +\n  geom_point(data = data.frame(x = Mu_est[1]), y = Mu_est[2], color = \"red\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nFinally, we can plot the points:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples$alpha %>% \n  as_tibble() %>% \n  pivot_longer(everything(), names_to = \"district_id_\", names_prefix = \"V\", values_to = \"alpha\") %>% \n  bind_cols(samples$beta %>% \n  as_tibble() %>% \n  pivot_longer(everything(), names_to = \"district_id\", names_prefix = \"V\", values_to = \"beta\")) %>% \n  group_by(district_id) %>% \n  median_qi(alpha, beta) %>% \n  select(district_id, alpha, beta) %>% \n  ggplot(aes(alpha, beta)) +\n  geom_point(alpha = 0.6) +\n  geom_path(data = data_elipses,\n            inherit.aes = F,\n            mapping = aes(x, y, group = level), linetype = 2, color = \"dodgerblue4\") +\n  geom_point(data = data.frame(x = Mu_est[1]), y = Mu_est[2], color = \"red\",\n             inherit.aes = FALSE,\n             mapping = aes(x, y)) +\n  labs(title = \"Negative correlation between intercepts and slopes per district\",\n       subtitle = \"Districts with higher overall use have lower differentials between urban and rural\",\n       x = expression(alpha),\n       y = expression(beta))\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n# 2nd question\n\nNow consider the predictor variables age.centered and living.children, also contained in data(bangladesh). Suppose that age influences contraceptive use (changing attitudes) and number of children (older people have had more time to have kids). Number of children may also directly influence contraceptive use. Draw a DAG that reflects these hypothetical relationships. Then build models needed to evaluate the DAG. You will need at least two models. Retain district and urban, as in Problem 1. What do you conclude about the causal influence of age and children?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndag <- dagitty::dagitty(\" dag {\n                        Age -> N_children\n                        Age -> contraception\n                        N_children -> contraception\n                        }\")\ndrawdag(dag)\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nConditional on this DAG, the total causal effect of Age on contraception is mediated (pipe) with Number of Children. Thus, to get the total effect we must not control by number of children.\n\nLet's fit this model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban,\n  age = bangladesh$age.centered, \n  kids = bangladesh$living.children\n)\nmodel_only_age <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban + gamma*age,\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 50.3 seconds.\nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 52.1 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 52.6 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 54.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 52.4 seconds.\nTotal execution time: 54.7 seconds.\n```\n:::\n:::\n\n\nLet's check our chains' health:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot_ulam(model_only_age)\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-12-9.png){width=672}\n:::\n:::\n\n\nThe chains look healthy enough. They are:\n\n1. They are stationary\n1. They mix well across the parameter space.\n1. Different chains converge to explore the same parameter space.\n\nLet's check the $\\hat{R}$ values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_only_age, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Rhat4       \n Min.   :0.9991  \n 1st Qu.:0.9996  \n Median :1.0000  \n Mean   :1.0002  \n 3rd Qu.:1.0005  \n Max.   :1.0069  \n NA's   :2       \n```\n:::\n:::\n\n\nThe $\\hat{R}$ look OK, indicating that the Markov chains are in close agreement with each other. Let's check the parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_only_age, depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 mean          sd          5.5%       94.5%     n_eff     Rhat4\na        -0.711537292 0.102268885 -0.8787672400 -0.55198898 2905.2135 1.0005300\nb         0.695464373 0.173304390  0.4243350000  0.97019381 2109.0982 1.0006393\ngamma     0.009474581 0.005552167  0.0005521033  0.01849995 8484.1344 0.9992501\nsigma[1]  0.584962697 0.100125045  0.4371533400  0.75428655 1260.7598 1.0016182\nsigma[2]  0.802377308 0.196794320  0.5076205600  1.13047220  586.1314 1.0069074\nRho[1,1]  1.000000000 0.000000000  1.0000000000  1.00000000       NaN       NaN\nRho[1,2] -0.650102143 0.167089307 -0.8636559950 -0.33474679  864.0654 1.0037881\nRho[2,1] -0.650102143 0.167089307 -0.8636559950 -0.33474679  864.0654 1.0037881\nRho[2,2]  1.000000000 0.000000000  1.0000000000  1.00000000       NaN       NaN\n```\n:::\n:::\n\n\nThe distribution of intercepts and slopes looks completely unchanged. For the $\\gamma$, our estimated effect has much of its probability mass around zero and 0.02. Therefore, we conclude that the total causal effect of age on the use of contraception is small. For example, let's take the woman from the first district and predict our expected probability that they use contraception, across both urban and rural areas, as  function of age:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(age, district_id = 1) %>% \n  add_fitted_draws(model_only_age) %>% \n  ggplot(aes(age, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of age\",\n       subtitle = \"Age has a positive small effect. No statistical adjustment by # of kids \",\n       y = \"predicted prob\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\nNow for the model that takes into account the number of children each woman has:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_age_kids <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alpha[district_id] + beta[district_id] * urban + gamma*age + delta*kids,\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    delta ~ normal(0, 1),\n    \n    # adaptive priors\n    c(alpha, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 72.7 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 73.5 seconds.\nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 75.1 seconds.\nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 78.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 75.0 seconds.\nTotal execution time: 78.9 seconds.\n```\n:::\n:::\n\n\nLet's look at our $\\hat{R}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_age_kids, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9997  \n Median :1.0000  \n Mean   :1.0002  \n 3rd Qu.:1.0005  \n Max.   :1.0041  \n NA's   :2       \n```\n:::\n:::\n\n\nThe $\\hat{R}$ look OK, indicating agreement between chains. Let's check our posterior's parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_age_kids,  depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\", \"delta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                mean          sd        5.5%       94.5%     n_eff     Rhat4\na        -1.83023872 0.189105439 -2.12943000 -1.53019765  737.8404 1.0010395\nb         0.74380971 0.170902543  0.47651030  1.01466715 1834.3857 1.0011111\ngamma    -0.02977116 0.007806217 -0.04238032 -0.01738009 1240.2477 0.9999487\nsigma[1]  0.60960022 0.103413091  0.45610425  0.77975195 1147.9766 1.0014546\nsigma[2]  0.77524901 0.204278550  0.46024833  1.10468865  480.3989 1.0041223\nRho[1,1]  1.00000000 0.000000000  1.00000000  1.00000000       NaN       NaN\nRho[1,2] -0.63624957 0.172037893 -0.85805022 -0.31101017  569.2614 1.0039501\nRho[2,1] -0.63624957 0.172037893 -0.85805022 -0.31101017  569.2614 1.0039501\nRho[2,2]  1.00000000 0.000000000  1.00000000  1.00000000       NaN       NaN\ndelta     0.41420075 0.056842441  0.32393682  0.50519928  715.3344 1.0002757\n```\n:::\n:::\n\nOur population distribution for slopes and parameters has shifted: the average probability of using contraception, for a woman with 1 kids, is much lower. That can be explained as our parameters for the number of children, $\\delta$, is clearly positive with an 87% compatibility interval between (0.33, 0.50) in the log-odds. Notice also that the effect of age has changed signs and it's mass is around (-0.04, -0.02) in the log odds scale. That is, older women, adjusting by the number of children they have, are less likely to use contraception. \n\nLet's plot the effect of having children for the women of the district 20 of average age:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(kids, district_id = 20, age = 0) %>% \n  add_fitted_draws(model_age_kids) %>% \n  ggplot(aes(kids, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of # of kids\",\n       subtitle = \"Women with more kids are more likely to use contraception\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nNow, for age:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(data_varying) %>% \n  group_by(urban) %>% \n  data_grid(age, district_id = 1, kids = 1) %>% \n  add_fitted_draws(model_age_kids) %>% \n  ggplot(aes(age, .value)) +\n  stat_lineribbon(fill = \"dodgerblue4\", alpha = 1/4) +\n  scale_fill_brewer(palette = \"Greys\") +\n  facet_wrap(~factor(urban, labels = c(\"Rural\", \"Urban\"))) +\n  labs(title = \"Predicted prob of using contraception as function of age\",\n       subtitle = \"Age has a negative effect. Statistically adjusting by # of kids\",\n       y = \"predicted prob\")\n```\n\n::: {.cell-output-display}\n![](2020-06-03-statistical-rethinking-week-9_files/figure-html/unnamed-chunk-20-1.png){width=768}\n:::\n:::\n\n\n**Going back to our DAG**, our findings are in accordance with it. **The total causal effect of age is less than the direct causal effect due to the pipe that goes through number of kids**. That is, older women have lower probabilities to use contraception once we statistically adjust by the number of kids they have. However, older women also tend to have more children and the direct effect of having more children is to be less likely to use contraception. Therefore, the mixed signal that we get from the total effect. \n\n# 3rd question\n\nModify any models from Problem 2 that contained that children variable and model the variable now as a monotonic ordered category, like education from the week we did ordered categories. Education in that example had 8 categories. Children here will have fewer (no one in the sample had 8 children). So modify the code appropriately. What do you conclude about the causal influence of each additional child on use of contraception?\n\nAlmost inadvertently, in our previous model we assumed that the additional effect of each kid in the log odds of using contraception was constant. By modelling as an ordered category, we let the data decide whether it should be so.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_varying <- list(\n  contraception = bangladesh$use.contraception,\n  district_id = bangladesh$district_id,\n  urban = bangladesh$urban,\n  age = bangladesh$age.centered, \n  kids = as.integer(bangladesh$living.children),\n  alpha = rep(2, 3)\n)\n\nmodel_age_kids_ord <- ulam(\n  alist(\n    contraception ~ dbinom(1, p),\n    logit(p) <- alp[district_id] + beta[district_id] * urban + gamma*age + bks*sum(delta_j[1:kids]),\n    \n    # traditional priors\n    gamma ~ normal(0, 1),\n    bks ~ normal(0, 1),\n    # adaptive priors\n    c(alp, beta)[district_id] ~ multi_normal(c(a, b), Rho, sigma),\n    \n    # hyper-priors\n    a ~ normal(-0.5, 1),\n    b ~ normal(0, 1),\n    sigma ~ exponential(1),\n    Rho ~ lkj_corr(2),\n    vector[4]: delta_j <<- append_row(0, delta),\n    simplex[3]: delta ~ dirichlet(alpha)\n  ),\n  chains = 4, cores = 4,\n  data = data_varying,\n  iter = 2000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains, with 1 thread(s) per chain...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 97.2 seconds.\nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 107.4 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 107.9 seconds.\nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 113.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 106.5 seconds.\nTotal execution time: 113.8 seconds.\n```\n:::\n:::\n\n\nLet's look at our $\\hat{R}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_age_kids_ord, depth = 3) %>% \n  data.frame() %>% \n  select(Rhat4) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Rhat4       \n Min.   :0.9992  \n 1st Qu.:0.9998  \n Median :1.0004  \n Mean   :1.0006  \n 3rd Qu.:1.0010  \n Max.   :1.0081  \n NA's   :2       \n```\n:::\n:::\n\n\nThe $\\hat{R}$ values look OK, indicating that the chains are in close agreement with each other. Let's check our parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_age_kids_ord,  depth = 3, pars = c(\"a\", \"b\", \"gamma\", \"sigma\", \"Rho\", \"bks\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                mean          sd        5.5%       94.5%     n_eff    Rhat4\na        -1.65414138 0.150164278 -1.89677880 -1.41186295 1077.8983 1.001821\nb         0.75640661 0.166677397  0.49969545  1.02912495 1975.9601 1.000855\ngamma    -0.02849433 0.007341003 -0.04022939 -0.01663006 2461.7104 1.000734\nsigma[1]  0.60865051 0.103430477  0.45507541  0.78141514 1311.5317 1.002836\nsigma[2]  0.77329681 0.212034191  0.44747856  1.11943225  483.0631 1.008150\nRho[1,1]  1.00000000 0.000000000  1.00000000  1.00000000       NaN      NaN\nRho[1,2] -0.64433118 0.169482917 -0.86035707 -0.33008590  860.8983 1.006834\nRho[2,1] -0.64433118 0.169482917 -0.86035707 -0.33008590  860.8983 1.006834\nRho[2,2]  1.00000000 0.000000000  1.00000000  1.00000000       NaN      NaN\nbks       1.37803430 0.160614910  1.12189780  1.62950030 1038.0832 1.002649\n```\n:::\n:::\n\nThe overall effect of the children variable, when a woman has 4 children, has the same sign and roughly the same magnitude as previous inferences. Let's look at the effect splitted by the number of children:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprecis(model_age_kids_ord, depth = 3, pars = \"delta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               mean         sd       5.5%     94.5%    n_eff     Rhat4\ndelta[1] 0.73293447 0.08141250 0.59898722 0.8600923 4670.253 0.9994883\ndelta[2] 0.16762106 0.07829638 0.05269849 0.2999460 5027.078 0.9994653\ndelta[3] 0.09944448 0.05476397 0.02512521 0.1978564 5644.457 0.9993787\n```\n:::\n:::\n\n\nRemember that these are percentages of the total effect. That is, around 73% of the total effect comes from having the second child. Therefore, we conclude that most of the effect that having children increases the chances of using contraception comes from having a second child. \n\n\n",
    "supporting": [
      "2020-06-03-statistical-rethinking-week-9_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}