<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.91.2" />


<title>Bayesian Data Analysis: Week 4 -&gt; Importance Sampling - David Salazar&#39;s blog</title>
<meta property="og:title" content="Bayesian Data Analysis: Week 4 -&gt; Importance Sampling - David Salazar&#39;s blog">


  <link href='https://david-salazar.github.io/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/booglee.jpeg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/David-Salazar">GitHub</a></li>
    
    <li><a href="https://david-salazar.github.io/">Posts</a></li>
    
    <li><a href="https://twitter.com/DavidSalazarVir">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">Bayesian Data Analysis: Week 4 -&gt; Importance Sampling</h1>

    
    <span class="article-date">2020-06-27</span>
    

    <div class="article-content">
      


<p>Bayesian Data Analysis (Gelman, Vehtari et. alter) is equals part a great introduction and THE reference for advanced Bayesian Statistics. Luckily, it’s <a href="http://www.stat.columbia.edu/~gelman/book/">freely available online</a>. To make things even better for the online learner, Aki Vehtari (one of the authors) has a set of online lectures and homeworks that go through the basics of Bayesian Data Analysis.</p>
<p>In this blogpost, I’ll go over one of the main topics of Week 4: Importance Sampling; I’ll also solve a couple of the exercises for Chapter 10 of the book. Week 4 was all about preparing the way for MCMC: if we <em>cannot fully</em> compute the <strong>posterior</strong>, but we <em>can</em> evaluate an unnormalized version, how can we <strong>approximate</strong> the posterior distribution?</p>
<div id="importance-sampling" class="section level2">
<h2>Importance Sampling</h2>
<p>Let <span class="math inline">\(q(\theta|y)\)</span> be the unnormalized posterior density that we can compute at some values for theta. Then, the posterior expectation is:</p>
<p><span class="math display">\[
E(\theta|y) = \dfrac{\int \theta q(\theta|y) d\theta}{\int q(\theta|y) d\theta}
\]</span>
However, the denominator is often intractable. Importance sampling, then, reduces to consider another density <span class="math inline">\(g(\theta)\)</span> from which we can draw direct samples. Then, if we multiply and divide by <span class="math inline">\(g(\theta)\)</span> in both numerator and denominator:</p>
<p><span class="math display">\[
\frac{\int[\theta q(\theta \mid y) / g(\theta)] g(\theta) d \theta}{\int[q(\theta \mid y) / g(\theta)] g(\theta) d \theta}
\]</span>
Which can be considered expectations with respect to <span class="math inline">\(g(\theta)\)</span>. Therefore, we can estimate both numerator and denominator by direct sampling from <span class="math inline">\(g(\theta)\)</span>:</p>
<p><span class="math display">\[
\frac{\frac{1}{S} \sum_{s=1}^{S} \left(\theta^{s}\right) w\left(\theta^{s}\right)}{\frac{1}{S} \sum_{s=1}^{S} w\left(\theta^{s}\right)}
\]</span></p>
<p>Where <span class="math inline">\(w\left(\theta^{s}\right)\)</span> are called the <strong>importance weights</strong> and are defined thus:</p>
<p><span class="math display">\[
w\left(\theta^{s}\right)=\frac{q\left(\theta^{s} \mid y\right)}{g\left(\theta^{s}\right)}
\]</span></p>
<p>Therefore, importance sampling is <em>sampling</em> from an <strong>approximation to the posterior</strong> and then <em>correcting</em> the importance that each sample has in the computation of a specific expectation. We correct by the ratio of the unnormalized posterior density to the density of the approximation.</p>
<p>Importance sampling will give precise estimates of the expectation if the weights are roughly uniform. However, if the importance weights vary substantially, the method will yield unsatisfactory estimates. Indeed:</p>
<blockquote>
<p>The worst possible scenario occurs when the importance ratios are small with high probability but with a low probability are huge, which happens, for example, if <span class="math inline">\(q\)</span> has wide tails compared to <span class="math inline">\(g,\)</span> as a function of <span class="math inline">\(\theta\)</span></p>
</blockquote>
</div>
<div id="putting-it-into-practice" class="section level2">
<h2>Putting it into practice</h2>
<p>Here I’ll solve exercise 6 and exercise 7</p>
<div id="exercise-6" class="section level3">
<h3>Exercise 6</h3>
<p>Importance sampling when the importance weights are well behaved: consider a univariate posterior distribution, <span class="math inline">\(p(\theta \mid y),\)</span> which we wish to approximate and then calculate moments of, using importance sampling from an unnormalized density, <span class="math inline">\(g(\theta)\)</span>. Suppose the posterior distribution is normal, and the approximation is <span class="math inline">\(t_{3}\)</span> with mode and curvature matched to the posterior density.</p>
<p>The <span class="math inline">\(t_{3}\)</span> has a variance of 3. Therefore, we sample from a normal with standard deviation of 3.</p>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Draw a sample of size <span class="math inline">\(S=100\)</span> from the approximate density and compute the importance ratios. Plot a histogram of the log importance ratios.</li>
</ol>
</blockquote>
<p>First, we draw samples from the approximate density <span class="math inline">\(t_{3}\)</span></p>
<pre class="r"><code>approximate_samples &lt;- rt(100, df = 3)</code></pre>
<p>Then, we compute the density <span class="math inline">\(g(\theta)\)</span> at these points:</p>
<pre class="r"><code>approximate_density &lt;- dt(approximate_samples, df = 3, log = TRUE)</code></pre>
<p>Finally, we compute the density at the sampled points:</p>
<pre class="r"><code>unnormalized_posterior &lt;- dnorm(approximate_samples, log = TRUE, sd = sqrt(3))</code></pre>
<p>The log importance weights are then:</p>
<pre class="r"><code>log_imp_weights = unnormalized_posterior - approximate_density
data.frame(log_imp_weights) %&gt;% 
  ggplot(aes(exp(log_imp_weights))) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  labs(title = &quot;Importance weights&quot;,
       subtitle = &quot;Approximating a normal with a t distribution&quot;)</code></pre>
<p><img src="/post/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-5-1.png" width="672" />
&gt; (b) Estimate <span class="math inline">\(\mathrm{E}(\theta \mid y)\)</span> and <span class="math inline">\(\operatorname{var}(\theta \mid y)\)</span> using importance sampling. Compare to the true values.</p>
<p>First, we exponentiate and normalize the weights:</p>
<pre class="r"><code>norm_weights &lt;- exp(log_imp_weights)/sum(exp(log_imp_weights))</code></pre>
<p>Finally, we compute <span class="math inline">\(\mathrm{E}(\theta \mid y)\)</span> simply as the sum of the product of the samples and these weights:</p>
<pre class="r"><code>mean_estimate &lt;- sum(approximate_samples*norm_weights)
glue::glue(&quot;Mean estimate: {round(mean_estimate, 2)}&quot;)</code></pre>
<pre><code>## Mean estimate: -0.12</code></pre>
<p>And <span class="math inline">\(\operatorname{var}(\theta \mid y)\)</span>:</p>
<pre class="r"><code>variance_estimate &lt;- sum(approximate_samples^2*norm_weights) + mean_estimate^2
glue::glue(&quot;Variance estimate: {round(variance_estimate, 2)}&quot;)</code></pre>
<pre><code>## Variance estimate: 3.41</code></pre>
<p>Which makes for a Monte-Carlo standard error for the mean of:</p>
<pre class="r"><code>variance_estimate/100</code></pre>
<pre><code>## [1] 0.03412438</code></pre>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Repeat (a) and (b) for <span class="math inline">\(S=10,000\)</span></li>
</ol>
</blockquote>
<pre class="r"><code>approximate_samples &lt;- rt(10000, df = 3)
approximate_density &lt;- dt(approximate_samples, df = 3, log = TRUE)
unnormalized_posterior &lt;- dnorm(approximate_samples, log = TRUE, sd = sqrt(3))

log_imp_weights = unnormalized_posterior - approximate_density
data.frame(log_imp_weights) %&gt;% 
  ggplot(aes(exp(log_imp_weights))) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  labs(title = &quot;Importance weights&quot;,
       subtitle = &quot;Approximating a normal with a t distribution&quot;)</code></pre>
<p><img src="/post/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-10-1.png" width="672" />
Finally, the computations:</p>
<pre class="r"><code>norm_weights &lt;- exp(log_imp_weights)/sum(exp(log_imp_weights))
mean_estimate &lt;- sum(approximate_samples*norm_weights)
glue::glue(&quot;Mean estimate: {round(mean_estimate, 3)}&quot;)</code></pre>
<pre><code>## Mean estimate: 0.031</code></pre>
<pre class="r"><code>variance_estimate &lt;- sum(approximate_samples^2*norm_weights) + mean_estimate^2
glue::glue(&quot;Variance estimate: {round(variance_estimate, 2)}&quot;)</code></pre>
<pre><code>## Variance estimate: 2.88</code></pre>
<p>Which makes for a Monte-Carlo standard error for the mean of:</p>
<pre class="r"><code>variance_estimate/10000</code></pre>
<pre><code>## [1] 0.0002883421</code></pre>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Using the sample obtained in (c), compute an estimate of effective sample size using (10.4) on page 266</li>
</ol>
</blockquote>
<p>The effective sample size is:</p>
<p><span class="math display">\[
S_{\mathrm{eff}}=\frac{1}{\sum_{s=1}^{S}\left(\tilde{w}\left(\theta^{s}\right)\right)^{2}}
\]</span>
where <span class="math inline">\(\tilde w\)</span> are the normalized weights.</p>
<pre class="r"><code>s_eff &lt;- 1/sum(norm_weights^2)
glue::glue(&quot;Effective sample size: {round(s_eff,0)}&quot;)</code></pre>
<pre><code>## Effective sample size: 8246</code></pre>
<p>Given that most of the weights are very small, we can have a reasonably efficient approximation of the posterior using importance sampling</p>
</div>
<div id="exercise-7" class="section level3">
<h3>Exercise 7</h3>
<p>Importance sampling when the importance weights are too variable: repeat the previous exercise, but with a <span class="math inline">\(t_{3}\)</span> posterior distribution and a normal approximation. Explain why the estimates of <span class="math inline">\(\operatorname{var}(\theta \mid y)\)</span> are systematically too low.</p>
<pre class="r"><code>approximate_samples &lt;- rnorm(100, sd = sqrt(3))
approximate_density &lt;- dnorm(approximate_samples,  log = TRUE, sd = sqrt(3))
unnormalized_posterior &lt;- dt(approximate_samples, df = 3, log = TRUE) 

log_imp_weights = unnormalized_posterior - approximate_density
data.frame(log_imp_weights) %&gt;% 
  ggplot(aes(exp(log_imp_weights))) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  labs(title = &quot;Importance weights&quot;,
       subtitle = &quot;Approximating a t-distribution with a normal&quot;)</code></pre>
<p><img src="/post/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>norm_weights &lt;- exp(log_imp_weights)/sum(exp(log_imp_weights))
mean_estimate &lt;- sum(approximate_samples*norm_weights)
glue::glue(&quot;Mean estimate: {round(mean_estimate, 3)}&quot;)</code></pre>
<pre><code>## Mean estimate: -0.115</code></pre>
<pre class="r"><code>variance_estimate &lt;- sum(approximate_samples^2*norm_weights) + mean_estimate^2
glue::glue(&quot;Variance estimate: {round(variance_estimate, 2)}&quot;)</code></pre>
<pre><code>## Variance estimate: 2.64</code></pre>
<pre class="r"><code>s_eff &lt;- 1/sum(norm_weights^2)
glue::glue(&quot;Effective sample size: {round(s_eff,0)}&quot;)</code></pre>
<pre><code>## Effective sample size: 86</code></pre>
<p>Even if we increase the sample size, the problems still remain:</p>
<pre class="r"><code>approximate_samples &lt;- rnorm(10000, sd = sqrt(3))
approximate_density &lt;- dnorm(approximate_samples,  log = TRUE, sd = sqrt(3))
unnormalized_posterior &lt;- dtnew(approximate_samples, df = 3, log = TRUE) 

log_imp_weights = unnormalized_posterior - approximate_density
data.frame(log_imp_weights) %&gt;% 
  ggplot(aes(exp(log_imp_weights))) +
  geom_density(fill = &quot;dodgerblue4&quot;, alpha = 0.5) +
  labs(title = &quot;Importance weights&quot;,
      subtitle = &quot;Approximating a t-distribution with a normal&quot;)</code></pre>
<p><img src="/post/2020-06-27-bayesian-data-analysis-week-4-importance-sampling_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>norm_weights &lt;- exp(log_imp_weights)/sum(exp(log_imp_weights))
mean_estimate &lt;- sum(approximate_samples*norm_weights)
glue::glue(&quot;Mean estimate: {round(mean_estimate, 3)}&quot;)</code></pre>
<pre><code>## Mean estimate: 0.03</code></pre>
<pre class="r"><code>variance_estimate &lt;- sum(approximate_samples^2*norm_weights) + mean_estimate^2
glue::glue(&quot;Variance estimate: {round(variance_estimate, 2)}&quot;)</code></pre>
<pre><code>## Variance estimate: 2.11</code></pre>
<p>Now, the posterior will have fatter tails than the normal approximation. Therefore, we will have to correct our normal samples with large importance weights, highlighting the inadequacy of our samples generated from the normal to approximate the tails of the posterior. Thus, we cannot adequately account for the tail effect in neither the mean nor the variance. However, the effect is worsened for the variance, as the L2 norm amplifies the consequences of inadequately sampling from the tail.</p>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//https-david-salazar-github-io.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

