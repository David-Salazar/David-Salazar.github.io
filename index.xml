<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dilettanting Data Science</title>
    <link>/</link>
    <description>Recent content on Dilettanting Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>What does it mean to fatten the tails?</title>
      <link>/2020/05/09/what-does-it-mean-to-fatten-the-tails/</link>
      <pubDate>Sat, 09 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/09/what-does-it-mean-to-fatten-the-tails/</guid>
      <description>What does it mean to fatten the tails?First, let’s define what we mean by fatter tails.
What are fatter tails?Intuitively, fat tails distribution are distributions for which their PDFs decay to zero very slowly. So slowly, that extreme values start gaining traction in the determination of the whole distribution. Thus, a distribution is fatter than another one if its PDF takes longer to decay to zero.
Fattening the tailsThus, if we wanted to fatten the tails, the intuitive response is to add more mass at the tails such that the PDF takes more time to decay.</description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 3</title>
      <link>/2020/05/03/statistical-rethinking-week-3/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/03/statistical-rethinking-week-3/</guid>
      <description>Statistical Rethinking: Week 3Week 3 gave the most interesting discussion of multiple regression. Why isn’t it enough with univariate regression? It allows us to disentagle two types of mistakes:
Spurious correlation between the predictor and independent variable.A masking relationship between two explanatory variables.It also started to introduce DAGs and how they are an incredible tool for thinking before fitting. Specially, it managed to convince me the frequent strategy of tossing everything into a multiple regression and hoping for the ebst is a recipe for disaster.</description>
    </item>
    
    <item>
      <title>Wittgenstein&#39;s Ruler: Fat or Thin?</title>
      <link>/2020/04/30/wittgenstein-s-ruler-fat-or-thin/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/30/wittgenstein-s-ruler-fat-or-thin/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 2</title>
      <link>/2020/04/28/statistical-rethinking-week-2/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/28/statistical-rethinking-week-2/</guid>
      <description>library(rethinking)library(tidyverse)library(ggridges)extrafont::loadfonts(device=&amp;quot;win&amp;quot;) set.seed(24)data(&amp;quot;Howell1&amp;quot;)precis(Howell1)## mean sd 5.5% 94.5%## height 138.2635963 27.6024476 81.108550 165.73500## weight 35.6106176 14.7191782 9.360721 54.50289## age 29.3443934 20.7468882 1.000000 66.13500## male 0.4724265 0.4996986 0.000000 1.00000## histogram## height &amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2581&amp;gt;## weight &amp;lt;U+2581&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2587&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;## age &amp;lt;U+2587&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2583&amp;gt;&amp;lt;U+2585&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2582&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;## male &amp;lt;U+2587&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2581&amp;gt;&amp;lt;U+2587&amp;gt;Week 2Week 2 has gotten us to start exploring linear regression from a bayesian perspective. I found it the most interesting to propagate uncertainty through the model.</description>
    </item>
    
    <item>
      <title>Spurious PCA under Thick Tails</title>
      <link>/2020/04/27/spurious-pca-under-thick-tails/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/27/spurious-pca-under-thick-tails/</guid>
      <description>Spurious PCA under Thick TailsPCA is a dimensionality reduction technique. It seeks to project the data onto a lower dimensional hyperplane such that as much of the original data variance is preserved. The underlying idea is that the vectors creating these lower dimensional hyperplanes reflect a latent structure in the data. However, what happens when there is no structure at all?
In his most recently published technical book,Taleb examines this question under two different regimes: Mediocristan and Extremistan.</description>
    </item>
    
    <item>
      <title>Pareto 80/20 and Maximum Likelihood</title>
      <link>/2020/04/23/pareto-80-20-and-maximum-likelihood/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/23/pareto-80-20-and-maximum-likelihood/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Statistical Rethinking: Week 1</title>
      <link>/2020/04/19/statistical-rethinking-week-1/</link>
      <pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/19/statistical-rethinking-week-1/</guid>
      <description>Week 1Week 1 tries to go as deep as possible in the intuition and the mechanics of a very simple model. As always with McElreath, he goes on with both clarity and erudition.
Suppose the globe tossing data had turned out to be 8 water in 15 tosses.Construct the posterior distribution, using grid approximation. Use thesame flat prior as before.
# define gridp_grid &amp;lt;- seq(from = 0, to = 1, length.</description>
    </item>
    
    <item>
      <title>Fat vs Thin: does LLN work?</title>
      <link>/2020/04/17/fat-vs-thin-does-lln-work/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/17/fat-vs-thin-does-lln-work/</guid>
      <description>Fat tails are a different beastStatistical estimation is based on the LLN and CLT. The CLT states that the sampling distribution will look like a normal. The LLN that the variance of the normal will decrease as our sampling size increases.
Or so does Nassim Nicholas Taleb says in his recently published technical book, wherein he explains how common practice statistical methodology breaks down under the Extremistan regime.</description>
    </item>
    
    <item>
      <title>Coursera Machine Learning: Neural Networks, Representation</title>
      <link>/2020/01/08/coursera-machine-learning-neural-networks-representation/</link>
      <pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/08/coursera-machine-learning-neural-networks-representation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coursera Machine Learning Logistic Regression and Regularization</title>
      <link>/2020/01/01/coursera-machine-learning-logistic-regression/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/01/01/coursera-machine-learning-logistic-regression/</guid>
      <description>Classification ProblemsLinear Regression?Logistic RegressionDecision BoundariesCost FunctionCross EntropyA maximum likelihood derivationVectorised implementationMulti Classification ProblemRegularizationSolutions to overfitting:Cost functionMoving towards neural networksClassification ProblemsIn all of these problems the variable that we’re trying to predict is a variable \(y\) that we can think of as taking on two values either zero or one, either spam or not spam, fraudulent or not fraudulent, related malignant or benign.</description>
    </item>
    
    <item>
      <title>Coursera Machine Learning: Introduction and Linear Regression</title>
      <link>/2019/12/26/coursera-machine-learning-week-1/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/26/coursera-machine-learning-week-1/</guid>
      <description>Why?Week 1Why Machine Learning?What is Machine LearningSupervised LearningRegression Problems and Classification ProblemsMath SettingExample of ML algorithm and a loss functionWhich ML Algorithm for which Loss Function?Unsupervised LearningLinear RegressionLoss FunctionGradient DescentGradient Descent JustificationGradient Descent in Linear RegressionImplementing Linear Regression with Gradient DescentProblems with linear regressionSensibility to OutliersMulticollinearityHeteroscedasticityWhy?</description>
    </item>
    
    <item>
      <title>Referential Arrays in Python</title>
      <link>/2019/12/19/referential-arrays-in-python/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/19/referential-arrays-in-python/</guid>
      <description>Referential ArrayArrays in pythons do not hold he objects themselves, but pointers to that objects. This can create problems with shallowcopying. For example:
Each index of each list contains a pointer to an object.
first_list = [1, 2, 3]second_list = [4, 5, 6]third_list = [7, 8, 9]print(first_list)## [1, 2, 3]What if we want to combine these lists?
first_list.extend(second_list)third_list.append(second_list)print(first_list)## [1, 2, 3, 4, 5, 6]print(third_list)## [7, 8, 9, [4, 5, 6]]Now, first_list points to the same object as second list.</description>
    </item>
    
    <item>
      <title>Refresher Big Oh Notation </title>
      <link>/2019/12/19/refresher-big-oh-notation/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/19/refresher-big-oh-notation/</guid>
      <description>Big Oh NotationFirst, let’s try to remember what problem we are actually trying to solve: compare the different runtime of different algorithms. We are not comparing them across time, but across number of some basic operations. And we care not about the time itself, but about the runtime growth as the input size also growths. Thus, we use asymptotic analysis.
\(O(n)\) can thus be described as: the runtime grows linearly as the input size increases.</description>
    </item>
    
    <item>
      <title>deeplearning.ai Specialization</title>
      <link>/2019/12/18/deeplearning-ai-specialization/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/18/deeplearning-ai-specialization/</guid>
      <description>deeplearning.aiOver the next few days, I’ll go over (this time I am paying and thus have access to the exams :)) the deeplearning.ai Coursera Specialization. Here, I’ll gather my notes of the course for easy access:
Neural Networks and Deep LearningImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.Structuring Machine Learning Projects.Convolutional Neural Networks.Sequence Models.Why?I have already used DL in a couple of personal projects.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning </title>
      <link>/2019/12/18/neural-networks-and-deep-learning/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/18/neural-networks-and-deep-learning/</guid>
      <description>Week 1Why Neural Networks?Economic value from Neural NetworksStructured vs UnstructuredWhy now? Scale, scale and scaleUniversal ApproximatorsWeek 2NotationLogistic RegressionA rational loss functionArriving at cost function: Maximum LikelihoodGradient DescentMinimizing directional derivativeComputational Graphs and backpropExample with logistic regressionVectorisationBroadcastingAssingments’ need to remember:Week 3Hidden LayerNotationActivation functionWhy bother with any activation function at all?</description>
    </item>
    
    <item>
      <title>Understanding Backtracking</title>
      <link>/2019/12/12/understanding-backtracking/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/12/understanding-backtracking/</guid>
      <description>Recursion black magicWhile working through some HackerRank’s problems, I came across the following exercise:
Find the number of ways that a given integer, \(X\), can be expressed as the sum of the \(N^{th}\) powers of unique, natural numbers.
However, I could not inmediately come up with a compelling way to programatically enumerate and examine all the possible sequences of numbers to find the required solution. Thus, it’s good time as ever to refresh my knowledge about recursion and specically, backtracking.</description>
    </item>
    
    <item>
      <title>Netflix Habits through data</title>
      <link>/2019/10/15/netflix-habits-through-data/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/15/netflix-habits-through-data/</guid>
      <description>Netflix HabitsIn the past, I believe I have spent an inordinate amount of time watching series and movies on Netflix. To try to gauge how my habits have changed through time, I downloaded the data that Netflix makes available and, of course, used ´R´ to analyze it.
Tidy Toolslibrary(tidyverse)library(tsibble)Let’s have a first look:
Sadly, there’s not that much information. However, let’s try to gauge how many shows have I watched:</description>
    </item>
    
    <item>
      <title>Qui mensis anni calidissimus est?</title>
      <link>/2018/07/19/qui-mensis-anni-calidissimus-est/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/19/qui-mensis-anni-calidissimus-est/</guid>
      <description>In capitulo XIII Linguae Latinae, cui nomen ‘Annus et Menses’ est, Quintus Aemiliam interrogat hoc:
Qui mensis anni calidissimus est?
Respondeo notitiae Nova Yorkae:
library(tidyverse)data &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KNYC.csv&amp;quot;)data &amp;lt;- data %&amp;gt;% mutate(date = lubridate::ymd(date),month = lubridate::month(date),mensis = case_when(month == 1 ~ &amp;quot;Ianuarius&amp;quot;,month == 2 ~ &amp;quot;Februarius&amp;quot;,month == 3 ~ &amp;quot;Martius&amp;quot;,month == 4 ~ &amp;quot;Aprilis&amp;quot;,month == 5 ~ &amp;quot;Maius&amp;quot;,month == 6 ~ &amp;quot;Iunius&amp;quot;,month == 7 ~ &amp;quot;Iulius&amp;quot;,month == 8 ~ &amp;quot;Augustus&amp;quot;,month == 9 ~ &amp;quot;Septiembre&amp;quot;,month == 10 ~ &amp;quot;October&amp;quot;,month == 11 ~ &amp;quot;November&amp;quot;, month == 12 ~ &amp;quot;December&amp;quot;),mensis = forcats::as_factor(mensis),mensis = forcats::fct_reorder(mensis, month, .</description>
    </item>
    
    <item>
      <title>How to spell HODL?</title>
      <link>/2018/07/17/how-spell-hodl/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/17/how-spell-hodl/</guid>
      <description>The moody Mr. MarketAnybody who has even a dime on the stock market will eventually get dragged on following the daily (or even hourly) moves in the market. However, this is not only a stressful idea, but also a very ineffective one. Most days in the market won’t even bulge your final total return. In fact, final market return is mostly determined by a handful of days alone.</description>
    </item>
    
    <item>
      <title>Trees, Ensembles and beyond, XGBoost and LGBM</title>
      <link>/2018/06/10/trees-ensembles-and-beyond/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/10/trees-ensembles-and-beyond/</guid>
      <description>Why?Set-upTreesFitting themInterpretationEnsemblesBaggingBootstrapingRandom ForestsConclusions for BaggingBoostingDirectional DerivativeGradient Boosting: Back to our problemConclusions for BoostingWhy?lightgbm and xgboost appear in every single competition at Kaggle. Thus, these boosting techniques must be able to learn something that cannot be easily learned from intelligent bagging techniques like Random Forests. This is my attempt to understand why and how they can do that.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hi! My name is David Salazar an I am dilettant&amp;rsquo;n my way into understanding Machine Learning and Data Science in general.</description>
    </item>
    
    <item>
      <title>The Adam Smith Problem: Tidytext in R</title>
      <link>/2018/06/07/the-adam-smith-problem-tidytext-in-r/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/07/the-adam-smith-problem-tidytext-in-r/</guid>
      <description>Why?This is a fun (for me) exercise to explore Text Mining with R and make sure I can follow along.
What is it ?Around the XIX century, some german scholars posited that Wealth of Nations’ Adam Smith was too different to Theory of Moral Sentiments’ Adam Smith, thus concluded that he must have had a change of heart somewhere along his life or that he was simply an incoherent man.</description>
    </item>
    
    <item>
      <title>Is it CR7 or Messi?: Using the fastai toolkit</title>
      <link>/2018/06/01/is-it-cr7-or-messi-using-the-fastai-toolkit/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/01/is-it-cr7-or-messi-using-the-fastai-toolkit/</guid>
      <description>World Cup Mode: CR vs Messi&amp;#182;To try to solidify what I have learned from Deep Learning for coders from fast.ai, I&#39;ll try to train a computer vision algorithm such that it can recognize whether is Messi or Cristiano Ronaldo in the picture. Notebook in github  or nbviewer 
Imports&amp;#182;In&amp;nbsp;[1]:# Put these at the top of every notebook, to get automatic reloading and inline plotting%reload_ext autoreload%autoreload 2%matplotlib inlineIn&amp;nbsp;[2]:# This file contains all the main external libs we&amp;#39;ll usefrom fastai.</description>
    </item>
    
    <item>
      <title>Plotting Supply and Demand Curves with ggplot2</title>
      <link>/2018/05/20/ggsupplydemand/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/20/ggsupplydemand/</guid>
      <description>What is it?ggsupplyDemand is an R package that makes it extremely easy to plot basic supply and demands using ggplot2
library(ggsupplyDemand)create_supply_and_demand() %&amp;gt;% shift_demand(outwards = TRUE) %&amp;gt;% plot_supply_and_demand(consumer_surplus = TRUE)Why?I needed to plot some basic supply and demand curves in R. Obviously, I thought of ggplot2. However, it is not that straightforward. The best resource I could find was this blogpost from Andrew Heiss. I recopilated most of his functions, created a simple API and put all the functions on an package.</description>
    </item>
    
    <item>
      <title>Shiny e iCOLCAP</title>
      <link>/2018/04/01/shiny-e-icolcap/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/01/shiny-e-icolcap/</guid>
      <description>Siguiendo una inversión decepecionanteEl mercado colombiano ha sido una decepción en los últimos cinco años. Para ver qué tan decepcionado debería estar, cree esta Shiny app. Para no ovlidarme de cómo se construyen en el futuro, acá un tutorial breve comentando selecciones del código:
ui: User InterfaceEn la ui especificamos la organización de los inputs y outputs, aún cuando no hayamos creado estos últimos.
ui &amp;lt;- fluidPage(# Application titletitlePanel(&amp;quot;Comparación de Mercado&amp;quot;),# Sidebar with a date inputsidebarLayout( dateInput(&amp;quot;fecha_inicial&amp;quot;, &amp;quot;Fecha de inicio de comparación:&amp;quot;, value = &amp;quot;2013-01-01&amp;quot;)# primer argumento input$fecha_inicial, como nos referiremos a esto en server.</description>
    </item>
    
  </channel>
</rss>